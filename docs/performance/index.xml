<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Performances on Apache Hadoop Ozone Development notes</title>
    <link>https://elek.github.io/ozone-notes/performance/</link>
    <description>Recent content in Performances on Apache Hadoop Ozone Development notes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 09 Aug 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://elek.github.io/ozone-notes/performance/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>HCFS test with instrumented FS</title>
      <link>https://elek.github.io/ozone-notes/performance/19_hcfs/</link>
      <pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://elek.github.io/ozone-notes/performance/19_hcfs/</guid>
      <description>Test method  Deployed Ozone and HDFS with the help of K8s Executed key generator using Hadoop Compatble File System ozone freon dfsg Tested with  default write buffer (using out.</description>
    </item>
    
    <item>
      <title>Teragen test (HDFS vs. Ozone)</title>
      <link>https://elek.github.io/ozone-notes/performance/20_teragen/</link>
      <pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://elek.github.io/ozone-notes/performance/20_teragen/</guid>
      <description>Test method  Deployed Ozone+Yarn and HDFS+Yarn (3.2.1) with k8s scripts Executed teragen MR with 10G data Using spinning disk HCFS usage is measured with instrumented bytecode  Real time spent in write and close methods   same storage is used for defaultFs (yarn data) and teragen data FileSystem usage from the yarn userlog directory is checked (see note below) One pipeline (3 datanodes) is used from both the HDFS and both Ozone  References:</description>
    </item>
    
    <item>
      <title>S3 performance test (Ozone, Minio)</title>
      <link>https://elek.github.io/ozone-notes/performance/18_warp/</link>
      <pubDate>Sat, 08 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://elek.github.io/ozone-notes/performance/18_warp/</guid>
      <description>Tests  Deployed Ozone and Minio Using empty dir for Ozone (disk) Executing S3 put test with using minio/warp  Summary Ozone seems to be smaller with the default settings (1.</description>
    </item>
    
    <item>
      <title>OM HA isolated tests (log segment size)</title>
      <link>https://elek.github.io/ozone-notes/performance/17_om_ha_isolated/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://elek.github.io/ozone-notes/performance/17_om_ha_isolated/</guid>
      <description>Hardware (y124) Three physical machine, spinning disks
VENDOR Manufacturer: Cisco Systems Inc Product Name: UCSC-C220-M4L MEMORY Size: 32 GB Size: 32 GB Size: 32 GB Size: 32 GB Size: 32 GB Size: 32 GB Size: 32 GB Size: 32 GB CPUs Version: Intel(R) Xeon(R) CPU E5-2630 v3 @ 2.</description>
    </item>
    
    <item>
      <title>OM isolated tests</title>
      <link>https://elek.github.io/ozone-notes/performance/16_om_isolated/</link>
      <pubDate>Wed, 13 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://elek.github.io/ozone-notes/performance/16_om_isolated/</guid>
      <description>docker image: elek/ozone-dev:be8a2fcc8  Tests Master / real hard disk 2020-05-13 09:25:22 INFO metrics:107 - type=TIMER, name=key-create, count=100000, min=99.738493, max=191.694249, mean=114.07656516845911, stddev=6.4852681842374595, median=116.563162, p75=116.767639, p95=117.051115, p98=117.197165, p99=133.151007, p999=191.694249, mean_rate=93.26821496222848, m1=87.</description>
    </item>
    
    <item>
      <title>Teragen crosscheck of master / HDDS-2717 (new / old layout)</title>
      <link>https://elek.github.io/ozone-notes/performance/15_teragen_new_old/</link>
      <pubDate>Tue, 10 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://elek.github.io/ozone-notes/performance/15_teragen_new_old/</guid>
      <description>Ozone Performance #15: Teragen crosscheck of master / HDDS-2717 (new / old layout) Test method  100G Teragen has been executed on Yarn with using o3fs as input / output dir 3 nodes are used for HDFS/Ozone 3 nodes for YARN Storage  Ratis log is saved to memdisk OM, SCM metadata is saved to disk (emptyDir) Chunks are saved to real disk (hostPath) Local disks are deleted after each sequence of tests   Default settings used except:  GC  HADOOP_OPTS: -server -XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly   Handlers:  OZONE-SITE.</description>
    </item>
    
    <item>
      <title>Teragen crosscheck of master / HDDS-3053 / HDDS-2717</title>
      <link>https://elek.github.io/ozone-notes/performance/14_teragen/</link>
      <pubDate>Tue, 10 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://elek.github.io/ozone-notes/performance/14_teragen/</guid>
      <description>Test method  100G Teragen has been executed on Yarn with using o3fs as input / output dir 3 nodes are used for HDFS/Ozone 3 nodes for YARN Storage  Ratis log is saved to memdisk OM, SCM metadata is saved to disk (emptyDir) Chunks are saved to real disk (hostPath)   Default settings used except:  GC  HADOOP_OPTS: -server -XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly   Handlers:  OZONE-SITE.</description>
    </item>
    
    <item>
      <title>Ratis log and chunk storage separation</title>
      <link>https://elek.github.io/ozone-notes/performance/13_ratis_log/</link>
      <pubDate>Tue, 03 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://elek.github.io/ozone-notes/performance/13_ratis_log/</guid>
      <description>Ozone Performance #13: Ratis log and chunk storage separation Test method  The same chunk writer freon test is executed: ozone freon dcg -n100000000 with the default 1k chunk size All the tests are executed on one pipeline.</description>
    </item>
    
    <item>
      <title>Teragen HDFS vs Ozone</title>
      <link>https://elek.github.io/ozone-notes/performance/11_teragen/</link>
      <pubDate>Mon, 24 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://elek.github.io/ozone-notes/performance/11_teragen/</guid>
      <description>Docker image: elek/ozone-dev:HDDS-2717-cmdw  Hadoop version:
Hadoop 3.2.1 Source code repository https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842 Compiled by rohithsharmaks on 2019-09-10T15:56Z Compiled with protoc 2.5.0 From source with checksum 776eaf9eee9c0ffc370bcbc1888737 This command was run using /opt/hadoop/share/hadoop/common/hadoop-common-3.</description>
    </item>
    
    <item>
      <title>Writing one file per chunks vs. one file per blocks</title>
      <link>https://elek.github.io/ozone-notes/performance/10_onefile/</link>
      <pubDate>Mon, 24 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://elek.github.io/ozone-notes/performance/10_onefile/</guid>
      <description>Docker image: elek/ozone-dev:HDDS-2717-cmdw  How to test?  Freon test executed Chunk Writes calling the ChunkManagerImpl.writeChunk method directly. Size of chunks has been modified between the tests All the results are All the tests are executed parallel on 6 machines (one test requires only one machine &amp;ndash;&amp;gt; 6 test results) After the freon test a sync call has been executed.</description>
    </item>
    
    <item>
      <title>Checking syscalls with different batch sizes</title>
      <link>https://elek.github.io/ozone-notes/performance/07_strace/</link>
      <pubDate>Tue, 11 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://elek.github.io/ozone-notes/performance/07_strace/</guid>
      <description>Ozone master: 3ddcdbbef Ratis master: 0.5.0-90cd474-SNAPSHOT Docker image: elek/ozonedev:20200211-1  How to test?  Deploy a single Datanode which is forced to be in FOLLOWER mode. Starting it with strace -f -c Stress test with freon (which behaves like a Ratis Leader) from the same container (one node) Killing datanode to have the strace summary  Environment Running k8s based containers on physical cluster, using memdisk.</description>
    </item>
    
  </channel>
</rss>