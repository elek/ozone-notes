Flokkr launcher script 1.1-18-g58a3efc

===== Plugin is activated ENVTOCONF =====
mapred-site.xml
File mapred-site.xml has been written out successfullly.
hdfs-site.xml
File hdfs-site.xml has been written out successfullly.
ozone-site.xml
File ozone-site.xml has been written out successfullly.
core-site.xml
File core-site.xml has been written out successfullly.
hadoop.conf
File hadoop.conf has been written out successfullly.
Non-spark-on-k8s command provided, proceeding in pass-through mode...
======================================
*** Launching "/opt/testscripts/parquet.sh copy hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1"
+ : /opt/spark
+ : /opt/spark-test/jars/
+ : watchforcommit.btm
+ /opt/spark/bin/spark-submit --conf spark.executor.memory=4g --jars /opt/ozonefs/hadoop-ozone-filesystem.jar /opt/spark-test/jars//spark-samples-1.0-SNAPSHOT.jar copy hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:19:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
21/01/18 18:19:05 INFO SparkContext: Running Spark version 3.0.1
21/01/18 18:19:05 INFO ResourceUtils: ==============================================================
21/01/18 18:19:05 INFO ResourceUtils: Resources for spark.driver:

21/01/18 18:19:05 INFO ResourceUtils: ==============================================================
21/01/18 18:19:05 INFO SparkContext: Submitted application: Copy
21/01/18 18:19:05 INFO SecurityManager: Changing view acls to: spark
21/01/18 18:19:05 INFO SecurityManager: Changing modify acls to: spark
21/01/18 18:19:05 INFO SecurityManager: Changing view acls groups to: 
21/01/18 18:19:05 INFO SecurityManager: Changing modify acls groups to: 
21/01/18 18:19:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
21/01/18 18:19:05 INFO Utils: Successfully started service 'sparkDriver' on port 44212.
21/01/18 18:19:05 INFO SparkEnv: Registering MapOutputTracker
21/01/18 18:19:05 INFO SparkEnv: Registering BlockManagerMaster
21/01/18 18:19:05 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/01/18 18:19:05 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/01/18 18:19:05 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/01/18 18:19:05 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-31b8cfba-e96c-4d7b-89c9-cd8c4bc4cb39
21/01/18 18:19:05 INFO MemoryStore: MemoryStore started with capacity 413.9 MiB
21/01/18 18:19:05 INFO SparkEnv: Registering OutputCommitCoordinator
21/01/18 18:19:06 INFO Utils: Successfully started service 'SparkUI' on port 4040.
21/01/18 18:19:06 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://test-runner-bnbrr:4040
21/01/18 18:19:06 INFO SparkContext: Added JAR file:///opt/ozonefs/hadoop-ozone-filesystem.jar at spark://test-runner-bnbrr:44212/jars/hadoop-ozone-filesystem.jar with timestamp 1610993946302
21/01/18 18:19:06 INFO SparkContext: Added JAR file:/opt/spark-test/jars/spark-samples-1.0-SNAPSHOT.jar at spark://test-runner-bnbrr:44212/jars/spark-samples-1.0-SNAPSHOT.jar with timestamp 1610993946303
21/01/18 18:19:06 INFO Executor: Starting executor ID driver on host test-runner-bnbrr
21/01/18 18:19:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38805.
21/01/18 18:19:06 INFO NettyBlockTransferService: Server created on test-runner-bnbrr:38805
21/01/18 18:19:06 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/01/18 18:19:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, test-runner-bnbrr, 38805, None)
21/01/18 18:19:06 INFO BlockManagerMasterEndpoint: Registering block manager test-runner-bnbrr:38805 with 413.9 MiB RAM, BlockManagerId(driver, test-runner-bnbrr, 38805, None)
21/01/18 18:19:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, test-runner-bnbrr, 38805, None)
21/01/18 18:19:06 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, test-runner-bnbrr, 38805, None)
21/01/18 18:19:06 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/opt/spark/spark-warehouse').
21/01/18 18:19:06 INFO SharedState: Warehouse path is 'file:/opt/spark/spark-warehouse'.
21/01/18 18:19:08 INFO InMemoryFileIndex: It took 232 ms to list leaf files for 1 paths.
21/01/18 18:19:08 INFO SparkContext: Starting job: parquet at Copy.java:26
21/01/18 18:19:08 INFO DAGScheduler: Got job 0 (parquet at Copy.java:26) with 1 output partitions
21/01/18 18:19:08 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at Copy.java:26)
21/01/18 18:19:08 INFO DAGScheduler: Parents of final stage: List()
21/01/18 18:19:08 INFO DAGScheduler: Missing parents: List()
21/01/18 18:19:08 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at Copy.java:26), which has no missing parents
21/01/18 18:19:08 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 84.2 KiB, free 413.8 MiB)
21/01/18 18:19:08 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 29.8 KiB, free 413.8 MiB)
21/01/18 18:19:08 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on test-runner-bnbrr:38805 (size: 29.8 KiB, free: 413.9 MiB)
21/01/18 18:19:08 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1223
21/01/18 18:19:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at Copy.java:26) (first 15 tasks are for partitions Vector(0))
21/01/18 18:19:08 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
21/01/18 18:19:08 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, test-runner-bnbrr, executor driver, partition 0, PROCESS_LOCAL, 7566 bytes)
21/01/18 18:19:08 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
21/01/18 18:19:08 INFO Executor: Fetching spark://test-runner-bnbrr:44212/jars/hadoop-ozone-filesystem.jar with timestamp 1610993946302
21/01/18 18:19:09 INFO TransportClientFactory: Successfully created connection to test-runner-bnbrr/10.42.1.56:44212 after 31 ms (0 ms spent in bootstraps)
21/01/18 18:19:09 INFO Utils: Fetching spark://test-runner-bnbrr:44212/jars/hadoop-ozone-filesystem.jar to /tmp/spark-8167179c-a1d6-4209-ad1d-885d7b347146/userFiles-4a0af0f1-a24f-437f-bcef-fa3c86bde7d3/fetchFileTemp8583811450323623955.tmp
21/01/18 18:19:09 INFO Executor: Adding file:/tmp/spark-8167179c-a1d6-4209-ad1d-885d7b347146/userFiles-4a0af0f1-a24f-437f-bcef-fa3c86bde7d3/hadoop-ozone-filesystem.jar to class loader
21/01/18 18:19:09 INFO Executor: Fetching spark://test-runner-bnbrr:44212/jars/spark-samples-1.0-SNAPSHOT.jar with timestamp 1610993946303
21/01/18 18:19:09 INFO Utils: Fetching spark://test-runner-bnbrr:44212/jars/spark-samples-1.0-SNAPSHOT.jar to /tmp/spark-8167179c-a1d6-4209-ad1d-885d7b347146/userFiles-4a0af0f1-a24f-437f-bcef-fa3c86bde7d3/fetchFileTemp7209986723354585504.tmp
21/01/18 18:19:09 INFO Executor: Adding file:/tmp/spark-8167179c-a1d6-4209-ad1d-885d7b347146/userFiles-4a0af0f1-a24f-437f-bcef-fa3c86bde7d3/spark-samples-1.0-SNAPSHOT.jar to class loader
21/01/18 18:19:09 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1780 bytes result sent to driver
21/01/18 18:19:09 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 887 ms on test-runner-bnbrr (executor driver) (1/1)
21/01/18 18:19:09 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
21/01/18 18:19:09 INFO DAGScheduler: ResultStage 0 (parquet at Copy.java:26) finished in 1.047 s
21/01/18 18:19:09 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/18 18:19:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
21/01/18 18:19:09 INFO DAGScheduler: Job 0 finished: parquet at Copy.java:26, took 1.093940 s
21/01/18 18:19:11 INFO BlockManagerInfo: Removed broadcast_0_piece0 on test-runner-bnbrr:38805 in memory (size: 29.8 KiB, free: 413.9 MiB)
21/01/18 18:19:11 INFO FileSourceStrategy: Pruning directories with: 
21/01/18 18:19:11 INFO FileSourceStrategy: Pushed Filters: 
21/01/18 18:19:11 INFO FileSourceStrategy: Post-Scan Filters: 
21/01/18 18:19:11 INFO FileSourceStrategy: Output Data Schema: struct<data: binary, index: int>
21/01/18 18:19:11 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:19:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:19:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:19:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:19:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:19:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:19:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:19:12 INFO CodeGenerator: Code generated in 221.68596 ms
21/01/18 18:19:12 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 307.2 KiB, free 413.6 MiB)
21/01/18 18:19:12 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 28.3 KiB, free 413.6 MiB)
21/01/18 18:19:12 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on test-runner-bnbrr:38805 (size: 28.3 KiB, free: 413.9 MiB)
21/01/18 18:19:12 INFO SparkContext: Created broadcast 1 from parquet at Copy.java:28
21/01/18 18:19:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
21/01/18 18:19:12 INFO SparkContext: Starting job: parquet at Copy.java:28
21/01/18 18:19:12 INFO DAGScheduler: Got job 1 (parquet at Copy.java:28) with 200 output partitions
21/01/18 18:19:12 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at Copy.java:28)
21/01/18 18:19:12 INFO DAGScheduler: Parents of final stage: List()
21/01/18 18:19:12 INFO DAGScheduler: Missing parents: List()
21/01/18 18:19:12 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at parquet at Copy.java:28), which has no missing parents
21/01/18 18:19:12 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 175.9 KiB, free 413.4 MiB)
21/01/18 18:19:12 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 62.2 KiB, free 413.4 MiB)
21/01/18 18:19:12 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on test-runner-bnbrr:38805 (size: 62.2 KiB, free: 413.8 MiB)
21/01/18 18:19:12 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
21/01/18 18:19:12 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at parquet at Copy.java:28) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
21/01/18 18:19:12 INFO TaskSchedulerImpl: Adding task set 1.0 with 200 tasks
21/01/18 18:19:12 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, test-runner-bnbrr, executor driver, partition 0, ANY, 7815 bytes)
21/01/18 18:19:12 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
21/01/18 18:19:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:19:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:19:12 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:19:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:19:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:19:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:19:12 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:19:12 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:19:12 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:19:12 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:19:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:19:12 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:19:12 INFO ParquetOutputFormat: Validation is off
21/01/18 18:19:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:19:12 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:19:12 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:19:12 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:19:12 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:19:12 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:19:12 INFO CodecPool: Got brand-new compressor [.snappy]
21/01/18 18:19:12 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-31b11617-3243-4561-b52e-58b9f29e1c09-c000.snappy.parquet, range: 0-104863442, partition values: [empty row]
21/01/18 18:19:13 INFO CodecPool: Got brand-new decompressor [.snappy]
21/01/18 18:19:50 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:19:56 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000000_1' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:19:56 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000000_1: Committed
21/01/18 18:19:56 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2570 bytes result sent to driver
21/01/18 18:19:56 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2, test-runner-bnbrr, executor driver, partition 1, ANY, 7815 bytes)
21/01/18 18:19:56 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
21/01/18 18:19:56 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 44237 ms on test-runner-bnbrr (executor driver) (1/200)
21/01/18 18:19:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:19:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:19:56 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:19:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:19:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:19:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:19:56 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-09334d37-b537-43d9-b277-914c4572a92a-c000.snappy.parquet, range: 0-104863441, partition values: [empty row]
21/01/18 18:19:57 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:19:57 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:19:57 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:19:57 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:19:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:19:57 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:19:57 INFO ParquetOutputFormat: Validation is off
21/01/18 18:19:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:19:57 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:19:57 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:19:57 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:19:57 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:19:57 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:19:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:19:57 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000001_2' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:19:57 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000001_2: Committed
21/01/18 18:19:57 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2527 bytes result sent to driver
21/01/18 18:19:57 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3, test-runner-bnbrr, executor driver, partition 2, ANY, 7815 bytes)
21/01/18 18:19:57 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
21/01/18 18:19:57 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 1212 ms on test-runner-bnbrr (executor driver) (2/200)
21/01/18 18:19:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:19:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:19:57 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:19:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:19:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:19:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:19:57 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-44ab9658-81cf-4f0d-9dac-69e2aa267a2e-c000.snappy.parquet, range: 0-104863439, partition values: [empty row]
21/01/18 18:19:58 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:19:58 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:19:58 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:19:58 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:19:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:19:58 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:19:58 INFO ParquetOutputFormat: Validation is off
21/01/18 18:19:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:19:58 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:19:58 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:19:58 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:19:58 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:19:58 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:19:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:19:59 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000002_3' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:19:59 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000002_3: Committed
21/01/18 18:19:59 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2527 bytes result sent to driver
21/01/18 18:19:59 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4, test-runner-bnbrr, executor driver, partition 3, ANY, 7815 bytes)
21/01/18 18:19:59 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
21/01/18 18:19:59 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 1311 ms on test-runner-bnbrr (executor driver) (3/200)
21/01/18 18:19:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:19:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:19:59 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:19:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:19:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:19:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:19:59 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-345c7b19-bf35-400f-889f-d7faeae87651-c000.snappy.parquet, range: 0-104863438, partition values: [empty row]
21/01/18 18:19:59 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:19:59 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:19:59 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:19:59 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:19:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:19:59 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:19:59 INFO ParquetOutputFormat: Validation is off
21/01/18 18:19:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:19:59 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:19:59 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:19:59 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:19:59 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:19:59 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:19:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:00 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000003_4' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:00 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000003_4: Committed
21/01/18 18:20:00 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2527 bytes result sent to driver
21/01/18 18:20:00 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5, test-runner-bnbrr, executor driver, partition 4, ANY, 7815 bytes)
21/01/18 18:20:00 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)
21/01/18 18:20:00 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 1286 ms on test-runner-bnbrr (executor driver) (4/200)
21/01/18 18:20:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:00 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-373f6422-0305-4ea9-bf5c-e49d798d17a7-c000.snappy.parquet, range: 0-104863438, partition values: [empty row]
21/01/18 18:20:01 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:01 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:01 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:01 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:01 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:01 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:01 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:01 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:01 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:01 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:01 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:01 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000004_5' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:01 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000004_5: Committed
21/01/18 18:20:01 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 2527 bytes result sent to driver
21/01/18 18:20:01 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6, test-runner-bnbrr, executor driver, partition 5, ANY, 7815 bytes)
21/01/18 18:20:01 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)
21/01/18 18:20:01 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 1221 ms on test-runner-bnbrr (executor driver) (5/200)
21/01/18 18:20:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:01 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:01 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-6aaf29a8-4e04-46b9-8868-0053d2d960c1-c000.snappy.parquet, range: 0-104863438, partition values: [empty row]
21/01/18 18:20:02 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:02 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:02 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:02 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:02 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:02 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:02 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:02 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:02 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:02 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:02 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:02 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:02 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:02 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000005_6' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:02 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000005_6: Committed
21/01/18 18:20:02 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 2527 bytes result sent to driver
21/01/18 18:20:02 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7, test-runner-bnbrr, executor driver, partition 6, ANY, 7815 bytes)
21/01/18 18:20:02 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)
21/01/18 18:20:02 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 1289 ms on test-runner-bnbrr (executor driver) (6/200)
21/01/18 18:20:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:03 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-6fafb026-b89c-4b9a-b152-662aae31e005-c000.snappy.parquet, range: 0-104863438, partition values: [empty row]
21/01/18 18:20:03 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:03 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:03 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:03 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:03 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:03 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:03 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:03 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:03 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:03 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:03 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:04 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000006_7' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:04 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000006_7: Committed
21/01/18 18:20:04 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2527 bytes result sent to driver
21/01/18 18:20:04 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8, test-runner-bnbrr, executor driver, partition 7, ANY, 7815 bytes)
21/01/18 18:20:04 INFO Executor: Running task 7.0 in stage 1.0 (TID 8)
21/01/18 18:20:04 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 1233 ms on test-runner-bnbrr (executor driver) (7/200)
21/01/18 18:20:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:04 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-8cef5bf0-4e63-445b-9c9f-a5f816c25cb4-c000.snappy.parquet, range: 0-104863438, partition values: [empty row]
21/01/18 18:20:04 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:04 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:04 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:04 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:04 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:04 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:04 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:04 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:04 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:04 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:04 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:05 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000007_8' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:05 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000007_8: Committed
21/01/18 18:20:05 INFO Executor: Finished task 7.0 in stage 1.0 (TID 8). 2527 bytes result sent to driver
21/01/18 18:20:05 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 9, test-runner-bnbrr, executor driver, partition 8, ANY, 7815 bytes)
21/01/18 18:20:05 INFO Executor: Running task 8.0 in stage 1.0 (TID 9)
21/01/18 18:20:05 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 1313 ms on test-runner-bnbrr (executor driver) (8/200)
21/01/18 18:20:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:05 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:05 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-b0c02e78-c8a2-4126-89b6-42ec3d2cfcf0-c000.snappy.parquet, range: 0-104863438, partition values: [empty row]
21/01/18 18:20:06 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:06 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:06 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:06 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:06 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:06 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:06 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:06 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:06 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:06 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:06 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:06 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000008_9' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:06 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000008_9: Committed
21/01/18 18:20:06 INFO Executor: Finished task 8.0 in stage 1.0 (TID 9). 2527 bytes result sent to driver
21/01/18 18:20:06 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 10, test-runner-bnbrr, executor driver, partition 9, ANY, 7815 bytes)
21/01/18 18:20:06 INFO Executor: Running task 9.0 in stage 1.0 (TID 10)
21/01/18 18:20:06 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 9) in 1299 ms on test-runner-bnbrr (executor driver) (9/200)
21/01/18 18:20:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:06 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:06 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-d77d3121-c720-4fec-bc2e-61e6bd58581b-c000.snappy.parquet, range: 0-104863438, partition values: [empty row]
21/01/18 18:20:07 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:07 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:07 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:07 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:07 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:07 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:07 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:07 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:07 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:07 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:07 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:08 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000009_10' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:08 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000009_10: Committed
21/01/18 18:20:08 INFO Executor: Finished task 9.0 in stage 1.0 (TID 10). 2527 bytes result sent to driver
21/01/18 18:20:08 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 11, test-runner-bnbrr, executor driver, partition 10, ANY, 7815 bytes)
21/01/18 18:20:08 INFO Executor: Running task 10.0 in stage 1.0 (TID 11)
21/01/18 18:20:08 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 10) in 1272 ms on test-runner-bnbrr (executor driver) (10/200)
21/01/18 18:20:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:08 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:08 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-e835ae88-968f-4733-b5e3-a86f32be55d3-c000.snappy.parquet, range: 0-104863438, partition values: [empty row]
21/01/18 18:20:08 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:08 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:08 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:08 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:08 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:08 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:08 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:08 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:08 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:08 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:08 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:09 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000010_11' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:09 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000010_11: Committed
21/01/18 18:20:09 INFO Executor: Finished task 10.0 in stage 1.0 (TID 11). 2527 bytes result sent to driver
21/01/18 18:20:09 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 12, test-runner-bnbrr, executor driver, partition 11, ANY, 7815 bytes)
21/01/18 18:20:09 INFO Executor: Running task 11.0 in stage 1.0 (TID 12)
21/01/18 18:20:09 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 11) in 1291 ms on test-runner-bnbrr (executor driver) (11/200)
21/01/18 18:20:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:09 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:09 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-fbe1741d-5521-448c-b688-eafd84daba79-c000.snappy.parquet, range: 0-104863438, partition values: [empty row]
21/01/18 18:20:09 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:09 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:09 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:09 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:09 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:09 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:09 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:09 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:09 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:09 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:09 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:10 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000011_12' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:10 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000011_12: Committed
21/01/18 18:20:10 INFO Executor: Finished task 11.0 in stage 1.0 (TID 12). 2527 bytes result sent to driver
21/01/18 18:20:10 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 13, test-runner-bnbrr, executor driver, partition 12, ANY, 7815 bytes)
21/01/18 18:20:10 INFO Executor: Running task 12.0 in stage 1.0 (TID 13)
21/01/18 18:20:10 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 12) in 1206 ms on test-runner-bnbrr (executor driver) (12/200)
21/01/18 18:20:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:10 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:10 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-237fc107-69ba-40bd-9d85-491a27b490eb-c000.snappy.parquet, range: 0-104863437, partition values: [empty row]
21/01/18 18:20:11 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:11 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:11 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:11 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:11 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:11 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:11 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:11 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:11 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:11 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:11 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:11 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000012_13' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:11 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000012_13: Committed
21/01/18 18:20:11 INFO Executor: Finished task 12.0 in stage 1.0 (TID 13). 2527 bytes result sent to driver
21/01/18 18:20:11 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 14, test-runner-bnbrr, executor driver, partition 13, ANY, 7815 bytes)
21/01/18 18:20:11 INFO Executor: Running task 13.0 in stage 1.0 (TID 14)
21/01/18 18:20:11 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 13) in 1298 ms on test-runner-bnbrr (executor driver) (13/200)
21/01/18 18:20:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:11 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-5ff11ce9-c84d-4528-a5cb-7eac9af6239a-c000.snappy.parquet, range: 0-104863437, partition values: [empty row]
21/01/18 18:20:12 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:12 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:12 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:12 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:12 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:12 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:12 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:12 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:12 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:12 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:12 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:13 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000013_14' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:13 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000013_14: Committed
21/01/18 18:20:13 INFO Executor: Finished task 13.0 in stage 1.0 (TID 14). 2527 bytes result sent to driver
21/01/18 18:20:13 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 15, test-runner-bnbrr, executor driver, partition 14, ANY, 7815 bytes)
21/01/18 18:20:13 INFO Executor: Running task 14.0 in stage 1.0 (TID 15)
21/01/18 18:20:13 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 14) in 1273 ms on test-runner-bnbrr (executor driver) (14/200)
21/01/18 18:20:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:13 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:13 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-6d72e498-f4d0-4eb1-bcd0-a419b3890045-c000.snappy.parquet, range: 0-104863437, partition values: [empty row]
21/01/18 18:20:13 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:13 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:13 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:13 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:13 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:13 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:13 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:13 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:13 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:13 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:13 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:14 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000014_15' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:14 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000014_15: Committed
21/01/18 18:20:14 INFO Executor: Finished task 14.0 in stage 1.0 (TID 15). 2527 bytes result sent to driver
21/01/18 18:20:14 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 16, test-runner-bnbrr, executor driver, partition 15, ANY, 7815 bytes)
21/01/18 18:20:14 INFO Executor: Running task 15.0 in stage 1.0 (TID 16)
21/01/18 18:20:14 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 15) in 1277 ms on test-runner-bnbrr (executor driver) (15/200)
21/01/18 18:20:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:14 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:14 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-752954a5-a849-4808-8ecf-6312f1e4e621-c000.snappy.parquet, range: 0-104863437, partition values: [empty row]
21/01/18 18:20:14 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:14 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:14 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:14 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:14 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:14 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:14 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:14 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:14 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:14 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:14 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:15 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000015_16' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:15 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000015_16: Committed
21/01/18 18:20:15 INFO Executor: Finished task 15.0 in stage 1.0 (TID 16). 2527 bytes result sent to driver
21/01/18 18:20:15 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 17, test-runner-bnbrr, executor driver, partition 16, ANY, 7815 bytes)
21/01/18 18:20:15 INFO Executor: Running task 16.0 in stage 1.0 (TID 17)
21/01/18 18:20:15 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 16) in 1175 ms on test-runner-bnbrr (executor driver) (16/200)
21/01/18 18:20:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:15 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:15 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-86d179cb-b858-4533-894c-5a719d711fd1-c000.snappy.parquet, range: 0-104863437, partition values: [empty row]
21/01/18 18:20:16 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:16 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:16 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:16 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:16 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:16 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:16 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:16 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:16 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:16 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:16 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:16 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000016_17' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:16 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000016_17: Committed
21/01/18 18:20:16 INFO Executor: Finished task 16.0 in stage 1.0 (TID 17). 2527 bytes result sent to driver
21/01/18 18:20:16 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 18, test-runner-bnbrr, executor driver, partition 17, ANY, 7815 bytes)
21/01/18 18:20:16 INFO Executor: Running task 17.0 in stage 1.0 (TID 18)
21/01/18 18:20:16 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 17) in 1329 ms on test-runner-bnbrr (executor driver) (17/200)
21/01/18 18:20:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:16 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:16 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-8fac3546-3939-4d88-8b58-1459740cfd32-c000.snappy.parquet, range: 0-104863437, partition values: [empty row]
21/01/18 18:20:17 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:17 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:17 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:17 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:17 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:17 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:17 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:17 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:17 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:17 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:17 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:18 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000017_18' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:18 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000017_18: Committed
21/01/18 18:20:18 INFO Executor: Finished task 17.0 in stage 1.0 (TID 18). 2527 bytes result sent to driver
21/01/18 18:20:18 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 19, test-runner-bnbrr, executor driver, partition 18, ANY, 7815 bytes)
21/01/18 18:20:18 INFO Executor: Running task 18.0 in stage 1.0 (TID 19)
21/01/18 18:20:18 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 18) in 1231 ms on test-runner-bnbrr (executor driver) (18/200)
21/01/18 18:20:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:18 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:18 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-b066934f-aa7c-4adc-a152-97ebb90634ca-c000.snappy.parquet, range: 0-104863437, partition values: [empty row]
21/01/18 18:20:18 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:18 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:18 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:18 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:18 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:18 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:18 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:18 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:18 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:18 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:18 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:19 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000018_19' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:19 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000018_19: Committed
21/01/18 18:20:19 INFO Executor: Finished task 18.0 in stage 1.0 (TID 19). 2527 bytes result sent to driver
21/01/18 18:20:19 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 20, test-runner-bnbrr, executor driver, partition 19, ANY, 7815 bytes)
21/01/18 18:20:19 INFO Executor: Running task 19.0 in stage 1.0 (TID 20)
21/01/18 18:20:19 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 19) in 1251 ms on test-runner-bnbrr (executor driver) (19/200)
21/01/18 18:20:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:19 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:19 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-fc9b9686-2956-4f94-908a-e3f65b0b5dec-c000.snappy.parquet, range: 0-104863437, partition values: [empty row]
21/01/18 18:20:19 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:19 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:19 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:19 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:19 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:19 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:19 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:19 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:19 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:19 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:19 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:20 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000019_20' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:20 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000019_20: Committed
21/01/18 18:20:20 INFO Executor: Finished task 19.0 in stage 1.0 (TID 20). 2527 bytes result sent to driver
21/01/18 18:20:20 INFO TaskSetManager: Starting task 20.0 in stage 1.0 (TID 21, test-runner-bnbrr, executor driver, partition 20, ANY, 7815 bytes)
21/01/18 18:20:20 INFO Executor: Running task 20.0 in stage 1.0 (TID 21)
21/01/18 18:20:20 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 20) in 1234 ms on test-runner-bnbrr (executor driver) (20/200)
21/01/18 18:20:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:20 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:20 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-00d83084-bb7a-4e0c-89d4-a4d329edf997-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:20:21 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:21 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:21 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:21 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:21 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:21 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:21 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:21 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:21 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:21 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:21 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:21 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000020_21' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:21 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000020_21: Committed
21/01/18 18:20:21 INFO Executor: Finished task 20.0 in stage 1.0 (TID 21). 2527 bytes result sent to driver
21/01/18 18:20:21 INFO TaskSetManager: Starting task 21.0 in stage 1.0 (TID 22, test-runner-bnbrr, executor driver, partition 21, ANY, 7815 bytes)
21/01/18 18:20:21 INFO Executor: Running task 21.0 in stage 1.0 (TID 22)
21/01/18 18:20:21 INFO TaskSetManager: Finished task 20.0 in stage 1.0 (TID 21) in 1219 ms on test-runner-bnbrr (executor driver) (21/200)
21/01/18 18:20:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:21 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:21 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-0200ce00-0a60-4e32-afa9-c614e963ff06-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:20:22 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:22 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:22 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:22 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:22 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:22 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:22 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:22 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:22 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:22 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:22 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:23 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000021_22' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:23 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000021_22: Committed
21/01/18 18:20:23 INFO Executor: Finished task 21.0 in stage 1.0 (TID 22). 2527 bytes result sent to driver
21/01/18 18:20:23 INFO TaskSetManager: Starting task 22.0 in stage 1.0 (TID 23, test-runner-bnbrr, executor driver, partition 22, ANY, 7815 bytes)
21/01/18 18:20:23 INFO Executor: Running task 22.0 in stage 1.0 (TID 23)
21/01/18 18:20:23 INFO TaskSetManager: Finished task 21.0 in stage 1.0 (TID 22) in 1254 ms on test-runner-bnbrr (executor driver) (22/200)
21/01/18 18:20:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:23 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:23 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-043d4e2d-9842-46ec-88b5-6eae790e7dc6-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:20:23 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:23 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:23 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:23 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:23 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:23 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:23 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:23 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:23 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:23 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:23 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:24 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000022_23' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:24 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000022_23: Committed
21/01/18 18:20:24 INFO Executor: Finished task 22.0 in stage 1.0 (TID 23). 2527 bytes result sent to driver
21/01/18 18:20:24 INFO TaskSetManager: Starting task 23.0 in stage 1.0 (TID 24, test-runner-bnbrr, executor driver, partition 23, ANY, 7815 bytes)
21/01/18 18:20:24 INFO Executor: Running task 23.0 in stage 1.0 (TID 24)
21/01/18 18:20:24 INFO TaskSetManager: Finished task 22.0 in stage 1.0 (TID 23) in 1287 ms on test-runner-bnbrr (executor driver) (23/200)
21/01/18 18:20:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:24 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:24 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-058cdb10-68a4-49bf-a1ab-915b3097b8cd-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:20:24 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:24 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:24 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:24 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:24 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:24 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:24 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:24 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:24 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:24 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:24 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:25 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000023_24' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:25 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000023_24: Committed
21/01/18 18:20:25 INFO Executor: Finished task 23.0 in stage 1.0 (TID 24). 2527 bytes result sent to driver
21/01/18 18:20:25 INFO TaskSetManager: Starting task 24.0 in stage 1.0 (TID 25, test-runner-bnbrr, executor driver, partition 24, ANY, 7815 bytes)
21/01/18 18:20:25 INFO Executor: Running task 24.0 in stage 1.0 (TID 25)
21/01/18 18:20:25 INFO TaskSetManager: Finished task 23.0 in stage 1.0 (TID 24) in 1264 ms on test-runner-bnbrr (executor driver) (24/200)
21/01/18 18:20:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:25 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:25 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-079717a9-24cb-4eaa-943f-70379fcf2a2b-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:20:26 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:26 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:26 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:26 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:26 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:26 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:26 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:26 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:26 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:26 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:26 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:26 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000024_25' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:26 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000024_25: Committed
21/01/18 18:20:26 INFO Executor: Finished task 24.0 in stage 1.0 (TID 25). 2527 bytes result sent to driver
21/01/18 18:20:26 INFO TaskSetManager: Starting task 25.0 in stage 1.0 (TID 26, test-runner-bnbrr, executor driver, partition 25, ANY, 7815 bytes)
21/01/18 18:20:26 INFO Executor: Running task 25.0 in stage 1.0 (TID 26)
21/01/18 18:20:26 INFO TaskSetManager: Finished task 24.0 in stage 1.0 (TID 25) in 1273 ms on test-runner-bnbrr (executor driver) (25/200)
21/01/18 18:20:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:26 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:26 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-0aad0e2e-352c-44fd-8f9b-b1c587048b21-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:20:27 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:27 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:27 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:27 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:27 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:27 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:27 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:27 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:27 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:27 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:27 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:28 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000025_26' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:28 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000025_26: Committed
21/01/18 18:20:28 INFO Executor: Finished task 25.0 in stage 1.0 (TID 26). 2527 bytes result sent to driver
21/01/18 18:20:28 INFO TaskSetManager: Starting task 26.0 in stage 1.0 (TID 27, test-runner-bnbrr, executor driver, partition 26, ANY, 7815 bytes)
21/01/18 18:20:28 INFO Executor: Running task 26.0 in stage 1.0 (TID 27)
21/01/18 18:20:28 INFO TaskSetManager: Finished task 25.0 in stage 1.0 (TID 26) in 1221 ms on test-runner-bnbrr (executor driver) (26/200)
21/01/18 18:20:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:28 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:28 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-0f4584ea-a510-4f9b-b5de-87ec3eec4518-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:20:28 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:28 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:28 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:28 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:28 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:28 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:28 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:28 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:28 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:28 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:28 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:29 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000026_27' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:29 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000026_27: Committed
21/01/18 18:20:29 INFO Executor: Finished task 26.0 in stage 1.0 (TID 27). 2527 bytes result sent to driver
21/01/18 18:20:29 INFO TaskSetManager: Starting task 27.0 in stage 1.0 (TID 28, test-runner-bnbrr, executor driver, partition 27, ANY, 7815 bytes)
21/01/18 18:20:29 INFO Executor: Running task 27.0 in stage 1.0 (TID 28)
21/01/18 18:20:29 INFO TaskSetManager: Finished task 26.0 in stage 1.0 (TID 27) in 1224 ms on test-runner-bnbrr (executor driver) (27/200)
21/01/18 18:20:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:29 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-16bfdaad-664d-4c31-88dd-68689cd713d1-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:20:29 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:29 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:29 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:29 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:29 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:29 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:29 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:29 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:29 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:29 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:29 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:30 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000027_28' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:30 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000027_28: Committed
21/01/18 18:20:30 INFO Executor: Finished task 27.0 in stage 1.0 (TID 28). 2527 bytes result sent to driver
21/01/18 18:20:30 INFO TaskSetManager: Starting task 28.0 in stage 1.0 (TID 29, test-runner-bnbrr, executor driver, partition 28, ANY, 7815 bytes)
21/01/18 18:20:30 INFO Executor: Running task 28.0 in stage 1.0 (TID 29)
21/01/18 18:20:30 INFO TaskSetManager: Finished task 27.0 in stage 1.0 (TID 28) in 1177 ms on test-runner-bnbrr (executor driver) (28/200)
21/01/18 18:20:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:30 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-25c14895-f8fb-41f4-b437-83ca7051738b-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:20:31 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:31 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:31 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:31 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:31 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:31 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:31 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:31 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:31 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:31 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:31 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:31 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000028_29' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:31 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000028_29: Committed
21/01/18 18:20:31 INFO Executor: Finished task 28.0 in stage 1.0 (TID 29). 2527 bytes result sent to driver
21/01/18 18:20:31 INFO TaskSetManager: Starting task 29.0 in stage 1.0 (TID 30, test-runner-bnbrr, executor driver, partition 29, ANY, 7815 bytes)
21/01/18 18:20:31 INFO Executor: Running task 29.0 in stage 1.0 (TID 30)
21/01/18 18:20:31 INFO TaskSetManager: Finished task 28.0 in stage 1.0 (TID 29) in 1243 ms on test-runner-bnbrr (executor driver) (29/200)
21/01/18 18:20:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:31 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-2f0e1f5f-82c4-4241-a94b-f6ab3f1c84b6-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:20:32 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:32 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:32 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:32 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:32 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:32 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:32 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:32 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:32 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:32 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:32 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:32 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000029_30' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:32 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000029_30: Committed
21/01/18 18:20:32 INFO Executor: Finished task 29.0 in stage 1.0 (TID 30). 2527 bytes result sent to driver
21/01/18 18:20:32 INFO TaskSetManager: Starting task 30.0 in stage 1.0 (TID 31, test-runner-bnbrr, executor driver, partition 30, ANY, 7815 bytes)
21/01/18 18:20:32 INFO Executor: Running task 30.0 in stage 1.0 (TID 31)
21/01/18 18:20:32 INFO TaskSetManager: Finished task 29.0 in stage 1.0 (TID 30) in 1172 ms on test-runner-bnbrr (executor driver) (30/200)
21/01/18 18:20:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:32 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:32 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-2f289ea5-6710-4878-ad1f-ab0717b5f894-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:20:33 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:33 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:33 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:33 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:33 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:33 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:33 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:33 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:33 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:33 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:33 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:34 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000030_31' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:34 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000030_31: Committed
21/01/18 18:20:34 INFO Executor: Finished task 30.0 in stage 1.0 (TID 31). 2527 bytes result sent to driver
21/01/18 18:20:34 INFO TaskSetManager: Starting task 31.0 in stage 1.0 (TID 32, test-runner-bnbrr, executor driver, partition 31, ANY, 7815 bytes)
21/01/18 18:20:34 INFO Executor: Running task 31.0 in stage 1.0 (TID 32)
21/01/18 18:20:34 INFO TaskSetManager: Finished task 30.0 in stage 1.0 (TID 31) in 1241 ms on test-runner-bnbrr (executor driver) (31/200)
21/01/18 18:20:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:34 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:34 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-358e3a8b-3bda-4582-af6d-d70146592cd1-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:20:34 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:34 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:34 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:34 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:34 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:34 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:34 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:34 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:34 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:34 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:34 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:34 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:34 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:35 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000031_32' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:35 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000031_32: Committed
21/01/18 18:20:35 INFO Executor: Finished task 31.0 in stage 1.0 (TID 32). 2527 bytes result sent to driver
21/01/18 18:20:35 INFO TaskSetManager: Starting task 32.0 in stage 1.0 (TID 33, test-runner-bnbrr, executor driver, partition 32, ANY, 7815 bytes)
21/01/18 18:20:35 INFO Executor: Running task 32.0 in stage 1.0 (TID 33)
21/01/18 18:20:35 INFO TaskSetManager: Finished task 31.0 in stage 1.0 (TID 32) in 1240 ms on test-runner-bnbrr (executor driver) (32/200)
21/01/18 18:20:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:35 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:35 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-376cbb36-4abf-4ba4-af2d-48d2d9d597df-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:20:35 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:35 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:35 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:35 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:35 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:35 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:35 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:35 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:35 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:35 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:35 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:36 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:36 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000032_33' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:36 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000032_33: Committed
21/01/18 18:20:36 INFO Executor: Finished task 32.0 in stage 1.0 (TID 33). 2527 bytes result sent to driver
21/01/18 18:20:36 INFO TaskSetManager: Starting task 33.0 in stage 1.0 (TID 34, test-runner-bnbrr, executor driver, partition 33, ANY, 7815 bytes)
21/01/18 18:20:36 INFO Executor: Running task 33.0 in stage 1.0 (TID 34)
21/01/18 18:20:36 INFO TaskSetManager: Finished task 32.0 in stage 1.0 (TID 33) in 1243 ms on test-runner-bnbrr (executor driver) (33/200)
21/01/18 18:20:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:36 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:36 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-3d69f169-0a81-47e0-815b-172d0b8626f9-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:20:37 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:37 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:37 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:37 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:37 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:37 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:37 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:37 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:37 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:37 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:37 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:37 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:38 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000033_34' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:38 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000033_34: Committed
21/01/18 18:20:38 INFO Executor: Finished task 33.0 in stage 1.0 (TID 34). 2527 bytes result sent to driver
21/01/18 18:20:38 INFO TaskSetManager: Starting task 34.0 in stage 1.0 (TID 35, test-runner-bnbrr, executor driver, partition 34, ANY, 7815 bytes)
21/01/18 18:20:38 INFO Executor: Running task 34.0 in stage 1.0 (TID 35)
21/01/18 18:20:38 INFO TaskSetManager: Finished task 33.0 in stage 1.0 (TID 34) in 1391 ms on test-runner-bnbrr (executor driver) (34/200)
21/01/18 18:20:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:38 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:38 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-3f96248f-0804-4295-803c-55ee51539a0f-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:20:38 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:38 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:38 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:38 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:38 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:38 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:38 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:38 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:38 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:38 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:38 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:39 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000034_35' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:39 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000034_35: Committed
21/01/18 18:20:39 INFO Executor: Finished task 34.0 in stage 1.0 (TID 35). 2527 bytes result sent to driver
21/01/18 18:20:39 INFO TaskSetManager: Starting task 35.0 in stage 1.0 (TID 36, test-runner-bnbrr, executor driver, partition 35, ANY, 7815 bytes)
21/01/18 18:20:39 INFO Executor: Running task 35.0 in stage 1.0 (TID 36)
21/01/18 18:20:39 INFO TaskSetManager: Finished task 34.0 in stage 1.0 (TID 35) in 1296 ms on test-runner-bnbrr (executor driver) (35/200)
21/01/18 18:20:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:39 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:39 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-405ea7e2-cb05-44c3-a47c-40d5ab9cc8c8-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:20:39 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:39 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:39 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:39 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:39 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:39 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:39 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:39 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:39 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:39 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:39 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:40 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000035_36' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:40 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000035_36: Committed
21/01/18 18:20:40 INFO Executor: Finished task 35.0 in stage 1.0 (TID 36). 2527 bytes result sent to driver
21/01/18 18:20:40 INFO TaskSetManager: Starting task 36.0 in stage 1.0 (TID 37, test-runner-bnbrr, executor driver, partition 36, ANY, 7815 bytes)
21/01/18 18:20:40 INFO Executor: Running task 36.0 in stage 1.0 (TID 37)
21/01/18 18:20:40 INFO TaskSetManager: Finished task 35.0 in stage 1.0 (TID 36) in 1236 ms on test-runner-bnbrr (executor driver) (36/200)
21/01/18 18:20:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:40 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:40 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-4f64e4e3-8cb7-4d8e-967b-1627626b2211-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:20:41 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:41 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:41 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:41 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:41 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:41 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:41 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:41 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:41 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:41 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:41 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:42 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000036_37' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:42 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000036_37: Committed
21/01/18 18:20:42 INFO Executor: Finished task 36.0 in stage 1.0 (TID 37). 2527 bytes result sent to driver
21/01/18 18:20:42 INFO TaskSetManager: Starting task 37.0 in stage 1.0 (TID 38, test-runner-bnbrr, executor driver, partition 37, ANY, 7815 bytes)
21/01/18 18:20:42 INFO Executor: Running task 37.0 in stage 1.0 (TID 38)
21/01/18 18:20:42 INFO TaskSetManager: Finished task 36.0 in stage 1.0 (TID 37) in 1388 ms on test-runner-bnbrr (executor driver) (37/200)
21/01/18 18:20:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:42 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:42 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-579518fa-6fd9-4928-bfb4-a6e063d077e7-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:20:42 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:42 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:42 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:42 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:42 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:42 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:42 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:42 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:42 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:42 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:42 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:43 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000037_38' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:43 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000037_38: Committed
21/01/18 18:20:43 INFO Executor: Finished task 37.0 in stage 1.0 (TID 38). 2527 bytes result sent to driver
21/01/18 18:20:43 INFO TaskSetManager: Starting task 38.0 in stage 1.0 (TID 39, test-runner-bnbrr, executor driver, partition 38, ANY, 7815 bytes)
21/01/18 18:20:43 INFO Executor: Running task 38.0 in stage 1.0 (TID 39)
21/01/18 18:20:43 INFO TaskSetManager: Finished task 37.0 in stage 1.0 (TID 38) in 1230 ms on test-runner-bnbrr (executor driver) (38/200)
21/01/18 18:20:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:43 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:43 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-58547651-d91d-4581-b604-36bc975a745b-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:20:43 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:43 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:43 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:43 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:43 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:43 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:43 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:43 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:43 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:43 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:43 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:44 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000038_39' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:44 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000038_39: Committed
21/01/18 18:20:44 INFO Executor: Finished task 38.0 in stage 1.0 (TID 39). 2527 bytes result sent to driver
21/01/18 18:20:44 INFO TaskSetManager: Starting task 39.0 in stage 1.0 (TID 40, test-runner-bnbrr, executor driver, partition 39, ANY, 7815 bytes)
21/01/18 18:20:44 INFO Executor: Running task 39.0 in stage 1.0 (TID 40)
21/01/18 18:20:44 INFO TaskSetManager: Finished task 38.0 in stage 1.0 (TID 39) in 1253 ms on test-runner-bnbrr (executor driver) (39/200)
21/01/18 18:20:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:44 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:44 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-61db9800-72c4-4699-9a53-294759a318bd-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:20:45 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:45 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:45 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:45 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:45 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:45 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:45 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:45 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:45 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:45 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:45 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:45 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000039_40' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:45 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000039_40: Committed
21/01/18 18:20:45 INFO Executor: Finished task 39.0 in stage 1.0 (TID 40). 2527 bytes result sent to driver
21/01/18 18:20:45 INFO TaskSetManager: Starting task 40.0 in stage 1.0 (TID 41, test-runner-bnbrr, executor driver, partition 40, ANY, 7815 bytes)
21/01/18 18:20:45 INFO Executor: Running task 40.0 in stage 1.0 (TID 41)
21/01/18 18:20:45 INFO TaskSetManager: Finished task 39.0 in stage 1.0 (TID 40) in 1255 ms on test-runner-bnbrr (executor driver) (40/200)
21/01/18 18:20:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:45 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:45 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-6264109b-46d2-4467-9f84-2590e55768cb-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:20:46 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:46 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:46 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:46 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:46 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:46 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:46 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:46 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:46 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:46 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:46 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:47 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000040_41' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:47 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000040_41: Committed
21/01/18 18:20:47 INFO Executor: Finished task 40.0 in stage 1.0 (TID 41). 2527 bytes result sent to driver
21/01/18 18:20:47 INFO TaskSetManager: Starting task 41.0 in stage 1.0 (TID 42, test-runner-bnbrr, executor driver, partition 41, ANY, 7815 bytes)
21/01/18 18:20:47 INFO Executor: Running task 41.0 in stage 1.0 (TID 42)
21/01/18 18:20:47 INFO TaskSetManager: Finished task 40.0 in stage 1.0 (TID 41) in 1298 ms on test-runner-bnbrr (executor driver) (41/200)
21/01/18 18:20:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:47 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-67ec9ab7-4851-475d-9c88-af225931a49e-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:20:47 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:47 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:47 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:47 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:47 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:47 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:47 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:47 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:47 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:47 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:47 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:48 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000041_42' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:48 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000041_42: Committed
21/01/18 18:20:48 INFO Executor: Finished task 41.0 in stage 1.0 (TID 42). 2527 bytes result sent to driver
21/01/18 18:20:48 INFO TaskSetManager: Starting task 42.0 in stage 1.0 (TID 43, test-runner-bnbrr, executor driver, partition 42, ANY, 7815 bytes)
21/01/18 18:20:48 INFO Executor: Running task 42.0 in stage 1.0 (TID 43)
21/01/18 18:20:48 INFO TaskSetManager: Finished task 41.0 in stage 1.0 (TID 42) in 1338 ms on test-runner-bnbrr (executor driver) (42/200)
21/01/18 18:20:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:48 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:48 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-6ce999d2-b239-4da7-b763-efe9a4637119-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:20:48 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:48 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:48 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:48 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:48 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:48 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:48 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:48 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:48 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:48 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:48 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:49 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000042_43' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:49 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000042_43: Committed
21/01/18 18:20:49 INFO Executor: Finished task 42.0 in stage 1.0 (TID 43). 2527 bytes result sent to driver
21/01/18 18:20:49 INFO TaskSetManager: Starting task 43.0 in stage 1.0 (TID 44, test-runner-bnbrr, executor driver, partition 43, ANY, 7815 bytes)
21/01/18 18:20:49 INFO Executor: Running task 43.0 in stage 1.0 (TID 44)
21/01/18 18:20:49 INFO TaskSetManager: Finished task 42.0 in stage 1.0 (TID 43) in 1274 ms on test-runner-bnbrr (executor driver) (43/200)
21/01/18 18:20:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:49 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-710d2e20-dba0-41a1-8a80-8f1b20768bab-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:20:50 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:50 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:50 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:50 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:50 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:50 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:50 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:50 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:50 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:50 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:50 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:50 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:50 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:50 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:50 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000043_44' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:50 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000043_44: Committed
21/01/18 18:20:50 INFO Executor: Finished task 43.0 in stage 1.0 (TID 44). 2527 bytes result sent to driver
21/01/18 18:20:50 INFO TaskSetManager: Starting task 44.0 in stage 1.0 (TID 45, test-runner-bnbrr, executor driver, partition 44, ANY, 7815 bytes)
21/01/18 18:20:50 INFO Executor: Running task 44.0 in stage 1.0 (TID 45)
21/01/18 18:20:50 INFO TaskSetManager: Finished task 43.0 in stage 1.0 (TID 44) in 1223 ms on test-runner-bnbrr (executor driver) (44/200)
21/01/18 18:20:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:50 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:50 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-888479e8-ea1c-4471-b80e-c4cd2de2428d-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:20:51 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:51 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:51 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:51 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:51 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:51 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:51 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:51 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:51 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:51 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:51 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:51 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:51 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:51 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:52 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000044_45' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:52 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000044_45: Committed
21/01/18 18:20:52 INFO Executor: Finished task 44.0 in stage 1.0 (TID 45). 2527 bytes result sent to driver
21/01/18 18:20:52 INFO TaskSetManager: Starting task 45.0 in stage 1.0 (TID 46, test-runner-bnbrr, executor driver, partition 45, ANY, 7815 bytes)
21/01/18 18:20:52 INFO Executor: Running task 45.0 in stage 1.0 (TID 46)
21/01/18 18:20:52 INFO TaskSetManager: Finished task 44.0 in stage 1.0 (TID 45) in 1285 ms on test-runner-bnbrr (executor driver) (45/200)
21/01/18 18:20:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:52 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:52 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-8b0b09b1-5ad2-4272-845c-addd9be4f297-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:20:52 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:52 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:52 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:52 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:52 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:52 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:52 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:52 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:52 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:52 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:52 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:53 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000045_46' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:53 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000045_46: Committed
21/01/18 18:20:53 INFO Executor: Finished task 45.0 in stage 1.0 (TID 46). 2527 bytes result sent to driver
21/01/18 18:20:53 INFO TaskSetManager: Starting task 46.0 in stage 1.0 (TID 47, test-runner-bnbrr, executor driver, partition 46, ANY, 7815 bytes)
21/01/18 18:20:53 INFO Executor: Running task 46.0 in stage 1.0 (TID 47)
21/01/18 18:20:53 INFO TaskSetManager: Finished task 45.0 in stage 1.0 (TID 46) in 1455 ms on test-runner-bnbrr (executor driver) (46/200)
21/01/18 18:20:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:53 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-8b5e1730-c73d-49f8-913a-931754c5342d-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:20:54 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:54 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:54 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:54 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:54 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:54 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:54 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:54 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:54 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:54 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:54 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:54 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000046_47' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:54 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000046_47: Committed
21/01/18 18:20:54 INFO Executor: Finished task 46.0 in stage 1.0 (TID 47). 2527 bytes result sent to driver
21/01/18 18:20:54 INFO TaskSetManager: Starting task 47.0 in stage 1.0 (TID 48, test-runner-bnbrr, executor driver, partition 47, ANY, 7815 bytes)
21/01/18 18:20:54 INFO Executor: Running task 47.0 in stage 1.0 (TID 48)
21/01/18 18:20:54 INFO TaskSetManager: Finished task 46.0 in stage 1.0 (TID 47) in 1219 ms on test-runner-bnbrr (executor driver) (47/200)
21/01/18 18:20:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:54 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:54 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-998527e2-da71-4696-9356-b18c437d71e7-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:20:55 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:55 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:55 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:55 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:55 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:55 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:55 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:55 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:55 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:55 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:55 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:56 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000047_48' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:56 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000047_48: Committed
21/01/18 18:20:56 INFO Executor: Finished task 47.0 in stage 1.0 (TID 48). 2527 bytes result sent to driver
21/01/18 18:20:56 INFO TaskSetManager: Starting task 48.0 in stage 1.0 (TID 49, test-runner-bnbrr, executor driver, partition 48, ANY, 7815 bytes)
21/01/18 18:20:56 INFO Executor: Running task 48.0 in stage 1.0 (TID 49)
21/01/18 18:20:56 INFO TaskSetManager: Finished task 47.0 in stage 1.0 (TID 48) in 1255 ms on test-runner-bnbrr (executor driver) (48/200)
21/01/18 18:20:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:56 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:56 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-99e02400-2030-497a-8cf0-604063603df6-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:20:56 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:56 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:56 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:56 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:56 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:56 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:56 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:56 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:56 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:56 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:56 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:57 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000048_49' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:57 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000048_49: Committed
21/01/18 18:20:57 INFO Executor: Finished task 48.0 in stage 1.0 (TID 49). 2527 bytes result sent to driver
21/01/18 18:20:57 INFO TaskSetManager: Starting task 49.0 in stage 1.0 (TID 50, test-runner-bnbrr, executor driver, partition 49, ANY, 7815 bytes)
21/01/18 18:20:57 INFO Executor: Running task 49.0 in stage 1.0 (TID 50)
21/01/18 18:20:57 INFO TaskSetManager: Finished task 48.0 in stage 1.0 (TID 49) in 1241 ms on test-runner-bnbrr (executor driver) (49/200)
21/01/18 18:20:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:57 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:57 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-9d8811fb-db2e-4a3c-9f40-5dd13456ff8f-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:20:57 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:57 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:57 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:57 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:57 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:57 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:57 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:57 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:57 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:57 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:57 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:58 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000049_50' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:58 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000049_50: Committed
21/01/18 18:20:58 INFO Executor: Finished task 49.0 in stage 1.0 (TID 50). 2527 bytes result sent to driver
21/01/18 18:20:58 INFO TaskSetManager: Starting task 50.0 in stage 1.0 (TID 51, test-runner-bnbrr, executor driver, partition 50, ANY, 7815 bytes)
21/01/18 18:20:58 INFO Executor: Running task 50.0 in stage 1.0 (TID 51)
21/01/18 18:20:58 INFO TaskSetManager: Finished task 49.0 in stage 1.0 (TID 50) in 1204 ms on test-runner-bnbrr (executor driver) (50/200)
21/01/18 18:20:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:58 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:58 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-a0601bf9-4b53-4fc2-af15-eae49fad7305-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:20:59 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:59 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:20:59 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:20:59 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:20:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:20:59 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:20:59 INFO ParquetOutputFormat: Validation is off
21/01/18 18:20:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:20:59 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:20:59 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:20:59 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:20:59 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:20:59 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:20:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:20:59 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000050_51' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:20:59 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000050_51: Committed
21/01/18 18:20:59 INFO Executor: Finished task 50.0 in stage 1.0 (TID 51). 2527 bytes result sent to driver
21/01/18 18:20:59 INFO TaskSetManager: Starting task 51.0 in stage 1.0 (TID 52, test-runner-bnbrr, executor driver, partition 51, ANY, 7815 bytes)
21/01/18 18:20:59 INFO TaskSetManager: Finished task 50.0 in stage 1.0 (TID 51) in 1387 ms on test-runner-bnbrr (executor driver) (51/200)
21/01/18 18:20:59 INFO Executor: Running task 51.0 in stage 1.0 (TID 52)
21/01/18 18:20:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:59 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:20:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:20:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:20:59 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-a6984976-cf2d-42e7-8c8c-68f53f464100-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:21:00 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:00 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:00 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:00 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:00 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:00 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:00 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:00 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:00 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:00 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:00 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:01 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000051_52' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:01 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000051_52: Committed
21/01/18 18:21:01 INFO Executor: Finished task 51.0 in stage 1.0 (TID 52). 2527 bytes result sent to driver
21/01/18 18:21:01 INFO TaskSetManager: Starting task 52.0 in stage 1.0 (TID 53, test-runner-bnbrr, executor driver, partition 52, ANY, 7815 bytes)
21/01/18 18:21:01 INFO Executor: Running task 52.0 in stage 1.0 (TID 53)
21/01/18 18:21:01 INFO TaskSetManager: Finished task 51.0 in stage 1.0 (TID 52) in 1204 ms on test-runner-bnbrr (executor driver) (52/200)
21/01/18 18:21:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:01 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:01 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-ae350ce2-b908-4848-a8f1-2b445cce3bda-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:21:01 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:01 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:01 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:01 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:01 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:01 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:01 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:01 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:01 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:01 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:01 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:02 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000052_53' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:02 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000052_53: Committed
21/01/18 18:21:02 INFO Executor: Finished task 52.0 in stage 1.0 (TID 53). 2527 bytes result sent to driver
21/01/18 18:21:02 INFO TaskSetManager: Starting task 53.0 in stage 1.0 (TID 54, test-runner-bnbrr, executor driver, partition 53, ANY, 7815 bytes)
21/01/18 18:21:02 INFO Executor: Running task 53.0 in stage 1.0 (TID 54)
21/01/18 18:21:02 INFO TaskSetManager: Finished task 52.0 in stage 1.0 (TID 53) in 1261 ms on test-runner-bnbrr (executor driver) (53/200)
21/01/18 18:21:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:02 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-b1e7e7d7-2461-4c4e-b7f4-69022aa8fdb3-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:21:02 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:02 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:02 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:02 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:02 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:02 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:02 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:02 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:02 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:02 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:02 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:02 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:02 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:03 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000053_54' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:03 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000053_54: Committed
21/01/18 18:21:03 INFO Executor: Finished task 53.0 in stage 1.0 (TID 54). 2527 bytes result sent to driver
21/01/18 18:21:03 INFO TaskSetManager: Starting task 54.0 in stage 1.0 (TID 55, test-runner-bnbrr, executor driver, partition 54, ANY, 7815 bytes)
21/01/18 18:21:03 INFO Executor: Running task 54.0 in stage 1.0 (TID 55)
21/01/18 18:21:03 INFO TaskSetManager: Finished task 53.0 in stage 1.0 (TID 54) in 1252 ms on test-runner-bnbrr (executor driver) (54/200)
21/01/18 18:21:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:03 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-b4c755c4-c761-4c69-82ca-da4906b87add-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:21:04 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:04 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:04 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:04 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:04 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:04 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:04 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:04 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:04 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:04 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:04 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:04 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000054_55' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:04 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000054_55: Committed
21/01/18 18:21:04 INFO Executor: Finished task 54.0 in stage 1.0 (TID 55). 2527 bytes result sent to driver
21/01/18 18:21:04 INFO TaskSetManager: Starting task 55.0 in stage 1.0 (TID 56, test-runner-bnbrr, executor driver, partition 55, ANY, 7815 bytes)
21/01/18 18:21:04 INFO Executor: Running task 55.0 in stage 1.0 (TID 56)
21/01/18 18:21:04 INFO TaskSetManager: Finished task 54.0 in stage 1.0 (TID 55) in 1245 ms on test-runner-bnbrr (executor driver) (55/200)
21/01/18 18:21:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:04 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-b63e0b5d-4d91-48b2-8cea-a54c1b36ce89-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:21:05 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:05 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:05 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:05 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:05 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:05 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:05 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:05 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:05 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:05 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:05 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:06 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000055_56' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:06 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000055_56: Committed
21/01/18 18:21:06 INFO Executor: Finished task 55.0 in stage 1.0 (TID 56). 2527 bytes result sent to driver
21/01/18 18:21:06 INFO TaskSetManager: Starting task 56.0 in stage 1.0 (TID 57, test-runner-bnbrr, executor driver, partition 56, ANY, 7815 bytes)
21/01/18 18:21:06 INFO Executor: Running task 56.0 in stage 1.0 (TID 57)
21/01/18 18:21:06 INFO TaskSetManager: Finished task 55.0 in stage 1.0 (TID 56) in 1199 ms on test-runner-bnbrr (executor driver) (56/200)
21/01/18 18:21:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:06 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:06 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-c15f5e1f-3599-4680-80c7-d5ce6a54c0eb-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:21:06 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:06 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:06 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:06 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:06 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:06 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:06 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:06 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:06 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:06 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:06 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:07 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000056_57' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:07 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000056_57: Committed
21/01/18 18:21:07 INFO Executor: Finished task 56.0 in stage 1.0 (TID 57). 2527 bytes result sent to driver
21/01/18 18:21:07 INFO TaskSetManager: Starting task 57.0 in stage 1.0 (TID 58, test-runner-bnbrr, executor driver, partition 57, ANY, 7815 bytes)
21/01/18 18:21:07 INFO TaskSetManager: Finished task 56.0 in stage 1.0 (TID 57) in 1369 ms on test-runner-bnbrr (executor driver) (57/200)
21/01/18 18:21:07 INFO Executor: Running task 57.0 in stage 1.0 (TID 58)
21/01/18 18:21:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:07 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:07 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-c8f25a7d-6217-488e-9beb-24b36a58777f-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:21:08 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:08 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:08 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:08 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:08 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:08 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:08 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:08 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:08 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:08 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:08 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:08 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000057_58' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:08 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000057_58: Committed
21/01/18 18:21:08 INFO Executor: Finished task 57.0 in stage 1.0 (TID 58). 2527 bytes result sent to driver
21/01/18 18:21:08 INFO TaskSetManager: Starting task 58.0 in stage 1.0 (TID 59, test-runner-bnbrr, executor driver, partition 58, ANY, 7815 bytes)
21/01/18 18:21:08 INFO Executor: Running task 58.0 in stage 1.0 (TID 59)
21/01/18 18:21:08 INFO TaskSetManager: Finished task 57.0 in stage 1.0 (TID 58) in 1204 ms on test-runner-bnbrr (executor driver) (58/200)
21/01/18 18:21:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:08 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:08 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-cbdedbaf-ec5b-4f35-99de-be2770d56e0e-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:21:09 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:09 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:09 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:09 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:09 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:09 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:09 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:09 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:09 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:09 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:09 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:09 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000058_59' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:09 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000058_59: Committed
21/01/18 18:21:09 INFO Executor: Finished task 58.0 in stage 1.0 (TID 59). 2527 bytes result sent to driver
21/01/18 18:21:09 INFO TaskSetManager: Starting task 59.0 in stage 1.0 (TID 60, test-runner-bnbrr, executor driver, partition 59, ANY, 7815 bytes)
21/01/18 18:21:09 INFO Executor: Running task 59.0 in stage 1.0 (TID 60)
21/01/18 18:21:09 INFO TaskSetManager: Finished task 58.0 in stage 1.0 (TID 59) in 1284 ms on test-runner-bnbrr (executor driver) (59/200)
21/01/18 18:21:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:09 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:09 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-d0849418-526b-423e-89a0-b2aef6f8ebd9-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:21:10 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:10 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:10 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:10 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:10 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:10 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:10 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:10 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:10 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:10 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:10 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:11 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000059_60' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:11 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000059_60: Committed
21/01/18 18:21:11 INFO Executor: Finished task 59.0 in stage 1.0 (TID 60). 2527 bytes result sent to driver
21/01/18 18:21:11 INFO TaskSetManager: Starting task 60.0 in stage 1.0 (TID 61, test-runner-bnbrr, executor driver, partition 60, ANY, 7815 bytes)
21/01/18 18:21:11 INFO Executor: Running task 60.0 in stage 1.0 (TID 61)
21/01/18 18:21:11 INFO TaskSetManager: Finished task 59.0 in stage 1.0 (TID 60) in 1244 ms on test-runner-bnbrr (executor driver) (60/200)
21/01/18 18:21:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:11 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-dc9a98c2-aea9-4ec3-842a-e189c8eec42d-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:21:11 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:11 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:11 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:11 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:11 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:11 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:11 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:11 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:11 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:11 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:11 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:12 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000060_61' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:12 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000060_61: Committed
21/01/18 18:21:12 INFO Executor: Finished task 60.0 in stage 1.0 (TID 61). 2527 bytes result sent to driver
21/01/18 18:21:12 INFO TaskSetManager: Starting task 61.0 in stage 1.0 (TID 62, test-runner-bnbrr, executor driver, partition 61, ANY, 7815 bytes)
21/01/18 18:21:12 INFO Executor: Running task 61.0 in stage 1.0 (TID 62)
21/01/18 18:21:12 INFO TaskSetManager: Finished task 60.0 in stage 1.0 (TID 61) in 1309 ms on test-runner-bnbrr (executor driver) (61/200)
21/01/18 18:21:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:12 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:12 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-dfae0c14-1b08-4311-96ff-5a8225042633-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:21:12 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:12 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:12 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:12 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:12 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:12 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:12 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:12 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:12 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:12 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:12 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:13 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000061_62' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:13 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000061_62: Committed
21/01/18 18:21:13 INFO Executor: Finished task 61.0 in stage 1.0 (TID 62). 2527 bytes result sent to driver
21/01/18 18:21:13 INFO TaskSetManager: Starting task 62.0 in stage 1.0 (TID 63, test-runner-bnbrr, executor driver, partition 62, ANY, 7815 bytes)
21/01/18 18:21:13 INFO Executor: Running task 62.0 in stage 1.0 (TID 63)
21/01/18 18:21:13 INFO TaskSetManager: Finished task 61.0 in stage 1.0 (TID 62) in 1152 ms on test-runner-bnbrr (executor driver) (62/200)
21/01/18 18:21:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:13 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:13 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-e794c625-95e3-4245-a0fb-4c7cb691c83d-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:21:14 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:14 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:14 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:14 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:14 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:14 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:14 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:14 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:14 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:14 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:14 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:14 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000062_63' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:14 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000062_63: Committed
21/01/18 18:21:14 INFO Executor: Finished task 62.0 in stage 1.0 (TID 63). 2527 bytes result sent to driver
21/01/18 18:21:14 INFO TaskSetManager: Starting task 63.0 in stage 1.0 (TID 64, test-runner-bnbrr, executor driver, partition 63, ANY, 7815 bytes)
21/01/18 18:21:14 INFO Executor: Running task 63.0 in stage 1.0 (TID 64)
21/01/18 18:21:14 INFO TaskSetManager: Finished task 62.0 in stage 1.0 (TID 63) in 1231 ms on test-runner-bnbrr (executor driver) (63/200)
21/01/18 18:21:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:14 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:14 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-eacbbf07-6c4d-44ef-aafa-dad754f43c32-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:21:15 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:15 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:15 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:15 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:15 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:15 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:15 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:15 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:15 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:15 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:15 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:16 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000063_64' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:16 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000063_64: Committed
21/01/18 18:21:16 INFO Executor: Finished task 63.0 in stage 1.0 (TID 64). 2527 bytes result sent to driver
21/01/18 18:21:16 INFO TaskSetManager: Starting task 64.0 in stage 1.0 (TID 65, test-runner-bnbrr, executor driver, partition 64, ANY, 7815 bytes)
21/01/18 18:21:16 INFO Executor: Running task 64.0 in stage 1.0 (TID 65)
21/01/18 18:21:16 INFO TaskSetManager: Finished task 63.0 in stage 1.0 (TID 64) in 1471 ms on test-runner-bnbrr (executor driver) (64/200)
21/01/18 18:21:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:16 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:16 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-f5e9b889-bb4d-46ad-a8f2-d0796459f11f-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:21:16 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:16 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:16 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:16 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:16 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:16 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:16 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:16 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:16 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:16 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:16 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:17 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000064_65' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:17 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000064_65: Committed
21/01/18 18:21:17 INFO Executor: Finished task 64.0 in stage 1.0 (TID 65). 2527 bytes result sent to driver
21/01/18 18:21:17 INFO TaskSetManager: Starting task 65.0 in stage 1.0 (TID 66, test-runner-bnbrr, executor driver, partition 65, ANY, 7815 bytes)
21/01/18 18:21:17 INFO Executor: Running task 65.0 in stage 1.0 (TID 66)
21/01/18 18:21:17 INFO TaskSetManager: Finished task 64.0 in stage 1.0 (TID 65) in 1248 ms on test-runner-bnbrr (executor driver) (65/200)
21/01/18 18:21:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:17 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:17 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-f6a486c8-440b-4d4e-8681-b4025dc3be42-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:21:18 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:18 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:18 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:18 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:18 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:18 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:18 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:18 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:18 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:18 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:18 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:18 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000065_66' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:18 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000065_66: Committed
21/01/18 18:21:18 INFO Executor: Finished task 65.0 in stage 1.0 (TID 66). 2527 bytes result sent to driver
21/01/18 18:21:18 INFO TaskSetManager: Starting task 66.0 in stage 1.0 (TID 67, test-runner-bnbrr, executor driver, partition 66, ANY, 7815 bytes)
21/01/18 18:21:18 INFO TaskSetManager: Finished task 65.0 in stage 1.0 (TID 66) in 1313 ms on test-runner-bnbrr (executor driver) (66/200)
21/01/18 18:21:18 INFO Executor: Running task 66.0 in stage 1.0 (TID 67)
21/01/18 18:21:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:18 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:18 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-fc1e9963-cbd0-4293-9073-ad9644a3842e-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:21:19 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:19 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:19 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:19 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:19 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:19 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:19 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:19 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:19 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:19 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:19 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:20 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000066_67' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:20 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000066_67: Committed
21/01/18 18:21:20 INFO Executor: Finished task 66.0 in stage 1.0 (TID 67). 2527 bytes result sent to driver
21/01/18 18:21:20 INFO TaskSetManager: Starting task 67.0 in stage 1.0 (TID 68, test-runner-bnbrr, executor driver, partition 67, ANY, 7815 bytes)
21/01/18 18:21:20 INFO Executor: Running task 67.0 in stage 1.0 (TID 68)
21/01/18 18:21:20 INFO TaskSetManager: Finished task 66.0 in stage 1.0 (TID 67) in 1264 ms on test-runner-bnbrr (executor driver) (67/200)
21/01/18 18:21:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:20 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:20 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-fd3f2d4a-7372-4e3d-8b04-e718373a8549-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/18 18:21:20 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:20 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:20 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:20 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:20 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:20 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:20 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:20 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:20 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:20 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:20 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:21 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000067_68' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:21 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000067_68: Committed
21/01/18 18:21:21 INFO Executor: Finished task 67.0 in stage 1.0 (TID 68). 2527 bytes result sent to driver
21/01/18 18:21:21 INFO TaskSetManager: Starting task 68.0 in stage 1.0 (TID 69, test-runner-bnbrr, executor driver, partition 68, ANY, 7815 bytes)
21/01/18 18:21:21 INFO Executor: Running task 68.0 in stage 1.0 (TID 69)
21/01/18 18:21:21 INFO TaskSetManager: Finished task 67.0 in stage 1.0 (TID 68) in 1301 ms on test-runner-bnbrr (executor driver) (68/200)
21/01/18 18:21:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:21 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:21 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-07b8d561-0a79-4949-adf1-b9622ee5938a-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/18 18:21:22 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:22 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:22 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:22 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:22 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:22 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:22 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:22 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:22 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:22 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:22 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:22 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000068_69' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:22 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000068_69: Committed
21/01/18 18:21:22 INFO Executor: Finished task 68.0 in stage 1.0 (TID 69). 2527 bytes result sent to driver
21/01/18 18:21:22 INFO TaskSetManager: Starting task 69.0 in stage 1.0 (TID 70, test-runner-bnbrr, executor driver, partition 69, ANY, 7815 bytes)
21/01/18 18:21:22 INFO Executor: Running task 69.0 in stage 1.0 (TID 70)
21/01/18 18:21:22 INFO TaskSetManager: Finished task 68.0 in stage 1.0 (TID 69) in 1240 ms on test-runner-bnbrr (executor driver) (69/200)
21/01/18 18:21:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:22 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:22 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-240958dc-bb82-43f9-b9d6-a8ac12add555-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/18 18:21:23 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:23 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:23 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:23 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:23 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:23 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:23 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:23 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:23 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:23 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:23 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:23 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000069_70' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:23 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000069_70: Committed
21/01/18 18:21:23 INFO Executor: Finished task 69.0 in stage 1.0 (TID 70). 2527 bytes result sent to driver
21/01/18 18:21:23 INFO TaskSetManager: Starting task 70.0 in stage 1.0 (TID 71, test-runner-bnbrr, executor driver, partition 70, ANY, 7815 bytes)
21/01/18 18:21:23 INFO Executor: Running task 70.0 in stage 1.0 (TID 71)
21/01/18 18:21:23 INFO TaskSetManager: Finished task 69.0 in stage 1.0 (TID 70) in 1275 ms on test-runner-bnbrr (executor driver) (70/200)
21/01/18 18:21:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:23 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:23 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-27809162-fd20-4da4-bbef-76d0dbee28f3-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/18 18:21:24 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:24 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:24 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:24 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:24 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:24 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:24 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:24 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:24 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:24 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:24 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:25 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000070_71' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:25 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000070_71: Committed
21/01/18 18:21:25 INFO Executor: Finished task 70.0 in stage 1.0 (TID 71). 2527 bytes result sent to driver
21/01/18 18:21:25 INFO TaskSetManager: Starting task 71.0 in stage 1.0 (TID 72, test-runner-bnbrr, executor driver, partition 71, ANY, 7815 bytes)
21/01/18 18:21:25 INFO Executor: Running task 71.0 in stage 1.0 (TID 72)
21/01/18 18:21:25 INFO TaskSetManager: Finished task 70.0 in stage 1.0 (TID 71) in 1274 ms on test-runner-bnbrr (executor driver) (71/200)
21/01/18 18:21:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:25 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:25 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-33d5d088-9b3b-4125-87b4-6843aee072ad-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/18 18:21:25 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:25 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:25 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:25 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:25 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:25 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:25 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:25 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:25 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:25 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:25 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:26 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000071_72' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:26 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000071_72: Committed
21/01/18 18:21:26 INFO Executor: Finished task 71.0 in stage 1.0 (TID 72). 2527 bytes result sent to driver
21/01/18 18:21:26 INFO TaskSetManager: Starting task 72.0 in stage 1.0 (TID 73, test-runner-bnbrr, executor driver, partition 72, ANY, 7815 bytes)
21/01/18 18:21:26 INFO Executor: Running task 72.0 in stage 1.0 (TID 73)
21/01/18 18:21:26 INFO TaskSetManager: Finished task 71.0 in stage 1.0 (TID 72) in 1218 ms on test-runner-bnbrr (executor driver) (72/200)
21/01/18 18:21:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:26 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:26 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-33db048b-4fde-4183-8d90-6621783eb92a-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/18 18:21:26 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:26 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:26 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:26 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:26 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:26 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:26 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:26 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:26 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:26 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:26 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:27 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000072_73' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:27 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000072_73: Committed
21/01/18 18:21:27 INFO Executor: Finished task 72.0 in stage 1.0 (TID 73). 2527 bytes result sent to driver
21/01/18 18:21:27 INFO TaskSetManager: Starting task 73.0 in stage 1.0 (TID 74, test-runner-bnbrr, executor driver, partition 73, ANY, 7815 bytes)
21/01/18 18:21:27 INFO Executor: Running task 73.0 in stage 1.0 (TID 74)
21/01/18 18:21:27 INFO TaskSetManager: Finished task 72.0 in stage 1.0 (TID 73) in 1228 ms on test-runner-bnbrr (executor driver) (73/200)
21/01/18 18:21:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:27 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:27 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-507b3af7-3fde-4201-bafc-8cfb0789cff5-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/18 18:21:28 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:28 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:28 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:28 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:28 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:28 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:28 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:28 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:28 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:28 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:28 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:28 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000073_74' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:28 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000073_74: Committed
21/01/18 18:21:28 INFO Executor: Finished task 73.0 in stage 1.0 (TID 74). 2527 bytes result sent to driver
21/01/18 18:21:28 INFO TaskSetManager: Starting task 74.0 in stage 1.0 (TID 75, test-runner-bnbrr, executor driver, partition 74, ANY, 7815 bytes)
21/01/18 18:21:28 INFO Executor: Running task 74.0 in stage 1.0 (TID 75)
21/01/18 18:21:28 INFO TaskSetManager: Finished task 73.0 in stage 1.0 (TID 74) in 1264 ms on test-runner-bnbrr (executor driver) (74/200)
21/01/18 18:21:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:28 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:28 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-5e31a833-49d0-4e4e-82fa-ed326990593b-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/18 18:21:29 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:29 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:29 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:29 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:29 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:29 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:29 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:29 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:29 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:29 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:29 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:30 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000074_75' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:30 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000074_75: Committed
21/01/18 18:21:30 INFO Executor: Finished task 74.0 in stage 1.0 (TID 75). 2527 bytes result sent to driver
21/01/18 18:21:30 INFO TaskSetManager: Starting task 75.0 in stage 1.0 (TID 76, test-runner-bnbrr, executor driver, partition 75, ANY, 7815 bytes)
21/01/18 18:21:30 INFO Executor: Running task 75.0 in stage 1.0 (TID 76)
21/01/18 18:21:30 INFO TaskSetManager: Finished task 74.0 in stage 1.0 (TID 75) in 1177 ms on test-runner-bnbrr (executor driver) (75/200)
21/01/18 18:21:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:30 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-61c24e89-670c-4814-94a6-e19482638abf-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/18 18:21:30 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:30 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:30 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:30 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:30 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:30 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:30 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:30 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:30 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:30 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:30 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:31 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000075_76' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:31 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000075_76: Committed
21/01/18 18:21:31 INFO Executor: Finished task 75.0 in stage 1.0 (TID 76). 2527 bytes result sent to driver
21/01/18 18:21:31 INFO TaskSetManager: Starting task 76.0 in stage 1.0 (TID 77, test-runner-bnbrr, executor driver, partition 76, ANY, 7815 bytes)
21/01/18 18:21:31 INFO Executor: Running task 76.0 in stage 1.0 (TID 77)
21/01/18 18:21:31 INFO TaskSetManager: Finished task 75.0 in stage 1.0 (TID 76) in 1185 ms on test-runner-bnbrr (executor driver) (76/200)
21/01/18 18:21:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:31 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-6b1ace46-23ee-45cd-898d-fd893fb9901d-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/18 18:21:31 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:31 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:31 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:31 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:31 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:31 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:31 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:31 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:31 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:31 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:31 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:32 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000076_77' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:32 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000076_77: Committed
21/01/18 18:21:32 INFO Executor: Finished task 76.0 in stage 1.0 (TID 77). 2527 bytes result sent to driver
21/01/18 18:21:32 INFO TaskSetManager: Starting task 77.0 in stage 1.0 (TID 78, test-runner-bnbrr, executor driver, partition 77, ANY, 7815 bytes)
21/01/18 18:21:32 INFO Executor: Running task 77.0 in stage 1.0 (TID 78)
21/01/18 18:21:32 INFO TaskSetManager: Finished task 76.0 in stage 1.0 (TID 77) in 1318 ms on test-runner-bnbrr (executor driver) (77/200)
21/01/18 18:21:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:32 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:32 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-7187973f-ccf9-49d5-bd77-169b2be8b617-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/18 18:21:33 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:33 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:33 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:33 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:33 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:33 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:33 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:33 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:33 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:33 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:33 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:33 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000077_78' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:33 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000077_78: Committed
21/01/18 18:21:33 INFO Executor: Finished task 77.0 in stage 1.0 (TID 78). 2527 bytes result sent to driver
21/01/18 18:21:33 INFO TaskSetManager: Starting task 78.0 in stage 1.0 (TID 79, test-runner-bnbrr, executor driver, partition 78, ANY, 7815 bytes)
21/01/18 18:21:33 INFO Executor: Running task 78.0 in stage 1.0 (TID 79)
21/01/18 18:21:33 INFO TaskSetManager: Finished task 77.0 in stage 1.0 (TID 78) in 1186 ms on test-runner-bnbrr (executor driver) (78/200)
21/01/18 18:21:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:33 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:33 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-7e00cb85-9ddd-49b6-a136-4d0c21c193df-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/18 18:21:34 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:34 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:34 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:34 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:34 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:34 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:34 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:34 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:34 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:34 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:34 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:34 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:34 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:35 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000078_79' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:35 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000078_79: Committed
21/01/18 18:21:35 INFO Executor: Finished task 78.0 in stage 1.0 (TID 79). 2527 bytes result sent to driver
21/01/18 18:21:35 INFO TaskSetManager: Starting task 79.0 in stage 1.0 (TID 80, test-runner-bnbrr, executor driver, partition 79, ANY, 7815 bytes)
21/01/18 18:21:35 INFO Executor: Running task 79.0 in stage 1.0 (TID 80)
21/01/18 18:21:35 INFO TaskSetManager: Finished task 78.0 in stage 1.0 (TID 79) in 1286 ms on test-runner-bnbrr (executor driver) (79/200)
21/01/18 18:21:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:35 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:35 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-88a2f85f-a38c-4237-be89-6e15c945b000-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/18 18:21:35 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:35 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:35 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:35 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:35 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:35 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:35 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:35 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:35 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:35 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:35 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:36 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000079_80' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:36 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000079_80: Committed
21/01/18 18:21:36 INFO Executor: Finished task 79.0 in stage 1.0 (TID 80). 2527 bytes result sent to driver
21/01/18 18:21:36 INFO TaskSetManager: Starting task 80.0 in stage 1.0 (TID 81, test-runner-bnbrr, executor driver, partition 80, ANY, 7815 bytes)
21/01/18 18:21:36 INFO Executor: Running task 80.0 in stage 1.0 (TID 81)
21/01/18 18:21:36 INFO TaskSetManager: Finished task 79.0 in stage 1.0 (TID 80) in 1371 ms on test-runner-bnbrr (executor driver) (80/200)
21/01/18 18:21:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:36 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:36 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-994cdcb4-e61c-4b79-adbd-d39d1b8145aa-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/18 18:21:37 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:37 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:37 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:37 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:37 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:37 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:37 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:37 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:37 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:37 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:37 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:37 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:37 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000080_81' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:37 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000080_81: Committed
21/01/18 18:21:37 INFO Executor: Finished task 80.0 in stage 1.0 (TID 81). 2527 bytes result sent to driver
21/01/18 18:21:37 INFO TaskSetManager: Starting task 81.0 in stage 1.0 (TID 82, test-runner-bnbrr, executor driver, partition 81, ANY, 7815 bytes)
21/01/18 18:21:37 INFO Executor: Running task 81.0 in stage 1.0 (TID 82)
21/01/18 18:21:37 INFO TaskSetManager: Finished task 80.0 in stage 1.0 (TID 81) in 1321 ms on test-runner-bnbrr (executor driver) (81/200)
21/01/18 18:21:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:37 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:37 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-a0ba90e1-67a1-4fcc-86c0-c6c5ec604943-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/18 18:21:38 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:38 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:38 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:38 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:38 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:38 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:38 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:38 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:38 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:38 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:38 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:39 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000081_82' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:39 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000081_82: Committed
21/01/18 18:21:39 INFO Executor: Finished task 81.0 in stage 1.0 (TID 82). 2527 bytes result sent to driver
21/01/18 18:21:39 INFO TaskSetManager: Starting task 82.0 in stage 1.0 (TID 83, test-runner-bnbrr, executor driver, partition 82, ANY, 7815 bytes)
21/01/18 18:21:39 INFO Executor: Running task 82.0 in stage 1.0 (TID 83)
21/01/18 18:21:39 INFO TaskSetManager: Finished task 81.0 in stage 1.0 (TID 82) in 1217 ms on test-runner-bnbrr (executor driver) (82/200)
21/01/18 18:21:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:39 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:39 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-a26c14ee-d480-4ed1-95f7-5bc47c555cba-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/18 18:21:39 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:39 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:39 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:39 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:39 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:39 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:39 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:39 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:39 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:39 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:39 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:40 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000082_83' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:40 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000082_83: Committed
21/01/18 18:21:40 INFO Executor: Finished task 82.0 in stage 1.0 (TID 83). 2527 bytes result sent to driver
21/01/18 18:21:40 INFO TaskSetManager: Starting task 83.0 in stage 1.0 (TID 84, test-runner-bnbrr, executor driver, partition 83, ANY, 7815 bytes)
21/01/18 18:21:40 INFO Executor: Running task 83.0 in stage 1.0 (TID 84)
21/01/18 18:21:40 INFO TaskSetManager: Finished task 82.0 in stage 1.0 (TID 83) in 1268 ms on test-runner-bnbrr (executor driver) (83/200)
21/01/18 18:21:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:40 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:40 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-a4b3c6e7-82b3-4ecf-a9a0-e9321a1cbbc1-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/18 18:21:40 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:40 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:40 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:40 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:40 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:40 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:40 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:40 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:40 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:40 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:40 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:41 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000083_84' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:41 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000083_84: Committed
21/01/18 18:21:41 INFO Executor: Finished task 83.0 in stage 1.0 (TID 84). 2527 bytes result sent to driver
21/01/18 18:21:41 INFO TaskSetManager: Starting task 84.0 in stage 1.0 (TID 85, test-runner-bnbrr, executor driver, partition 84, ANY, 7815 bytes)
21/01/18 18:21:41 INFO Executor: Running task 84.0 in stage 1.0 (TID 85)
21/01/18 18:21:41 INFO TaskSetManager: Finished task 83.0 in stage 1.0 (TID 84) in 1235 ms on test-runner-bnbrr (executor driver) (84/200)
21/01/18 18:21:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:41 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:41 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-ac3bd021-a97f-40db-820f-3fc5e70d1b68-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/18 18:21:42 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:42 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:42 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:42 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:42 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:42 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:42 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:42 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:42 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:42 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:42 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:42 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000084_85' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:42 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000084_85: Committed
21/01/18 18:21:42 INFO Executor: Finished task 84.0 in stage 1.0 (TID 85). 2527 bytes result sent to driver
21/01/18 18:21:42 INFO TaskSetManager: Starting task 85.0 in stage 1.0 (TID 86, test-runner-bnbrr, executor driver, partition 85, ANY, 7815 bytes)
21/01/18 18:21:42 INFO Executor: Running task 85.0 in stage 1.0 (TID 86)
21/01/18 18:21:42 INFO TaskSetManager: Finished task 84.0 in stage 1.0 (TID 85) in 1221 ms on test-runner-bnbrr (executor driver) (85/200)
21/01/18 18:21:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:42 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:42 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-aca95a8c-a228-4212-a40f-809699f094ff-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/18 18:21:43 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:43 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:43 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:43 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:43 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:43 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:43 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:43 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:43 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:43 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:43 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:44 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000085_86' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:44 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000085_86: Committed
21/01/18 18:21:44 INFO Executor: Finished task 85.0 in stage 1.0 (TID 86). 2527 bytes result sent to driver
21/01/18 18:21:44 INFO TaskSetManager: Starting task 86.0 in stage 1.0 (TID 87, test-runner-bnbrr, executor driver, partition 86, ANY, 7815 bytes)
21/01/18 18:21:44 INFO Executor: Running task 86.0 in stage 1.0 (TID 87)
21/01/18 18:21:44 INFO TaskSetManager: Finished task 85.0 in stage 1.0 (TID 86) in 1330 ms on test-runner-bnbrr (executor driver) (86/200)
21/01/18 18:21:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:44 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:44 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-d889e109-14f8-44cd-a974-4a5e02877873-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/18 18:21:44 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:44 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:44 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:44 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:44 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:44 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:44 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:44 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:44 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:44 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:44 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:45 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000086_87' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:45 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000086_87: Committed
21/01/18 18:21:45 INFO Executor: Finished task 86.0 in stage 1.0 (TID 87). 2527 bytes result sent to driver
21/01/18 18:21:45 INFO TaskSetManager: Starting task 87.0 in stage 1.0 (TID 88, test-runner-bnbrr, executor driver, partition 87, ANY, 7815 bytes)
21/01/18 18:21:45 INFO Executor: Running task 87.0 in stage 1.0 (TID 88)
21/01/18 18:21:45 INFO TaskSetManager: Finished task 86.0 in stage 1.0 (TID 87) in 1253 ms on test-runner-bnbrr (executor driver) (87/200)
21/01/18 18:21:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:45 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:45 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-d891ab94-366c-44a9-8f24-49de6ee21806-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/18 18:21:45 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:45 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:45 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:45 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:45 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:45 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:45 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:45 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:45 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:45 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:45 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:46 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000087_88' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:46 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000087_88: Committed
21/01/18 18:21:46 INFO Executor: Finished task 87.0 in stage 1.0 (TID 88). 2527 bytes result sent to driver
21/01/18 18:21:46 INFO TaskSetManager: Starting task 88.0 in stage 1.0 (TID 89, test-runner-bnbrr, executor driver, partition 88, ANY, 7815 bytes)
21/01/18 18:21:46 INFO Executor: Running task 88.0 in stage 1.0 (TID 89)
21/01/18 18:21:46 INFO TaskSetManager: Finished task 87.0 in stage 1.0 (TID 88) in 1227 ms on test-runner-bnbrr (executor driver) (88/200)
21/01/18 18:21:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:46 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:46 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-df79502e-c32f-4019-ba77-4eddad2dffaa-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/18 18:21:47 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:47 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:47 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:47 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:47 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:47 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:47 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:47 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:47 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:47 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:47 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:47 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000088_89' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:47 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000088_89: Committed
21/01/18 18:21:47 INFO Executor: Finished task 88.0 in stage 1.0 (TID 89). 2527 bytes result sent to driver
21/01/18 18:21:47 INFO TaskSetManager: Starting task 89.0 in stage 1.0 (TID 90, test-runner-bnbrr, executor driver, partition 89, ANY, 7815 bytes)
21/01/18 18:21:47 INFO Executor: Running task 89.0 in stage 1.0 (TID 90)
21/01/18 18:21:47 INFO TaskSetManager: Finished task 88.0 in stage 1.0 (TID 89) in 1200 ms on test-runner-bnbrr (executor driver) (89/200)
21/01/18 18:21:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:47 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-e55ee486-93de-4f04-b8a9-8c214403478f-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/18 18:21:48 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:48 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:48 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:48 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:48 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:48 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:48 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:48 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:48 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:48 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:48 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:49 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000089_90' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:49 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000089_90: Committed
21/01/18 18:21:49 INFO Executor: Finished task 89.0 in stage 1.0 (TID 90). 2527 bytes result sent to driver
21/01/18 18:21:49 INFO TaskSetManager: Starting task 90.0 in stage 1.0 (TID 91, test-runner-bnbrr, executor driver, partition 90, ANY, 7815 bytes)
21/01/18 18:21:49 INFO Executor: Running task 90.0 in stage 1.0 (TID 91)
21/01/18 18:21:49 INFO TaskSetManager: Finished task 89.0 in stage 1.0 (TID 90) in 1395 ms on test-runner-bnbrr (executor driver) (90/200)
21/01/18 18:21:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:49 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-ea07fd7a-0c55-472e-941c-d4c1277bc74d-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/18 18:21:49 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:49 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:49 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:49 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:49 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:49 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:49 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:49 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:49 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:49 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:49 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:50 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000090_91' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:50 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000090_91: Committed
21/01/18 18:21:50 INFO Executor: Finished task 90.0 in stage 1.0 (TID 91). 2527 bytes result sent to driver
21/01/18 18:21:50 INFO TaskSetManager: Starting task 91.0 in stage 1.0 (TID 92, test-runner-bnbrr, executor driver, partition 91, ANY, 7815 bytes)
21/01/18 18:21:50 INFO Executor: Running task 91.0 in stage 1.0 (TID 92)
21/01/18 18:21:50 INFO TaskSetManager: Finished task 90.0 in stage 1.0 (TID 91) in 1286 ms on test-runner-bnbrr (executor driver) (91/200)
21/01/18 18:21:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:50 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:50 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-f827fe44-9e8e-45f0-aa0d-b9fd341cb4e0-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/18 18:21:51 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:51 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:51 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:51 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:51 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:51 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:51 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:51 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:51 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:51 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:51 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:51 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:51 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:51 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:51 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000091_92' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:51 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000091_92: Committed
21/01/18 18:21:51 INFO Executor: Finished task 91.0 in stage 1.0 (TID 92). 2527 bytes result sent to driver
21/01/18 18:21:51 INFO TaskSetManager: Starting task 92.0 in stage 1.0 (TID 93, test-runner-bnbrr, executor driver, partition 92, ANY, 7815 bytes)
21/01/18 18:21:51 INFO Executor: Running task 92.0 in stage 1.0 (TID 93)
21/01/18 18:21:51 INFO TaskSetManager: Finished task 91.0 in stage 1.0 (TID 92) in 1260 ms on test-runner-bnbrr (executor driver) (92/200)
21/01/18 18:21:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:51 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-014b687c-8a3e-4184-9579-db8e10921a06-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:21:52 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:52 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:52 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:52 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:52 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:52 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:52 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:52 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:52 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:52 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:52 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:52 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000092_93' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:52 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000092_93: Committed
21/01/18 18:21:52 INFO Executor: Finished task 92.0 in stage 1.0 (TID 93). 2527 bytes result sent to driver
21/01/18 18:21:52 INFO TaskSetManager: Starting task 93.0 in stage 1.0 (TID 94, test-runner-bnbrr, executor driver, partition 93, ANY, 7815 bytes)
21/01/18 18:21:52 INFO Executor: Running task 93.0 in stage 1.0 (TID 94)
21/01/18 18:21:52 INFO TaskSetManager: Finished task 92.0 in stage 1.0 (TID 93) in 1214 ms on test-runner-bnbrr (executor driver) (93/200)
21/01/18 18:21:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:52 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:52 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-02c8b828-9768-4c70-90ca-c57e2e4d7076-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:21:53 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:53 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:53 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:53 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:53 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:53 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:53 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:53 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:53 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:53 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:53 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:54 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000093_94' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:54 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000093_94: Committed
21/01/18 18:21:54 INFO Executor: Finished task 93.0 in stage 1.0 (TID 94). 2527 bytes result sent to driver
21/01/18 18:21:54 INFO TaskSetManager: Starting task 94.0 in stage 1.0 (TID 95, test-runner-bnbrr, executor driver, partition 94, ANY, 7815 bytes)
21/01/18 18:21:54 INFO Executor: Running task 94.0 in stage 1.0 (TID 95)
21/01/18 18:21:54 INFO TaskSetManager: Finished task 93.0 in stage 1.0 (TID 94) in 1188 ms on test-runner-bnbrr (executor driver) (94/200)
21/01/18 18:21:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:54 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:54 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-0cdf5c2d-b738-4d4d-912b-03e8430855b4-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:21:54 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:54 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:54 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:54 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:54 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:54 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:54 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:54 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:54 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:54 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:54 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:55 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000094_95' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:55 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000094_95: Committed
21/01/18 18:21:55 INFO Executor: Finished task 94.0 in stage 1.0 (TID 95). 2527 bytes result sent to driver
21/01/18 18:21:55 INFO TaskSetManager: Starting task 95.0 in stage 1.0 (TID 96, test-runner-bnbrr, executor driver, partition 95, ANY, 7815 bytes)
21/01/18 18:21:55 INFO Executor: Running task 95.0 in stage 1.0 (TID 96)
21/01/18 18:21:55 INFO TaskSetManager: Finished task 94.0 in stage 1.0 (TID 95) in 1264 ms on test-runner-bnbrr (executor driver) (95/200)
21/01/18 18:21:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:55 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-0d2c9280-1ceb-4b89-8531-6a75c75c0f9c-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:21:55 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:55 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:55 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:55 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:55 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:55 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:55 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:55 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:55 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:55 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:55 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:56 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000095_96' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:56 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000095_96: Committed
21/01/18 18:21:56 INFO Executor: Finished task 95.0 in stage 1.0 (TID 96). 2527 bytes result sent to driver
21/01/18 18:21:56 INFO TaskSetManager: Starting task 96.0 in stage 1.0 (TID 97, test-runner-bnbrr, executor driver, partition 96, ANY, 7815 bytes)
21/01/18 18:21:56 INFO Executor: Running task 96.0 in stage 1.0 (TID 97)
21/01/18 18:21:56 INFO TaskSetManager: Finished task 95.0 in stage 1.0 (TID 96) in 1183 ms on test-runner-bnbrr (executor driver) (96/200)
21/01/18 18:21:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:56 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:56 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-10b5f934-04df-451d-80e4-c8fca69fb3ec-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:21:57 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:57 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:57 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:57 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:57 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:57 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:57 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:57 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:57 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:57 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:57 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:57 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000096_97' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:57 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000096_97: Committed
21/01/18 18:21:57 INFO Executor: Finished task 96.0 in stage 1.0 (TID 97). 2527 bytes result sent to driver
21/01/18 18:21:57 INFO TaskSetManager: Starting task 97.0 in stage 1.0 (TID 98, test-runner-bnbrr, executor driver, partition 97, ANY, 7815 bytes)
21/01/18 18:21:57 INFO Executor: Running task 97.0 in stage 1.0 (TID 98)
21/01/18 18:21:57 INFO TaskSetManager: Finished task 96.0 in stage 1.0 (TID 97) in 1184 ms on test-runner-bnbrr (executor driver) (97/200)
21/01/18 18:21:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:57 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:57 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-10df400d-dff3-47fb-aab1-a7dc8fc5267d-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:21:58 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:58 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:58 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:58 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:58 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:58 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:58 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:58 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:58 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:58 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:58 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:21:58 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000097_98' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:21:58 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000097_98: Committed
21/01/18 18:21:58 INFO Executor: Finished task 97.0 in stage 1.0 (TID 98). 2527 bytes result sent to driver
21/01/18 18:21:58 INFO TaskSetManager: Starting task 98.0 in stage 1.0 (TID 99, test-runner-bnbrr, executor driver, partition 98, ANY, 7815 bytes)
21/01/18 18:21:58 INFO Executor: Running task 98.0 in stage 1.0 (TID 99)
21/01/18 18:21:58 INFO TaskSetManager: Finished task 97.0 in stage 1.0 (TID 98) in 1277 ms on test-runner-bnbrr (executor driver) (98/200)
21/01/18 18:21:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:58 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:21:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:21:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:21:58 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-1228b8ed-a0dc-4929-9cda-2d6d1ff0099c-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:21:59 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:59 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:21:59 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:21:59 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:21:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:21:59 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:21:59 INFO ParquetOutputFormat: Validation is off
21/01/18 18:21:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:21:59 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:21:59 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:21:59 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:21:59 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:21:59 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:21:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:00 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000098_99' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:00 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000098_99: Committed
21/01/18 18:22:00 INFO Executor: Finished task 98.0 in stage 1.0 (TID 99). 2527 bytes result sent to driver
21/01/18 18:22:00 INFO TaskSetManager: Starting task 99.0 in stage 1.0 (TID 100, test-runner-bnbrr, executor driver, partition 99, ANY, 7815 bytes)
21/01/18 18:22:00 INFO Executor: Running task 99.0 in stage 1.0 (TID 100)
21/01/18 18:22:00 INFO TaskSetManager: Finished task 98.0 in stage 1.0 (TID 99) in 1484 ms on test-runner-bnbrr (executor driver) (99/200)
21/01/18 18:22:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:00 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-1256eefe-e22b-4a70-9cd1-1ade0b5006e6-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:01 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:01 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:01 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:01 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:01 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:01 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:01 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:01 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:01 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:01 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:01 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:01 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000099_100' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:01 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000099_100: Committed
21/01/18 18:22:01 INFO Executor: Finished task 99.0 in stage 1.0 (TID 100). 2527 bytes result sent to driver
21/01/18 18:22:01 INFO TaskSetManager: Starting task 100.0 in stage 1.0 (TID 101, test-runner-bnbrr, executor driver, partition 100, ANY, 7815 bytes)
21/01/18 18:22:01 INFO Executor: Running task 100.0 in stage 1.0 (TID 101)
21/01/18 18:22:01 INFO TaskSetManager: Finished task 99.0 in stage 1.0 (TID 100) in 1363 ms on test-runner-bnbrr (executor driver) (100/200)
21/01/18 18:22:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:01 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:01 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-16a830b6-0737-4622-879d-573f00d936dc-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:02 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:02 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:02 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:02 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:02 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:02 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:02 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:02 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:02 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:02 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:02 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:02 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:02 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:03 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000100_101' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:03 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000100_101: Committed
21/01/18 18:22:03 INFO Executor: Finished task 100.0 in stage 1.0 (TID 101). 2527 bytes result sent to driver
21/01/18 18:22:03 INFO TaskSetManager: Starting task 101.0 in stage 1.0 (TID 102, test-runner-bnbrr, executor driver, partition 101, ANY, 7815 bytes)
21/01/18 18:22:03 INFO Executor: Running task 101.0 in stage 1.0 (TID 102)
21/01/18 18:22:03 INFO TaskSetManager: Finished task 100.0 in stage 1.0 (TID 101) in 1305 ms on test-runner-bnbrr (executor driver) (101/200)
21/01/18 18:22:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:03 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-19081d08-04d8-4295-9908-a2a24f4d2ed5-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:03 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:03 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:03 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:03 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:03 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:03 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:03 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:03 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:03 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:03 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:03 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:04 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000101_102' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:04 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000101_102: Committed
21/01/18 18:22:04 INFO Executor: Finished task 101.0 in stage 1.0 (TID 102). 2527 bytes result sent to driver
21/01/18 18:22:04 INFO TaskSetManager: Starting task 102.0 in stage 1.0 (TID 103, test-runner-bnbrr, executor driver, partition 102, ANY, 7815 bytes)
21/01/18 18:22:04 INFO Executor: Running task 102.0 in stage 1.0 (TID 103)
21/01/18 18:22:04 INFO TaskSetManager: Finished task 101.0 in stage 1.0 (TID 102) in 1195 ms on test-runner-bnbrr (executor driver) (102/200)
21/01/18 18:22:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:04 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-192806f0-3beb-48cb-b38e-741a6925f467-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:04 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:04 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:04 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:04 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:04 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:04 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:04 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:04 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:04 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:04 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:04 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000102_103' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:05 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000102_103: Committed
21/01/18 18:22:05 INFO Executor: Finished task 102.0 in stage 1.0 (TID 103). 2527 bytes result sent to driver
21/01/18 18:22:05 INFO TaskSetManager: Starting task 103.0 in stage 1.0 (TID 104, test-runner-bnbrr, executor driver, partition 103, ANY, 7815 bytes)
21/01/18 18:22:05 INFO Executor: Running task 103.0 in stage 1.0 (TID 104)
21/01/18 18:22:05 INFO TaskSetManager: Finished task 102.0 in stage 1.0 (TID 103) in 1278 ms on test-runner-bnbrr (executor driver) (103/200)
21/01/18 18:22:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:05 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:05 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-1dc03705-0639-4c24-a1a4-9ea5aca66bbf-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:06 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:06 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:06 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:06 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:06 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:06 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:06 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:06 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:06 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:06 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:06 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:06 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000103_104' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:06 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000103_104: Committed
21/01/18 18:22:06 INFO Executor: Finished task 103.0 in stage 1.0 (TID 104). 2527 bytes result sent to driver
21/01/18 18:22:06 INFO TaskSetManager: Starting task 104.0 in stage 1.0 (TID 105, test-runner-bnbrr, executor driver, partition 104, ANY, 7815 bytes)
21/01/18 18:22:06 INFO Executor: Running task 104.0 in stage 1.0 (TID 105)
21/01/18 18:22:06 INFO TaskSetManager: Finished task 103.0 in stage 1.0 (TID 104) in 1254 ms on test-runner-bnbrr (executor driver) (104/200)
21/01/18 18:22:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:06 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:06 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-1f9c452d-2e08-4033-8993-117952fd0a1c-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:07 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:07 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:07 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:07 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:07 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:07 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:07 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:07 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:07 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000104_105' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:08 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000104_105: Committed
21/01/18 18:22:08 INFO Executor: Finished task 104.0 in stage 1.0 (TID 105). 2527 bytes result sent to driver
21/01/18 18:22:08 INFO TaskSetManager: Starting task 105.0 in stage 1.0 (TID 106, test-runner-bnbrr, executor driver, partition 105, ANY, 7815 bytes)
21/01/18 18:22:08 INFO Executor: Running task 105.0 in stage 1.0 (TID 106)
21/01/18 18:22:08 INFO TaskSetManager: Finished task 104.0 in stage 1.0 (TID 105) in 1198 ms on test-runner-bnbrr (executor driver) (105/200)
21/01/18 18:22:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:08 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:08 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-20223fb3-fda2-4185-8a01-33c5feec792c-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:08 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:08 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:08 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:08 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:08 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:08 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:08 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:08 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:08 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:08 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:08 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:09 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000105_106' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:09 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000105_106: Committed
21/01/18 18:22:09 INFO Executor: Finished task 105.0 in stage 1.0 (TID 106). 2527 bytes result sent to driver
21/01/18 18:22:09 INFO TaskSetManager: Starting task 106.0 in stage 1.0 (TID 107, test-runner-bnbrr, executor driver, partition 106, ANY, 7815 bytes)
21/01/18 18:22:09 INFO Executor: Running task 106.0 in stage 1.0 (TID 107)
21/01/18 18:22:09 INFO TaskSetManager: Finished task 105.0 in stage 1.0 (TID 106) in 1257 ms on test-runner-bnbrr (executor driver) (106/200)
21/01/18 18:22:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:09 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:09 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-223110f9-469d-41ae-a998-f07b5f963482-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:09 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:09 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:09 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:09 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:09 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:09 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:09 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:09 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:09 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:09 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:09 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:10 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000106_107' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:10 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000106_107: Committed
21/01/18 18:22:10 INFO Executor: Finished task 106.0 in stage 1.0 (TID 107). 2527 bytes result sent to driver
21/01/18 18:22:10 INFO TaskSetManager: Starting task 107.0 in stage 1.0 (TID 108, test-runner-bnbrr, executor driver, partition 107, ANY, 7815 bytes)
21/01/18 18:22:10 INFO Executor: Running task 107.0 in stage 1.0 (TID 108)
21/01/18 18:22:10 INFO TaskSetManager: Finished task 106.0 in stage 1.0 (TID 107) in 1236 ms on test-runner-bnbrr (executor driver) (107/200)
21/01/18 18:22:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:10 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:10 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-258101ba-e64a-4c8a-b972-31523a2f4bc0-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:11 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:11 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:11 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:11 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:11 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:11 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:11 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:11 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:11 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:11 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:11 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:11 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000107_108' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:11 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000107_108: Committed
21/01/18 18:22:11 INFO Executor: Finished task 107.0 in stage 1.0 (TID 108). 2527 bytes result sent to driver
21/01/18 18:22:11 INFO TaskSetManager: Starting task 108.0 in stage 1.0 (TID 109, test-runner-bnbrr, executor driver, partition 108, ANY, 7815 bytes)
21/01/18 18:22:11 INFO Executor: Running task 108.0 in stage 1.0 (TID 109)
21/01/18 18:22:11 INFO TaskSetManager: Finished task 107.0 in stage 1.0 (TID 108) in 1409 ms on test-runner-bnbrr (executor driver) (108/200)
21/01/18 18:22:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:11 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-25b96886-3c62-41c7-b137-229c53d3a23d-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:12 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:12 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:12 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:12 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:12 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:12 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:12 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:12 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:12 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:12 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:12 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:13 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000108_109' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:13 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000108_109: Committed
21/01/18 18:22:13 INFO Executor: Finished task 108.0 in stage 1.0 (TID 109). 2527 bytes result sent to driver
21/01/18 18:22:13 INFO TaskSetManager: Starting task 109.0 in stage 1.0 (TID 110, test-runner-bnbrr, executor driver, partition 109, ANY, 7815 bytes)
21/01/18 18:22:13 INFO Executor: Running task 109.0 in stage 1.0 (TID 110)
21/01/18 18:22:13 INFO TaskSetManager: Finished task 108.0 in stage 1.0 (TID 109) in 1241 ms on test-runner-bnbrr (executor driver) (109/200)
21/01/18 18:22:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:13 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:13 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-27a464c5-2452-44f7-ae00-f5e9e072af4d-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:13 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:13 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:13 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:13 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:13 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:13 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:13 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:13 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:13 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:13 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:13 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:14 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000109_110' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:14 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000109_110: Committed
21/01/18 18:22:14 INFO Executor: Finished task 109.0 in stage 1.0 (TID 110). 2527 bytes result sent to driver
21/01/18 18:22:14 INFO TaskSetManager: Starting task 110.0 in stage 1.0 (TID 111, test-runner-bnbrr, executor driver, partition 110, ANY, 7815 bytes)
21/01/18 18:22:14 INFO Executor: Running task 110.0 in stage 1.0 (TID 111)
21/01/18 18:22:14 INFO TaskSetManager: Finished task 109.0 in stage 1.0 (TID 110) in 1269 ms on test-runner-bnbrr (executor driver) (110/200)
21/01/18 18:22:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:14 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:14 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-280ca397-a31b-4c42-a9ef-bf0d80aee11d-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:15 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:15 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:15 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:15 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:15 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:15 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:15 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:15 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:15 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:15 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:15 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:15 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000110_111' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:15 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000110_111: Committed
21/01/18 18:22:15 INFO Executor: Finished task 110.0 in stage 1.0 (TID 111). 2527 bytes result sent to driver
21/01/18 18:22:15 INFO TaskSetManager: Starting task 111.0 in stage 1.0 (TID 112, test-runner-bnbrr, executor driver, partition 111, ANY, 7815 bytes)
21/01/18 18:22:15 INFO Executor: Running task 111.0 in stage 1.0 (TID 112)
21/01/18 18:22:15 INFO TaskSetManager: Finished task 110.0 in stage 1.0 (TID 111) in 1249 ms on test-runner-bnbrr (executor driver) (111/200)
21/01/18 18:22:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:15 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:15 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-28dcc08c-c405-4c25-83d3-429b689144e0-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:16 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:16 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:16 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:16 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:16 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:16 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:16 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:16 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:16 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:16 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:16 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:17 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000111_112' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:17 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000111_112: Committed
21/01/18 18:22:17 INFO Executor: Finished task 111.0 in stage 1.0 (TID 112). 2527 bytes result sent to driver
21/01/18 18:22:17 INFO TaskSetManager: Starting task 112.0 in stage 1.0 (TID 113, test-runner-bnbrr, executor driver, partition 112, ANY, 7815 bytes)
21/01/18 18:22:17 INFO Executor: Running task 112.0 in stage 1.0 (TID 113)
21/01/18 18:22:17 INFO TaskSetManager: Finished task 111.0 in stage 1.0 (TID 112) in 1422 ms on test-runner-bnbrr (executor driver) (112/200)
21/01/18 18:22:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:17 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:17 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-297025cf-de94-46ce-84f6-098285d93297-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:17 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:17 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:17 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:17 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:17 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:17 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:17 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:17 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:17 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:17 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:17 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:18 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000112_113' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:18 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000112_113: Committed
21/01/18 18:22:18 INFO Executor: Finished task 112.0 in stage 1.0 (TID 113). 2527 bytes result sent to driver
21/01/18 18:22:18 INFO TaskSetManager: Starting task 113.0 in stage 1.0 (TID 114, test-runner-bnbrr, executor driver, partition 113, ANY, 7815 bytes)
21/01/18 18:22:18 INFO Executor: Running task 113.0 in stage 1.0 (TID 114)
21/01/18 18:22:18 INFO TaskSetManager: Finished task 112.0 in stage 1.0 (TID 113) in 1307 ms on test-runner-bnbrr (executor driver) (113/200)
21/01/18 18:22:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:18 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:18 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-2bc23bd3-8b32-4600-9204-df16c2d44d50-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:19 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:19 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:19 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:19 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:19 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:19 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:19 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:19 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:19 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:19 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:19 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:19 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000113_114' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:19 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000113_114: Committed
21/01/18 18:22:19 INFO Executor: Finished task 113.0 in stage 1.0 (TID 114). 2527 bytes result sent to driver
21/01/18 18:22:19 INFO TaskSetManager: Starting task 114.0 in stage 1.0 (TID 115, test-runner-bnbrr, executor driver, partition 114, ANY, 7815 bytes)
21/01/18 18:22:19 INFO Executor: Running task 114.0 in stage 1.0 (TID 115)
21/01/18 18:22:19 INFO TaskSetManager: Finished task 113.0 in stage 1.0 (TID 114) in 1323 ms on test-runner-bnbrr (executor driver) (114/200)
21/01/18 18:22:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:19 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:19 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-30f3b106-318c-4b3e-b41f-13aaeeb71a13-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:20 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:20 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:20 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:20 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:20 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:20 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:20 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:20 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:20 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:20 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:20 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:20 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000114_115' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:20 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000114_115: Committed
21/01/18 18:22:20 INFO Executor: Finished task 114.0 in stage 1.0 (TID 115). 2527 bytes result sent to driver
21/01/18 18:22:20 INFO TaskSetManager: Starting task 115.0 in stage 1.0 (TID 116, test-runner-bnbrr, executor driver, partition 115, ANY, 7815 bytes)
21/01/18 18:22:20 INFO Executor: Running task 115.0 in stage 1.0 (TID 116)
21/01/18 18:22:20 INFO TaskSetManager: Finished task 114.0 in stage 1.0 (TID 115) in 1174 ms on test-runner-bnbrr (executor driver) (115/200)
21/01/18 18:22:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:20 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:20 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-336bfdaf-38df-450f-abb1-c750110c3003-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:21 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:21 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:21 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:21 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:21 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:21 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:21 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:21 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:21 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:21 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:21 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:22 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000115_116' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:22 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000115_116: Committed
21/01/18 18:22:22 INFO Executor: Finished task 115.0 in stage 1.0 (TID 116). 2527 bytes result sent to driver
21/01/18 18:22:22 INFO TaskSetManager: Starting task 116.0 in stage 1.0 (TID 117, test-runner-bnbrr, executor driver, partition 116, ANY, 7815 bytes)
21/01/18 18:22:22 INFO Executor: Running task 116.0 in stage 1.0 (TID 117)
21/01/18 18:22:22 INFO TaskSetManager: Finished task 115.0 in stage 1.0 (TID 116) in 1282 ms on test-runner-bnbrr (executor driver) (116/200)
21/01/18 18:22:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:22 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:22 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-359e7f38-44ed-44dd-bbd8-f30c71a9c00d-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:22 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:22 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:22 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:22 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:22 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:22 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:22 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:22 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:22 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:22 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:22 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:23 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000116_117' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:23 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000116_117: Committed
21/01/18 18:22:23 INFO Executor: Finished task 116.0 in stage 1.0 (TID 117). 2527 bytes result sent to driver
21/01/18 18:22:23 INFO TaskSetManager: Starting task 117.0 in stage 1.0 (TID 118, test-runner-bnbrr, executor driver, partition 117, ANY, 7815 bytes)
21/01/18 18:22:23 INFO Executor: Running task 117.0 in stage 1.0 (TID 118)
21/01/18 18:22:23 INFO TaskSetManager: Finished task 116.0 in stage 1.0 (TID 117) in 1263 ms on test-runner-bnbrr (executor driver) (117/200)
21/01/18 18:22:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:23 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:23 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-377bead6-c429-47c9-9dd4-5561e9f5a529-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:24 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:24 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:24 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:24 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:24 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:24 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:24 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:24 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:24 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:24 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:24 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:24 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000117_118' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:24 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000117_118: Committed
21/01/18 18:22:24 INFO Executor: Finished task 117.0 in stage 1.0 (TID 118). 2527 bytes result sent to driver
21/01/18 18:22:24 INFO TaskSetManager: Starting task 118.0 in stage 1.0 (TID 119, test-runner-bnbrr, executor driver, partition 118, ANY, 7815 bytes)
21/01/18 18:22:24 INFO Executor: Running task 118.0 in stage 1.0 (TID 119)
21/01/18 18:22:24 INFO TaskSetManager: Finished task 117.0 in stage 1.0 (TID 118) in 1249 ms on test-runner-bnbrr (executor driver) (118/200)
21/01/18 18:22:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:24 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:24 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-37a83322-e170-4b22-a7c0-7fdec00e1b42-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:25 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:25 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:25 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:25 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:25 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:25 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:25 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:25 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:25 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:25 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:25 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:25 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000118_119' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:25 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000118_119: Committed
21/01/18 18:22:25 INFO Executor: Finished task 118.0 in stage 1.0 (TID 119). 2527 bytes result sent to driver
21/01/18 18:22:25 INFO TaskSetManager: Starting task 119.0 in stage 1.0 (TID 120, test-runner-bnbrr, executor driver, partition 119, ANY, 7815 bytes)
21/01/18 18:22:25 INFO Executor: Running task 119.0 in stage 1.0 (TID 120)
21/01/18 18:22:25 INFO TaskSetManager: Finished task 118.0 in stage 1.0 (TID 119) in 1198 ms on test-runner-bnbrr (executor driver) (119/200)
21/01/18 18:22:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:25 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:25 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-382fe44a-fa11-4aec-9724-d3dff64b5bb8-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:26 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:26 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:26 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:26 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:26 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:26 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:26 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:26 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:26 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:26 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:26 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:27 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000119_120' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:27 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000119_120: Committed
21/01/18 18:22:27 INFO Executor: Finished task 119.0 in stage 1.0 (TID 120). 2527 bytes result sent to driver
21/01/18 18:22:27 INFO TaskSetManager: Starting task 120.0 in stage 1.0 (TID 121, test-runner-bnbrr, executor driver, partition 120, ANY, 7815 bytes)
21/01/18 18:22:27 INFO Executor: Running task 120.0 in stage 1.0 (TID 121)
21/01/18 18:22:27 INFO TaskSetManager: Finished task 119.0 in stage 1.0 (TID 120) in 1240 ms on test-runner-bnbrr (executor driver) (120/200)
21/01/18 18:22:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:27 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:27 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-39980a12-a79e-4d01-a913-2d3033142908-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:27 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:27 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:27 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:27 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:27 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:27 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:27 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:27 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:27 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:27 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:27 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:28 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000120_121' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:28 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000120_121: Committed
21/01/18 18:22:28 INFO Executor: Finished task 120.0 in stage 1.0 (TID 121). 2527 bytes result sent to driver
21/01/18 18:22:28 INFO TaskSetManager: Starting task 121.0 in stage 1.0 (TID 122, test-runner-bnbrr, executor driver, partition 121, ANY, 7815 bytes)
21/01/18 18:22:28 INFO Executor: Running task 121.0 in stage 1.0 (TID 122)
21/01/18 18:22:28 INFO TaskSetManager: Finished task 120.0 in stage 1.0 (TID 121) in 1392 ms on test-runner-bnbrr (executor driver) (121/200)
21/01/18 18:22:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:28 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:28 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-3c0fe828-ce26-4a3c-b7be-36477c4409a4-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:29 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:29 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:29 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:29 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:29 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:29 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:29 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:29 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:29 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:29 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:29 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:29 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000121_122' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:29 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000121_122: Committed
21/01/18 18:22:29 INFO Executor: Finished task 121.0 in stage 1.0 (TID 122). 2527 bytes result sent to driver
21/01/18 18:22:29 INFO TaskSetManager: Starting task 122.0 in stage 1.0 (TID 123, test-runner-bnbrr, executor driver, partition 122, ANY, 7815 bytes)
21/01/18 18:22:29 INFO Executor: Running task 122.0 in stage 1.0 (TID 123)
21/01/18 18:22:29 INFO TaskSetManager: Finished task 121.0 in stage 1.0 (TID 122) in 1355 ms on test-runner-bnbrr (executor driver) (122/200)
21/01/18 18:22:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:29 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-402be5de-ecad-4624-9cc2-544acc320ea5-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:30 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:30 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:30 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:30 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:30 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:30 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:30 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:30 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:30 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:30 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:30 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:31 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000122_123' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:31 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000122_123: Committed
21/01/18 18:22:31 INFO Executor: Finished task 122.0 in stage 1.0 (TID 123). 2527 bytes result sent to driver
21/01/18 18:22:31 INFO TaskSetManager: Starting task 123.0 in stage 1.0 (TID 124, test-runner-bnbrr, executor driver, partition 123, ANY, 7815 bytes)
21/01/18 18:22:31 INFO Executor: Running task 123.0 in stage 1.0 (TID 124)
21/01/18 18:22:31 INFO TaskSetManager: Finished task 122.0 in stage 1.0 (TID 123) in 1252 ms on test-runner-bnbrr (executor driver) (123/200)
21/01/18 18:22:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:31 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-4549c406-7304-4e87-8ed8-175acf74175f-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:31 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:31 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:31 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:31 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:31 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:31 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:31 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:31 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:31 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:31 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:31 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:32 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000123_124' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:32 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000123_124: Committed
21/01/18 18:22:32 INFO Executor: Finished task 123.0 in stage 1.0 (TID 124). 2527 bytes result sent to driver
21/01/18 18:22:32 INFO TaskSetManager: Starting task 124.0 in stage 1.0 (TID 125, test-runner-bnbrr, executor driver, partition 124, ANY, 7815 bytes)
21/01/18 18:22:32 INFO Executor: Running task 124.0 in stage 1.0 (TID 125)
21/01/18 18:22:32 INFO TaskSetManager: Finished task 123.0 in stage 1.0 (TID 124) in 1235 ms on test-runner-bnbrr (executor driver) (124/200)
21/01/18 18:22:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:32 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:32 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-493c722e-e9f2-4d60-9c99-280dbc4d03f2-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:32 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:32 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:32 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:32 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:32 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:32 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:32 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:32 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:32 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:32 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:32 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:33 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000124_125' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:33 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000124_125: Committed
21/01/18 18:22:33 INFO Executor: Finished task 124.0 in stage 1.0 (TID 125). 2527 bytes result sent to driver
21/01/18 18:22:33 INFO TaskSetManager: Starting task 125.0 in stage 1.0 (TID 126, test-runner-bnbrr, executor driver, partition 125, ANY, 7815 bytes)
21/01/18 18:22:33 INFO Executor: Running task 125.0 in stage 1.0 (TID 126)
21/01/18 18:22:33 INFO TaskSetManager: Finished task 124.0 in stage 1.0 (TID 125) in 1262 ms on test-runner-bnbrr (executor driver) (125/200)
21/01/18 18:22:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:33 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:33 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-4f66c5fe-b995-49c3-8c94-d55a9ed1d60d-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:34 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:34 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:34 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:34 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:34 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:34 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:34 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:34 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:34 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:34 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:34 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:34 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:34 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:34 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000125_126' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:34 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000125_126: Committed
21/01/18 18:22:34 INFO Executor: Finished task 125.0 in stage 1.0 (TID 126). 2527 bytes result sent to driver
21/01/18 18:22:34 INFO TaskSetManager: Starting task 126.0 in stage 1.0 (TID 127, test-runner-bnbrr, executor driver, partition 126, ANY, 7815 bytes)
21/01/18 18:22:34 INFO Executor: Running task 126.0 in stage 1.0 (TID 127)
21/01/18 18:22:34 INFO TaskSetManager: Finished task 125.0 in stage 1.0 (TID 126) in 1243 ms on test-runner-bnbrr (executor driver) (126/200)
21/01/18 18:22:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:34 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:34 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-56037bac-fbb8-45a8-9841-2282be707128-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:35 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:35 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:35 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:35 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:35 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:35 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:35 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:35 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:35 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:35 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:35 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:36 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000126_127' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:36 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000126_127: Committed
21/01/18 18:22:36 INFO Executor: Finished task 126.0 in stage 1.0 (TID 127). 2527 bytes result sent to driver
21/01/18 18:22:36 INFO TaskSetManager: Starting task 127.0 in stage 1.0 (TID 128, test-runner-bnbrr, executor driver, partition 127, ANY, 7815 bytes)
21/01/18 18:22:36 INFO Executor: Running task 127.0 in stage 1.0 (TID 128)
21/01/18 18:22:36 INFO TaskSetManager: Finished task 126.0 in stage 1.0 (TID 127) in 1265 ms on test-runner-bnbrr (executor driver) (127/200)
21/01/18 18:22:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:36 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:36 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-5be6a0d1-27bd-46e1-9e97-3cc6e1f7d38d-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:36 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:36 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:36 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:36 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:36 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:36 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:36 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:36 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:36 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:36 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:36 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:36 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:36 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:36 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:37 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000127_128' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:37 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000127_128: Committed
21/01/18 18:22:37 INFO Executor: Finished task 127.0 in stage 1.0 (TID 128). 2527 bytes result sent to driver
21/01/18 18:22:37 INFO TaskSetManager: Starting task 128.0 in stage 1.0 (TID 129, test-runner-bnbrr, executor driver, partition 128, ANY, 7815 bytes)
21/01/18 18:22:37 INFO Executor: Running task 128.0 in stage 1.0 (TID 129)
21/01/18 18:22:37 INFO TaskSetManager: Finished task 127.0 in stage 1.0 (TID 128) in 1292 ms on test-runner-bnbrr (executor driver) (128/200)
21/01/18 18:22:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:37 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:37 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-5d132288-f449-4928-9e9c-8601d3eda702-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:37 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:37 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:37 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:37 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:37 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:37 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:37 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:37 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:37 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:37 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:37 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:38 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000128_129' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:38 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000128_129: Committed
21/01/18 18:22:38 INFO Executor: Finished task 128.0 in stage 1.0 (TID 129). 2527 bytes result sent to driver
21/01/18 18:22:38 INFO TaskSetManager: Starting task 129.0 in stage 1.0 (TID 130, test-runner-bnbrr, executor driver, partition 129, ANY, 7815 bytes)
21/01/18 18:22:38 INFO Executor: Running task 129.0 in stage 1.0 (TID 130)
21/01/18 18:22:38 INFO TaskSetManager: Finished task 128.0 in stage 1.0 (TID 129) in 1223 ms on test-runner-bnbrr (executor driver) (129/200)
21/01/18 18:22:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:38 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:38 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-637d4b65-972d-4be6-868f-765c97db819f-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:39 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:39 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:39 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:39 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:39 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:39 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:39 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:39 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:39 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:39 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:39 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:39 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000129_130' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:39 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000129_130: Committed
21/01/18 18:22:39 INFO Executor: Finished task 129.0 in stage 1.0 (TID 130). 2527 bytes result sent to driver
21/01/18 18:22:39 INFO TaskSetManager: Starting task 130.0 in stage 1.0 (TID 131, test-runner-bnbrr, executor driver, partition 130, ANY, 7815 bytes)
21/01/18 18:22:39 INFO Executor: Running task 130.0 in stage 1.0 (TID 131)
21/01/18 18:22:39 INFO TaskSetManager: Finished task 129.0 in stage 1.0 (TID 130) in 1274 ms on test-runner-bnbrr (executor driver) (130/200)
21/01/18 18:22:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:39 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:39 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-646e6385-9ed1-42e9-8bb0-fe8f1eeac8dc-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:40 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:40 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:40 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:40 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:40 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:40 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:40 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:40 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:40 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:40 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:40 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:41 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000130_131' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:41 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000130_131: Committed
21/01/18 18:22:41 INFO Executor: Finished task 130.0 in stage 1.0 (TID 131). 2527 bytes result sent to driver
21/01/18 18:22:41 INFO TaskSetManager: Starting task 131.0 in stage 1.0 (TID 132, test-runner-bnbrr, executor driver, partition 131, ANY, 7815 bytes)
21/01/18 18:22:41 INFO Executor: Running task 131.0 in stage 1.0 (TID 132)
21/01/18 18:22:41 INFO TaskSetManager: Finished task 130.0 in stage 1.0 (TID 131) in 1281 ms on test-runner-bnbrr (executor driver) (131/200)
21/01/18 18:22:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:41 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:41 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-65e43861-f43b-4aa7-97c9-6d2cea80a1fc-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:41 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:41 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:41 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:41 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:41 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:41 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:41 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:41 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:41 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:41 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:41 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:42 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000131_132' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:42 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000131_132: Committed
21/01/18 18:22:42 INFO Executor: Finished task 131.0 in stage 1.0 (TID 132). 2527 bytes result sent to driver
21/01/18 18:22:42 INFO TaskSetManager: Starting task 132.0 in stage 1.0 (TID 133, test-runner-bnbrr, executor driver, partition 132, ANY, 7815 bytes)
21/01/18 18:22:42 INFO Executor: Running task 132.0 in stage 1.0 (TID 133)
21/01/18 18:22:42 INFO TaskSetManager: Finished task 131.0 in stage 1.0 (TID 132) in 1231 ms on test-runner-bnbrr (executor driver) (132/200)
21/01/18 18:22:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:42 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:42 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-671fa286-53a9-4a6d-8dce-0639fd0c70b0-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:43 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:43 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:43 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:43 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:43 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:43 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:43 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:43 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:43 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:43 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:43 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:43 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000132_133' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:43 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000132_133: Committed
21/01/18 18:22:43 INFO Executor: Finished task 132.0 in stage 1.0 (TID 133). 2527 bytes result sent to driver
21/01/18 18:22:43 INFO TaskSetManager: Starting task 133.0 in stage 1.0 (TID 134, test-runner-bnbrr, executor driver, partition 133, ANY, 7815 bytes)
21/01/18 18:22:43 INFO Executor: Running task 133.0 in stage 1.0 (TID 134)
21/01/18 18:22:43 INFO TaskSetManager: Finished task 132.0 in stage 1.0 (TID 133) in 1268 ms on test-runner-bnbrr (executor driver) (133/200)
21/01/18 18:22:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:43 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:43 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-6e765797-feaf-4f7a-9b43-e6af151110dd-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:44 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:44 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:44 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:44 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:44 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:44 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:44 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:44 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:44 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:44 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:44 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:45 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000133_134' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:45 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000133_134: Committed
21/01/18 18:22:45 INFO Executor: Finished task 133.0 in stage 1.0 (TID 134). 2527 bytes result sent to driver
21/01/18 18:22:45 INFO TaskSetManager: Starting task 134.0 in stage 1.0 (TID 135, test-runner-bnbrr, executor driver, partition 134, ANY, 7815 bytes)
21/01/18 18:22:45 INFO Executor: Running task 134.0 in stage 1.0 (TID 135)
21/01/18 18:22:45 INFO TaskSetManager: Finished task 133.0 in stage 1.0 (TID 134) in 1335 ms on test-runner-bnbrr (executor driver) (134/200)
21/01/18 18:22:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:45 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:45 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-6f2ce0d5-6eb2-4078-8d4e-fdcc640fb5bf-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:45 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:45 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:45 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:45 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:45 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:45 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:45 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:45 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:45 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:45 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:45 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:46 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000134_135' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:46 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000134_135: Committed
21/01/18 18:22:46 INFO Executor: Finished task 134.0 in stage 1.0 (TID 135). 2527 bytes result sent to driver
21/01/18 18:22:46 INFO TaskSetManager: Starting task 135.0 in stage 1.0 (TID 136, test-runner-bnbrr, executor driver, partition 135, ANY, 7815 bytes)
21/01/18 18:22:46 INFO Executor: Running task 135.0 in stage 1.0 (TID 136)
21/01/18 18:22:46 INFO TaskSetManager: Finished task 134.0 in stage 1.0 (TID 135) in 1236 ms on test-runner-bnbrr (executor driver) (135/200)
21/01/18 18:22:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:46 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:46 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-7321ea9b-8d62-40ce-aebf-c67e56d08bb4-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:46 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:46 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:46 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:46 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:46 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:46 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:46 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:46 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:46 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:46 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:46 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:47 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000135_136' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:47 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000135_136: Committed
21/01/18 18:22:47 INFO Executor: Finished task 135.0 in stage 1.0 (TID 136). 2527 bytes result sent to driver
21/01/18 18:22:47 INFO TaskSetManager: Starting task 136.0 in stage 1.0 (TID 137, test-runner-bnbrr, executor driver, partition 136, ANY, 7815 bytes)
21/01/18 18:22:47 INFO Executor: Running task 136.0 in stage 1.0 (TID 137)
21/01/18 18:22:47 INFO TaskSetManager: Finished task 135.0 in stage 1.0 (TID 136) in 1233 ms on test-runner-bnbrr (executor driver) (136/200)
21/01/18 18:22:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:47 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-7339b8e6-081b-46b8-bb54-31dc6468162b-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:48 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:48 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:48 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:48 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:48 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:48 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:48 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:48 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:48 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:48 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:48 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:48 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000136_137' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:48 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000136_137: Committed
21/01/18 18:22:48 INFO Executor: Finished task 136.0 in stage 1.0 (TID 137). 2527 bytes result sent to driver
21/01/18 18:22:48 INFO TaskSetManager: Starting task 137.0 in stage 1.0 (TID 138, test-runner-bnbrr, executor driver, partition 137, ANY, 7815 bytes)
21/01/18 18:22:48 INFO Executor: Running task 137.0 in stage 1.0 (TID 138)
21/01/18 18:22:48 INFO TaskSetManager: Finished task 136.0 in stage 1.0 (TID 137) in 1262 ms on test-runner-bnbrr (executor driver) (137/200)
21/01/18 18:22:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:48 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:48 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-7666f9bb-5db9-4a23-91c4-b90eeeec3cdf-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:49 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:49 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:49 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:49 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:49 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:49 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:49 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:49 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:49 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:49 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:49 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:50 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000137_138' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:50 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000137_138: Committed
21/01/18 18:22:50 INFO Executor: Finished task 137.0 in stage 1.0 (TID 138). 2527 bytes result sent to driver
21/01/18 18:22:50 INFO TaskSetManager: Starting task 138.0 in stage 1.0 (TID 139, test-runner-bnbrr, executor driver, partition 138, ANY, 7815 bytes)
21/01/18 18:22:50 INFO Executor: Running task 138.0 in stage 1.0 (TID 139)
21/01/18 18:22:50 INFO TaskSetManager: Finished task 137.0 in stage 1.0 (TID 138) in 1256 ms on test-runner-bnbrr (executor driver) (138/200)
21/01/18 18:22:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:50 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:50 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-78932b13-bf55-4666-b512-18be2e3dad9b-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:50 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:50 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:50 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:50 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:50 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:50 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:50 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:50 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:50 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:50 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:50 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:50 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:50 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:50 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:51 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000138_139' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:51 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000138_139: Committed
21/01/18 18:22:51 INFO Executor: Finished task 138.0 in stage 1.0 (TID 139). 2527 bytes result sent to driver
21/01/18 18:22:51 INFO TaskSetManager: Starting task 139.0 in stage 1.0 (TID 140, test-runner-bnbrr, executor driver, partition 139, ANY, 7815 bytes)
21/01/18 18:22:51 INFO Executor: Running task 139.0 in stage 1.0 (TID 140)
21/01/18 18:22:51 INFO TaskSetManager: Finished task 138.0 in stage 1.0 (TID 139) in 1226 ms on test-runner-bnbrr (executor driver) (139/200)
21/01/18 18:22:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:51 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-79d76633-df71-4d88-822e-77ca037f808f-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:51 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:51 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:51 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:51 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:51 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:51 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:51 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:51 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:51 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:51 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:51 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:51 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:51 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:51 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:52 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000139_140' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:52 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000139_140: Committed
21/01/18 18:22:52 INFO Executor: Finished task 139.0 in stage 1.0 (TID 140). 2527 bytes result sent to driver
21/01/18 18:22:52 INFO TaskSetManager: Starting task 140.0 in stage 1.0 (TID 141, test-runner-bnbrr, executor driver, partition 140, ANY, 7815 bytes)
21/01/18 18:22:52 INFO Executor: Running task 140.0 in stage 1.0 (TID 141)
21/01/18 18:22:52 INFO TaskSetManager: Finished task 139.0 in stage 1.0 (TID 140) in 1234 ms on test-runner-bnbrr (executor driver) (140/200)
21/01/18 18:22:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:52 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:52 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-7ed0a1fd-0932-48f3-ae89-10e91fafef26-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:53 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:53 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:53 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:53 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:53 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:53 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:53 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:53 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:53 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:53 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:53 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:53 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000140_141' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:53 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000140_141: Committed
21/01/18 18:22:53 INFO Executor: Finished task 140.0 in stage 1.0 (TID 141). 2527 bytes result sent to driver
21/01/18 18:22:53 INFO TaskSetManager: Starting task 141.0 in stage 1.0 (TID 142, test-runner-bnbrr, executor driver, partition 141, ANY, 7815 bytes)
21/01/18 18:22:53 INFO Executor: Running task 141.0 in stage 1.0 (TID 142)
21/01/18 18:22:53 INFO TaskSetManager: Finished task 140.0 in stage 1.0 (TID 141) in 1306 ms on test-runner-bnbrr (executor driver) (141/200)
21/01/18 18:22:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:53 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-83e4f18f-9961-4721-bf0b-3257b2053dc0-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:54 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:54 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:54 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:54 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:54 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:54 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:54 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:54 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:54 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:54 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:54 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:54 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000141_142' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:54 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000141_142: Committed
21/01/18 18:22:54 INFO Executor: Finished task 141.0 in stage 1.0 (TID 142). 2527 bytes result sent to driver
21/01/18 18:22:54 INFO TaskSetManager: Starting task 142.0 in stage 1.0 (TID 143, test-runner-bnbrr, executor driver, partition 142, ANY, 7815 bytes)
21/01/18 18:22:54 INFO Executor: Running task 142.0 in stage 1.0 (TID 143)
21/01/18 18:22:54 INFO TaskSetManager: Finished task 141.0 in stage 1.0 (TID 142) in 1157 ms on test-runner-bnbrr (executor driver) (142/200)
21/01/18 18:22:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:54 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:54 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-8b4f586e-3924-409e-8165-48c0c00bc4f3-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:55 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:55 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:55 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:55 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:55 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:55 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:55 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:55 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:55 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:55 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:55 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:56 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000142_143' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:56 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000142_143: Committed
21/01/18 18:22:56 INFO Executor: Finished task 142.0 in stage 1.0 (TID 143). 2527 bytes result sent to driver
21/01/18 18:22:56 INFO TaskSetManager: Starting task 143.0 in stage 1.0 (TID 144, test-runner-bnbrr, executor driver, partition 143, ANY, 7815 bytes)
21/01/18 18:22:56 INFO Executor: Running task 143.0 in stage 1.0 (TID 144)
21/01/18 18:22:56 INFO TaskSetManager: Finished task 142.0 in stage 1.0 (TID 143) in 1198 ms on test-runner-bnbrr (executor driver) (143/200)
21/01/18 18:22:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:56 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:56 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-8c148bc1-5755-41c5-8ecf-dbdce168eb66-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:56 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:56 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:56 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:56 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:56 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:56 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:56 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:56 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:56 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:56 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:56 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:57 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000143_144' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:57 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000143_144: Committed
21/01/18 18:22:57 INFO Executor: Finished task 143.0 in stage 1.0 (TID 144). 2527 bytes result sent to driver
21/01/18 18:22:57 INFO TaskSetManager: Starting task 144.0 in stage 1.0 (TID 145, test-runner-bnbrr, executor driver, partition 144, ANY, 7815 bytes)
21/01/18 18:22:57 INFO Executor: Running task 144.0 in stage 1.0 (TID 145)
21/01/18 18:22:57 INFO TaskSetManager: Finished task 143.0 in stage 1.0 (TID 144) in 1315 ms on test-runner-bnbrr (executor driver) (144/200)
21/01/18 18:22:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:57 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:57 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-8f170627-713d-4581-9d1f-46ae8c87e0ba-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:58 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:58 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:58 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:58 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:58 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:58 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:58 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:58 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:58 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:58 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:58 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:22:58 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000144_145' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:22:58 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000144_145: Committed
21/01/18 18:22:58 INFO Executor: Finished task 144.0 in stage 1.0 (TID 145). 2527 bytes result sent to driver
21/01/18 18:22:58 INFO TaskSetManager: Starting task 145.0 in stage 1.0 (TID 146, test-runner-bnbrr, executor driver, partition 145, ANY, 7815 bytes)
21/01/18 18:22:58 INFO Executor: Running task 145.0 in stage 1.0 (TID 146)
21/01/18 18:22:58 INFO TaskSetManager: Finished task 144.0 in stage 1.0 (TID 145) in 1307 ms on test-runner-bnbrr (executor driver) (145/200)
21/01/18 18:22:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:58 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:22:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:22:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:22:58 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-9018b1a3-5fbd-4417-8a4e-7f6bf4df020c-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:22:59 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:59 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:22:59 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:22:59 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:22:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:22:59 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:22:59 INFO ParquetOutputFormat: Validation is off
21/01/18 18:22:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:22:59 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:22:59 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:22:59 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:22:59 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:22:59 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:22:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:00 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000145_146' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:00 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000145_146: Committed
21/01/18 18:23:00 INFO Executor: Finished task 145.0 in stage 1.0 (TID 146). 2527 bytes result sent to driver
21/01/18 18:23:00 INFO TaskSetManager: Starting task 146.0 in stage 1.0 (TID 147, test-runner-bnbrr, executor driver, partition 146, ANY, 7815 bytes)
21/01/18 18:23:00 INFO Executor: Running task 146.0 in stage 1.0 (TID 147)
21/01/18 18:23:00 INFO TaskSetManager: Finished task 145.0 in stage 1.0 (TID 146) in 1263 ms on test-runner-bnbrr (executor driver) (146/200)
21/01/18 18:23:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:00 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-9659e280-8e82-4fe0-ada6-f9cd583b7415-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:00 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:00 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:00 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:00 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:00 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:00 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:00 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:00 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:00 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:00 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:00 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:01 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000146_147' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:01 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000146_147: Committed
21/01/18 18:23:01 INFO Executor: Finished task 146.0 in stage 1.0 (TID 147). 2527 bytes result sent to driver
21/01/18 18:23:01 INFO TaskSetManager: Starting task 147.0 in stage 1.0 (TID 148, test-runner-bnbrr, executor driver, partition 147, ANY, 7815 bytes)
21/01/18 18:23:01 INFO Executor: Running task 147.0 in stage 1.0 (TID 148)
21/01/18 18:23:01 INFO TaskSetManager: Finished task 146.0 in stage 1.0 (TID 147) in 1564 ms on test-runner-bnbrr (executor driver) (147/200)
21/01/18 18:23:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:01 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:01 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-974850b8-b60c-46a6-8c46-58a3108ef731-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:02 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:02 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:02 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:02 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:02 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:02 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:02 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:02 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:02 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:02 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:02 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:02 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:02 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:02 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000147_148' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:02 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000147_148: Committed
21/01/18 18:23:02 INFO Executor: Finished task 147.0 in stage 1.0 (TID 148). 2527 bytes result sent to driver
21/01/18 18:23:02 INFO TaskSetManager: Starting task 148.0 in stage 1.0 (TID 149, test-runner-bnbrr, executor driver, partition 148, ANY, 7815 bytes)
21/01/18 18:23:02 INFO Executor: Running task 148.0 in stage 1.0 (TID 149)
21/01/18 18:23:02 INFO TaskSetManager: Finished task 147.0 in stage 1.0 (TID 148) in 1279 ms on test-runner-bnbrr (executor driver) (148/200)
21/01/18 18:23:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:02 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-97c8ec63-93e1-455a-b72e-dacc790074c1-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:03 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:03 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:03 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:03 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:03 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:03 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:03 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:03 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:03 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:03 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:03 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:04 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000148_149' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:04 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000148_149: Committed
21/01/18 18:23:04 INFO Executor: Finished task 148.0 in stage 1.0 (TID 149). 2527 bytes result sent to driver
21/01/18 18:23:04 INFO TaskSetManager: Starting task 149.0 in stage 1.0 (TID 150, test-runner-bnbrr, executor driver, partition 149, ANY, 7815 bytes)
21/01/18 18:23:04 INFO Executor: Running task 149.0 in stage 1.0 (TID 150)
21/01/18 18:23:04 INFO TaskSetManager: Finished task 148.0 in stage 1.0 (TID 149) in 1265 ms on test-runner-bnbrr (executor driver) (149/200)
21/01/18 18:23:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:04 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-9847f8de-258c-4a3c-816e-01b3ce1f38b9-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:04 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:04 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:04 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:04 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:04 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:04 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:04 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:04 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:04 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:04 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:04 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:05 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000149_150' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:05 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000149_150: Committed
21/01/18 18:23:05 INFO Executor: Finished task 149.0 in stage 1.0 (TID 150). 2527 bytes result sent to driver
21/01/18 18:23:05 INFO TaskSetManager: Starting task 150.0 in stage 1.0 (TID 151, test-runner-bnbrr, executor driver, partition 150, ANY, 7815 bytes)
21/01/18 18:23:05 INFO Executor: Running task 150.0 in stage 1.0 (TID 151)
21/01/18 18:23:05 INFO TaskSetManager: Finished task 149.0 in stage 1.0 (TID 150) in 1234 ms on test-runner-bnbrr (executor driver) (150/200)
21/01/18 18:23:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:05 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:05 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-984ee6a7-3ffc-4ec2-bdd9-6c88b116c2c1-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:05 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:05 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:05 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:05 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:05 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:05 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:05 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:05 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:05 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:05 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:05 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:06 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000150_151' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:06 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000150_151: Committed
21/01/18 18:23:06 INFO Executor: Finished task 150.0 in stage 1.0 (TID 151). 2527 bytes result sent to driver
21/01/18 18:23:06 INFO TaskSetManager: Starting task 151.0 in stage 1.0 (TID 152, test-runner-bnbrr, executor driver, partition 151, ANY, 7815 bytes)
21/01/18 18:23:06 INFO Executor: Running task 151.0 in stage 1.0 (TID 152)
21/01/18 18:23:06 INFO TaskSetManager: Finished task 150.0 in stage 1.0 (TID 151) in 1222 ms on test-runner-bnbrr (executor driver) (151/200)
21/01/18 18:23:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:06 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:06 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-99fc47a2-8b5a-40ed-9d6f-d46a9aed4b4b-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:07 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:07 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:07 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:07 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:07 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:07 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:07 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:07 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:07 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:07 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:07 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:07 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000151_152' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:07 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000151_152: Committed
21/01/18 18:23:07 INFO Executor: Finished task 151.0 in stage 1.0 (TID 152). 2527 bytes result sent to driver
21/01/18 18:23:07 INFO TaskSetManager: Starting task 152.0 in stage 1.0 (TID 153, test-runner-bnbrr, executor driver, partition 152, ANY, 7815 bytes)
21/01/18 18:23:07 INFO Executor: Running task 152.0 in stage 1.0 (TID 153)
21/01/18 18:23:07 INFO TaskSetManager: Finished task 151.0 in stage 1.0 (TID 152) in 1256 ms on test-runner-bnbrr (executor driver) (152/200)
21/01/18 18:23:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:07 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:07 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-9acc2ed8-6c7b-4940-9077-e109a0dc963c-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:08 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:08 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:08 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:08 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:08 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:08 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:08 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:08 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:08 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:08 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:08 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:09 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000152_153' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:09 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000152_153: Committed
21/01/18 18:23:09 INFO Executor: Finished task 152.0 in stage 1.0 (TID 153). 2527 bytes result sent to driver
21/01/18 18:23:09 INFO TaskSetManager: Starting task 153.0 in stage 1.0 (TID 154, test-runner-bnbrr, executor driver, partition 153, ANY, 7815 bytes)
21/01/18 18:23:09 INFO Executor: Running task 153.0 in stage 1.0 (TID 154)
21/01/18 18:23:09 INFO TaskSetManager: Finished task 152.0 in stage 1.0 (TID 153) in 1296 ms on test-runner-bnbrr (executor driver) (153/200)
21/01/18 18:23:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:09 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:09 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-9faf117f-f23d-4f1b-95e5-fe8905c3dd4b-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:09 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:09 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:09 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:09 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:09 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:09 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:09 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:09 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:09 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:09 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:09 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:10 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000153_154' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:10 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000153_154: Committed
21/01/18 18:23:10 INFO Executor: Finished task 153.0 in stage 1.0 (TID 154). 2527 bytes result sent to driver
21/01/18 18:23:10 INFO TaskSetManager: Starting task 154.0 in stage 1.0 (TID 155, test-runner-bnbrr, executor driver, partition 154, ANY, 7815 bytes)
21/01/18 18:23:10 INFO Executor: Running task 154.0 in stage 1.0 (TID 155)
21/01/18 18:23:10 INFO TaskSetManager: Finished task 153.0 in stage 1.0 (TID 154) in 1223 ms on test-runner-bnbrr (executor driver) (154/200)
21/01/18 18:23:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:10 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:10 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-a13764c2-4d8d-42f4-8809-717b0b02cd80-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:10 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:10 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:10 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:10 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:10 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:10 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:10 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:10 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:10 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:10 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:10 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:11 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000154_155' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:11 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000154_155: Committed
21/01/18 18:23:11 INFO Executor: Finished task 154.0 in stage 1.0 (TID 155). 2527 bytes result sent to driver
21/01/18 18:23:11 INFO TaskSetManager: Starting task 155.0 in stage 1.0 (TID 156, test-runner-bnbrr, executor driver, partition 155, ANY, 7815 bytes)
21/01/18 18:23:11 INFO Executor: Running task 155.0 in stage 1.0 (TID 156)
21/01/18 18:23:11 INFO TaskSetManager: Finished task 154.0 in stage 1.0 (TID 155) in 1286 ms on test-runner-bnbrr (executor driver) (155/200)
21/01/18 18:23:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:11 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-a1fee28b-0eb5-4f2e-a746-5fcafd5f6c0e-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:12 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:12 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:12 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:12 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:12 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:12 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:12 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:12 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:12 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:12 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:12 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:12 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000155_156' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:12 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000155_156: Committed
21/01/18 18:23:12 INFO Executor: Finished task 155.0 in stage 1.0 (TID 156). 2527 bytes result sent to driver
21/01/18 18:23:12 INFO TaskSetManager: Starting task 156.0 in stage 1.0 (TID 157, test-runner-bnbrr, executor driver, partition 156, ANY, 7815 bytes)
21/01/18 18:23:12 INFO Executor: Running task 156.0 in stage 1.0 (TID 157)
21/01/18 18:23:12 INFO TaskSetManager: Finished task 155.0 in stage 1.0 (TID 156) in 1284 ms on test-runner-bnbrr (executor driver) (156/200)
21/01/18 18:23:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:12 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:12 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-a4f7c748-6a34-44bb-a295-f3a76009e53a-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:13 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:13 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:13 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:13 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:13 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:13 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:13 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:13 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:13 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:13 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:13 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:14 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000156_157' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:14 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000156_157: Committed
21/01/18 18:23:14 INFO Executor: Finished task 156.0 in stage 1.0 (TID 157). 2527 bytes result sent to driver
21/01/18 18:23:14 INFO TaskSetManager: Starting task 157.0 in stage 1.0 (TID 158, test-runner-bnbrr, executor driver, partition 157, ANY, 7815 bytes)
21/01/18 18:23:14 INFO Executor: Running task 157.0 in stage 1.0 (TID 158)
21/01/18 18:23:14 INFO TaskSetManager: Finished task 156.0 in stage 1.0 (TID 157) in 1195 ms on test-runner-bnbrr (executor driver) (157/200)
21/01/18 18:23:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:14 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:14 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-a6802d43-3cef-4ba5-af79-e002cdf33a61-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:14 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:14 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:14 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:14 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:14 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:14 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:14 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:14 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:14 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:14 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:14 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:15 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000157_158' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:15 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000157_158: Committed
21/01/18 18:23:15 INFO Executor: Finished task 157.0 in stage 1.0 (TID 158). 2527 bytes result sent to driver
21/01/18 18:23:15 INFO TaskSetManager: Starting task 158.0 in stage 1.0 (TID 159, test-runner-bnbrr, executor driver, partition 158, ANY, 7815 bytes)
21/01/18 18:23:15 INFO Executor: Running task 158.0 in stage 1.0 (TID 159)
21/01/18 18:23:15 INFO TaskSetManager: Finished task 157.0 in stage 1.0 (TID 158) in 1256 ms on test-runner-bnbrr (executor driver) (158/200)
21/01/18 18:23:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:15 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:15 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-a80feff1-2e92-4721-8a2d-8c3167e55675-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:16 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:16 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:16 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:16 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:16 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:16 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:16 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:16 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:16 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:16 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:16 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:16 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000158_159' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:16 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000158_159: Committed
21/01/18 18:23:16 INFO Executor: Finished task 158.0 in stage 1.0 (TID 159). 2527 bytes result sent to driver
21/01/18 18:23:16 INFO TaskSetManager: Starting task 159.0 in stage 1.0 (TID 160, test-runner-bnbrr, executor driver, partition 159, ANY, 7815 bytes)
21/01/18 18:23:16 INFO Executor: Running task 159.0 in stage 1.0 (TID 160)
21/01/18 18:23:16 INFO TaskSetManager: Finished task 158.0 in stage 1.0 (TID 159) in 1356 ms on test-runner-bnbrr (executor driver) (159/200)
21/01/18 18:23:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:16 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:16 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-aa8c27be-9d72-48e1-bb99-1fdb3df7cd72-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:17 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:17 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:17 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:17 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:17 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:17 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:17 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:17 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:17 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:17 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:17 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:18 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000159_160' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:18 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000159_160: Committed
21/01/18 18:23:18 INFO Executor: Finished task 159.0 in stage 1.0 (TID 160). 2527 bytes result sent to driver
21/01/18 18:23:18 INFO TaskSetManager: Starting task 160.0 in stage 1.0 (TID 161, test-runner-bnbrr, executor driver, partition 160, ANY, 7815 bytes)
21/01/18 18:23:18 INFO Executor: Running task 160.0 in stage 1.0 (TID 161)
21/01/18 18:23:18 INFO TaskSetManager: Finished task 159.0 in stage 1.0 (TID 160) in 1306 ms on test-runner-bnbrr (executor driver) (160/200)
21/01/18 18:23:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:18 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:18 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-af341d34-c71a-4e27-8c22-9e70608c5320-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:18 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:18 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:18 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:18 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:18 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:18 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:18 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:18 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:18 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:18 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:18 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:19 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000160_161' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:19 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000160_161: Committed
21/01/18 18:23:19 INFO Executor: Finished task 160.0 in stage 1.0 (TID 161). 2527 bytes result sent to driver
21/01/18 18:23:19 INFO TaskSetManager: Starting task 161.0 in stage 1.0 (TID 162, test-runner-bnbrr, executor driver, partition 161, ANY, 7815 bytes)
21/01/18 18:23:19 INFO Executor: Running task 161.0 in stage 1.0 (TID 162)
21/01/18 18:23:19 INFO TaskSetManager: Finished task 160.0 in stage 1.0 (TID 161) in 1237 ms on test-runner-bnbrr (executor driver) (161/200)
21/01/18 18:23:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:19 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:19 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-af9dfc1a-e51b-4c1a-8327-dcf9123817ba-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:19 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:19 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:19 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:19 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:19 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:19 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:19 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:19 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:19 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:19 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:19 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:20 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000161_162' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:20 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000161_162: Committed
21/01/18 18:23:20 INFO Executor: Finished task 161.0 in stage 1.0 (TID 162). 2527 bytes result sent to driver
21/01/18 18:23:20 INFO TaskSetManager: Starting task 162.0 in stage 1.0 (TID 163, test-runner-bnbrr, executor driver, partition 162, ANY, 7815 bytes)
21/01/18 18:23:20 INFO Executor: Running task 162.0 in stage 1.0 (TID 163)
21/01/18 18:23:20 INFO TaskSetManager: Finished task 161.0 in stage 1.0 (TID 162) in 1348 ms on test-runner-bnbrr (executor driver) (162/200)
21/01/18 18:23:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:20 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:20 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-b64b724c-1deb-4da6-88ea-b6052ca7a188-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:21 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:21 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:21 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:21 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:21 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:21 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:21 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:21 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:21 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:21 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:21 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:21 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000162_163' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:21 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000162_163: Committed
21/01/18 18:23:21 INFO Executor: Finished task 162.0 in stage 1.0 (TID 163). 2527 bytes result sent to driver
21/01/18 18:23:21 INFO TaskSetManager: Starting task 163.0 in stage 1.0 (TID 164, test-runner-bnbrr, executor driver, partition 163, ANY, 7815 bytes)
21/01/18 18:23:21 INFO Executor: Running task 163.0 in stage 1.0 (TID 164)
21/01/18 18:23:21 INFO TaskSetManager: Finished task 162.0 in stage 1.0 (TID 163) in 1226 ms on test-runner-bnbrr (executor driver) (163/200)
21/01/18 18:23:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:21 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:21 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-b7aee53a-d10b-4dee-8daf-a3dfdd08ab54-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:22 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:22 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:22 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:22 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:22 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:22 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:22 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:22 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:22 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:22 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:22 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:23 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000163_164' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:23 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000163_164: Committed
21/01/18 18:23:23 INFO Executor: Finished task 163.0 in stage 1.0 (TID 164). 2527 bytes result sent to driver
21/01/18 18:23:23 INFO TaskSetManager: Starting task 164.0 in stage 1.0 (TID 165, test-runner-bnbrr, executor driver, partition 164, ANY, 7815 bytes)
21/01/18 18:23:23 INFO Executor: Running task 164.0 in stage 1.0 (TID 165)
21/01/18 18:23:23 INFO TaskSetManager: Finished task 163.0 in stage 1.0 (TID 164) in 1436 ms on test-runner-bnbrr (executor driver) (164/200)
21/01/18 18:23:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:23 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:23 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-b860f8a5-5ebd-4080-be13-6296f642326d-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:23 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:23 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:23 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:23 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:23 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:23 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:23 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:23 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:23 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:23 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:23 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:24 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000164_165' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:24 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000164_165: Committed
21/01/18 18:23:24 INFO Executor: Finished task 164.0 in stage 1.0 (TID 165). 2527 bytes result sent to driver
21/01/18 18:23:24 INFO TaskSetManager: Starting task 165.0 in stage 1.0 (TID 166, test-runner-bnbrr, executor driver, partition 165, ANY, 7815 bytes)
21/01/18 18:23:24 INFO Executor: Running task 165.0 in stage 1.0 (TID 166)
21/01/18 18:23:24 INFO TaskSetManager: Finished task 164.0 in stage 1.0 (TID 165) in 1244 ms on test-runner-bnbrr (executor driver) (165/200)
21/01/18 18:23:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:24 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:24 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-b8ef6d75-394e-4c25-be72-8db610cfdaa1-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:25 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:25 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:25 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:25 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:25 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:25 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:25 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:25 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:25 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:25 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:25 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:25 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000165_166' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:25 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000165_166: Committed
21/01/18 18:23:25 INFO Executor: Finished task 165.0 in stage 1.0 (TID 166). 2527 bytes result sent to driver
21/01/18 18:23:25 INFO TaskSetManager: Starting task 166.0 in stage 1.0 (TID 167, test-runner-bnbrr, executor driver, partition 166, ANY, 7815 bytes)
21/01/18 18:23:25 INFO Executor: Running task 166.0 in stage 1.0 (TID 167)
21/01/18 18:23:25 INFO TaskSetManager: Finished task 165.0 in stage 1.0 (TID 166) in 1302 ms on test-runner-bnbrr (executor driver) (166/200)
21/01/18 18:23:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:25 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:25 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-ba212e89-9929-499d-8edf-dfd174a605d9-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:26 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:26 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:26 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:26 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:26 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:26 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:26 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:26 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:26 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:26 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:26 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:27 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000166_167' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:27 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000166_167: Committed
21/01/18 18:23:27 INFO Executor: Finished task 166.0 in stage 1.0 (TID 167). 2527 bytes result sent to driver
21/01/18 18:23:27 INFO TaskSetManager: Starting task 167.0 in stage 1.0 (TID 168, test-runner-bnbrr, executor driver, partition 167, ANY, 7815 bytes)
21/01/18 18:23:27 INFO Executor: Running task 167.0 in stage 1.0 (TID 168)
21/01/18 18:23:27 INFO TaskSetManager: Finished task 166.0 in stage 1.0 (TID 167) in 1215 ms on test-runner-bnbrr (executor driver) (167/200)
21/01/18 18:23:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:27 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:27 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-bce7a893-ca65-44c4-a3ce-42affce3575a-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:27 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:27 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:27 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:27 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:27 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:27 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:27 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:27 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:27 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:27 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:27 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:28 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000167_168' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:28 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000167_168: Committed
21/01/18 18:23:28 INFO Executor: Finished task 167.0 in stage 1.0 (TID 168). 2527 bytes result sent to driver
21/01/18 18:23:28 INFO TaskSetManager: Starting task 168.0 in stage 1.0 (TID 169, test-runner-bnbrr, executor driver, partition 168, ANY, 7815 bytes)
21/01/18 18:23:28 INFO Executor: Running task 168.0 in stage 1.0 (TID 169)
21/01/18 18:23:28 INFO TaskSetManager: Finished task 167.0 in stage 1.0 (TID 168) in 1189 ms on test-runner-bnbrr (executor driver) (168/200)
21/01/18 18:23:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:28 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:28 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-bd4a7e26-8a3a-4961-b547-e4c57b6b38f8-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:28 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:28 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:28 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:28 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:28 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:28 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:28 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:28 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:28 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:28 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:28 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:29 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000168_169' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:29 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000168_169: Committed
21/01/18 18:23:29 INFO Executor: Finished task 168.0 in stage 1.0 (TID 169). 2527 bytes result sent to driver
21/01/18 18:23:29 INFO TaskSetManager: Starting task 169.0 in stage 1.0 (TID 170, test-runner-bnbrr, executor driver, partition 169, ANY, 7815 bytes)
21/01/18 18:23:29 INFO Executor: Running task 169.0 in stage 1.0 (TID 170)
21/01/18 18:23:29 INFO TaskSetManager: Finished task 168.0 in stage 1.0 (TID 169) in 1350 ms on test-runner-bnbrr (executor driver) (169/200)
21/01/18 18:23:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:29 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-bd6689a6-b25b-4830-b347-4a8efada6ccb-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:30 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:30 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:30 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:30 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:30 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:30 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:30 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:30 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:30 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:30 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:30 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:30 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000169_170' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:30 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000169_170: Committed
21/01/18 18:23:30 INFO Executor: Finished task 169.0 in stage 1.0 (TID 170). 2527 bytes result sent to driver
21/01/18 18:23:30 INFO TaskSetManager: Starting task 170.0 in stage 1.0 (TID 171, test-runner-bnbrr, executor driver, partition 170, ANY, 7815 bytes)
21/01/18 18:23:30 INFO Executor: Running task 170.0 in stage 1.0 (TID 171)
21/01/18 18:23:30 INFO TaskSetManager: Finished task 169.0 in stage 1.0 (TID 170) in 1294 ms on test-runner-bnbrr (executor driver) (170/200)
21/01/18 18:23:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:30 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-bd98edd8-2073-4fdb-b4d8-0efeaa4936fb-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:31 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:31 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:31 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:31 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:31 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:31 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:31 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:31 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:31 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:31 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:31 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:32 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000170_171' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:32 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000170_171: Committed
21/01/18 18:23:32 INFO Executor: Finished task 170.0 in stage 1.0 (TID 171). 2527 bytes result sent to driver
21/01/18 18:23:32 INFO TaskSetManager: Starting task 171.0 in stage 1.0 (TID 172, test-runner-bnbrr, executor driver, partition 171, ANY, 7815 bytes)
21/01/18 18:23:32 INFO Executor: Running task 171.0 in stage 1.0 (TID 172)
21/01/18 18:23:32 INFO TaskSetManager: Finished task 170.0 in stage 1.0 (TID 171) in 1268 ms on test-runner-bnbrr (executor driver) (171/200)
21/01/18 18:23:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:32 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:32 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-be47f291-4c4a-449d-a4f4-defef5673643-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:32 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:32 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:32 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:32 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:32 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:32 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:32 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:32 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:32 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:32 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:32 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:33 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000171_172' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:33 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000171_172: Committed
21/01/18 18:23:33 INFO Executor: Finished task 171.0 in stage 1.0 (TID 172). 2527 bytes result sent to driver
21/01/18 18:23:33 INFO TaskSetManager: Starting task 172.0 in stage 1.0 (TID 173, test-runner-bnbrr, executor driver, partition 172, ANY, 7815 bytes)
21/01/18 18:23:33 INFO Executor: Running task 172.0 in stage 1.0 (TID 173)
21/01/18 18:23:33 INFO TaskSetManager: Finished task 171.0 in stage 1.0 (TID 172) in 1143 ms on test-runner-bnbrr (executor driver) (172/200)
21/01/18 18:23:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:33 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:33 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-c0dc5f72-aea9-4933-bab3-e5581c9b8fb8-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:33 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:33 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:33 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:33 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:33 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:33 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:33 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:33 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:33 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:33 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:33 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:34 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000172_173' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:34 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000172_173: Committed
21/01/18 18:23:34 INFO Executor: Finished task 172.0 in stage 1.0 (TID 173). 2527 bytes result sent to driver
21/01/18 18:23:34 INFO TaskSetManager: Starting task 173.0 in stage 1.0 (TID 174, test-runner-bnbrr, executor driver, partition 173, ANY, 7815 bytes)
21/01/18 18:23:34 INFO Executor: Running task 173.0 in stage 1.0 (TID 174)
21/01/18 18:23:34 INFO TaskSetManager: Finished task 172.0 in stage 1.0 (TID 173) in 1203 ms on test-runner-bnbrr (executor driver) (173/200)
21/01/18 18:23:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:34 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:34 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-c40d8f9f-5b6a-430d-8eb6-9cfac822bd14-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:35 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:35 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:35 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:35 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:35 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:35 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:35 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:35 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:35 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:35 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:35 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:35 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000173_174' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:35 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000173_174: Committed
21/01/18 18:23:35 INFO Executor: Finished task 173.0 in stage 1.0 (TID 174). 2527 bytes result sent to driver
21/01/18 18:23:35 INFO TaskSetManager: Starting task 174.0 in stage 1.0 (TID 175, test-runner-bnbrr, executor driver, partition 174, ANY, 7815 bytes)
21/01/18 18:23:35 INFO Executor: Running task 174.0 in stage 1.0 (TID 175)
21/01/18 18:23:35 INFO TaskSetManager: Finished task 173.0 in stage 1.0 (TID 174) in 1250 ms on test-runner-bnbrr (executor driver) (174/200)
21/01/18 18:23:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:35 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:35 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-c461a99d-12a2-4387-a1fd-085c0a1f0f79-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:36 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:36 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:36 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:36 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:36 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:36 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:36 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:36 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:36 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:36 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:36 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:36 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:36 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:36 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:37 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000174_175' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:37 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000174_175: Committed
21/01/18 18:23:37 INFO Executor: Finished task 174.0 in stage 1.0 (TID 175). 2527 bytes result sent to driver
21/01/18 18:23:37 INFO TaskSetManager: Starting task 175.0 in stage 1.0 (TID 176, test-runner-bnbrr, executor driver, partition 175, ANY, 7815 bytes)
21/01/18 18:23:37 INFO Executor: Running task 175.0 in stage 1.0 (TID 176)
21/01/18 18:23:37 INFO TaskSetManager: Finished task 174.0 in stage 1.0 (TID 175) in 1266 ms on test-runner-bnbrr (executor driver) (175/200)
21/01/18 18:23:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:37 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:37 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-c595aac7-0d5b-4907-9070-ed63cb2b0033-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:37 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:37 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:37 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:37 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:37 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:37 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:37 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:37 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:37 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:37 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:37 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:37 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:38 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000175_176' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:38 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000175_176: Committed
21/01/18 18:23:38 INFO Executor: Finished task 175.0 in stage 1.0 (TID 176). 2527 bytes result sent to driver
21/01/18 18:23:38 INFO TaskSetManager: Starting task 176.0 in stage 1.0 (TID 177, test-runner-bnbrr, executor driver, partition 176, ANY, 7815 bytes)
21/01/18 18:23:38 INFO Executor: Running task 176.0 in stage 1.0 (TID 177)
21/01/18 18:23:38 INFO TaskSetManager: Finished task 175.0 in stage 1.0 (TID 176) in 1285 ms on test-runner-bnbrr (executor driver) (176/200)
21/01/18 18:23:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:38 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:38 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-cda237fa-07a5-4c6f-8f91-a8c291f21603-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:38 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:38 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:38 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:38 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:38 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:38 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:38 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:38 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:38 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:38 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:38 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:39 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000176_177' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:39 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000176_177: Committed
21/01/18 18:23:39 INFO Executor: Finished task 176.0 in stage 1.0 (TID 177). 2527 bytes result sent to driver
21/01/18 18:23:39 INFO TaskSetManager: Starting task 177.0 in stage 1.0 (TID 178, test-runner-bnbrr, executor driver, partition 177, ANY, 7815 bytes)
21/01/18 18:23:39 INFO Executor: Running task 177.0 in stage 1.0 (TID 178)
21/01/18 18:23:39 INFO TaskSetManager: Finished task 176.0 in stage 1.0 (TID 177) in 1346 ms on test-runner-bnbrr (executor driver) (177/200)
21/01/18 18:23:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:39 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:39 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-ce417699-ef26-4183-bd1c-f0d444f21aea-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:40 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:40 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:40 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:40 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:40 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:40 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:40 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:40 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:40 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:40 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:40 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:40 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000177_178' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:40 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000177_178: Committed
21/01/18 18:23:40 INFO Executor: Finished task 177.0 in stage 1.0 (TID 178). 2527 bytes result sent to driver
21/01/18 18:23:40 INFO TaskSetManager: Starting task 178.0 in stage 1.0 (TID 179, test-runner-bnbrr, executor driver, partition 178, ANY, 7815 bytes)
21/01/18 18:23:40 INFO Executor: Running task 178.0 in stage 1.0 (TID 179)
21/01/18 18:23:40 INFO TaskSetManager: Finished task 177.0 in stage 1.0 (TID 178) in 1312 ms on test-runner-bnbrr (executor driver) (178/200)
21/01/18 18:23:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:40 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:40 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-cf7c1803-f712-44fd-8dd7-f21f1797cadb-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:41 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:41 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:41 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:41 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:41 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:41 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:41 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:41 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:41 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:41 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:41 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:42 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000178_179' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:42 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000178_179: Committed
21/01/18 18:23:42 INFO Executor: Finished task 178.0 in stage 1.0 (TID 179). 2527 bytes result sent to driver
21/01/18 18:23:42 INFO TaskSetManager: Starting task 179.0 in stage 1.0 (TID 180, test-runner-bnbrr, executor driver, partition 179, ANY, 7815 bytes)
21/01/18 18:23:42 INFO Executor: Running task 179.0 in stage 1.0 (TID 180)
21/01/18 18:23:42 INFO TaskSetManager: Finished task 178.0 in stage 1.0 (TID 179) in 1335 ms on test-runner-bnbrr (executor driver) (179/200)
21/01/18 18:23:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:42 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:42 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-d2124b7b-53f5-423a-970d-125509499a3e-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:42 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:42 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:42 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:42 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:42 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:42 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:42 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:42 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:42 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:42 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:42 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:43 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000179_180' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:43 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000179_180: Committed
21/01/18 18:23:43 INFO Executor: Finished task 179.0 in stage 1.0 (TID 180). 2527 bytes result sent to driver
21/01/18 18:23:43 INFO TaskSetManager: Starting task 180.0 in stage 1.0 (TID 181, test-runner-bnbrr, executor driver, partition 180, ANY, 7815 bytes)
21/01/18 18:23:43 INFO Executor: Running task 180.0 in stage 1.0 (TID 181)
21/01/18 18:23:43 INFO TaskSetManager: Finished task 179.0 in stage 1.0 (TID 180) in 1265 ms on test-runner-bnbrr (executor driver) (180/200)
21/01/18 18:23:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:43 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:43 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-d7ff7795-8d95-46a4-9342-f8de0d52fb13-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:44 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:44 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:44 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:44 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:44 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:44 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:44 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:44 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:44 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:44 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:44 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:44 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000180_181' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:44 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000180_181: Committed
21/01/18 18:23:44 INFO Executor: Finished task 180.0 in stage 1.0 (TID 181). 2527 bytes result sent to driver
21/01/18 18:23:44 INFO TaskSetManager: Starting task 181.0 in stage 1.0 (TID 182, test-runner-bnbrr, executor driver, partition 181, ANY, 7815 bytes)
21/01/18 18:23:44 INFO Executor: Running task 181.0 in stage 1.0 (TID 182)
21/01/18 18:23:44 INFO TaskSetManager: Finished task 180.0 in stage 1.0 (TID 181) in 1188 ms on test-runner-bnbrr (executor driver) (181/200)
21/01/18 18:23:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:44 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:44 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-df4130dd-fe40-4bf8-9ac5-f36c41cb6a0c-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:45 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:45 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:45 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:45 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:45 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:45 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:45 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:45 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:45 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:45 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:45 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:46 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000181_182' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:46 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000181_182: Committed
21/01/18 18:23:46 INFO Executor: Finished task 181.0 in stage 1.0 (TID 182). 2527 bytes result sent to driver
21/01/18 18:23:46 INFO TaskSetManager: Starting task 182.0 in stage 1.0 (TID 183, test-runner-bnbrr, executor driver, partition 182, ANY, 7815 bytes)
21/01/18 18:23:46 INFO Executor: Running task 182.0 in stage 1.0 (TID 183)
21/01/18 18:23:46 INFO TaskSetManager: Finished task 181.0 in stage 1.0 (TID 182) in 1429 ms on test-runner-bnbrr (executor driver) (182/200)
21/01/18 18:23:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:46 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:46 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-e10d47b5-fb8a-4c94-92ae-e03942c14618-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:46 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:46 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:46 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:46 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:46 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:46 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:46 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:46 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:46 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:46 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:46 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:47 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000182_183' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:47 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000182_183: Committed
21/01/18 18:23:47 INFO Executor: Finished task 182.0 in stage 1.0 (TID 183). 2527 bytes result sent to driver
21/01/18 18:23:47 INFO TaskSetManager: Starting task 183.0 in stage 1.0 (TID 184, test-runner-bnbrr, executor driver, partition 183, ANY, 7815 bytes)
21/01/18 18:23:47 INFO Executor: Running task 183.0 in stage 1.0 (TID 184)
21/01/18 18:23:47 INFO TaskSetManager: Finished task 182.0 in stage 1.0 (TID 183) in 1191 ms on test-runner-bnbrr (executor driver) (183/200)
21/01/18 18:23:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:47 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-e310852b-1f58-477a-b1f1-167e86bfb199-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:47 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:47 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:47 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:47 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:47 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:47 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:47 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:47 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:47 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:47 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:47 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:48 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000183_184' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:48 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000183_184: Committed
21/01/18 18:23:48 INFO Executor: Finished task 183.0 in stage 1.0 (TID 184). 2527 bytes result sent to driver
21/01/18 18:23:48 INFO TaskSetManager: Starting task 184.0 in stage 1.0 (TID 185, test-runner-bnbrr, executor driver, partition 184, ANY, 7815 bytes)
21/01/18 18:23:48 INFO Executor: Running task 184.0 in stage 1.0 (TID 185)
21/01/18 18:23:48 INFO TaskSetManager: Finished task 183.0 in stage 1.0 (TID 184) in 1400 ms on test-runner-bnbrr (executor driver) (184/200)
21/01/18 18:23:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:48 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:48 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-e422f988-87b9-493c-8bfa-2a5919233b83-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:49 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:49 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:49 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:49 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:49 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:49 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:49 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:49 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:49 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:49 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:49 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:50 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000184_185' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:50 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000184_185: Committed
21/01/18 18:23:50 INFO Executor: Finished task 184.0 in stage 1.0 (TID 185). 2527 bytes result sent to driver
21/01/18 18:23:50 INFO TaskSetManager: Starting task 185.0 in stage 1.0 (TID 186, test-runner-bnbrr, executor driver, partition 185, ANY, 7815 bytes)
21/01/18 18:23:50 INFO Executor: Running task 185.0 in stage 1.0 (TID 186)
21/01/18 18:23:50 INFO TaskSetManager: Finished task 184.0 in stage 1.0 (TID 185) in 1266 ms on test-runner-bnbrr (executor driver) (185/200)
21/01/18 18:23:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:50 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:50 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-e4a34e98-4216-48de-adc1-376994a2e867-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:50 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:50 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:50 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:50 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:50 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:50 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:50 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:50 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:50 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:50 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:50 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:50 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:50 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:50 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:51 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000185_186' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:51 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000185_186: Committed
21/01/18 18:23:51 INFO Executor: Finished task 185.0 in stage 1.0 (TID 186). 2527 bytes result sent to driver
21/01/18 18:23:51 INFO TaskSetManager: Starting task 186.0 in stage 1.0 (TID 187, test-runner-bnbrr, executor driver, partition 186, ANY, 7815 bytes)
21/01/18 18:23:51 INFO Executor: Running task 186.0 in stage 1.0 (TID 187)
21/01/18 18:23:51 INFO TaskSetManager: Finished task 185.0 in stage 1.0 (TID 186) in 1371 ms on test-runner-bnbrr (executor driver) (186/200)
21/01/18 18:23:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:51 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-e56dd0d1-7c85-4305-a186-ecd212953910-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:51 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:51 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:51 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:51 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:51 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:51 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:51 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:51 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:51 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:51 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:51 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:51 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:51 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:52 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000186_187' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:52 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000186_187: Committed
21/01/18 18:23:52 INFO Executor: Finished task 186.0 in stage 1.0 (TID 187). 2527 bytes result sent to driver
21/01/18 18:23:52 INFO TaskSetManager: Starting task 187.0 in stage 1.0 (TID 188, test-runner-bnbrr, executor driver, partition 187, ANY, 7815 bytes)
21/01/18 18:23:52 INFO Executor: Running task 187.0 in stage 1.0 (TID 188)
21/01/18 18:23:52 INFO TaskSetManager: Finished task 186.0 in stage 1.0 (TID 187) in 1188 ms on test-runner-bnbrr (executor driver) (187/200)
21/01/18 18:23:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:52 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:52 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-e7c50e8a-f082-4ce1-963c-99ce8e7f76e3-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:53 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:53 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:53 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:53 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:53 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:53 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:53 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:53 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:53 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:53 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:53 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:53 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000187_188' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:53 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000187_188: Committed
21/01/18 18:23:53 INFO Executor: Finished task 187.0 in stage 1.0 (TID 188). 2527 bytes result sent to driver
21/01/18 18:23:53 INFO TaskSetManager: Starting task 188.0 in stage 1.0 (TID 189, test-runner-bnbrr, executor driver, partition 188, ANY, 7815 bytes)
21/01/18 18:23:53 INFO Executor: Running task 188.0 in stage 1.0 (TID 189)
21/01/18 18:23:53 INFO TaskSetManager: Finished task 187.0 in stage 1.0 (TID 188) in 1187 ms on test-runner-bnbrr (executor driver) (188/200)
21/01/18 18:23:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:53 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-eecdcc93-fdbf-4e44-914e-83c904d61cdb-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:54 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:54 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:54 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:54 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:54 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:54 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:54 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:54 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:54 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:54 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:54 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:55 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000188_189' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:55 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000188_189: Committed
21/01/18 18:23:55 INFO Executor: Finished task 188.0 in stage 1.0 (TID 189). 2527 bytes result sent to driver
21/01/18 18:23:55 INFO TaskSetManager: Starting task 189.0 in stage 1.0 (TID 190, test-runner-bnbrr, executor driver, partition 189, ANY, 7815 bytes)
21/01/18 18:23:55 INFO Executor: Running task 189.0 in stage 1.0 (TID 190)
21/01/18 18:23:55 INFO TaskSetManager: Finished task 188.0 in stage 1.0 (TID 189) in 1318 ms on test-runner-bnbrr (executor driver) (189/200)
21/01/18 18:23:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:55 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-f00215b3-35a7-441b-a773-bda17a2ae2d4-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:55 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:55 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:55 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:55 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:55 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:55 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:55 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:55 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:55 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:55 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:55 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:56 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000189_190' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:56 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000189_190: Committed
21/01/18 18:23:56 INFO Executor: Finished task 189.0 in stage 1.0 (TID 190). 2527 bytes result sent to driver
21/01/18 18:23:56 INFO TaskSetManager: Starting task 190.0 in stage 1.0 (TID 191, test-runner-bnbrr, executor driver, partition 190, ANY, 7815 bytes)
21/01/18 18:23:56 INFO Executor: Running task 190.0 in stage 1.0 (TID 191)
21/01/18 18:23:56 INFO TaskSetManager: Finished task 189.0 in stage 1.0 (TID 190) in 1239 ms on test-runner-bnbrr (executor driver) (190/200)
21/01/18 18:23:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:56 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:56 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-f2b26521-1c63-4cfc-8f5b-627aab1845bf-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:56 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:56 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:56 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:56 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:56 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:56 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:56 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:56 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:56 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:56 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:56 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:57 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000190_191' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:57 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000190_191: Committed
21/01/18 18:23:57 INFO Executor: Finished task 190.0 in stage 1.0 (TID 191). 2527 bytes result sent to driver
21/01/18 18:23:57 INFO TaskSetManager: Starting task 191.0 in stage 1.0 (TID 192, test-runner-bnbrr, executor driver, partition 191, ANY, 7815 bytes)
21/01/18 18:23:57 INFO Executor: Running task 191.0 in stage 1.0 (TID 192)
21/01/18 18:23:57 INFO TaskSetManager: Finished task 190.0 in stage 1.0 (TID 191) in 1353 ms on test-runner-bnbrr (executor driver) (191/200)
21/01/18 18:23:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:57 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:57 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-f302582d-917f-4baa-a728-fde414d77039-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:58 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:58 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:58 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:58 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:58 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:58 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:58 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:58 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:58 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:58 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:58 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:23:58 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000191_192' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:23:58 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000191_192: Committed
21/01/18 18:23:58 INFO Executor: Finished task 191.0 in stage 1.0 (TID 192). 2527 bytes result sent to driver
21/01/18 18:23:58 INFO TaskSetManager: Starting task 192.0 in stage 1.0 (TID 193, test-runner-bnbrr, executor driver, partition 192, ANY, 7815 bytes)
21/01/18 18:23:58 INFO Executor: Running task 192.0 in stage 1.0 (TID 193)
21/01/18 18:23:58 INFO TaskSetManager: Finished task 191.0 in stage 1.0 (TID 192) in 1294 ms on test-runner-bnbrr (executor driver) (192/200)
21/01/18 18:23:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:59 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:23:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:23:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:23:59 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-f5623f46-2aa4-4214-8087-b3e1535b8045-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:23:59 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:59 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:23:59 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:23:59 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:23:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:23:59 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:23:59 INFO ParquetOutputFormat: Validation is off
21/01/18 18:23:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:23:59 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:23:59 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:23:59 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:23:59 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:23:59 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:23:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:24:00 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000192_193' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:24:00 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000192_193: Committed
21/01/18 18:24:00 INFO Executor: Finished task 192.0 in stage 1.0 (TID 193). 2527 bytes result sent to driver
21/01/18 18:24:00 INFO TaskSetManager: Starting task 193.0 in stage 1.0 (TID 194, test-runner-bnbrr, executor driver, partition 193, ANY, 7815 bytes)
21/01/18 18:24:00 INFO Executor: Running task 193.0 in stage 1.0 (TID 194)
21/01/18 18:24:00 INFO TaskSetManager: Finished task 192.0 in stage 1.0 (TID 193) in 1284 ms on test-runner-bnbrr (executor driver) (193/200)
21/01/18 18:24:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:24:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:24:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:24:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:24:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:24:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:24:00 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-f5dc5f0b-a333-4a9f-af07-0cca483c614e-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:24:00 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:24:00 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:24:00 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:24:00 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:24:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:24:00 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:24:00 INFO ParquetOutputFormat: Validation is off
21/01/18 18:24:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:24:00 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:24:00 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:24:00 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:24:00 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:24:00 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:24:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:24:01 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000193_194' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:24:01 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000193_194: Committed
21/01/18 18:24:01 INFO Executor: Finished task 193.0 in stage 1.0 (TID 194). 2527 bytes result sent to driver
21/01/18 18:24:01 INFO TaskSetManager: Starting task 194.0 in stage 1.0 (TID 195, test-runner-bnbrr, executor driver, partition 194, ANY, 7815 bytes)
21/01/18 18:24:01 INFO Executor: Running task 194.0 in stage 1.0 (TID 195)
21/01/18 18:24:01 INFO TaskSetManager: Finished task 193.0 in stage 1.0 (TID 194) in 1335 ms on test-runner-bnbrr (executor driver) (194/200)
21/01/18 18:24:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:24:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:24:01 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:24:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:24:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:24:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:24:01 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-f6068e7e-d560-48f2-9b57-29c201703662-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:24:02 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:24:02 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:24:02 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:24:02 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:24:02 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:24:02 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:24:02 INFO ParquetOutputFormat: Validation is off
21/01/18 18:24:02 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:24:02 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:24:02 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:24:02 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:24:02 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:24:02 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:24:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:24:03 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000194_195' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:24:03 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000194_195: Committed
21/01/18 18:24:03 INFO Executor: Finished task 194.0 in stage 1.0 (TID 195). 2527 bytes result sent to driver
21/01/18 18:24:03 INFO TaskSetManager: Starting task 195.0 in stage 1.0 (TID 196, test-runner-bnbrr, executor driver, partition 195, ANY, 7815 bytes)
21/01/18 18:24:03 INFO Executor: Running task 195.0 in stage 1.0 (TID 196)
21/01/18 18:24:03 INFO TaskSetManager: Finished task 194.0 in stage 1.0 (TID 195) in 1412 ms on test-runner-bnbrr (executor driver) (195/200)
21/01/18 18:24:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:24:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:24:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:24:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:24:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:24:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:24:03 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-f8d5fdee-480e-45d9-a4c1-ce9270a41cf2-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:24:03 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:24:03 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:24:03 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:24:03 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:24:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:24:03 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:24:03 INFO ParquetOutputFormat: Validation is off
21/01/18 18:24:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:24:03 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:24:03 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:24:03 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:24:03 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:24:03 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:24:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:24:04 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000195_196' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:24:04 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000195_196: Committed
21/01/18 18:24:04 INFO Executor: Finished task 195.0 in stage 1.0 (TID 196). 2527 bytes result sent to driver
21/01/18 18:24:04 INFO TaskSetManager: Starting task 196.0 in stage 1.0 (TID 197, test-runner-bnbrr, executor driver, partition 196, ANY, 7815 bytes)
21/01/18 18:24:04 INFO Executor: Running task 196.0 in stage 1.0 (TID 197)
21/01/18 18:24:04 INFO TaskSetManager: Finished task 195.0 in stage 1.0 (TID 196) in 1198 ms on test-runner-bnbrr (executor driver) (196/200)
21/01/18 18:24:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:24:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:24:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:24:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:24:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:24:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:24:04 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-f9c8549e-8ae5-4ed9-86d9-cd0bfd1dd572-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:24:04 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:24:04 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:24:04 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:24:04 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:24:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:24:04 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:24:04 INFO ParquetOutputFormat: Validation is off
21/01/18 18:24:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:24:04 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:24:04 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:24:04 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:24:04 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:24:04 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:24:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:24:05 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000196_197' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:24:05 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000196_197: Committed
21/01/18 18:24:05 INFO Executor: Finished task 196.0 in stage 1.0 (TID 197). 2527 bytes result sent to driver
21/01/18 18:24:05 INFO TaskSetManager: Starting task 197.0 in stage 1.0 (TID 198, test-runner-bnbrr, executor driver, partition 197, ANY, 7815 bytes)
21/01/18 18:24:05 INFO Executor: Running task 197.0 in stage 1.0 (TID 198)
21/01/18 18:24:05 INFO TaskSetManager: Finished task 196.0 in stage 1.0 (TID 197) in 1346 ms on test-runner-bnbrr (executor driver) (197/200)
21/01/18 18:24:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:24:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:24:05 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:24:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:24:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:24:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:24:05 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-fd1c05a9-7014-47d1-bf16-32be67340eaa-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:24:06 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:24:06 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:24:06 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:24:06 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:24:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:24:06 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:24:06 INFO ParquetOutputFormat: Validation is off
21/01/18 18:24:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:24:06 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:24:06 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:24:06 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:24:06 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:24:06 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:24:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:24:06 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000197_198' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:24:06 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000197_198: Committed
21/01/18 18:24:06 INFO Executor: Finished task 197.0 in stage 1.0 (TID 198). 2527 bytes result sent to driver
21/01/18 18:24:06 INFO TaskSetManager: Starting task 198.0 in stage 1.0 (TID 199, test-runner-bnbrr, executor driver, partition 198, ANY, 7815 bytes)
21/01/18 18:24:06 INFO Executor: Running task 198.0 in stage 1.0 (TID 199)
21/01/18 18:24:06 INFO TaskSetManager: Finished task 197.0 in stage 1.0 (TID 198) in 1259 ms on test-runner-bnbrr (executor driver) (198/200)
21/01/18 18:24:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:24:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:24:06 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:24:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:24:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:24:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:24:06 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-fe246f8a-974b-4e17-a635-437dcb730429-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/18 18:24:07 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:24:07 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:24:07 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:24:07 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:24:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:24:07 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:24:07 INFO ParquetOutputFormat: Validation is off
21/01/18 18:24:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:24:07 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:24:07 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:24:07 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:24:07 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:24:07 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:24:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/18 18:24:08 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000198_199' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:24:08 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000198_199: Committed
21/01/18 18:24:08 INFO Executor: Finished task 198.0 in stage 1.0 (TID 199). 2527 bytes result sent to driver
21/01/18 18:24:08 INFO TaskSetManager: Starting task 199.0 in stage 1.0 (TID 200, test-runner-bnbrr, executor driver, partition 199, ANY, 7963 bytes)
21/01/18 18:24:08 INFO Executor: Running task 199.0 in stage 1.0 (TID 200)
21/01/18 18:24:08 INFO TaskSetManager: Finished task 198.0 in stage 1.0 (TID 199) in 1304 ms on test-runner-bnbrr (executor driver) (199/200)
21/01/18 18:24:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:24:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:24:08 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:24:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/18 18:24:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/18 18:24:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/18 18:24:08 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-f62408be-4427-4404-ab87-d6cceda9ecfe-c000.snappy.parquet, range: 0-104863433, partition values: [empty row]
21/01/18 18:24:08 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:24:08 INFO CodecConfig: Compression: SNAPPY
21/01/18 18:24:08 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/18 18:24:08 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/18 18:24:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/18 18:24:08 INFO ParquetOutputFormat: Dictionary is on
21/01/18 18:24:08 INFO ParquetOutputFormat: Validation is off
21/01/18 18:24:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/18 18:24:08 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/18 18:24:08 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/18 18:24:08 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/18 18:24:08 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/18 18:24:08 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/18 18:24:08 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-b0c8e1cf-1fa4-49d1-b5d1-fb352aea27ca-c000.snappy.parquet, range: 0-608, partition values: [empty row]
21/01/18 18:24:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104863256
21/01/18 18:24:09 INFO FileOutputCommitter: Saved output of task 'attempt_20210118181912_0001_m_000199_200' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/18 18:24:09 INFO SparkHadoopMapRedUtil: attempt_20210118181912_0001_m_000199_200: Committed
21/01/18 18:24:09 INFO Executor: Finished task 199.0 in stage 1.0 (TID 200). 2527 bytes result sent to driver
21/01/18 18:24:09 INFO TaskSetManager: Finished task 199.0 in stage 1.0 (TID 200) in 1314 ms on test-runner-bnbrr (executor driver) (200/200)
21/01/18 18:24:09 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
21/01/18 18:24:09 INFO DAGScheduler: ResultStage 1 (parquet at Copy.java:28) finished in 297.053 s
21/01/18 18:24:09 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/18 18:24:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
21/01/18 18:24:09 INFO DAGScheduler: Job 1 finished: parquet at Copy.java:28, took 297.099993 s
21/01/18 18:24:09 INFO FileFormatWriter: Write Job aaf763f0-0edd-4fcd-8ced-f4455a91172d committed.
21/01/18 18:24:09 INFO FileFormatWriter: Finished processing stats for write job aaf763f0-0edd-4fcd-8ced-f4455a91172d.
21/01/18 18:24:09 INFO SparkUI: Stopped Spark web UI at http://test-runner-bnbrr:4040
21/01/18 18:24:09 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
21/01/18 18:24:09 INFO MemoryStore: MemoryStore cleared
21/01/18 18:24:09 INFO BlockManager: BlockManager stopped
21/01/18 18:24:09 INFO BlockManagerMaster: BlockManagerMaster stopped
21/01/18 18:24:09 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
21/01/18 18:24:09 INFO SparkContext: Successfully stopped SparkContext
21/01/18 18:24:09 INFO ShutdownHookManager: Shutdown hook called
21/01/18 18:24:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-db356603-19a5-4208-962e-5f1cfbc9b361
21/01/18 18:24:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-8167179c-a1d6-4209-ad1d-885d7b347146

real	5m6.345s
user	4m27.870s
sys	1m0.091s
Process exited with exit code 0
