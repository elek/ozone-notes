Flokkr launcher script 1.1-18-g58a3efc

===== Plugin is activated ENVTOCONF =====
hadoop.conf
File hadoop.conf has been written out successfullly.
mapred-site.xml
File mapred-site.xml has been written out successfullly.
ozone-site.xml
File ozone-site.xml has been written out successfullly.
core-site.xml
File core-site.xml has been written out successfullly.
hdfs-site.xml
File hdfs-site.xml has been written out successfullly.
Non-spark-on-k8s command provided, proceeding in pass-through mode...
======================================
*** Launching "/opt/testscripts/parquet.sh copy hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1"
+ : /opt/spark
+ : /opt/spark-test/jars/
+ : watchforcommit.btm
+ /opt/spark/bin/spark-submit --conf spark.executor.memory=4g --jars /opt/ozonefs/hadoop-ozone-filesystem.jar /opt/spark-test/jars//spark-samples-1.0-SNAPSHOT.jar copy hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:43:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
21/01/19 12:43:49 INFO SparkContext: Running Spark version 3.0.1
21/01/19 12:43:49 INFO ResourceUtils: ==============================================================
21/01/19 12:43:49 INFO ResourceUtils: Resources for spark.driver:

21/01/19 12:43:49 INFO ResourceUtils: ==============================================================
21/01/19 12:43:49 INFO SparkContext: Submitted application: Copy
21/01/19 12:43:49 INFO SecurityManager: Changing view acls to: spark
21/01/19 12:43:49 INFO SecurityManager: Changing modify acls to: spark
21/01/19 12:43:49 INFO SecurityManager: Changing view acls groups to: 
21/01/19 12:43:49 INFO SecurityManager: Changing modify acls groups to: 
21/01/19 12:43:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
21/01/19 12:43:49 INFO Utils: Successfully started service 'sparkDriver' on port 41734.
21/01/19 12:43:49 INFO SparkEnv: Registering MapOutputTracker
21/01/19 12:43:49 INFO SparkEnv: Registering BlockManagerMaster
21/01/19 12:43:49 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/01/19 12:43:49 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/01/19 12:43:49 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/01/19 12:43:49 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-31df5dd2-9870-4768-8761-5b17646a02d0
21/01/19 12:43:49 INFO MemoryStore: MemoryStore started with capacity 413.9 MiB
21/01/19 12:43:49 INFO SparkEnv: Registering OutputCommitCoordinator
21/01/19 12:43:49 INFO Utils: Successfully started service 'SparkUI' on port 4040.
21/01/19 12:43:49 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://test-runner-pkwzc:4040
21/01/19 12:43:50 INFO SparkContext: Added JAR file:///opt/ozonefs/hadoop-ozone-filesystem.jar at spark://test-runner-pkwzc:41734/jars/hadoop-ozone-filesystem.jar with timestamp 1611060230013
21/01/19 12:43:50 INFO SparkContext: Added JAR file:/opt/spark-test/jars/spark-samples-1.0-SNAPSHOT.jar at spark://test-runner-pkwzc:41734/jars/spark-samples-1.0-SNAPSHOT.jar with timestamp 1611060230013
21/01/19 12:43:50 INFO Executor: Starting executor ID driver on host test-runner-pkwzc
21/01/19 12:43:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37864.
21/01/19 12:43:50 INFO NettyBlockTransferService: Server created on test-runner-pkwzc:37864
21/01/19 12:43:50 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/01/19 12:43:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, test-runner-pkwzc, 37864, None)
21/01/19 12:43:50 INFO BlockManagerMasterEndpoint: Registering block manager test-runner-pkwzc:37864 with 413.9 MiB RAM, BlockManagerId(driver, test-runner-pkwzc, 37864, None)
21/01/19 12:43:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, test-runner-pkwzc, 37864, None)
21/01/19 12:43:50 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, test-runner-pkwzc, 37864, None)
21/01/19 12:43:50 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/opt/spark/spark-warehouse').
21/01/19 12:43:50 INFO SharedState: Warehouse path is 'file:/opt/spark/spark-warehouse'.
21/01/19 12:43:51 INFO InMemoryFileIndex: It took 230 ms to list leaf files for 1 paths.
21/01/19 12:43:52 INFO SparkContext: Starting job: parquet at Copy.java:26
21/01/19 12:43:52 INFO DAGScheduler: Got job 0 (parquet at Copy.java:26) with 1 output partitions
21/01/19 12:43:52 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at Copy.java:26)
21/01/19 12:43:52 INFO DAGScheduler: Parents of final stage: List()
21/01/19 12:43:52 INFO DAGScheduler: Missing parents: List()
21/01/19 12:43:52 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at Copy.java:26), which has no missing parents
21/01/19 12:43:52 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 84.2 KiB, free 413.8 MiB)
21/01/19 12:43:52 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 29.8 KiB, free 413.8 MiB)
21/01/19 12:43:52 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on test-runner-pkwzc:37864 (size: 29.8 KiB, free: 413.9 MiB)
21/01/19 12:43:52 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1223
21/01/19 12:43:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at Copy.java:26) (first 15 tasks are for partitions Vector(0))
21/01/19 12:43:52 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
21/01/19 12:43:52 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, test-runner-pkwzc, executor driver, partition 0, PROCESS_LOCAL, 7566 bytes)
21/01/19 12:43:52 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
21/01/19 12:43:52 INFO Executor: Fetching spark://test-runner-pkwzc:41734/jars/spark-samples-1.0-SNAPSHOT.jar with timestamp 1611060230013
21/01/19 12:43:52 INFO TransportClientFactory: Successfully created connection to test-runner-pkwzc/10.42.1.79:41734 after 32 ms (0 ms spent in bootstraps)
21/01/19 12:43:52 INFO Utils: Fetching spark://test-runner-pkwzc:41734/jars/spark-samples-1.0-SNAPSHOT.jar to /tmp/spark-a037648d-209e-4b2c-bb6c-566370f50e22/userFiles-e744c083-c2f7-4d97-b052-15d48af582a5/fetchFileTemp1845472623499682567.tmp
21/01/19 12:43:52 INFO Executor: Adding file:/tmp/spark-a037648d-209e-4b2c-bb6c-566370f50e22/userFiles-e744c083-c2f7-4d97-b052-15d48af582a5/spark-samples-1.0-SNAPSHOT.jar to class loader
21/01/19 12:43:52 INFO Executor: Fetching spark://test-runner-pkwzc:41734/jars/hadoop-ozone-filesystem.jar with timestamp 1611060230013
21/01/19 12:43:52 INFO Utils: Fetching spark://test-runner-pkwzc:41734/jars/hadoop-ozone-filesystem.jar to /tmp/spark-a037648d-209e-4b2c-bb6c-566370f50e22/userFiles-e744c083-c2f7-4d97-b052-15d48af582a5/fetchFileTemp8349098072521747582.tmp
21/01/19 12:43:52 INFO Executor: Adding file:/tmp/spark-a037648d-209e-4b2c-bb6c-566370f50e22/userFiles-e744c083-c2f7-4d97-b052-15d48af582a5/hadoop-ozone-filesystem.jar to class loader
21/01/19 12:43:53 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1780 bytes result sent to driver
21/01/19 12:43:53 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 897 ms on test-runner-pkwzc (executor driver) (1/1)
21/01/19 12:43:53 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
21/01/19 12:43:53 INFO DAGScheduler: ResultStage 0 (parquet at Copy.java:26) finished in 1.052 s
21/01/19 12:43:53 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/19 12:43:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
21/01/19 12:43:53 INFO DAGScheduler: Job 0 finished: parquet at Copy.java:26, took 1.104689 s
21/01/19 12:43:55 INFO BlockManagerInfo: Removed broadcast_0_piece0 on test-runner-pkwzc:37864 in memory (size: 29.8 KiB, free: 413.9 MiB)
21/01/19 12:43:55 INFO FileSourceStrategy: Pruning directories with: 
21/01/19 12:43:55 INFO FileSourceStrategy: Pushed Filters: 
21/01/19 12:43:55 INFO FileSourceStrategy: Post-Scan Filters: 
21/01/19 12:43:55 INFO FileSourceStrategy: Output Data Schema: struct<data: binary, index: int>
21/01/19 12:43:55 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:43:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:43:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:43:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:43:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:43:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:43:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:43:55 INFO CodeGenerator: Code generated in 230.419043 ms
21/01/19 12:43:55 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 307.2 KiB, free 413.6 MiB)
21/01/19 12:43:55 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 28.3 KiB, free 413.6 MiB)
21/01/19 12:43:55 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on test-runner-pkwzc:37864 (size: 28.3 KiB, free: 413.9 MiB)
21/01/19 12:43:55 INFO SparkContext: Created broadcast 1 from parquet at Copy.java:28
21/01/19 12:43:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
21/01/19 12:43:56 INFO SparkContext: Starting job: parquet at Copy.java:28
21/01/19 12:43:56 INFO DAGScheduler: Got job 1 (parquet at Copy.java:28) with 200 output partitions
21/01/19 12:43:56 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at Copy.java:28)
21/01/19 12:43:56 INFO DAGScheduler: Parents of final stage: List()
21/01/19 12:43:56 INFO DAGScheduler: Missing parents: List()
21/01/19 12:43:56 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at parquet at Copy.java:28), which has no missing parents
21/01/19 12:43:56 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 176.1 KiB, free 413.4 MiB)
21/01/19 12:43:56 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 62.4 KiB, free 413.4 MiB)
21/01/19 12:43:56 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on test-runner-pkwzc:37864 (size: 62.4 KiB, free: 413.8 MiB)
21/01/19 12:43:56 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
21/01/19 12:43:56 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at parquet at Copy.java:28) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
21/01/19 12:43:56 INFO TaskSchedulerImpl: Adding task set 1.0 with 200 tasks
21/01/19 12:43:56 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, test-runner-pkwzc, executor driver, partition 0, ANY, 7815 bytes)
21/01/19 12:43:56 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
21/01/19 12:43:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:43:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:43:56 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:43:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:43:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:43:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:43:56 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:43:56 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:43:56 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:43:56 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:43:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:43:56 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:43:56 INFO ParquetOutputFormat: Validation is off
21/01/19 12:43:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:43:56 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:43:56 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:43:56 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:43:56 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:43:56 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:43:56 INFO CodecPool: Got brand-new compressor [.snappy]
21/01/19 12:43:56 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-aaa5a965-8829-463d-9192-1cb65594ad9f-c000.snappy.parquet, range: 0-104863439, partition values: [empty row]
21/01/19 12:43:56 INFO CodecPool: Got brand-new decompressor [.snappy]
21/01/19 12:44:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:44:34 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000000_1' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:44:34 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000000_1: Committed
21/01/19 12:44:34 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2570 bytes result sent to driver
21/01/19 12:44:34 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2, test-runner-pkwzc, executor driver, partition 1, ANY, 7815 bytes)
21/01/19 12:44:34 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
21/01/19 12:44:34 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 37937 ms on test-runner-pkwzc (executor driver) (1/200)
21/01/19 12:44:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:34 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:34 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-b29eb974-c5c0-45de-b5d6-ee6209f13d69-c000.snappy.parquet, range: 0-104863439, partition values: [empty row]
21/01/19 12:44:34 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:34 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:34 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:44:34 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:44:34 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:44:34 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:44:34 INFO ParquetOutputFormat: Validation is off
21/01/19 12:44:34 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:44:34 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:44:34 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:44:34 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:44:34 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:44:34 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:44:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:44:35 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000001_2' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:44:35 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000001_2: Committed
21/01/19 12:44:35 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2527 bytes result sent to driver
21/01/19 12:44:35 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3, test-runner-pkwzc, executor driver, partition 2, ANY, 7815 bytes)
21/01/19 12:44:35 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
21/01/19 12:44:35 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 1131 ms on test-runner-pkwzc (executor driver) (2/200)
21/01/19 12:44:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:35 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:35 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-bbd43881-3851-4eec-81d7-bed35c606c7b-c000.snappy.parquet, range: 0-104863439, partition values: [empty row]
21/01/19 12:44:35 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:35 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:35 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:44:35 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:44:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:44:35 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:44:35 INFO ParquetOutputFormat: Validation is off
21/01/19 12:44:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:44:35 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:44:35 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:44:35 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:44:35 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:44:35 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:44:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:44:36 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000002_3' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:44:36 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000002_3: Committed
21/01/19 12:44:36 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2527 bytes result sent to driver
21/01/19 12:44:36 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4, test-runner-pkwzc, executor driver, partition 3, ANY, 7815 bytes)
21/01/19 12:44:36 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
21/01/19 12:44:36 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 1242 ms on test-runner-pkwzc (executor driver) (3/200)
21/01/19 12:44:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:36 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:36 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-16b8cfed-862c-450f-9111-7f7e0abc98b7-c000.snappy.parquet, range: 0-104863438, partition values: [empty row]
21/01/19 12:44:36 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:36 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:36 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:44:36 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:44:36 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:44:36 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:44:36 INFO ParquetOutputFormat: Validation is off
21/01/19 12:44:36 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:44:36 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:44:36 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:44:36 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:44:36 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:44:36 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:44:37 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:44:37 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000003_4' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:44:37 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000003_4: Committed
21/01/19 12:44:37 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2527 bytes result sent to driver
21/01/19 12:44:37 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5, test-runner-pkwzc, executor driver, partition 4, ANY, 7815 bytes)
21/01/19 12:44:37 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)
21/01/19 12:44:37 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 1184 ms on test-runner-pkwzc (executor driver) (4/200)
21/01/19 12:44:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:37 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:37 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-27804906-2474-4045-b4a8-cd4fcb878417-c000.snappy.parquet, range: 0-104863438, partition values: [empty row]
21/01/19 12:44:38 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:38 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:38 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:44:38 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:44:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:44:38 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:44:38 INFO ParquetOutputFormat: Validation is off
21/01/19 12:44:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:44:38 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:44:38 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:44:38 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:44:38 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:44:38 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:44:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:44:38 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000004_5' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:44:38 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000004_5: Committed
21/01/19 12:44:38 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 2527 bytes result sent to driver
21/01/19 12:44:38 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6, test-runner-pkwzc, executor driver, partition 5, ANY, 7815 bytes)
21/01/19 12:44:38 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)
21/01/19 12:44:38 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 1197 ms on test-runner-pkwzc (executor driver) (5/200)
21/01/19 12:44:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:38 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:38 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-2a0f898d-0f6f-457e-8914-71cdcda227ee-c000.snappy.parquet, range: 0-104863438, partition values: [empty row]
21/01/19 12:44:39 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:39 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:39 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:44:39 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:44:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:44:39 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:44:39 INFO ParquetOutputFormat: Validation is off
21/01/19 12:44:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:44:39 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:44:39 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:44:39 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:44:39 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:44:39 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:44:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:44:39 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000005_6' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:44:39 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000005_6: Committed
21/01/19 12:44:39 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 2527 bytes result sent to driver
21/01/19 12:44:39 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7, test-runner-pkwzc, executor driver, partition 6, ANY, 7815 bytes)
21/01/19 12:44:39 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)
21/01/19 12:44:39 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 1160 ms on test-runner-pkwzc (executor driver) (6/200)
21/01/19 12:44:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:39 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:39 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-62608243-2950-4f47-99d7-118e0405c6c0-c000.snappy.parquet, range: 0-104863438, partition values: [empty row]
21/01/19 12:44:40 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:40 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:40 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:44:40 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:44:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:44:40 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:44:40 INFO ParquetOutputFormat: Validation is off
21/01/19 12:44:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:44:40 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:44:40 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:44:40 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:44:40 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:44:40 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:44:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:44:41 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000006_7' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:44:41 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000006_7: Committed
21/01/19 12:44:41 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2527 bytes result sent to driver
21/01/19 12:44:41 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8, test-runner-pkwzc, executor driver, partition 7, ANY, 7815 bytes)
21/01/19 12:44:41 INFO Executor: Running task 7.0 in stage 1.0 (TID 8)
21/01/19 12:44:41 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 1174 ms on test-runner-pkwzc (executor driver) (7/200)
21/01/19 12:44:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:41 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:41 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-745164e4-9f9e-4262-9ebe-b25ff779cbcf-c000.snappy.parquet, range: 0-104863438, partition values: [empty row]
21/01/19 12:44:41 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:41 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:41 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:44:41 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:44:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:44:41 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:44:41 INFO ParquetOutputFormat: Validation is off
21/01/19 12:44:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:44:41 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:44:41 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:44:41 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:44:41 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:44:41 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:44:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:44:42 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000007_8' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:44:42 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000007_8: Committed
21/01/19 12:44:42 INFO Executor: Finished task 7.0 in stage 1.0 (TID 8). 2527 bytes result sent to driver
21/01/19 12:44:42 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 9, test-runner-pkwzc, executor driver, partition 8, ANY, 7815 bytes)
21/01/19 12:44:42 INFO Executor: Running task 8.0 in stage 1.0 (TID 9)
21/01/19 12:44:42 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 1245 ms on test-runner-pkwzc (executor driver) (8/200)
21/01/19 12:44:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:42 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:42 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-a0ea959a-c937-4d55-9191-48f0033622f7-c000.snappy.parquet, range: 0-104863438, partition values: [empty row]
21/01/19 12:44:42 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:42 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:42 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:44:42 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:44:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:44:42 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:44:42 INFO ParquetOutputFormat: Validation is off
21/01/19 12:44:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:44:42 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:44:42 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:44:42 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:44:42 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:44:42 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:44:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:44:43 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000008_9' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:44:43 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000008_9: Committed
21/01/19 12:44:43 INFO Executor: Finished task 8.0 in stage 1.0 (TID 9). 2527 bytes result sent to driver
21/01/19 12:44:43 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 10, test-runner-pkwzc, executor driver, partition 9, ANY, 7815 bytes)
21/01/19 12:44:43 INFO Executor: Running task 9.0 in stage 1.0 (TID 10)
21/01/19 12:44:43 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 9) in 1135 ms on test-runner-pkwzc (executor driver) (9/200)
21/01/19 12:44:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:43 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:43 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-c16cab9e-3298-4515-8d2f-4c5ba6cf953f-c000.snappy.parquet, range: 0-104863438, partition values: [empty row]
21/01/19 12:44:44 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:44 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:44 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:44:44 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:44:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:44:44 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:44:44 INFO ParquetOutputFormat: Validation is off
21/01/19 12:44:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:44:44 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:44:44 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:44:44 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:44:44 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:44:44 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:44:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:44:44 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000009_10' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:44:44 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000009_10: Committed
21/01/19 12:44:44 INFO Executor: Finished task 9.0 in stage 1.0 (TID 10). 2527 bytes result sent to driver
21/01/19 12:44:44 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 11, test-runner-pkwzc, executor driver, partition 10, ANY, 7815 bytes)
21/01/19 12:44:44 INFO Executor: Running task 10.0 in stage 1.0 (TID 11)
21/01/19 12:44:44 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 10) in 1158 ms on test-runner-pkwzc (executor driver) (10/200)
21/01/19 12:44:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:44 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:44 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-d6100d0a-3dfc-4d0c-b36a-4d8da527ae3c-c000.snappy.parquet, range: 0-104863438, partition values: [empty row]
21/01/19 12:44:45 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:45 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:45 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:44:45 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:44:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:44:45 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:44:45 INFO ParquetOutputFormat: Validation is off
21/01/19 12:44:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:44:45 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:44:45 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:44:45 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:44:45 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:44:45 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:44:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:44:45 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000010_11' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:44:45 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000010_11: Committed
21/01/19 12:44:45 INFO Executor: Finished task 10.0 in stage 1.0 (TID 11). 2527 bytes result sent to driver
21/01/19 12:44:45 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 12, test-runner-pkwzc, executor driver, partition 11, ANY, 7815 bytes)
21/01/19 12:44:45 INFO Executor: Running task 11.0 in stage 1.0 (TID 12)
21/01/19 12:44:45 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 11) in 1256 ms on test-runner-pkwzc (executor driver) (11/200)
21/01/19 12:44:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:45 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:45 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-d6562cc6-bc69-4af0-bea0-7aa55a5b1ef3-c000.snappy.parquet, range: 0-104863438, partition values: [empty row]
21/01/19 12:44:46 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:46 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:46 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:44:46 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:44:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:44:46 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:44:46 INFO ParquetOutputFormat: Validation is off
21/01/19 12:44:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:44:46 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:44:46 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:44:46 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:44:46 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:44:46 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:44:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:44:47 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000011_12' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:44:47 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000011_12: Committed
21/01/19 12:44:47 INFO Executor: Finished task 11.0 in stage 1.0 (TID 12). 2527 bytes result sent to driver
21/01/19 12:44:47 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 13, test-runner-pkwzc, executor driver, partition 12, ANY, 7815 bytes)
21/01/19 12:44:47 INFO Executor: Running task 12.0 in stage 1.0 (TID 13)
21/01/19 12:44:47 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 12) in 1137 ms on test-runner-pkwzc (executor driver) (12/200)
21/01/19 12:44:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:47 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-e3bfc9e4-b466-4d3b-98f8-b05dcd5bb2b0-c000.snappy.parquet, range: 0-104863438, partition values: [empty row]
21/01/19 12:44:47 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:47 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:47 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:44:47 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:44:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:44:47 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:44:47 INFO ParquetOutputFormat: Validation is off
21/01/19 12:44:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:44:47 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:44:47 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:44:47 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:44:47 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:44:47 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:44:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:44:48 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000012_13' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:44:48 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000012_13: Committed
21/01/19 12:44:48 INFO Executor: Finished task 12.0 in stage 1.0 (TID 13). 2527 bytes result sent to driver
21/01/19 12:44:48 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 14, test-runner-pkwzc, executor driver, partition 13, ANY, 7815 bytes)
21/01/19 12:44:48 INFO Executor: Running task 13.0 in stage 1.0 (TID 14)
21/01/19 12:44:48 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 13) in 1207 ms on test-runner-pkwzc (executor driver) (13/200)
21/01/19 12:44:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:48 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:48 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-0354bce4-13d3-410b-92c1-96369135c8e5-c000.snappy.parquet, range: 0-104863437, partition values: [empty row]
21/01/19 12:44:48 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:48 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:48 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:44:48 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:44:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:44:48 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:44:48 INFO ParquetOutputFormat: Validation is off
21/01/19 12:44:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:44:48 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:44:48 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:44:48 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:44:48 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:44:48 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:44:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:44:49 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000013_14' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:44:49 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000013_14: Committed
21/01/19 12:44:49 INFO Executor: Finished task 13.0 in stage 1.0 (TID 14). 2527 bytes result sent to driver
21/01/19 12:44:49 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 15, test-runner-pkwzc, executor driver, partition 14, ANY, 7815 bytes)
21/01/19 12:44:49 INFO Executor: Running task 14.0 in stage 1.0 (TID 15)
21/01/19 12:44:49 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 14) in 1138 ms on test-runner-pkwzc (executor driver) (14/200)
21/01/19 12:44:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:49 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-2779aa27-2245-4ed4-96e9-f81ed9f843a8-c000.snappy.parquet, range: 0-104863437, partition values: [empty row]
21/01/19 12:44:49 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:49 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:49 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:44:49 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:44:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:44:49 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:44:49 INFO ParquetOutputFormat: Validation is off
21/01/19 12:44:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:44:49 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:44:49 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:44:49 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:44:49 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:44:49 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:44:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:44:50 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000014_15' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:44:50 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000014_15: Committed
21/01/19 12:44:50 INFO Executor: Finished task 14.0 in stage 1.0 (TID 15). 2527 bytes result sent to driver
21/01/19 12:44:50 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 16, test-runner-pkwzc, executor driver, partition 15, ANY, 7815 bytes)
21/01/19 12:44:50 INFO Executor: Running task 15.0 in stage 1.0 (TID 16)
21/01/19 12:44:50 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 15) in 1167 ms on test-runner-pkwzc (executor driver) (15/200)
21/01/19 12:44:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:50 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:50 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-6ece1f5b-8668-4ae2-8dd5-9efa8181a90e-c000.snappy.parquet, range: 0-104863437, partition values: [empty row]
21/01/19 12:44:51 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:51 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:51 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:44:51 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:44:51 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:44:51 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:44:51 INFO ParquetOutputFormat: Validation is off
21/01/19 12:44:51 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:44:51 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:44:51 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:44:51 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:44:51 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:44:51 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:44:51 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:44:51 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000015_16' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:44:51 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000015_16: Committed
21/01/19 12:44:51 INFO Executor: Finished task 15.0 in stage 1.0 (TID 16). 2527 bytes result sent to driver
21/01/19 12:44:51 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 17, test-runner-pkwzc, executor driver, partition 16, ANY, 7815 bytes)
21/01/19 12:44:51 INFO Executor: Running task 16.0 in stage 1.0 (TID 17)
21/01/19 12:44:51 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 16) in 1221 ms on test-runner-pkwzc (executor driver) (16/200)
21/01/19 12:44:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:51 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-a0eda42d-d6dc-46c4-925e-b9bba4e40c94-c000.snappy.parquet, range: 0-104863437, partition values: [empty row]
21/01/19 12:44:52 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:52 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:52 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:44:52 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:44:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:44:52 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:44:52 INFO ParquetOutputFormat: Validation is off
21/01/19 12:44:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:44:52 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:44:52 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:44:52 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:44:52 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:44:52 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:44:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:44:53 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000016_17' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:44:53 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000016_17: Committed
21/01/19 12:44:53 INFO Executor: Finished task 16.0 in stage 1.0 (TID 17). 2527 bytes result sent to driver
21/01/19 12:44:53 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 18, test-runner-pkwzc, executor driver, partition 17, ANY, 7815 bytes)
21/01/19 12:44:53 INFO Executor: Running task 17.0 in stage 1.0 (TID 18)
21/01/19 12:44:53 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 17) in 1212 ms on test-runner-pkwzc (executor driver) (17/200)
21/01/19 12:44:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:53 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-b168d065-b289-484f-a566-4a21cf0dcb4d-c000.snappy.parquet, range: 0-104863437, partition values: [empty row]
21/01/19 12:44:53 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:53 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:53 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:44:53 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:44:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:44:53 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:44:53 INFO ParquetOutputFormat: Validation is off
21/01/19 12:44:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:44:53 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:44:53 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:44:53 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:44:53 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:44:53 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:44:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:44:54 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000017_18' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:44:54 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000017_18: Committed
21/01/19 12:44:54 INFO Executor: Finished task 17.0 in stage 1.0 (TID 18). 2527 bytes result sent to driver
21/01/19 12:44:54 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 19, test-runner-pkwzc, executor driver, partition 18, ANY, 7815 bytes)
21/01/19 12:44:54 INFO Executor: Running task 18.0 in stage 1.0 (TID 19)
21/01/19 12:44:54 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 18) in 1133 ms on test-runner-pkwzc (executor driver) (18/200)
21/01/19 12:44:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:54 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:54 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-c684d21c-3c1f-4427-aef4-f443b5087555-c000.snappy.parquet, range: 0-104863437, partition values: [empty row]
21/01/19 12:44:54 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:54 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:54 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:44:54 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:44:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:44:54 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:44:54 INFO ParquetOutputFormat: Validation is off
21/01/19 12:44:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:44:54 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:44:54 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:44:54 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:44:54 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:44:54 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:44:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:44:55 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000018_19' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:44:55 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000018_19: Committed
21/01/19 12:44:55 INFO Executor: Finished task 18.0 in stage 1.0 (TID 19). 2527 bytes result sent to driver
21/01/19 12:44:55 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 20, test-runner-pkwzc, executor driver, partition 19, ANY, 7815 bytes)
21/01/19 12:44:55 INFO Executor: Running task 19.0 in stage 1.0 (TID 20)
21/01/19 12:44:55 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 19) in 1145 ms on test-runner-pkwzc (executor driver) (19/200)
21/01/19 12:44:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:55 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-ef89e86e-f972-4289-a841-f8ae5e338ed3-c000.snappy.parquet, range: 0-104863437, partition values: [empty row]
21/01/19 12:44:55 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:55 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:55 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:44:55 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:44:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:44:55 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:44:55 INFO ParquetOutputFormat: Validation is off
21/01/19 12:44:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:44:55 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:44:55 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:44:55 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:44:55 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:44:55 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:44:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:44:56 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000019_20' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:44:56 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000019_20: Committed
21/01/19 12:44:56 INFO Executor: Finished task 19.0 in stage 1.0 (TID 20). 2527 bytes result sent to driver
21/01/19 12:44:56 INFO TaskSetManager: Starting task 20.0 in stage 1.0 (TID 21, test-runner-pkwzc, executor driver, partition 20, ANY, 7815 bytes)
21/01/19 12:44:56 INFO Executor: Running task 20.0 in stage 1.0 (TID 21)
21/01/19 12:44:56 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 20) in 1172 ms on test-runner-pkwzc (executor driver) (20/200)
21/01/19 12:44:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:56 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:56 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-06793d25-22a1-48e8-a1f4-c6d80bcf066b-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:44:56 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:56 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:56 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:44:56 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:44:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:44:56 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:44:56 INFO ParquetOutputFormat: Validation is off
21/01/19 12:44:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:44:56 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:44:56 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:44:56 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:44:56 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:44:56 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:44:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:44:57 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000020_21' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:44:57 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000020_21: Committed
21/01/19 12:44:57 INFO Executor: Finished task 20.0 in stage 1.0 (TID 21). 2527 bytes result sent to driver
21/01/19 12:44:57 INFO TaskSetManager: Starting task 21.0 in stage 1.0 (TID 22, test-runner-pkwzc, executor driver, partition 21, ANY, 7815 bytes)
21/01/19 12:44:57 INFO Executor: Running task 21.0 in stage 1.0 (TID 22)
21/01/19 12:44:57 INFO TaskSetManager: Finished task 20.0 in stage 1.0 (TID 21) in 1141 ms on test-runner-pkwzc (executor driver) (21/200)
21/01/19 12:44:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:57 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:57 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-0a7c73e9-0377-47a5-9e73-6ebb77cfe1f6-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:44:58 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:58 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:44:58 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:44:58 INFO ParquetOutputFormat: Validation is off
21/01/19 12:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:44:58 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:44:58 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:44:58 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:44:58 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:44:58 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000021_22' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:44:58 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000021_22: Committed
21/01/19 12:44:58 INFO Executor: Finished task 21.0 in stage 1.0 (TID 22). 2527 bytes result sent to driver
21/01/19 12:44:58 INFO TaskSetManager: Starting task 22.0 in stage 1.0 (TID 23, test-runner-pkwzc, executor driver, partition 22, ANY, 7815 bytes)
21/01/19 12:44:58 INFO Executor: Running task 22.0 in stage 1.0 (TID 23)
21/01/19 12:44:58 INFO TaskSetManager: Finished task 21.0 in stage 1.0 (TID 22) in 1173 ms on test-runner-pkwzc (executor driver) (22/200)
21/01/19 12:44:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:58 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:58 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-0cb77c21-bd3b-4e57-9144-b57f0920fed3-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:44:59 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:59 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:44:59 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:44:59 INFO ParquetOutputFormat: Validation is off
21/01/19 12:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:44:59 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:44:59 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:44:59 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:44:59 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:44:59 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000022_23' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:44:59 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000022_23: Committed
21/01/19 12:44:59 INFO Executor: Finished task 22.0 in stage 1.0 (TID 23). 2527 bytes result sent to driver
21/01/19 12:44:59 INFO TaskSetManager: Starting task 23.0 in stage 1.0 (TID 24, test-runner-pkwzc, executor driver, partition 23, ANY, 7815 bytes)
21/01/19 12:44:59 INFO Executor: Running task 23.0 in stage 1.0 (TID 24)
21/01/19 12:44:59 INFO TaskSetManager: Finished task 22.0 in stage 1.0 (TID 23) in 1153 ms on test-runner-pkwzc (executor driver) (23/200)
21/01/19 12:44:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:59 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:44:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:44:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:44:59 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-13728d8c-46ac-4a7b-8adc-bf26485a7b4a-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:00 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:00 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:00 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:00 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:00 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:00 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:00 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:00 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:00 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:01 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000023_24' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:01 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000023_24: Committed
21/01/19 12:45:01 INFO Executor: Finished task 23.0 in stage 1.0 (TID 24). 2527 bytes result sent to driver
21/01/19 12:45:01 INFO TaskSetManager: Starting task 24.0 in stage 1.0 (TID 25, test-runner-pkwzc, executor driver, partition 24, ANY, 7815 bytes)
21/01/19 12:45:01 INFO Executor: Running task 24.0 in stage 1.0 (TID 25)
21/01/19 12:45:01 INFO TaskSetManager: Finished task 23.0 in stage 1.0 (TID 24) in 1148 ms on test-runner-pkwzc (executor driver) (24/200)
21/01/19 12:45:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:01 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:01 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-14d7bc63-bb75-4715-aac0-b78d61c78f69-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:01 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:01 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:01 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:01 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:01 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:01 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:01 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:01 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:01 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:01 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:01 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:02 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000024_25' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:02 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000024_25: Committed
21/01/19 12:45:02 INFO Executor: Finished task 24.0 in stage 1.0 (TID 25). 2527 bytes result sent to driver
21/01/19 12:45:02 INFO TaskSetManager: Starting task 25.0 in stage 1.0 (TID 26, test-runner-pkwzc, executor driver, partition 25, ANY, 7815 bytes)
21/01/19 12:45:02 INFO Executor: Running task 25.0 in stage 1.0 (TID 26)
21/01/19 12:45:02 INFO TaskSetManager: Finished task 24.0 in stage 1.0 (TID 25) in 1147 ms on test-runner-pkwzc (executor driver) (25/200)
21/01/19 12:45:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:02 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-1ee9f130-8fcc-4af4-a036-4ead4d4f8572-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:02 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:02 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:02 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:02 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:02 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:02 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:02 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:02 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:02 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:02 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:02 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:02 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:02 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:03 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000025_26' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:03 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000025_26: Committed
21/01/19 12:45:03 INFO Executor: Finished task 25.0 in stage 1.0 (TID 26). 2527 bytes result sent to driver
21/01/19 12:45:03 INFO TaskSetManager: Starting task 26.0 in stage 1.0 (TID 27, test-runner-pkwzc, executor driver, partition 26, ANY, 7815 bytes)
21/01/19 12:45:03 INFO Executor: Running task 26.0 in stage 1.0 (TID 27)
21/01/19 12:45:03 INFO TaskSetManager: Finished task 25.0 in stage 1.0 (TID 26) in 1077 ms on test-runner-pkwzc (executor driver) (26/200)
21/01/19 12:45:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:03 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-1f5049a0-97fd-4e08-9886-221be1a5e9ef-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:03 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:03 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:03 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:03 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:03 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:03 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:03 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:03 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:03 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:03 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:03 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:04 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000026_27' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:04 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000026_27: Committed
21/01/19 12:45:04 INFO Executor: Finished task 26.0 in stage 1.0 (TID 27). 2527 bytes result sent to driver
21/01/19 12:45:04 INFO TaskSetManager: Starting task 27.0 in stage 1.0 (TID 28, test-runner-pkwzc, executor driver, partition 27, ANY, 7815 bytes)
21/01/19 12:45:04 INFO Executor: Running task 27.0 in stage 1.0 (TID 28)
21/01/19 12:45:04 INFO TaskSetManager: Finished task 26.0 in stage 1.0 (TID 27) in 1191 ms on test-runner-pkwzc (executor driver) (27/200)
21/01/19 12:45:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:04 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-23bcc7b1-7b91-40ef-bbf5-4238ab707676-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:04 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:04 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:04 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:04 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:04 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:04 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:04 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:04 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:04 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:04 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:04 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:05 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000027_28' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:05 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000027_28: Committed
21/01/19 12:45:05 INFO Executor: Finished task 27.0 in stage 1.0 (TID 28). 2527 bytes result sent to driver
21/01/19 12:45:05 INFO TaskSetManager: Starting task 28.0 in stage 1.0 (TID 29, test-runner-pkwzc, executor driver, partition 28, ANY, 7815 bytes)
21/01/19 12:45:05 INFO Executor: Running task 28.0 in stage 1.0 (TID 29)
21/01/19 12:45:05 INFO TaskSetManager: Finished task 27.0 in stage 1.0 (TID 28) in 1189 ms on test-runner-pkwzc (executor driver) (28/200)
21/01/19 12:45:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:05 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:05 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-2749b922-1d0b-44ef-beb9-b8fa50d5cc23-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:06 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:06 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:06 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:06 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:06 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:06 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:06 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:06 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:06 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:06 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:06 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:06 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000028_29' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:06 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000028_29: Committed
21/01/19 12:45:06 INFO Executor: Finished task 28.0 in stage 1.0 (TID 29). 2527 bytes result sent to driver
21/01/19 12:45:06 INFO TaskSetManager: Starting task 29.0 in stage 1.0 (TID 30, test-runner-pkwzc, executor driver, partition 29, ANY, 7815 bytes)
21/01/19 12:45:06 INFO Executor: Running task 29.0 in stage 1.0 (TID 30)
21/01/19 12:45:06 INFO TaskSetManager: Finished task 28.0 in stage 1.0 (TID 29) in 1181 ms on test-runner-pkwzc (executor driver) (29/200)
21/01/19 12:45:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:06 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:06 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-2d467614-14bd-4639-801c-8a4d4d307067-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:07 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:07 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:07 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:07 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:07 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:07 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:07 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:07 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:07 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:07 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:07 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:08 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000029_30' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:08 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000029_30: Committed
21/01/19 12:45:08 INFO Executor: Finished task 29.0 in stage 1.0 (TID 30). 2527 bytes result sent to driver
21/01/19 12:45:08 INFO TaskSetManager: Starting task 30.0 in stage 1.0 (TID 31, test-runner-pkwzc, executor driver, partition 30, ANY, 7815 bytes)
21/01/19 12:45:08 INFO Executor: Running task 30.0 in stage 1.0 (TID 31)
21/01/19 12:45:08 INFO TaskSetManager: Finished task 29.0 in stage 1.0 (TID 30) in 1163 ms on test-runner-pkwzc (executor driver) (30/200)
21/01/19 12:45:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:08 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:08 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-2edb578a-7682-4d12-9ac1-280d39d0af72-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:08 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:08 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:08 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:08 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:08 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:08 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:08 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:08 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:08 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:08 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:08 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:09 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000030_31' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:09 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000030_31: Committed
21/01/19 12:45:09 INFO Executor: Finished task 30.0 in stage 1.0 (TID 31). 2527 bytes result sent to driver
21/01/19 12:45:09 INFO TaskSetManager: Starting task 31.0 in stage 1.0 (TID 32, test-runner-pkwzc, executor driver, partition 31, ANY, 7815 bytes)
21/01/19 12:45:09 INFO Executor: Running task 31.0 in stage 1.0 (TID 32)
21/01/19 12:45:09 INFO TaskSetManager: Finished task 30.0 in stage 1.0 (TID 31) in 1148 ms on test-runner-pkwzc (executor driver) (31/200)
21/01/19 12:45:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:09 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:09 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-2ef14fc5-d074-4871-893b-51d4185cb021-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:09 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:09 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:09 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:09 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:09 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:09 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:09 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:09 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:09 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:09 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:09 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:10 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000031_32' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:10 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000031_32: Committed
21/01/19 12:45:10 INFO Executor: Finished task 31.0 in stage 1.0 (TID 32). 2527 bytes result sent to driver
21/01/19 12:45:10 INFO TaskSetManager: Starting task 32.0 in stage 1.0 (TID 33, test-runner-pkwzc, executor driver, partition 32, ANY, 7815 bytes)
21/01/19 12:45:10 INFO Executor: Running task 32.0 in stage 1.0 (TID 33)
21/01/19 12:45:10 INFO TaskSetManager: Finished task 31.0 in stage 1.0 (TID 32) in 1125 ms on test-runner-pkwzc (executor driver) (32/200)
21/01/19 12:45:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:10 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:10 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-33ff67e7-864b-4d22-b06e-5ee71ba62a14-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:10 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:10 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:10 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:10 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:10 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:10 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:10 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:10 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:10 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:10 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:10 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:11 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000032_33' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:11 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000032_33: Committed
21/01/19 12:45:11 INFO Executor: Finished task 32.0 in stage 1.0 (TID 33). 2527 bytes result sent to driver
21/01/19 12:45:11 INFO TaskSetManager: Starting task 33.0 in stage 1.0 (TID 34, test-runner-pkwzc, executor driver, partition 33, ANY, 7815 bytes)
21/01/19 12:45:11 INFO Executor: Running task 33.0 in stage 1.0 (TID 34)
21/01/19 12:45:11 INFO TaskSetManager: Finished task 32.0 in stage 1.0 (TID 33) in 1157 ms on test-runner-pkwzc (executor driver) (33/200)
21/01/19 12:45:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:11 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-3784dad9-3cb4-41e2-b18b-1fe92b576d82-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:11 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:11 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:11 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:11 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:11 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:11 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:11 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:11 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:11 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:11 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:11 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:12 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000033_34' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:12 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000033_34: Committed
21/01/19 12:45:12 INFO Executor: Finished task 33.0 in stage 1.0 (TID 34). 2527 bytes result sent to driver
21/01/19 12:45:12 INFO TaskSetManager: Starting task 34.0 in stage 1.0 (TID 35, test-runner-pkwzc, executor driver, partition 34, ANY, 7815 bytes)
21/01/19 12:45:12 INFO Executor: Running task 34.0 in stage 1.0 (TID 35)
21/01/19 12:45:12 INFO TaskSetManager: Finished task 33.0 in stage 1.0 (TID 34) in 1104 ms on test-runner-pkwzc (executor driver) (34/200)
21/01/19 12:45:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:12 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:12 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-3ac69452-a21a-44ab-a5fe-b2d9af874241-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:12 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:12 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:12 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:12 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:12 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:12 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:12 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:12 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:12 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:12 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:12 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:13 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000034_35' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:13 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000034_35: Committed
21/01/19 12:45:13 INFO Executor: Finished task 34.0 in stage 1.0 (TID 35). 2527 bytes result sent to driver
21/01/19 12:45:13 INFO TaskSetManager: Starting task 35.0 in stage 1.0 (TID 36, test-runner-pkwzc, executor driver, partition 35, ANY, 7815 bytes)
21/01/19 12:45:13 INFO Executor: Running task 35.0 in stage 1.0 (TID 36)
21/01/19 12:45:13 INFO TaskSetManager: Finished task 34.0 in stage 1.0 (TID 35) in 1101 ms on test-runner-pkwzc (executor driver) (35/200)
21/01/19 12:45:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:13 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:13 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-3f2df710-a4a8-489f-8d48-1f75b2e94dbf-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:14 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:14 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:14 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:14 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:14 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:14 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:14 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:14 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:14 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:14 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:14 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:14 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000035_36' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:14 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000035_36: Committed
21/01/19 12:45:14 INFO Executor: Finished task 35.0 in stage 1.0 (TID 36). 2527 bytes result sent to driver
21/01/19 12:45:14 INFO TaskSetManager: Starting task 36.0 in stage 1.0 (TID 37, test-runner-pkwzc, executor driver, partition 36, ANY, 7815 bytes)
21/01/19 12:45:14 INFO Executor: Running task 36.0 in stage 1.0 (TID 37)
21/01/19 12:45:14 INFO TaskSetManager: Finished task 35.0 in stage 1.0 (TID 36) in 1264 ms on test-runner-pkwzc (executor driver) (36/200)
21/01/19 12:45:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:14 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:14 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-50dd480f-3535-4003-8e37-dbb4f3393402-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:15 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:15 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:15 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:15 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:15 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:15 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:15 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:15 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:15 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:15 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:15 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000036_37' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:16 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000036_37: Committed
21/01/19 12:45:16 INFO Executor: Finished task 36.0 in stage 1.0 (TID 37). 2527 bytes result sent to driver
21/01/19 12:45:16 INFO TaskSetManager: Starting task 37.0 in stage 1.0 (TID 38, test-runner-pkwzc, executor driver, partition 37, ANY, 7815 bytes)
21/01/19 12:45:16 INFO Executor: Running task 37.0 in stage 1.0 (TID 38)
21/01/19 12:45:16 INFO TaskSetManager: Finished task 36.0 in stage 1.0 (TID 37) in 1132 ms on test-runner-pkwzc (executor driver) (37/200)
21/01/19 12:45:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:16 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:16 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-553f51eb-ab71-45e9-a001-79177ba30c3c-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:16 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:16 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:16 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:16 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:16 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:16 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:16 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:16 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:16 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:17 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000037_38' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:17 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000037_38: Committed
21/01/19 12:45:17 INFO Executor: Finished task 37.0 in stage 1.0 (TID 38). 2527 bytes result sent to driver
21/01/19 12:45:17 INFO TaskSetManager: Starting task 38.0 in stage 1.0 (TID 39, test-runner-pkwzc, executor driver, partition 38, ANY, 7815 bytes)
21/01/19 12:45:17 INFO Executor: Running task 38.0 in stage 1.0 (TID 39)
21/01/19 12:45:17 INFO TaskSetManager: Finished task 37.0 in stage 1.0 (TID 38) in 1148 ms on test-runner-pkwzc (executor driver) (38/200)
21/01/19 12:45:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:17 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:17 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-5a24c4b0-e1c5-48ba-9480-1d1c80fa4f02-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:17 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:17 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:17 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:17 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:17 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:17 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:17 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:17 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:17 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000038_39' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:18 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000038_39: Committed
21/01/19 12:45:18 INFO Executor: Finished task 38.0 in stage 1.0 (TID 39). 2527 bytes result sent to driver
21/01/19 12:45:18 INFO TaskSetManager: Starting task 39.0 in stage 1.0 (TID 40, test-runner-pkwzc, executor driver, partition 39, ANY, 7815 bytes)
21/01/19 12:45:18 INFO Executor: Running task 39.0 in stage 1.0 (TID 40)
21/01/19 12:45:18 INFO TaskSetManager: Finished task 38.0 in stage 1.0 (TID 39) in 1154 ms on test-runner-pkwzc (executor driver) (39/200)
21/01/19 12:45:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:18 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:18 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-5c23de30-a25d-432d-945c-c8acc5589e63-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:18 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:18 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:18 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:18 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:18 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:18 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:18 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:18 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:18 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000039_40' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:19 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000039_40: Committed
21/01/19 12:45:19 INFO Executor: Finished task 39.0 in stage 1.0 (TID 40). 2527 bytes result sent to driver
21/01/19 12:45:19 INFO TaskSetManager: Starting task 40.0 in stage 1.0 (TID 41, test-runner-pkwzc, executor driver, partition 40, ANY, 7815 bytes)
21/01/19 12:45:19 INFO Executor: Running task 40.0 in stage 1.0 (TID 41)
21/01/19 12:45:19 INFO TaskSetManager: Finished task 39.0 in stage 1.0 (TID 40) in 1105 ms on test-runner-pkwzc (executor driver) (40/200)
21/01/19 12:45:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:19 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:19 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-5cdf7619-08e1-417b-8f4c-5c52d1898bd6-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:19 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:19 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:19 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:19 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:19 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:19 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:19 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:19 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:19 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:20 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000040_41' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:20 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000040_41: Committed
21/01/19 12:45:20 INFO Executor: Finished task 40.0 in stage 1.0 (TID 41). 2527 bytes result sent to driver
21/01/19 12:45:20 INFO TaskSetManager: Starting task 41.0 in stage 1.0 (TID 42, test-runner-pkwzc, executor driver, partition 41, ANY, 7815 bytes)
21/01/19 12:45:20 INFO Executor: Running task 41.0 in stage 1.0 (TID 42)
21/01/19 12:45:20 INFO TaskSetManager: Finished task 40.0 in stage 1.0 (TID 41) in 1312 ms on test-runner-pkwzc (executor driver) (41/200)
21/01/19 12:45:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:20 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:20 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-5faef0e3-1603-45e5-9cad-5b96f0f874eb-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:21 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:21 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:21 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:21 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:21 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:21 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:21 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:21 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:21 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:21 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:21 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:21 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000041_42' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:21 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000041_42: Committed
21/01/19 12:45:21 INFO Executor: Finished task 41.0 in stage 1.0 (TID 42). 2527 bytes result sent to driver
21/01/19 12:45:21 INFO TaskSetManager: Starting task 42.0 in stage 1.0 (TID 43, test-runner-pkwzc, executor driver, partition 42, ANY, 7815 bytes)
21/01/19 12:45:21 INFO Executor: Running task 42.0 in stage 1.0 (TID 43)
21/01/19 12:45:21 INFO TaskSetManager: Finished task 41.0 in stage 1.0 (TID 42) in 1145 ms on test-runner-pkwzc (executor driver) (42/200)
21/01/19 12:45:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:21 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:21 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-61ba731f-b51b-471f-927b-cb37943b9881-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:22 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:22 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:22 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:22 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:22 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:22 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:22 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:22 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:22 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:22 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:22 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:23 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000042_43' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:23 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000042_43: Committed
21/01/19 12:45:23 INFO Executor: Finished task 42.0 in stage 1.0 (TID 43). 2527 bytes result sent to driver
21/01/19 12:45:23 INFO TaskSetManager: Starting task 43.0 in stage 1.0 (TID 44, test-runner-pkwzc, executor driver, partition 43, ANY, 7815 bytes)
21/01/19 12:45:23 INFO Executor: Running task 43.0 in stage 1.0 (TID 44)
21/01/19 12:45:23 INFO TaskSetManager: Finished task 42.0 in stage 1.0 (TID 43) in 1105 ms on test-runner-pkwzc (executor driver) (43/200)
21/01/19 12:45:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:23 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:23 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-62b9d41b-7ea7-4560-95ce-e8a000b13524-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:23 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:23 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:23 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:23 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:23 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:23 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:23 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:23 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:23 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:23 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:23 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:24 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000043_44' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:24 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000043_44: Committed
21/01/19 12:45:24 INFO Executor: Finished task 43.0 in stage 1.0 (TID 44). 2527 bytes result sent to driver
21/01/19 12:45:24 INFO TaskSetManager: Starting task 44.0 in stage 1.0 (TID 45, test-runner-pkwzc, executor driver, partition 44, ANY, 7815 bytes)
21/01/19 12:45:24 INFO Executor: Running task 44.0 in stage 1.0 (TID 45)
21/01/19 12:45:24 INFO TaskSetManager: Finished task 43.0 in stage 1.0 (TID 44) in 1112 ms on test-runner-pkwzc (executor driver) (44/200)
21/01/19 12:45:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:24 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:24 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-656d19f7-b108-4ae8-82d4-334f749d9e0c-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:24 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:24 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:24 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:24 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:24 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:24 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:24 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:24 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:24 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:24 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:24 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:25 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000044_45' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:25 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000044_45: Committed
21/01/19 12:45:25 INFO Executor: Finished task 44.0 in stage 1.0 (TID 45). 2527 bytes result sent to driver
21/01/19 12:45:25 INFO TaskSetManager: Starting task 45.0 in stage 1.0 (TID 46, test-runner-pkwzc, executor driver, partition 45, ANY, 7815 bytes)
21/01/19 12:45:25 INFO Executor: Running task 45.0 in stage 1.0 (TID 46)
21/01/19 12:45:25 INFO TaskSetManager: Finished task 44.0 in stage 1.0 (TID 45) in 1152 ms on test-runner-pkwzc (executor driver) (45/200)
21/01/19 12:45:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:25 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:25 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-6a7e8aae-92e3-4094-b354-24a9e569b845-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:25 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:25 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:25 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:25 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:25 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:25 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:25 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:25 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:25 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:25 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:25 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:26 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000045_46' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:26 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000045_46: Committed
21/01/19 12:45:26 INFO Executor: Finished task 45.0 in stage 1.0 (TID 46). 2527 bytes result sent to driver
21/01/19 12:45:26 INFO TaskSetManager: Starting task 46.0 in stage 1.0 (TID 47, test-runner-pkwzc, executor driver, partition 46, ANY, 7815 bytes)
21/01/19 12:45:26 INFO Executor: Running task 46.0 in stage 1.0 (TID 47)
21/01/19 12:45:26 INFO TaskSetManager: Finished task 45.0 in stage 1.0 (TID 46) in 1179 ms on test-runner-pkwzc (executor driver) (46/200)
21/01/19 12:45:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:26 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:26 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-6cbb45a0-df7e-4929-acf5-7fc20da40152-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:26 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:26 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:26 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:26 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:26 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:26 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:26 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:26 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:26 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:26 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:26 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:27 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000046_47' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:27 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000046_47: Committed
21/01/19 12:45:27 INFO Executor: Finished task 46.0 in stage 1.0 (TID 47). 2527 bytes result sent to driver
21/01/19 12:45:27 INFO TaskSetManager: Starting task 47.0 in stage 1.0 (TID 48, test-runner-pkwzc, executor driver, partition 47, ANY, 7815 bytes)
21/01/19 12:45:27 INFO Executor: Running task 47.0 in stage 1.0 (TID 48)
21/01/19 12:45:27 INFO TaskSetManager: Finished task 46.0 in stage 1.0 (TID 47) in 1159 ms on test-runner-pkwzc (executor driver) (47/200)
21/01/19 12:45:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:27 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:27 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-6faed507-1efa-4770-9b21-c61006afdac0-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:28 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:28 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:28 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:28 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:28 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:28 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:28 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:28 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:28 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:28 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:28 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:28 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000047_48' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:28 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000047_48: Committed
21/01/19 12:45:28 INFO Executor: Finished task 47.0 in stage 1.0 (TID 48). 2527 bytes result sent to driver
21/01/19 12:45:28 INFO TaskSetManager: Starting task 48.0 in stage 1.0 (TID 49, test-runner-pkwzc, executor driver, partition 48, ANY, 7815 bytes)
21/01/19 12:45:28 INFO Executor: Running task 48.0 in stage 1.0 (TID 49)
21/01/19 12:45:28 INFO TaskSetManager: Finished task 47.0 in stage 1.0 (TID 48) in 1134 ms on test-runner-pkwzc (executor driver) (48/200)
21/01/19 12:45:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:28 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:28 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-70013753-d86b-4c1c-80e1-22317d511189-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:29 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:29 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:29 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:29 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:29 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:29 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:29 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:29 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:29 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:29 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:29 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:29 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000048_49' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:29 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000048_49: Committed
21/01/19 12:45:29 INFO Executor: Finished task 48.0 in stage 1.0 (TID 49). 2527 bytes result sent to driver
21/01/19 12:45:29 INFO TaskSetManager: Starting task 49.0 in stage 1.0 (TID 50, test-runner-pkwzc, executor driver, partition 49, ANY, 7815 bytes)
21/01/19 12:45:29 INFO Executor: Running task 49.0 in stage 1.0 (TID 50)
21/01/19 12:45:29 INFO TaskSetManager: Finished task 48.0 in stage 1.0 (TID 49) in 1114 ms on test-runner-pkwzc (executor driver) (49/200)
21/01/19 12:45:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:29 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-7ab06e0c-cf36-4a7e-9fc1-ae62d8a6c75d-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:30 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:30 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:30 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:30 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:30 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:30 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:30 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:30 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:30 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:30 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:30 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:31 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000049_50' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:31 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000049_50: Committed
21/01/19 12:45:31 INFO Executor: Finished task 49.0 in stage 1.0 (TID 50). 2527 bytes result sent to driver
21/01/19 12:45:31 INFO TaskSetManager: Starting task 50.0 in stage 1.0 (TID 51, test-runner-pkwzc, executor driver, partition 50, ANY, 7815 bytes)
21/01/19 12:45:31 INFO Executor: Running task 50.0 in stage 1.0 (TID 51)
21/01/19 12:45:31 INFO TaskSetManager: Finished task 49.0 in stage 1.0 (TID 50) in 1201 ms on test-runner-pkwzc (executor driver) (50/200)
21/01/19 12:45:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:31 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-7fbc4115-8e5e-4c80-b8c6-86e8d3a3421f-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:31 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:31 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:31 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:31 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:31 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:31 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:31 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:31 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:31 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:31 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:31 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:32 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000050_51' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:32 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000050_51: Committed
21/01/19 12:45:32 INFO Executor: Finished task 50.0 in stage 1.0 (TID 51). 2527 bytes result sent to driver
21/01/19 12:45:32 INFO TaskSetManager: Starting task 51.0 in stage 1.0 (TID 52, test-runner-pkwzc, executor driver, partition 51, ANY, 7815 bytes)
21/01/19 12:45:32 INFO Executor: Running task 51.0 in stage 1.0 (TID 52)
21/01/19 12:45:32 INFO TaskSetManager: Finished task 50.0 in stage 1.0 (TID 51) in 1558 ms on test-runner-pkwzc (executor driver) (51/200)
21/01/19 12:45:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:32 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:32 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-96e31860-e51c-4479-87f3-29fa1854824e-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:33 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:33 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:33 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:33 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:33 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:33 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:33 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:33 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:33 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:33 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:33 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:33 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000051_52' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:33 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000051_52: Committed
21/01/19 12:45:33 INFO Executor: Finished task 51.0 in stage 1.0 (TID 52). 2527 bytes result sent to driver
21/01/19 12:45:33 INFO TaskSetManager: Starting task 52.0 in stage 1.0 (TID 53, test-runner-pkwzc, executor driver, partition 52, ANY, 7815 bytes)
21/01/19 12:45:33 INFO Executor: Running task 52.0 in stage 1.0 (TID 53)
21/01/19 12:45:33 INFO TaskSetManager: Finished task 51.0 in stage 1.0 (TID 52) in 1149 ms on test-runner-pkwzc (executor driver) (52/200)
21/01/19 12:45:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:33 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:33 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-9781534b-8e47-4256-87da-7bd54e843f1b-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:34 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:34 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:34 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:34 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:34 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:34 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:34 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:34 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:34 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:34 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:34 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:34 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:34 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:34 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000052_53' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:34 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000052_53: Committed
21/01/19 12:45:34 INFO Executor: Finished task 52.0 in stage 1.0 (TID 53). 2527 bytes result sent to driver
21/01/19 12:45:34 INFO TaskSetManager: Starting task 53.0 in stage 1.0 (TID 54, test-runner-pkwzc, executor driver, partition 53, ANY, 7815 bytes)
21/01/19 12:45:34 INFO Executor: Running task 53.0 in stage 1.0 (TID 54)
21/01/19 12:45:34 INFO TaskSetManager: Finished task 52.0 in stage 1.0 (TID 53) in 1162 ms on test-runner-pkwzc (executor driver) (53/200)
21/01/19 12:45:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:34 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:34 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-9f77cd15-ed72-45e6-84a4-e0daa01b1b44-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:35 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:35 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:35 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:35 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:35 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:35 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:35 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:35 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:35 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:35 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:35 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:36 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000053_54' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:36 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000053_54: Committed
21/01/19 12:45:36 INFO Executor: Finished task 53.0 in stage 1.0 (TID 54). 2527 bytes result sent to driver
21/01/19 12:45:36 INFO TaskSetManager: Starting task 54.0 in stage 1.0 (TID 55, test-runner-pkwzc, executor driver, partition 54, ANY, 7815 bytes)
21/01/19 12:45:36 INFO Executor: Running task 54.0 in stage 1.0 (TID 55)
21/01/19 12:45:36 INFO TaskSetManager: Finished task 53.0 in stage 1.0 (TID 54) in 1155 ms on test-runner-pkwzc (executor driver) (54/200)
21/01/19 12:45:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:36 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:36 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-a33faf02-ef70-4049-b5e6-30202ac95a9a-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:36 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:36 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:36 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:36 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:36 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:36 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:36 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:36 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:36 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:36 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:36 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:36 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:36 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:36 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:37 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000054_55' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:37 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000054_55: Committed
21/01/19 12:45:37 INFO Executor: Finished task 54.0 in stage 1.0 (TID 55). 2527 bytes result sent to driver
21/01/19 12:45:37 INFO TaskSetManager: Starting task 55.0 in stage 1.0 (TID 56, test-runner-pkwzc, executor driver, partition 55, ANY, 7815 bytes)
21/01/19 12:45:37 INFO Executor: Running task 55.0 in stage 1.0 (TID 56)
21/01/19 12:45:37 INFO TaskSetManager: Finished task 54.0 in stage 1.0 (TID 55) in 1072 ms on test-runner-pkwzc (executor driver) (55/200)
21/01/19 12:45:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:37 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:37 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-a86a18a0-e199-455d-9a84-4773e5a5572e-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:37 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:37 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:37 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:37 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:37 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:37 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:37 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:37 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:37 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:37 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:37 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:37 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:38 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000055_56' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:38 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000055_56: Committed
21/01/19 12:45:38 INFO Executor: Finished task 55.0 in stage 1.0 (TID 56). 2527 bytes result sent to driver
21/01/19 12:45:38 INFO TaskSetManager: Starting task 56.0 in stage 1.0 (TID 57, test-runner-pkwzc, executor driver, partition 56, ANY, 7815 bytes)
21/01/19 12:45:38 INFO Executor: Running task 56.0 in stage 1.0 (TID 57)
21/01/19 12:45:38 INFO TaskSetManager: Finished task 55.0 in stage 1.0 (TID 56) in 1238 ms on test-runner-pkwzc (executor driver) (56/200)
21/01/19 12:45:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:38 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:38 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-ae919f8b-9865-4421-b50d-2ebce5dbc778-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:38 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:38 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:38 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:38 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:38 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:38 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:38 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:38 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:38 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:38 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:38 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:39 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000056_57' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:39 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000056_57: Committed
21/01/19 12:45:39 INFO Executor: Finished task 56.0 in stage 1.0 (TID 57). 2527 bytes result sent to driver
21/01/19 12:45:39 INFO TaskSetManager: Starting task 57.0 in stage 1.0 (TID 58, test-runner-pkwzc, executor driver, partition 57, ANY, 7815 bytes)
21/01/19 12:45:39 INFO Executor: Running task 57.0 in stage 1.0 (TID 58)
21/01/19 12:45:39 INFO TaskSetManager: Finished task 56.0 in stage 1.0 (TID 57) in 1067 ms on test-runner-pkwzc (executor driver) (57/200)
21/01/19 12:45:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:39 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:39 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-b0415463-232e-4409-bbd8-0e771d00b684-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:39 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:39 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:39 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:39 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:39 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:39 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:39 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:39 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:39 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:39 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:39 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:40 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000057_58' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:40 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000057_58: Committed
21/01/19 12:45:40 INFO Executor: Finished task 57.0 in stage 1.0 (TID 58). 2527 bytes result sent to driver
21/01/19 12:45:40 INFO TaskSetManager: Starting task 58.0 in stage 1.0 (TID 59, test-runner-pkwzc, executor driver, partition 58, ANY, 7815 bytes)
21/01/19 12:45:40 INFO Executor: Running task 58.0 in stage 1.0 (TID 59)
21/01/19 12:45:40 INFO TaskSetManager: Finished task 57.0 in stage 1.0 (TID 58) in 1123 ms on test-runner-pkwzc (executor driver) (58/200)
21/01/19 12:45:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:40 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:40 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-bddd0be4-fad3-4d65-a0b0-484008b03142-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:41 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:41 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:41 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:41 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:41 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:41 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:41 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:41 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:41 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:41 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:41 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:41 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000058_59' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:41 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000058_59: Committed
21/01/19 12:45:41 INFO Executor: Finished task 58.0 in stage 1.0 (TID 59). 2527 bytes result sent to driver
21/01/19 12:45:41 INFO TaskSetManager: Starting task 59.0 in stage 1.0 (TID 60, test-runner-pkwzc, executor driver, partition 59, ANY, 7815 bytes)
21/01/19 12:45:41 INFO Executor: Running task 59.0 in stage 1.0 (TID 60)
21/01/19 12:45:41 INFO TaskSetManager: Finished task 58.0 in stage 1.0 (TID 59) in 1119 ms on test-runner-pkwzc (executor driver) (59/200)
21/01/19 12:45:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:41 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:41 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-bfe30963-8ccb-4b7f-953c-0e254572b12c-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:42 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:42 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:42 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:42 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:42 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:42 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:42 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:42 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:42 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:42 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:42 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:42 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000059_60' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:42 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000059_60: Committed
21/01/19 12:45:42 INFO Executor: Finished task 59.0 in stage 1.0 (TID 60). 2527 bytes result sent to driver
21/01/19 12:45:42 INFO TaskSetManager: Starting task 60.0 in stage 1.0 (TID 61, test-runner-pkwzc, executor driver, partition 60, ANY, 7815 bytes)
21/01/19 12:45:42 INFO Executor: Running task 60.0 in stage 1.0 (TID 61)
21/01/19 12:45:42 INFO TaskSetManager: Finished task 59.0 in stage 1.0 (TID 60) in 1112 ms on test-runner-pkwzc (executor driver) (60/200)
21/01/19 12:45:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:42 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:42 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-c42bfd05-80f1-4edf-aa81-7f04608da015-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:43 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:43 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:43 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:43 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:43 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:43 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:43 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:43 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:43 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:43 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:43 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:44 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000060_61' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:44 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000060_61: Committed
21/01/19 12:45:44 INFO Executor: Finished task 60.0 in stage 1.0 (TID 61). 2527 bytes result sent to driver
21/01/19 12:45:44 INFO TaskSetManager: Starting task 61.0 in stage 1.0 (TID 62, test-runner-pkwzc, executor driver, partition 61, ANY, 7815 bytes)
21/01/19 12:45:44 INFO Executor: Running task 61.0 in stage 1.0 (TID 62)
21/01/19 12:45:44 INFO TaskSetManager: Finished task 60.0 in stage 1.0 (TID 61) in 1315 ms on test-runner-pkwzc (executor driver) (61/200)
21/01/19 12:45:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:44 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:44 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-cb5cd67f-c35a-457c-9815-10573735ea03-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:44 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:44 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:44 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:44 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:44 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:44 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:44 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:44 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:44 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:44 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:44 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:45 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000061_62' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:45 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000061_62: Committed
21/01/19 12:45:45 INFO Executor: Finished task 61.0 in stage 1.0 (TID 62). 2527 bytes result sent to driver
21/01/19 12:45:45 INFO TaskSetManager: Starting task 62.0 in stage 1.0 (TID 63, test-runner-pkwzc, executor driver, partition 62, ANY, 7815 bytes)
21/01/19 12:45:45 INFO Executor: Running task 62.0 in stage 1.0 (TID 63)
21/01/19 12:45:45 INFO TaskSetManager: Finished task 61.0 in stage 1.0 (TID 62) in 1163 ms on test-runner-pkwzc (executor driver) (62/200)
21/01/19 12:45:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:45 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:45 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-dc50d141-29da-43ec-8bc1-7d4245ffa128-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:45 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:45 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:45 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:45 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:45 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:45 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:45 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:45 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:45 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:45 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:45 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:46 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000062_63' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:46 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000062_63: Committed
21/01/19 12:45:46 INFO Executor: Finished task 62.0 in stage 1.0 (TID 63). 2527 bytes result sent to driver
21/01/19 12:45:46 INFO TaskSetManager: Starting task 63.0 in stage 1.0 (TID 64, test-runner-pkwzc, executor driver, partition 63, ANY, 7815 bytes)
21/01/19 12:45:46 INFO Executor: Running task 63.0 in stage 1.0 (TID 64)
21/01/19 12:45:46 INFO TaskSetManager: Finished task 62.0 in stage 1.0 (TID 63) in 1143 ms on test-runner-pkwzc (executor driver) (63/200)
21/01/19 12:45:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:46 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:46 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-deb9a1a0-2c68-4922-828d-e0cbe187fb6c-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:46 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:46 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:46 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:46 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:46 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:46 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:46 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:46 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:46 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:46 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:46 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:47 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000063_64' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:47 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000063_64: Committed
21/01/19 12:45:47 INFO Executor: Finished task 63.0 in stage 1.0 (TID 64). 2527 bytes result sent to driver
21/01/19 12:45:47 INFO TaskSetManager: Starting task 64.0 in stage 1.0 (TID 65, test-runner-pkwzc, executor driver, partition 64, ANY, 7815 bytes)
21/01/19 12:45:47 INFO Executor: Running task 64.0 in stage 1.0 (TID 65)
21/01/19 12:45:47 INFO TaskSetManager: Finished task 63.0 in stage 1.0 (TID 64) in 1076 ms on test-runner-pkwzc (executor driver) (64/200)
21/01/19 12:45:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:47 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-e13f38d6-3e80-4767-a2bc-3a02b9c21256-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:47 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:47 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:47 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:47 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:47 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:47 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:47 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:47 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:47 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:47 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:47 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:48 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000064_65' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:48 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000064_65: Committed
21/01/19 12:45:48 INFO Executor: Finished task 64.0 in stage 1.0 (TID 65). 2527 bytes result sent to driver
21/01/19 12:45:48 INFO TaskSetManager: Starting task 65.0 in stage 1.0 (TID 66, test-runner-pkwzc, executor driver, partition 65, ANY, 7815 bytes)
21/01/19 12:45:48 INFO Executor: Running task 65.0 in stage 1.0 (TID 66)
21/01/19 12:45:48 INFO TaskSetManager: Finished task 64.0 in stage 1.0 (TID 65) in 1141 ms on test-runner-pkwzc (executor driver) (65/200)
21/01/19 12:45:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:48 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:48 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-e2b010ba-c9a5-4a8f-9412-4a245a176a93-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:49 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:49 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:49 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:49 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:49 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:49 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:49 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:49 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:49 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:49 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:49 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:49 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000065_66' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:49 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000065_66: Committed
21/01/19 12:45:49 INFO Executor: Finished task 65.0 in stage 1.0 (TID 66). 2527 bytes result sent to driver
21/01/19 12:45:49 INFO TaskSetManager: Starting task 66.0 in stage 1.0 (TID 67, test-runner-pkwzc, executor driver, partition 66, ANY, 7815 bytes)
21/01/19 12:45:49 INFO Executor: Running task 66.0 in stage 1.0 (TID 67)
21/01/19 12:45:49 INFO TaskSetManager: Finished task 65.0 in stage 1.0 (TID 66) in 1171 ms on test-runner-pkwzc (executor driver) (66/200)
21/01/19 12:45:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:49 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-e4e00720-247e-4bb0-ba19-ed5a87aade1c-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:50 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:50 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:50 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:50 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:50 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:50 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:50 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:50 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:50 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:50 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:50 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:50 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:50 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:50 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:51 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000066_67' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:51 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000066_67: Committed
21/01/19 12:45:51 INFO Executor: Finished task 66.0 in stage 1.0 (TID 67). 2527 bytes result sent to driver
21/01/19 12:45:51 INFO TaskSetManager: Starting task 67.0 in stage 1.0 (TID 68, test-runner-pkwzc, executor driver, partition 67, ANY, 7815 bytes)
21/01/19 12:45:51 INFO Executor: Running task 67.0 in stage 1.0 (TID 68)
21/01/19 12:45:51 INFO TaskSetManager: Finished task 66.0 in stage 1.0 (TID 67) in 1282 ms on test-runner-pkwzc (executor driver) (67/200)
21/01/19 12:45:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:51 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-e84c4b89-a671-48da-82f6-383e3271e2a9-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:51 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:51 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:51 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:51 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:51 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:51 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:51 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:51 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:51 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:51 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:51 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:51 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:51 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:51 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:52 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000067_68' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:52 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000067_68: Committed
21/01/19 12:45:52 INFO Executor: Finished task 67.0 in stage 1.0 (TID 68). 2527 bytes result sent to driver
21/01/19 12:45:52 INFO TaskSetManager: Starting task 68.0 in stage 1.0 (TID 69, test-runner-pkwzc, executor driver, partition 68, ANY, 7815 bytes)
21/01/19 12:45:52 INFO Executor: Running task 68.0 in stage 1.0 (TID 69)
21/01/19 12:45:52 INFO TaskSetManager: Finished task 67.0 in stage 1.0 (TID 68) in 1058 ms on test-runner-pkwzc (executor driver) (68/200)
21/01/19 12:45:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:52 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:52 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-f6d0f407-bb68-414e-b75c-7ffa145c1610-c000.snappy.parquet, range: 0-104863436, partition values: [empty row]
21/01/19 12:45:52 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:52 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:52 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:52 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:52 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:52 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:52 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:52 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:52 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:52 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:52 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:53 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000068_69' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:53 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000068_69: Committed
21/01/19 12:45:53 INFO Executor: Finished task 68.0 in stage 1.0 (TID 69). 2527 bytes result sent to driver
21/01/19 12:45:53 INFO TaskSetManager: Starting task 69.0 in stage 1.0 (TID 70, test-runner-pkwzc, executor driver, partition 69, ANY, 7815 bytes)
21/01/19 12:45:53 INFO Executor: Running task 69.0 in stage 1.0 (TID 70)
21/01/19 12:45:53 INFO TaskSetManager: Finished task 68.0 in stage 1.0 (TID 69) in 1099 ms on test-runner-pkwzc (executor driver) (69/200)
21/01/19 12:45:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:53 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-0909d2f8-f10b-4ab6-bcb6-54ef7c70a370-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/19 12:45:53 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:53 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:53 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:53 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:53 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:53 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:53 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:53 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:53 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:53 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:53 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:54 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000069_70' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:54 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000069_70: Committed
21/01/19 12:45:54 INFO Executor: Finished task 69.0 in stage 1.0 (TID 70). 2527 bytes result sent to driver
21/01/19 12:45:54 INFO TaskSetManager: Starting task 70.0 in stage 1.0 (TID 71, test-runner-pkwzc, executor driver, partition 70, ANY, 7815 bytes)
21/01/19 12:45:54 INFO Executor: Running task 70.0 in stage 1.0 (TID 71)
21/01/19 12:45:54 INFO TaskSetManager: Finished task 69.0 in stage 1.0 (TID 70) in 1279 ms on test-runner-pkwzc (executor driver) (70/200)
21/01/19 12:45:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:54 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:54 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-1a54b65c-cffc-4305-b4fa-cea9785ada6b-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/19 12:45:54 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:54 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:54 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:54 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:54 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:54 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:54 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:54 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:54 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:54 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:54 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:55 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000070_71' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:55 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000070_71: Committed
21/01/19 12:45:55 INFO Executor: Finished task 70.0 in stage 1.0 (TID 71). 2527 bytes result sent to driver
21/01/19 12:45:55 INFO TaskSetManager: Starting task 71.0 in stage 1.0 (TID 72, test-runner-pkwzc, executor driver, partition 71, ANY, 7815 bytes)
21/01/19 12:45:55 INFO Executor: Running task 71.0 in stage 1.0 (TID 72)
21/01/19 12:45:55 INFO TaskSetManager: Finished task 70.0 in stage 1.0 (TID 71) in 1246 ms on test-runner-pkwzc (executor driver) (71/200)
21/01/19 12:45:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:55 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-260ca17a-9354-459e-af22-95c69cdc7258-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/19 12:45:56 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:56 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:56 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:56 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:56 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:56 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:56 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:56 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:56 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:56 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:56 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:56 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000071_72' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:56 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000071_72: Committed
21/01/19 12:45:56 INFO Executor: Finished task 71.0 in stage 1.0 (TID 72). 2527 bytes result sent to driver
21/01/19 12:45:56 INFO TaskSetManager: Starting task 72.0 in stage 1.0 (TID 73, test-runner-pkwzc, executor driver, partition 72, ANY, 7815 bytes)
21/01/19 12:45:56 INFO Executor: Running task 72.0 in stage 1.0 (TID 73)
21/01/19 12:45:56 INFO TaskSetManager: Finished task 71.0 in stage 1.0 (TID 72) in 1090 ms on test-runner-pkwzc (executor driver) (72/200)
21/01/19 12:45:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:56 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:56 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-36a31152-cb4a-485d-ba1f-4151669418bf-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/19 12:45:57 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:57 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:57 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:57 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:57 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:57 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:57 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:57 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:57 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:57 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:57 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:58 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000072_73' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:58 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000072_73: Committed
21/01/19 12:45:58 INFO Executor: Finished task 72.0 in stage 1.0 (TID 73). 2527 bytes result sent to driver
21/01/19 12:45:58 INFO TaskSetManager: Starting task 73.0 in stage 1.0 (TID 74, test-runner-pkwzc, executor driver, partition 73, ANY, 7815 bytes)
21/01/19 12:45:58 INFO Executor: Running task 73.0 in stage 1.0 (TID 74)
21/01/19 12:45:58 INFO TaskSetManager: Finished task 72.0 in stage 1.0 (TID 73) in 1233 ms on test-runner-pkwzc (executor driver) (73/200)
21/01/19 12:45:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:58 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:58 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-397e5c89-1fa5-447f-9f28-95736980f438-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/19 12:45:58 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:58 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:58 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:58 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:58 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:58 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:58 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:58 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:58 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:58 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:58 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:45:59 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000073_74' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:45:59 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000073_74: Committed
21/01/19 12:45:59 INFO Executor: Finished task 73.0 in stage 1.0 (TID 74). 2527 bytes result sent to driver
21/01/19 12:45:59 INFO TaskSetManager: Starting task 74.0 in stage 1.0 (TID 75, test-runner-pkwzc, executor driver, partition 74, ANY, 7815 bytes)
21/01/19 12:45:59 INFO Executor: Running task 74.0 in stage 1.0 (TID 75)
21/01/19 12:45:59 INFO TaskSetManager: Finished task 73.0 in stage 1.0 (TID 74) in 1059 ms on test-runner-pkwzc (executor driver) (74/200)
21/01/19 12:45:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:59 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:45:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:45:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:45:59 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-3baf25fb-b0eb-4a63-a195-6cd153e25ad3-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/19 12:45:59 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:59 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:45:59 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:45:59 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:45:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:45:59 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:45:59 INFO ParquetOutputFormat: Validation is off
21/01/19 12:45:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:45:59 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:45:59 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:45:59 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:45:59 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:45:59 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:45:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:00 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000074_75' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:00 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000074_75: Committed
21/01/19 12:46:00 INFO Executor: Finished task 74.0 in stage 1.0 (TID 75). 2527 bytes result sent to driver
21/01/19 12:46:00 INFO TaskSetManager: Starting task 75.0 in stage 1.0 (TID 76, test-runner-pkwzc, executor driver, partition 75, ANY, 7815 bytes)
21/01/19 12:46:00 INFO Executor: Running task 75.0 in stage 1.0 (TID 76)
21/01/19 12:46:00 INFO TaskSetManager: Finished task 74.0 in stage 1.0 (TID 75) in 1111 ms on test-runner-pkwzc (executor driver) (75/200)
21/01/19 12:46:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:00 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-402952ee-337b-499d-8c1a-d513fb155649-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/19 12:46:00 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:00 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:00 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:00 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:00 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:00 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:00 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:00 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:00 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:00 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:00 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:01 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000075_76' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:01 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000075_76: Committed
21/01/19 12:46:01 INFO Executor: Finished task 75.0 in stage 1.0 (TID 76). 2527 bytes result sent to driver
21/01/19 12:46:01 INFO TaskSetManager: Starting task 76.0 in stage 1.0 (TID 77, test-runner-pkwzc, executor driver, partition 76, ANY, 7815 bytes)
21/01/19 12:46:01 INFO Executor: Running task 76.0 in stage 1.0 (TID 77)
21/01/19 12:46:01 INFO TaskSetManager: Finished task 75.0 in stage 1.0 (TID 76) in 1145 ms on test-runner-pkwzc (executor driver) (76/200)
21/01/19 12:46:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:01 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:01 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-495758c2-6fa3-43c8-8722-4c0b9f7ff1bf-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/19 12:46:01 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:01 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:01 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:01 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:01 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:01 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:01 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:01 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:01 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:01 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:01 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:02 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000076_77' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:02 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000076_77: Committed
21/01/19 12:46:02 INFO Executor: Finished task 76.0 in stage 1.0 (TID 77). 2527 bytes result sent to driver
21/01/19 12:46:02 INFO TaskSetManager: Starting task 77.0 in stage 1.0 (TID 78, test-runner-pkwzc, executor driver, partition 77, ANY, 7815 bytes)
21/01/19 12:46:02 INFO Executor: Running task 77.0 in stage 1.0 (TID 78)
21/01/19 12:46:02 INFO TaskSetManager: Finished task 76.0 in stage 1.0 (TID 77) in 1164 ms on test-runner-pkwzc (executor driver) (77/200)
21/01/19 12:46:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:02 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-4b1ba7bc-e410-4c89-83e7-d1236460a348-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/19 12:46:03 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:03 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:03 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:03 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:03 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:03 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:03 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:03 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:03 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:03 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:03 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:03 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000077_78' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:03 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000077_78: Committed
21/01/19 12:46:03 INFO Executor: Finished task 77.0 in stage 1.0 (TID 78). 2527 bytes result sent to driver
21/01/19 12:46:03 INFO TaskSetManager: Starting task 78.0 in stage 1.0 (TID 79, test-runner-pkwzc, executor driver, partition 78, ANY, 7815 bytes)
21/01/19 12:46:03 INFO Executor: Running task 78.0 in stage 1.0 (TID 79)
21/01/19 12:46:03 INFO TaskSetManager: Finished task 77.0 in stage 1.0 (TID 78) in 1139 ms on test-runner-pkwzc (executor driver) (78/200)
21/01/19 12:46:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:03 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-620bb1a8-357a-4760-b130-ddebd5373c9b-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/19 12:46:04 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:04 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:04 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:04 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:04 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:04 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:04 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:04 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:04 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:04 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:04 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:04 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000078_79' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:04 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000078_79: Committed
21/01/19 12:46:04 INFO Executor: Finished task 78.0 in stage 1.0 (TID 79). 2527 bytes result sent to driver
21/01/19 12:46:04 INFO TaskSetManager: Starting task 79.0 in stage 1.0 (TID 80, test-runner-pkwzc, executor driver, partition 79, ANY, 7815 bytes)
21/01/19 12:46:04 INFO Executor: Running task 79.0 in stage 1.0 (TID 80)
21/01/19 12:46:04 INFO TaskSetManager: Finished task 78.0 in stage 1.0 (TID 79) in 1127 ms on test-runner-pkwzc (executor driver) (79/200)
21/01/19 12:46:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:04 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-65c0de9e-ec71-414c-a23c-3beb6bebc879-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/19 12:46:05 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:05 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:05 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:05 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:05 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:05 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:05 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:05 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:05 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:05 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:05 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:05 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000079_80' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:05 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000079_80: Committed
21/01/19 12:46:05 INFO Executor: Finished task 79.0 in stage 1.0 (TID 80). 2527 bytes result sent to driver
21/01/19 12:46:05 INFO TaskSetManager: Starting task 80.0 in stage 1.0 (TID 81, test-runner-pkwzc, executor driver, partition 80, ANY, 7815 bytes)
21/01/19 12:46:05 INFO Executor: Running task 80.0 in stage 1.0 (TID 81)
21/01/19 12:46:05 INFO TaskSetManager: Finished task 79.0 in stage 1.0 (TID 80) in 1152 ms on test-runner-pkwzc (executor driver) (80/200)
21/01/19 12:46:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:06 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:06 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-780e9e82-8cae-4c78-ad7d-d31385465e27-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/19 12:46:06 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:06 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:06 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:06 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:06 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:06 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:06 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:06 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:06 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:06 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:06 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:07 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000080_81' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:07 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000080_81: Committed
21/01/19 12:46:07 INFO Executor: Finished task 80.0 in stage 1.0 (TID 81). 2527 bytes result sent to driver
21/01/19 12:46:07 INFO TaskSetManager: Starting task 81.0 in stage 1.0 (TID 82, test-runner-pkwzc, executor driver, partition 81, ANY, 7815 bytes)
21/01/19 12:46:07 INFO Executor: Running task 81.0 in stage 1.0 (TID 82)
21/01/19 12:46:07 INFO TaskSetManager: Finished task 80.0 in stage 1.0 (TID 81) in 1084 ms on test-runner-pkwzc (executor driver) (81/200)
21/01/19 12:46:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:07 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:07 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-8b5794f6-9fe2-4588-b2f4-e1a598cbf8fc-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/19 12:46:07 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:07 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:07 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:07 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:07 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:07 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:07 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:07 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:07 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:07 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:07 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:08 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000081_82' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:08 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000081_82: Committed
21/01/19 12:46:08 INFO Executor: Finished task 81.0 in stage 1.0 (TID 82). 2527 bytes result sent to driver
21/01/19 12:46:08 INFO TaskSetManager: Starting task 82.0 in stage 1.0 (TID 83, test-runner-pkwzc, executor driver, partition 82, ANY, 7815 bytes)
21/01/19 12:46:08 INFO Executor: Running task 82.0 in stage 1.0 (TID 83)
21/01/19 12:46:08 INFO TaskSetManager: Finished task 81.0 in stage 1.0 (TID 82) in 1167 ms on test-runner-pkwzc (executor driver) (82/200)
21/01/19 12:46:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:08 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:08 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-8fa5192a-9d44-42f4-9c62-28e5f0bdb5f2-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/19 12:46:08 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:08 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:08 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:08 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:08 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:08 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:08 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:08 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:08 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:08 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:08 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:09 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000082_83' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:09 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000082_83: Committed
21/01/19 12:46:09 INFO Executor: Finished task 82.0 in stage 1.0 (TID 83). 2527 bytes result sent to driver
21/01/19 12:46:09 INFO TaskSetManager: Starting task 83.0 in stage 1.0 (TID 84, test-runner-pkwzc, executor driver, partition 83, ANY, 7815 bytes)
21/01/19 12:46:09 INFO Executor: Running task 83.0 in stage 1.0 (TID 84)
21/01/19 12:46:09 INFO TaskSetManager: Finished task 82.0 in stage 1.0 (TID 83) in 1128 ms on test-runner-pkwzc (executor driver) (83/200)
21/01/19 12:46:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:09 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:09 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-9a33b055-fb5f-4abf-96ca-1c8226c73333-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/19 12:46:09 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:09 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:09 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:09 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:09 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:09 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:09 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:09 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:09 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:09 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:09 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:10 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000083_84' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:10 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000083_84: Committed
21/01/19 12:46:10 INFO Executor: Finished task 83.0 in stage 1.0 (TID 84). 2527 bytes result sent to driver
21/01/19 12:46:10 INFO TaskSetManager: Starting task 84.0 in stage 1.0 (TID 85, test-runner-pkwzc, executor driver, partition 84, ANY, 7815 bytes)
21/01/19 12:46:10 INFO Executor: Running task 84.0 in stage 1.0 (TID 85)
21/01/19 12:46:10 INFO TaskSetManager: Finished task 83.0 in stage 1.0 (TID 84) in 1164 ms on test-runner-pkwzc (executor driver) (84/200)
21/01/19 12:46:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:10 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:10 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-a3cdb8ef-19f2-4fac-819c-d6c3171a4339-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/19 12:46:11 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:11 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:11 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:11 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:11 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:11 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:11 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:11 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:11 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:11 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:11 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:11 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000084_85' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:11 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000084_85: Committed
21/01/19 12:46:11 INFO Executor: Finished task 84.0 in stage 1.0 (TID 85). 2527 bytes result sent to driver
21/01/19 12:46:11 INFO TaskSetManager: Starting task 85.0 in stage 1.0 (TID 86, test-runner-pkwzc, executor driver, partition 85, ANY, 7815 bytes)
21/01/19 12:46:11 INFO TaskSetManager: Finished task 84.0 in stage 1.0 (TID 85) in 1305 ms on test-runner-pkwzc (executor driver) (85/200)
21/01/19 12:46:11 INFO Executor: Running task 85.0 in stage 1.0 (TID 86)
21/01/19 12:46:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:11 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-b7980f18-bacf-438f-af48-34726dfe90f4-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/19 12:46:12 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:12 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:12 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:12 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:12 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:12 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:12 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:12 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:12 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:12 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:12 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:13 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000085_86' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:13 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000085_86: Committed
21/01/19 12:46:13 INFO Executor: Finished task 85.0 in stage 1.0 (TID 86). 2527 bytes result sent to driver
21/01/19 12:46:13 INFO TaskSetManager: Starting task 86.0 in stage 1.0 (TID 87, test-runner-pkwzc, executor driver, partition 86, ANY, 7815 bytes)
21/01/19 12:46:13 INFO Executor: Running task 86.0 in stage 1.0 (TID 87)
21/01/19 12:46:13 INFO TaskSetManager: Finished task 85.0 in stage 1.0 (TID 86) in 1178 ms on test-runner-pkwzc (executor driver) (86/200)
21/01/19 12:46:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:13 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:13 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-c1521c82-9467-444c-b1e6-e6bdbfeaf651-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/19 12:46:13 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:13 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:13 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:13 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:13 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:13 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:13 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:13 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:13 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:13 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:13 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:14 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000086_87' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:14 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000086_87: Committed
21/01/19 12:46:14 INFO Executor: Finished task 86.0 in stage 1.0 (TID 87). 2527 bytes result sent to driver
21/01/19 12:46:14 INFO TaskSetManager: Starting task 87.0 in stage 1.0 (TID 88, test-runner-pkwzc, executor driver, partition 87, ANY, 7815 bytes)
21/01/19 12:46:14 INFO Executor: Running task 87.0 in stage 1.0 (TID 88)
21/01/19 12:46:14 INFO TaskSetManager: Finished task 86.0 in stage 1.0 (TID 87) in 1120 ms on test-runner-pkwzc (executor driver) (87/200)
21/01/19 12:46:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:14 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:14 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-c2ac599c-8488-4777-a1bf-3f225b14c9e9-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/19 12:46:14 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:14 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:14 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:14 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:14 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:14 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:14 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:14 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:14 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:14 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:14 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:15 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000087_88' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:15 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000087_88: Committed
21/01/19 12:46:15 INFO Executor: Finished task 87.0 in stage 1.0 (TID 88). 2527 bytes result sent to driver
21/01/19 12:46:15 INFO TaskSetManager: Starting task 88.0 in stage 1.0 (TID 89, test-runner-pkwzc, executor driver, partition 88, ANY, 7815 bytes)
21/01/19 12:46:15 INFO Executor: Running task 88.0 in stage 1.0 (TID 89)
21/01/19 12:46:15 INFO TaskSetManager: Finished task 87.0 in stage 1.0 (TID 88) in 1117 ms on test-runner-pkwzc (executor driver) (88/200)
21/01/19 12:46:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:15 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:15 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-c63f4598-3d2d-4b72-abda-8e1aeeacd75a-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/19 12:46:15 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:15 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:15 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:15 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:15 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:15 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:15 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:15 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:15 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:15 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:15 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:16 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000088_89' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:16 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000088_89: Committed
21/01/19 12:46:16 INFO Executor: Finished task 88.0 in stage 1.0 (TID 89). 2527 bytes result sent to driver
21/01/19 12:46:16 INFO TaskSetManager: Starting task 89.0 in stage 1.0 (TID 90, test-runner-pkwzc, executor driver, partition 89, ANY, 7815 bytes)
21/01/19 12:46:16 INFO Executor: Running task 89.0 in stage 1.0 (TID 90)
21/01/19 12:46:16 INFO TaskSetManager: Finished task 88.0 in stage 1.0 (TID 89) in 1208 ms on test-runner-pkwzc (executor driver) (89/200)
21/01/19 12:46:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:16 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:16 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-ed595b4f-bbaf-4768-811c-73d026933b95-c000.snappy.parquet, range: 0-104863435, partition values: [empty row]
21/01/19 12:46:16 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:16 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:16 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:16 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:16 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:16 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:16 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:16 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:16 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:16 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:16 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:17 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000089_90' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:17 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000089_90: Committed
21/01/19 12:46:17 INFO Executor: Finished task 89.0 in stage 1.0 (TID 90). 2527 bytes result sent to driver
21/01/19 12:46:17 INFO TaskSetManager: Starting task 90.0 in stage 1.0 (TID 91, test-runner-pkwzc, executor driver, partition 90, ANY, 7815 bytes)
21/01/19 12:46:17 INFO Executor: Running task 90.0 in stage 1.0 (TID 91)
21/01/19 12:46:17 INFO TaskSetManager: Finished task 89.0 in stage 1.0 (TID 90) in 1322 ms on test-runner-pkwzc (executor driver) (90/200)
21/01/19 12:46:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:17 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:17 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-032d61dd-e07a-47a9-b612-716db204a85d-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:46:18 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:18 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:18 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:18 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:18 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:18 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:18 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:18 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:18 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:18 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:18 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:18 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000090_91' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:18 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000090_91: Committed
21/01/19 12:46:18 INFO Executor: Finished task 90.0 in stage 1.0 (TID 91). 2527 bytes result sent to driver
21/01/19 12:46:18 INFO TaskSetManager: Starting task 91.0 in stage 1.0 (TID 92, test-runner-pkwzc, executor driver, partition 91, ANY, 7815 bytes)
21/01/19 12:46:18 INFO Executor: Running task 91.0 in stage 1.0 (TID 92)
21/01/19 12:46:18 INFO TaskSetManager: Finished task 90.0 in stage 1.0 (TID 91) in 1172 ms on test-runner-pkwzc (executor driver) (91/200)
21/01/19 12:46:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:18 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:18 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-036c1d4c-f883-4877-b4ef-28e97d6b3447-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:46:19 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:19 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:19 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:19 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:19 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:19 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:19 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:19 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:19 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:19 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:19 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:20 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000091_92' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:20 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000091_92: Committed
21/01/19 12:46:20 INFO Executor: Finished task 91.0 in stage 1.0 (TID 92). 2527 bytes result sent to driver
21/01/19 12:46:20 INFO TaskSetManager: Starting task 92.0 in stage 1.0 (TID 93, test-runner-pkwzc, executor driver, partition 92, ANY, 7815 bytes)
21/01/19 12:46:20 INFO Executor: Running task 92.0 in stage 1.0 (TID 93)
21/01/19 12:46:20 INFO TaskSetManager: Finished task 91.0 in stage 1.0 (TID 92) in 1126 ms on test-runner-pkwzc (executor driver) (92/200)
21/01/19 12:46:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:20 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:20 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-08954339-6874-462b-8cc8-0319948fc571-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:46:20 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:20 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:20 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:20 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:20 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:20 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:20 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:20 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:20 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:20 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:20 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:21 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000092_93' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:21 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000092_93: Committed
21/01/19 12:46:21 INFO Executor: Finished task 92.0 in stage 1.0 (TID 93). 2527 bytes result sent to driver
21/01/19 12:46:21 INFO TaskSetManager: Starting task 93.0 in stage 1.0 (TID 94, test-runner-pkwzc, executor driver, partition 93, ANY, 7815 bytes)
21/01/19 12:46:21 INFO Executor: Running task 93.0 in stage 1.0 (TID 94)
21/01/19 12:46:21 INFO TaskSetManager: Finished task 92.0 in stage 1.0 (TID 93) in 1176 ms on test-runner-pkwzc (executor driver) (93/200)
21/01/19 12:46:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:21 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:21 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-08b4cddf-c45c-4b22-b02d-c84e0ccb06d5-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:46:21 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:21 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:21 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:21 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:21 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:21 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:21 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:21 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:21 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:21 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:21 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:22 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000093_94' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:22 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000093_94: Committed
21/01/19 12:46:22 INFO Executor: Finished task 93.0 in stage 1.0 (TID 94). 2527 bytes result sent to driver
21/01/19 12:46:22 INFO TaskSetManager: Starting task 94.0 in stage 1.0 (TID 95, test-runner-pkwzc, executor driver, partition 94, ANY, 7815 bytes)
21/01/19 12:46:22 INFO Executor: Running task 94.0 in stage 1.0 (TID 95)
21/01/19 12:46:22 INFO TaskSetManager: Finished task 93.0 in stage 1.0 (TID 94) in 1220 ms on test-runner-pkwzc (executor driver) (94/200)
21/01/19 12:46:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:22 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:22 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-0a75882c-9284-4357-9ead-2384605761b7-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:46:22 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:22 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:22 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:22 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:22 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:22 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:22 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:22 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:22 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:22 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:22 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:23 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000094_95' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:23 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000094_95: Committed
21/01/19 12:46:23 INFO Executor: Finished task 94.0 in stage 1.0 (TID 95). 2527 bytes result sent to driver
21/01/19 12:46:23 INFO TaskSetManager: Starting task 95.0 in stage 1.0 (TID 96, test-runner-pkwzc, executor driver, partition 95, ANY, 7815 bytes)
21/01/19 12:46:23 INFO Executor: Running task 95.0 in stage 1.0 (TID 96)
21/01/19 12:46:23 INFO TaskSetManager: Finished task 94.0 in stage 1.0 (TID 95) in 1275 ms on test-runner-pkwzc (executor driver) (95/200)
21/01/19 12:46:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:23 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:23 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-0bda8a6c-1e64-444a-88af-295ae6054330-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:46:24 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:24 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:24 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:24 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:24 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:24 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:24 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:24 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:24 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:24 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:24 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:24 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000095_96' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:24 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000095_96: Committed
21/01/19 12:46:24 INFO Executor: Finished task 95.0 in stage 1.0 (TID 96). 2527 bytes result sent to driver
21/01/19 12:46:24 INFO TaskSetManager: Starting task 96.0 in stage 1.0 (TID 97, test-runner-pkwzc, executor driver, partition 96, ANY, 7815 bytes)
21/01/19 12:46:24 INFO Executor: Running task 96.0 in stage 1.0 (TID 97)
21/01/19 12:46:24 INFO TaskSetManager: Finished task 95.0 in stage 1.0 (TID 96) in 1186 ms on test-runner-pkwzc (executor driver) (96/200)
21/01/19 12:46:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:24 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:24 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-0e8388af-821c-44b9-83d0-e60152bbd561-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:46:25 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:25 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:25 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:25 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:25 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:25 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:25 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:25 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:25 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:25 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:25 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:26 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000096_97' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:26 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000096_97: Committed
21/01/19 12:46:26 INFO Executor: Finished task 96.0 in stage 1.0 (TID 97). 2527 bytes result sent to driver
21/01/19 12:46:26 INFO TaskSetManager: Starting task 97.0 in stage 1.0 (TID 98, test-runner-pkwzc, executor driver, partition 97, ANY, 7815 bytes)
21/01/19 12:46:26 INFO Executor: Running task 97.0 in stage 1.0 (TID 98)
21/01/19 12:46:26 INFO TaskSetManager: Finished task 96.0 in stage 1.0 (TID 97) in 1139 ms on test-runner-pkwzc (executor driver) (97/200)
21/01/19 12:46:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:26 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:26 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-164379aa-19e0-43c4-bd85-acbbc33ec47c-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:46:26 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:26 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:26 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:26 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:26 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:26 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:26 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:26 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:26 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:26 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:26 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:27 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000097_98' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:27 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000097_98: Committed
21/01/19 12:46:27 INFO Executor: Finished task 97.0 in stage 1.0 (TID 98). 2527 bytes result sent to driver
21/01/19 12:46:27 INFO TaskSetManager: Starting task 98.0 in stage 1.0 (TID 99, test-runner-pkwzc, executor driver, partition 98, ANY, 7815 bytes)
21/01/19 12:46:27 INFO Executor: Running task 98.0 in stage 1.0 (TID 99)
21/01/19 12:46:27 INFO TaskSetManager: Finished task 97.0 in stage 1.0 (TID 98) in 1111 ms on test-runner-pkwzc (executor driver) (98/200)
21/01/19 12:46:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:27 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:27 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-18663ce0-59f2-416a-a313-a3179f6096a7-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:46:27 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:27 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:27 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:27 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:27 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:27 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:27 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:27 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:27 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:27 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:27 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:28 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000098_99' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:28 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000098_99: Committed
21/01/19 12:46:28 INFO Executor: Finished task 98.0 in stage 1.0 (TID 99). 2527 bytes result sent to driver
21/01/19 12:46:28 INFO TaskSetManager: Starting task 99.0 in stage 1.0 (TID 100, test-runner-pkwzc, executor driver, partition 99, ANY, 7815 bytes)
21/01/19 12:46:28 INFO Executor: Running task 99.0 in stage 1.0 (TID 100)
21/01/19 12:46:28 INFO TaskSetManager: Finished task 98.0 in stage 1.0 (TID 99) in 1047 ms on test-runner-pkwzc (executor driver) (99/200)
21/01/19 12:46:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:28 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:28 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-23c4f164-1513-44bb-bff8-a4b6dbbb131f-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:46:28 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:28 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:28 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:28 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:28 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:28 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:28 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:28 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:28 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:28 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:28 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:29 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000099_100' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:29 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000099_100: Committed
21/01/19 12:46:29 INFO Executor: Finished task 99.0 in stage 1.0 (TID 100). 2527 bytes result sent to driver
21/01/19 12:46:29 INFO TaskSetManager: Starting task 100.0 in stage 1.0 (TID 101, test-runner-pkwzc, executor driver, partition 100, ANY, 7815 bytes)
21/01/19 12:46:29 INFO Executor: Running task 100.0 in stage 1.0 (TID 101)
21/01/19 12:46:29 INFO TaskSetManager: Finished task 99.0 in stage 1.0 (TID 100) in 1243 ms on test-runner-pkwzc (executor driver) (100/200)
21/01/19 12:46:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:29 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-24a3a298-dd3a-4ab9-9cb8-7183f6c0a5f5-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:46:29 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:29 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:29 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:29 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:29 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:29 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:29 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:29 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:29 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:29 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:29 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:30 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000100_101' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:30 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000100_101: Committed
21/01/19 12:46:30 INFO Executor: Finished task 100.0 in stage 1.0 (TID 101). 2527 bytes result sent to driver
21/01/19 12:46:30 INFO TaskSetManager: Starting task 101.0 in stage 1.0 (TID 102, test-runner-pkwzc, executor driver, partition 101, ANY, 7815 bytes)
21/01/19 12:46:30 INFO Executor: Running task 101.0 in stage 1.0 (TID 102)
21/01/19 12:46:30 INFO TaskSetManager: Finished task 100.0 in stage 1.0 (TID 101) in 1154 ms on test-runner-pkwzc (executor driver) (101/200)
21/01/19 12:46:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:30 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-27da6240-a3df-4519-bf9f-bb6fa40f5624-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:46:31 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:31 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:31 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:31 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:31 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:31 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:31 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:31 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:31 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:31 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:31 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:31 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000101_102' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:31 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000101_102: Committed
21/01/19 12:46:31 INFO Executor: Finished task 101.0 in stage 1.0 (TID 102). 2527 bytes result sent to driver
21/01/19 12:46:31 INFO TaskSetManager: Starting task 102.0 in stage 1.0 (TID 103, test-runner-pkwzc, executor driver, partition 102, ANY, 7815 bytes)
21/01/19 12:46:31 INFO Executor: Running task 102.0 in stage 1.0 (TID 103)
21/01/19 12:46:31 INFO TaskSetManager: Finished task 101.0 in stage 1.0 (TID 102) in 1102 ms on test-runner-pkwzc (executor driver) (102/200)
21/01/19 12:46:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:31 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-295f937b-9fb5-4a76-9eaf-7e44e2f9a235-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:46:32 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:32 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:32 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:32 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:32 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:32 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:32 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:32 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:32 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:32 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:32 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:32 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000102_103' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:32 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000102_103: Committed
21/01/19 12:46:32 INFO Executor: Finished task 102.0 in stage 1.0 (TID 103). 2527 bytes result sent to driver
21/01/19 12:46:32 INFO TaskSetManager: Starting task 103.0 in stage 1.0 (TID 104, test-runner-pkwzc, executor driver, partition 103, ANY, 7815 bytes)
21/01/19 12:46:32 INFO Executor: Running task 103.0 in stage 1.0 (TID 104)
21/01/19 12:46:32 INFO TaskSetManager: Finished task 102.0 in stage 1.0 (TID 103) in 1140 ms on test-runner-pkwzc (executor driver) (103/200)
21/01/19 12:46:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:32 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:32 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-2a2b53e5-4dda-4321-868b-fbbeed7293f7-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:46:33 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:33 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:33 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:33 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:33 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:33 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:33 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:33 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:33 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:33 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:33 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:33 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000103_104' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:33 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000103_104: Committed
21/01/19 12:46:33 INFO Executor: Finished task 103.0 in stage 1.0 (TID 104). 2527 bytes result sent to driver
21/01/19 12:46:33 INFO TaskSetManager: Starting task 104.0 in stage 1.0 (TID 105, test-runner-pkwzc, executor driver, partition 104, ANY, 7815 bytes)
21/01/19 12:46:33 INFO Executor: Running task 104.0 in stage 1.0 (TID 105)
21/01/19 12:46:33 INFO TaskSetManager: Finished task 103.0 in stage 1.0 (TID 104) in 1096 ms on test-runner-pkwzc (executor driver) (104/200)
21/01/19 12:46:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:33 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:33 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-2a6d51e9-1abe-44ba-847b-52621b351339-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:46:34 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:34 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:34 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:34 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:34 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:34 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:34 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:34 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:34 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:34 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:34 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:34 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:34 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:35 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000104_105' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:35 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000104_105: Committed
21/01/19 12:46:35 INFO Executor: Finished task 104.0 in stage 1.0 (TID 105). 2527 bytes result sent to driver
21/01/19 12:46:35 INFO TaskSetManager: Starting task 105.0 in stage 1.0 (TID 106, test-runner-pkwzc, executor driver, partition 105, ANY, 7815 bytes)
21/01/19 12:46:35 INFO Executor: Running task 105.0 in stage 1.0 (TID 106)
21/01/19 12:46:35 INFO TaskSetManager: Finished task 104.0 in stage 1.0 (TID 105) in 1411 ms on test-runner-pkwzc (executor driver) (105/200)
21/01/19 12:46:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:35 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:35 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-2cd79918-71db-447d-822b-80aacfa86217-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:46:35 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:35 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:35 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:35 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:35 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:35 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:35 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:35 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:35 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:35 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:35 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:36 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000105_106' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:36 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000105_106: Committed
21/01/19 12:46:36 INFO Executor: Finished task 105.0 in stage 1.0 (TID 106). 2527 bytes result sent to driver
21/01/19 12:46:36 INFO TaskSetManager: Starting task 106.0 in stage 1.0 (TID 107, test-runner-pkwzc, executor driver, partition 106, ANY, 7815 bytes)
21/01/19 12:46:36 INFO Executor: Running task 106.0 in stage 1.0 (TID 107)
21/01/19 12:46:36 INFO TaskSetManager: Finished task 105.0 in stage 1.0 (TID 106) in 1073 ms on test-runner-pkwzc (executor driver) (106/200)
21/01/19 12:46:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:36 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:36 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-3077984a-5ced-418c-b2f9-99c9b42ed790-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:46:36 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:36 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:36 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:36 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:36 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:36 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:36 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:36 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:36 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:36 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:36 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:36 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:36 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:36 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:37 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000106_107' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:37 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000106_107: Committed
21/01/19 12:46:37 INFO Executor: Finished task 106.0 in stage 1.0 (TID 107). 2527 bytes result sent to driver
21/01/19 12:46:37 INFO TaskSetManager: Starting task 107.0 in stage 1.0 (TID 108, test-runner-pkwzc, executor driver, partition 107, ANY, 7815 bytes)
21/01/19 12:46:37 INFO Executor: Running task 107.0 in stage 1.0 (TID 108)
21/01/19 12:46:37 INFO TaskSetManager: Finished task 106.0 in stage 1.0 (TID 107) in 1080 ms on test-runner-pkwzc (executor driver) (107/200)
21/01/19 12:46:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:37 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:37 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-30c173b5-e976-4aa1-9fe1-63f762f16a6b-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:46:37 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:37 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:37 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:37 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:37 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:37 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:37 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:37 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:37 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:37 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:37 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:38 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000107_108' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:38 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000107_108: Committed
21/01/19 12:46:38 INFO Executor: Finished task 107.0 in stage 1.0 (TID 108). 2527 bytes result sent to driver
21/01/19 12:46:38 INFO TaskSetManager: Starting task 108.0 in stage 1.0 (TID 109, test-runner-pkwzc, executor driver, partition 108, ANY, 7815 bytes)
21/01/19 12:46:38 INFO Executor: Running task 108.0 in stage 1.0 (TID 109)
21/01/19 12:46:38 INFO TaskSetManager: Finished task 107.0 in stage 1.0 (TID 108) in 1118 ms on test-runner-pkwzc (executor driver) (108/200)
21/01/19 12:46:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:38 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:38 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-3120932e-7a8c-4a41-8a7d-212daa6a976f-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:46:39 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:39 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:39 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:39 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:39 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:39 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:39 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:39 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:39 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:39 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:39 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:39 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000108_109' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:39 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000108_109: Committed
21/01/19 12:46:39 INFO Executor: Finished task 108.0 in stage 1.0 (TID 109). 2527 bytes result sent to driver
21/01/19 12:46:39 INFO TaskSetManager: Starting task 109.0 in stage 1.0 (TID 110, test-runner-pkwzc, executor driver, partition 109, ANY, 7815 bytes)
21/01/19 12:46:39 INFO Executor: Running task 109.0 in stage 1.0 (TID 110)
21/01/19 12:46:39 INFO TaskSetManager: Finished task 108.0 in stage 1.0 (TID 109) in 1123 ms on test-runner-pkwzc (executor driver) (109/200)
21/01/19 12:46:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:39 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:39 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-3154794b-3613-416a-a843-97622a934f7a-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:46:40 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:40 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:40 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:40 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:40 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:40 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:40 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:40 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:40 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:40 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:40 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:40 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000109_110' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:40 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000109_110: Committed
21/01/19 12:46:40 INFO Executor: Finished task 109.0 in stage 1.0 (TID 110). 2527 bytes result sent to driver
21/01/19 12:46:40 INFO TaskSetManager: Starting task 110.0 in stage 1.0 (TID 111, test-runner-pkwzc, executor driver, partition 110, ANY, 7815 bytes)
21/01/19 12:46:40 INFO Executor: Running task 110.0 in stage 1.0 (TID 111)
21/01/19 12:46:40 INFO TaskSetManager: Finished task 109.0 in stage 1.0 (TID 110) in 1238 ms on test-runner-pkwzc (executor driver) (110/200)
21/01/19 12:46:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:41 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:41 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-318c5892-fb3e-4730-9f76-33a9319ca5c9-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:46:41 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:41 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:41 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:41 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:41 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:41 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:41 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:41 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:41 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:41 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:41 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:42 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000110_111' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:42 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000110_111: Committed
21/01/19 12:46:42 INFO Executor: Finished task 110.0 in stage 1.0 (TID 111). 2527 bytes result sent to driver
21/01/19 12:46:42 INFO TaskSetManager: Starting task 111.0 in stage 1.0 (TID 112, test-runner-pkwzc, executor driver, partition 111, ANY, 7815 bytes)
21/01/19 12:46:42 INFO Executor: Running task 111.0 in stage 1.0 (TID 112)
21/01/19 12:46:42 INFO TaskSetManager: Finished task 110.0 in stage 1.0 (TID 111) in 1097 ms on test-runner-pkwzc (executor driver) (111/200)
21/01/19 12:46:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:42 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:42 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-340dcca3-5e54-4fe9-8ce3-3fb65a0c0737-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:46:42 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:42 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:42 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:42 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:42 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:42 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:42 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:42 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:42 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:42 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:42 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:43 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000111_112' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:43 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000111_112: Committed
21/01/19 12:46:43 INFO Executor: Finished task 111.0 in stage 1.0 (TID 112). 2527 bytes result sent to driver
21/01/19 12:46:43 INFO TaskSetManager: Starting task 112.0 in stage 1.0 (TID 113, test-runner-pkwzc, executor driver, partition 112, ANY, 7815 bytes)
21/01/19 12:46:43 INFO Executor: Running task 112.0 in stage 1.0 (TID 113)
21/01/19 12:46:43 INFO TaskSetManager: Finished task 111.0 in stage 1.0 (TID 112) in 1059 ms on test-runner-pkwzc (executor driver) (112/200)
21/01/19 12:46:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:43 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:43 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-38f77e61-9571-4e0b-8b33-62c2390d74fe-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:46:43 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:43 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:43 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:43 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:43 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:43 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:43 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:43 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:43 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:43 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:43 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:44 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000112_113' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:44 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000112_113: Committed
21/01/19 12:46:44 INFO Executor: Finished task 112.0 in stage 1.0 (TID 113). 2527 bytes result sent to driver
21/01/19 12:46:44 INFO TaskSetManager: Starting task 113.0 in stage 1.0 (TID 114, test-runner-pkwzc, executor driver, partition 113, ANY, 7815 bytes)
21/01/19 12:46:44 INFO Executor: Running task 113.0 in stage 1.0 (TID 114)
21/01/19 12:46:44 INFO TaskSetManager: Finished task 112.0 in stage 1.0 (TID 113) in 1151 ms on test-runner-pkwzc (executor driver) (113/200)
21/01/19 12:46:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:44 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:44 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-3ee56063-b310-4f6e-8d91-9985fbdb6870-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:46:44 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:44 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:44 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:44 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:44 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:44 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:44 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:44 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:44 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:44 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:44 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:45 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000113_114' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:45 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000113_114: Committed
21/01/19 12:46:45 INFO Executor: Finished task 113.0 in stage 1.0 (TID 114). 2527 bytes result sent to driver
21/01/19 12:46:45 INFO TaskSetManager: Starting task 114.0 in stage 1.0 (TID 115, test-runner-pkwzc, executor driver, partition 114, ANY, 7815 bytes)
21/01/19 12:46:45 INFO Executor: Running task 114.0 in stage 1.0 (TID 115)
21/01/19 12:46:45 INFO TaskSetManager: Finished task 113.0 in stage 1.0 (TID 114) in 1094 ms on test-runner-pkwzc (executor driver) (114/200)
21/01/19 12:46:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:45 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:45 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-3f63fe47-cca6-4b2f-b1c7-29e8e200876e-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:46:45 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:45 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:45 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:45 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:45 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:45 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:45 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:45 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:45 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:45 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:45 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:46 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000114_115' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:46 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000114_115: Committed
21/01/19 12:46:46 INFO Executor: Finished task 114.0 in stage 1.0 (TID 115). 2527 bytes result sent to driver
21/01/19 12:46:46 INFO TaskSetManager: Starting task 115.0 in stage 1.0 (TID 116, test-runner-pkwzc, executor driver, partition 115, ANY, 7815 bytes)
21/01/19 12:46:46 INFO TaskSetManager: Finished task 114.0 in stage 1.0 (TID 115) in 1154 ms on test-runner-pkwzc (executor driver) (115/200)
21/01/19 12:46:46 INFO Executor: Running task 115.0 in stage 1.0 (TID 116)
21/01/19 12:46:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:46 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:46 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-3fc3ccb3-805c-456f-b3d7-6ddab958a657-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:46:47 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:47 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:47 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:47 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:47 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:47 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:47 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:47 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:47 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:47 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:47 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:47 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000115_116' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:47 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000115_116: Committed
21/01/19 12:46:47 INFO Executor: Finished task 115.0 in stage 1.0 (TID 116). 2527 bytes result sent to driver
21/01/19 12:46:47 INFO TaskSetManager: Starting task 116.0 in stage 1.0 (TID 117, test-runner-pkwzc, executor driver, partition 116, ANY, 7815 bytes)
21/01/19 12:46:47 INFO Executor: Running task 116.0 in stage 1.0 (TID 117)
21/01/19 12:46:47 INFO TaskSetManager: Finished task 115.0 in stage 1.0 (TID 116) in 1130 ms on test-runner-pkwzc (executor driver) (116/200)
21/01/19 12:46:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:47 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-40d00de8-4c6f-4c81-90df-8d616abf3b76-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:46:48 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:48 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:48 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:48 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:48 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:48 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:48 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:48 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:48 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:48 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:48 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:48 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000116_117' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:48 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000116_117: Committed
21/01/19 12:46:48 INFO Executor: Finished task 116.0 in stage 1.0 (TID 117). 2527 bytes result sent to driver
21/01/19 12:46:48 INFO TaskSetManager: Starting task 117.0 in stage 1.0 (TID 118, test-runner-pkwzc, executor driver, partition 117, ANY, 7815 bytes)
21/01/19 12:46:48 INFO Executor: Running task 117.0 in stage 1.0 (TID 118)
21/01/19 12:46:48 INFO TaskSetManager: Finished task 116.0 in stage 1.0 (TID 117) in 1173 ms on test-runner-pkwzc (executor driver) (117/200)
21/01/19 12:46:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:48 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:48 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-4285993c-c175-4c84-93af-8f5e92c6ccf8-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:46:49 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:49 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:49 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:49 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:49 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:49 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:49 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:49 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:49 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:49 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:49 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:49 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000117_118' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:49 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000117_118: Committed
21/01/19 12:46:49 INFO Executor: Finished task 117.0 in stage 1.0 (TID 118). 2527 bytes result sent to driver
21/01/19 12:46:49 INFO TaskSetManager: Starting task 118.0 in stage 1.0 (TID 119, test-runner-pkwzc, executor driver, partition 118, ANY, 7815 bytes)
21/01/19 12:46:49 INFO Executor: Running task 118.0 in stage 1.0 (TID 119)
21/01/19 12:46:49 INFO TaskSetManager: Finished task 117.0 in stage 1.0 (TID 118) in 1121 ms on test-runner-pkwzc (executor driver) (118/200)
21/01/19 12:46:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:49 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-43d52807-f512-4189-8a38-173f5b6d0a03-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:46:50 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:50 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:50 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:50 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:50 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:50 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:50 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:50 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:50 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:50 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:50 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:50 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:50 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:50 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:51 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000118_119' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:51 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000118_119: Committed
21/01/19 12:46:51 INFO Executor: Finished task 118.0 in stage 1.0 (TID 119). 2527 bytes result sent to driver
21/01/19 12:46:51 INFO TaskSetManager: Starting task 119.0 in stage 1.0 (TID 120, test-runner-pkwzc, executor driver, partition 119, ANY, 7815 bytes)
21/01/19 12:46:51 INFO Executor: Running task 119.0 in stage 1.0 (TID 120)
21/01/19 12:46:51 INFO TaskSetManager: Finished task 118.0 in stage 1.0 (TID 119) in 1120 ms on test-runner-pkwzc (executor driver) (119/200)
21/01/19 12:46:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:51 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-49257cc6-4480-4692-9b4d-291d14f14372-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:46:51 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:51 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:51 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:51 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:51 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:51 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:51 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:51 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:51 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:51 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:51 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:51 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:51 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:51 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:52 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000119_120' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:52 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000119_120: Committed
21/01/19 12:46:52 INFO Executor: Finished task 119.0 in stage 1.0 (TID 120). 2527 bytes result sent to driver
21/01/19 12:46:52 INFO TaskSetManager: Starting task 120.0 in stage 1.0 (TID 121, test-runner-pkwzc, executor driver, partition 120, ANY, 7815 bytes)
21/01/19 12:46:52 INFO Executor: Running task 120.0 in stage 1.0 (TID 121)
21/01/19 12:46:52 INFO TaskSetManager: Finished task 119.0 in stage 1.0 (TID 120) in 1127 ms on test-runner-pkwzc (executor driver) (120/200)
21/01/19 12:46:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:52 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:52 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-4a4d6996-1a07-4ed4-8bc6-fe2e6de8e332-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:46:52 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:52 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:52 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:52 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:52 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:52 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:52 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:52 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:52 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:52 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:52 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:53 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000120_121' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:53 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000120_121: Committed
21/01/19 12:46:53 INFO Executor: Finished task 120.0 in stage 1.0 (TID 121). 2527 bytes result sent to driver
21/01/19 12:46:53 INFO TaskSetManager: Starting task 121.0 in stage 1.0 (TID 122, test-runner-pkwzc, executor driver, partition 121, ANY, 7815 bytes)
21/01/19 12:46:53 INFO Executor: Running task 121.0 in stage 1.0 (TID 122)
21/01/19 12:46:53 INFO TaskSetManager: Finished task 120.0 in stage 1.0 (TID 121) in 1211 ms on test-runner-pkwzc (executor driver) (121/200)
21/01/19 12:46:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:53 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-4e9d3018-f3a5-4c60-8f2f-ef19801d4f33-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:46:53 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:53 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:53 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:53 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:53 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:53 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:53 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:53 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:53 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:53 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:53 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:54 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000121_122' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:54 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000121_122: Committed
21/01/19 12:46:54 INFO Executor: Finished task 121.0 in stage 1.0 (TID 122). 2527 bytes result sent to driver
21/01/19 12:46:54 INFO TaskSetManager: Starting task 122.0 in stage 1.0 (TID 123, test-runner-pkwzc, executor driver, partition 122, ANY, 7815 bytes)
21/01/19 12:46:54 INFO Executor: Running task 122.0 in stage 1.0 (TID 123)
21/01/19 12:46:54 INFO TaskSetManager: Finished task 121.0 in stage 1.0 (TID 122) in 1071 ms on test-runner-pkwzc (executor driver) (122/200)
21/01/19 12:46:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:54 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:54 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-4f931553-eff1-4d5a-8b0f-4656e5708759-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:46:54 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:54 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:54 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:54 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:54 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:54 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:54 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:54 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:54 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:54 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:54 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:55 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000122_123' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:55 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000122_123: Committed
21/01/19 12:46:55 INFO Executor: Finished task 122.0 in stage 1.0 (TID 123). 2527 bytes result sent to driver
21/01/19 12:46:55 INFO TaskSetManager: Starting task 123.0 in stage 1.0 (TID 124, test-runner-pkwzc, executor driver, partition 123, ANY, 7815 bytes)
21/01/19 12:46:55 INFO Executor: Running task 123.0 in stage 1.0 (TID 124)
21/01/19 12:46:55 INFO TaskSetManager: Finished task 122.0 in stage 1.0 (TID 123) in 1103 ms on test-runner-pkwzc (executor driver) (123/200)
21/01/19 12:46:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:55 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-50aef3a3-bf15-4537-9361-806313b7c2d3-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:46:56 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:56 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:56 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:56 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:56 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:56 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:56 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:56 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:56 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:56 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:56 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:56 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000123_124' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:56 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000123_124: Committed
21/01/19 12:46:56 INFO Executor: Finished task 123.0 in stage 1.0 (TID 124). 2527 bytes result sent to driver
21/01/19 12:46:56 INFO TaskSetManager: Starting task 124.0 in stage 1.0 (TID 125, test-runner-pkwzc, executor driver, partition 124, ANY, 7815 bytes)
21/01/19 12:46:56 INFO Executor: Running task 124.0 in stage 1.0 (TID 125)
21/01/19 12:46:56 INFO TaskSetManager: Finished task 123.0 in stage 1.0 (TID 124) in 1143 ms on test-runner-pkwzc (executor driver) (124/200)
21/01/19 12:46:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:56 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:56 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-50ed7e1d-2099-4a82-b8f2-1f01aad20f75-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:46:57 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:57 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:57 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:57 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:57 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:57 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:57 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:57 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:57 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:57 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:57 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:57 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000124_125' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:57 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000124_125: Committed
21/01/19 12:46:57 INFO Executor: Finished task 124.0 in stage 1.0 (TID 125). 2527 bytes result sent to driver
21/01/19 12:46:57 INFO TaskSetManager: Starting task 125.0 in stage 1.0 (TID 126, test-runner-pkwzc, executor driver, partition 125, ANY, 7815 bytes)
21/01/19 12:46:57 INFO Executor: Running task 125.0 in stage 1.0 (TID 126)
21/01/19 12:46:57 INFO TaskSetManager: Finished task 124.0 in stage 1.0 (TID 125) in 1090 ms on test-runner-pkwzc (executor driver) (125/200)
21/01/19 12:46:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:57 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:57 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-543cd0cf-e16c-4aec-b3b1-6ff693fc6c26-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:46:58 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:58 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:58 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:58 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:58 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:58 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:58 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:58 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:58 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:58 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:58 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:58 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000125_126' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:58 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000125_126: Committed
21/01/19 12:46:58 INFO Executor: Finished task 125.0 in stage 1.0 (TID 126). 2527 bytes result sent to driver
21/01/19 12:46:58 INFO TaskSetManager: Starting task 126.0 in stage 1.0 (TID 127, test-runner-pkwzc, executor driver, partition 126, ANY, 7815 bytes)
21/01/19 12:46:58 INFO Executor: Running task 126.0 in stage 1.0 (TID 127)
21/01/19 12:46:58 INFO TaskSetManager: Finished task 125.0 in stage 1.0 (TID 126) in 1084 ms on test-runner-pkwzc (executor driver) (126/200)
21/01/19 12:46:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:58 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:58 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-5598129b-c2b0-4924-88d3-d5c9803edde1-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:46:59 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:59 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:46:59 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:46:59 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:46:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:46:59 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:46:59 INFO ParquetOutputFormat: Validation is off
21/01/19 12:46:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:46:59 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:46:59 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:46:59 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:46:59 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:46:59 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:46:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:46:59 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000126_127' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:46:59 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000126_127: Committed
21/01/19 12:46:59 INFO Executor: Finished task 126.0 in stage 1.0 (TID 127). 2527 bytes result sent to driver
21/01/19 12:46:59 INFO TaskSetManager: Starting task 127.0 in stage 1.0 (TID 128, test-runner-pkwzc, executor driver, partition 127, ANY, 7815 bytes)
21/01/19 12:46:59 INFO Executor: Running task 127.0 in stage 1.0 (TID 128)
21/01/19 12:46:59 INFO TaskSetManager: Finished task 126.0 in stage 1.0 (TID 127) in 1073 ms on test-runner-pkwzc (executor driver) (127/200)
21/01/19 12:46:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:59 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:46:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:46:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:46:59 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-5980737f-19cd-4933-95c4-b1a7e7fca6eb-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:00 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:00 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:00 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:00 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:00 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:00 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:00 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:00 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:00 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:00 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:00 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:01 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000127_128' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:01 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000127_128: Committed
21/01/19 12:47:01 INFO Executor: Finished task 127.0 in stage 1.0 (TID 128). 2527 bytes result sent to driver
21/01/19 12:47:01 INFO TaskSetManager: Starting task 128.0 in stage 1.0 (TID 129, test-runner-pkwzc, executor driver, partition 128, ANY, 7815 bytes)
21/01/19 12:47:01 INFO TaskSetManager: Finished task 127.0 in stage 1.0 (TID 128) in 1142 ms on test-runner-pkwzc (executor driver) (128/200)
21/01/19 12:47:01 INFO Executor: Running task 128.0 in stage 1.0 (TID 129)
21/01/19 12:47:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:01 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:01 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-5d64a35a-8c8d-4098-b436-d394ed1f844f-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:01 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:01 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:01 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:01 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:01 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:01 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:01 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:01 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:01 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:01 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:01 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:02 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000128_129' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:02 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000128_129: Committed
21/01/19 12:47:02 INFO Executor: Finished task 128.0 in stage 1.0 (TID 129). 2527 bytes result sent to driver
21/01/19 12:47:02 INFO TaskSetManager: Starting task 129.0 in stage 1.0 (TID 130, test-runner-pkwzc, executor driver, partition 129, ANY, 7815 bytes)
21/01/19 12:47:02 INFO Executor: Running task 129.0 in stage 1.0 (TID 130)
21/01/19 12:47:02 INFO TaskSetManager: Finished task 128.0 in stage 1.0 (TID 129) in 1253 ms on test-runner-pkwzc (executor driver) (129/200)
21/01/19 12:47:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:02 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-5f3df22d-3aa8-42c6-95ac-b43f9733ab94-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:02 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:02 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:02 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:02 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:02 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:02 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:02 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:02 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:02 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:02 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:02 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:02 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:02 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:03 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000129_130' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:03 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000129_130: Committed
21/01/19 12:47:03 INFO Executor: Finished task 129.0 in stage 1.0 (TID 130). 2527 bytes result sent to driver
21/01/19 12:47:03 INFO TaskSetManager: Starting task 130.0 in stage 1.0 (TID 131, test-runner-pkwzc, executor driver, partition 130, ANY, 7815 bytes)
21/01/19 12:47:03 INFO Executor: Running task 130.0 in stage 1.0 (TID 131)
21/01/19 12:47:03 INFO TaskSetManager: Finished task 129.0 in stage 1.0 (TID 130) in 1105 ms on test-runner-pkwzc (executor driver) (130/200)
21/01/19 12:47:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:03 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-660b8f8d-a3b6-40e6-a7a6-a97fa9885e8f-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:03 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:03 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:03 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:03 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:03 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:03 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:03 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:03 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:03 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:03 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:03 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:04 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000130_131' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:04 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000130_131: Committed
21/01/19 12:47:04 INFO Executor: Finished task 130.0 in stage 1.0 (TID 131). 2527 bytes result sent to driver
21/01/19 12:47:04 INFO TaskSetManager: Starting task 131.0 in stage 1.0 (TID 132, test-runner-pkwzc, executor driver, partition 131, ANY, 7815 bytes)
21/01/19 12:47:04 INFO Executor: Running task 131.0 in stage 1.0 (TID 132)
21/01/19 12:47:04 INFO TaskSetManager: Finished task 130.0 in stage 1.0 (TID 131) in 1168 ms on test-runner-pkwzc (executor driver) (131/200)
21/01/19 12:47:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:04 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-66546fb9-eb49-4fa9-b7e5-7022b6c6e8b1-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:05 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:05 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:05 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:05 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:05 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:05 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:05 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:05 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:05 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:05 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:05 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:05 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000131_132' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:05 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000131_132: Committed
21/01/19 12:47:05 INFO Executor: Finished task 131.0 in stage 1.0 (TID 132). 2527 bytes result sent to driver
21/01/19 12:47:05 INFO TaskSetManager: Starting task 132.0 in stage 1.0 (TID 133, test-runner-pkwzc, executor driver, partition 132, ANY, 7815 bytes)
21/01/19 12:47:05 INFO Executor: Running task 132.0 in stage 1.0 (TID 133)
21/01/19 12:47:05 INFO TaskSetManager: Finished task 131.0 in stage 1.0 (TID 132) in 1140 ms on test-runner-pkwzc (executor driver) (132/200)
21/01/19 12:47:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:05 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:05 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-69f20f31-8fef-495b-906d-fe74a1a1cf34-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:06 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:06 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:06 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:06 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:06 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:06 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:06 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:06 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:06 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:06 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:06 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:06 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000132_133' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:06 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000132_133: Committed
21/01/19 12:47:06 INFO Executor: Finished task 132.0 in stage 1.0 (TID 133). 2527 bytes result sent to driver
21/01/19 12:47:06 INFO TaskSetManager: Starting task 133.0 in stage 1.0 (TID 134, test-runner-pkwzc, executor driver, partition 133, ANY, 7815 bytes)
21/01/19 12:47:06 INFO Executor: Running task 133.0 in stage 1.0 (TID 134)
21/01/19 12:47:06 INFO TaskSetManager: Finished task 132.0 in stage 1.0 (TID 133) in 1108 ms on test-runner-pkwzc (executor driver) (133/200)
21/01/19 12:47:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:06 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:06 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-6a364f96-843c-45a0-955b-86f8ac66d4df-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:07 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:07 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:07 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:07 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:07 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:07 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:07 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:07 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:07 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:07 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:07 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:08 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000133_134' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:08 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000133_134: Committed
21/01/19 12:47:08 INFO Executor: Finished task 133.0 in stage 1.0 (TID 134). 2527 bytes result sent to driver
21/01/19 12:47:08 INFO TaskSetManager: Starting task 134.0 in stage 1.0 (TID 135, test-runner-pkwzc, executor driver, partition 134, ANY, 7815 bytes)
21/01/19 12:47:08 INFO Executor: Running task 134.0 in stage 1.0 (TID 135)
21/01/19 12:47:08 INFO TaskSetManager: Finished task 133.0 in stage 1.0 (TID 134) in 1215 ms on test-runner-pkwzc (executor driver) (134/200)
21/01/19 12:47:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:08 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:08 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-6b297aea-a677-4269-8877-0f2864e52308-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:08 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:08 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:08 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:08 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:08 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:08 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:08 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:08 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:08 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:08 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:08 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:09 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000134_135' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:09 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000134_135: Committed
21/01/19 12:47:09 INFO Executor: Finished task 134.0 in stage 1.0 (TID 135). 2527 bytes result sent to driver
21/01/19 12:47:09 INFO TaskSetManager: Starting task 135.0 in stage 1.0 (TID 136, test-runner-pkwzc, executor driver, partition 135, ANY, 7815 bytes)
21/01/19 12:47:09 INFO Executor: Running task 135.0 in stage 1.0 (TID 136)
21/01/19 12:47:09 INFO TaskSetManager: Finished task 134.0 in stage 1.0 (TID 135) in 1081 ms on test-runner-pkwzc (executor driver) (135/200)
21/01/19 12:47:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:09 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:09 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-6cca2d52-80fd-4124-8312-4cfec2f4764e-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:09 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:09 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:09 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:09 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:09 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:09 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:09 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:09 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:09 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:09 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:09 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:10 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000135_136' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:10 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000135_136: Committed
21/01/19 12:47:10 INFO Executor: Finished task 135.0 in stage 1.0 (TID 136). 2527 bytes result sent to driver
21/01/19 12:47:10 INFO TaskSetManager: Starting task 136.0 in stage 1.0 (TID 137, test-runner-pkwzc, executor driver, partition 136, ANY, 7815 bytes)
21/01/19 12:47:10 INFO Executor: Running task 136.0 in stage 1.0 (TID 137)
21/01/19 12:47:10 INFO TaskSetManager: Finished task 135.0 in stage 1.0 (TID 136) in 1083 ms on test-runner-pkwzc (executor driver) (136/200)
21/01/19 12:47:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:10 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:10 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-6dc52c9b-0ff4-4d04-8916-018276249982-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:10 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:10 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:10 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:10 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:10 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:10 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:10 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:10 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:10 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:10 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:10 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:11 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000136_137' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:11 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000136_137: Committed
21/01/19 12:47:11 INFO Executor: Finished task 136.0 in stage 1.0 (TID 137). 2527 bytes result sent to driver
21/01/19 12:47:11 INFO TaskSetManager: Starting task 137.0 in stage 1.0 (TID 138, test-runner-pkwzc, executor driver, partition 137, ANY, 7815 bytes)
21/01/19 12:47:11 INFO Executor: Running task 137.0 in stage 1.0 (TID 138)
21/01/19 12:47:11 INFO TaskSetManager: Finished task 136.0 in stage 1.0 (TID 137) in 1088 ms on test-runner-pkwzc (executor driver) (137/200)
21/01/19 12:47:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:11 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-6f02c2ed-b506-41c2-9d17-2fc74dfa9e9c-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:11 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:11 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:11 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:11 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:11 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:11 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:11 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:11 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:11 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:11 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:11 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:12 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000137_138' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:12 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000137_138: Committed
21/01/19 12:47:12 INFO Executor: Finished task 137.0 in stage 1.0 (TID 138). 2527 bytes result sent to driver
21/01/19 12:47:12 INFO TaskSetManager: Starting task 138.0 in stage 1.0 (TID 139, test-runner-pkwzc, executor driver, partition 138, ANY, 7815 bytes)
21/01/19 12:47:12 INFO Executor: Running task 138.0 in stage 1.0 (TID 139)
21/01/19 12:47:12 INFO TaskSetManager: Finished task 137.0 in stage 1.0 (TID 138) in 1132 ms on test-runner-pkwzc (executor driver) (138/200)
21/01/19 12:47:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:12 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:12 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-6ff6e69d-15a9-462e-b3be-9a753c3c1a60-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:12 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:12 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:12 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:12 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:12 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:12 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:12 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:12 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:12 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:12 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:12 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:13 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000138_139' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:13 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000138_139: Committed
21/01/19 12:47:13 INFO Executor: Finished task 138.0 in stage 1.0 (TID 139). 2527 bytes result sent to driver
21/01/19 12:47:13 INFO TaskSetManager: Starting task 139.0 in stage 1.0 (TID 140, test-runner-pkwzc, executor driver, partition 139, ANY, 7815 bytes)
21/01/19 12:47:13 INFO Executor: Running task 139.0 in stage 1.0 (TID 140)
21/01/19 12:47:13 INFO TaskSetManager: Finished task 138.0 in stage 1.0 (TID 139) in 1217 ms on test-runner-pkwzc (executor driver) (139/200)
21/01/19 12:47:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:13 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:13 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-7342166f-bf0a-4022-a9a9-dac2c0bc5af7-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:14 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:14 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:14 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:14 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:14 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:14 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:14 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:14 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:14 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:14 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:14 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:14 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000139_140' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:14 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000139_140: Committed
21/01/19 12:47:14 INFO Executor: Finished task 139.0 in stage 1.0 (TID 140). 2527 bytes result sent to driver
21/01/19 12:47:14 INFO TaskSetManager: Starting task 140.0 in stage 1.0 (TID 141, test-runner-pkwzc, executor driver, partition 140, ANY, 7815 bytes)
21/01/19 12:47:14 INFO Executor: Running task 140.0 in stage 1.0 (TID 141)
21/01/19 12:47:14 INFO TaskSetManager: Finished task 139.0 in stage 1.0 (TID 140) in 1135 ms on test-runner-pkwzc (executor driver) (140/200)
21/01/19 12:47:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:14 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:14 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-73db53ed-30ca-4bd0-83d0-cfcb5e480558-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:15 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:15 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:15 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:15 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:15 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:15 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:15 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:15 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:15 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:15 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:15 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:15 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000140_141' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:15 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000140_141: Committed
21/01/19 12:47:15 INFO Executor: Finished task 140.0 in stage 1.0 (TID 141). 2527 bytes result sent to driver
21/01/19 12:47:15 INFO TaskSetManager: Starting task 141.0 in stage 1.0 (TID 142, test-runner-pkwzc, executor driver, partition 141, ANY, 7815 bytes)
21/01/19 12:47:15 INFO Executor: Running task 141.0 in stage 1.0 (TID 142)
21/01/19 12:47:15 INFO TaskSetManager: Finished task 140.0 in stage 1.0 (TID 141) in 1045 ms on test-runner-pkwzc (executor driver) (141/200)
21/01/19 12:47:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:15 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:15 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-75afd0f5-05cc-46c0-8ec7-73deadfba19a-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:16 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:16 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:16 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:16 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:16 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:16 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:16 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:16 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:16 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:16 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:16 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:17 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000141_142' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:17 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000141_142: Committed
21/01/19 12:47:17 INFO Executor: Finished task 141.0 in stage 1.0 (TID 142). 2527 bytes result sent to driver
21/01/19 12:47:17 INFO TaskSetManager: Starting task 142.0 in stage 1.0 (TID 143, test-runner-pkwzc, executor driver, partition 142, ANY, 7815 bytes)
21/01/19 12:47:17 INFO Executor: Running task 142.0 in stage 1.0 (TID 143)
21/01/19 12:47:17 INFO TaskSetManager: Finished task 141.0 in stage 1.0 (TID 142) in 1134 ms on test-runner-pkwzc (executor driver) (142/200)
21/01/19 12:47:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:17 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:17 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-77dde976-d0b4-433a-9b69-5d0ca6345a58-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:17 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:17 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:17 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:17 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:17 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:17 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:17 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:17 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:17 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:17 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:17 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:18 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000142_143' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:18 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000142_143: Committed
21/01/19 12:47:18 INFO Executor: Finished task 142.0 in stage 1.0 (TID 143). 2527 bytes result sent to driver
21/01/19 12:47:18 INFO TaskSetManager: Starting task 143.0 in stage 1.0 (TID 144, test-runner-pkwzc, executor driver, partition 143, ANY, 7815 bytes)
21/01/19 12:47:18 INFO Executor: Running task 143.0 in stage 1.0 (TID 144)
21/01/19 12:47:18 INFO TaskSetManager: Finished task 142.0 in stage 1.0 (TID 143) in 1106 ms on test-runner-pkwzc (executor driver) (143/200)
21/01/19 12:47:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:18 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:18 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-79cf30a1-648f-442e-8aac-f5d2de925c74-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:18 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:18 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:18 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:18 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:18 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:18 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:18 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:18 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:18 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:18 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:18 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:19 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000143_144' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:19 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000143_144: Committed
21/01/19 12:47:19 INFO Executor: Finished task 143.0 in stage 1.0 (TID 144). 2527 bytes result sent to driver
21/01/19 12:47:19 INFO TaskSetManager: Starting task 144.0 in stage 1.0 (TID 145, test-runner-pkwzc, executor driver, partition 144, ANY, 7815 bytes)
21/01/19 12:47:19 INFO Executor: Running task 144.0 in stage 1.0 (TID 145)
21/01/19 12:47:19 INFO TaskSetManager: Finished task 143.0 in stage 1.0 (TID 144) in 1399 ms on test-runner-pkwzc (executor driver) (144/200)
21/01/19 12:47:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:19 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:19 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-7f3197dd-5e28-4a84-9f34-e3656c61cb98-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:19 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:19 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:19 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:19 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:19 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:19 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:19 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:19 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:19 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:19 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:19 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:20 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000144_145' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:20 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000144_145: Committed
21/01/19 12:47:20 INFO Executor: Finished task 144.0 in stage 1.0 (TID 145). 2527 bytes result sent to driver
21/01/19 12:47:20 INFO TaskSetManager: Starting task 145.0 in stage 1.0 (TID 146, test-runner-pkwzc, executor driver, partition 145, ANY, 7815 bytes)
21/01/19 12:47:20 INFO Executor: Running task 145.0 in stage 1.0 (TID 146)
21/01/19 12:47:20 INFO TaskSetManager: Finished task 144.0 in stage 1.0 (TID 145) in 1105 ms on test-runner-pkwzc (executor driver) (145/200)
21/01/19 12:47:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:20 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:20 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-80eca001-a5a4-4147-a897-fece1cadb60a-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:21 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:21 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:21 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:21 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:21 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:21 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:21 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:21 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:21 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:21 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:21 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:21 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000145_146' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:21 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000145_146: Committed
21/01/19 12:47:21 INFO Executor: Finished task 145.0 in stage 1.0 (TID 146). 2527 bytes result sent to driver
21/01/19 12:47:21 INFO TaskSetManager: Starting task 146.0 in stage 1.0 (TID 147, test-runner-pkwzc, executor driver, partition 146, ANY, 7815 bytes)
21/01/19 12:47:21 INFO Executor: Running task 146.0 in stage 1.0 (TID 147)
21/01/19 12:47:21 INFO TaskSetManager: Finished task 145.0 in stage 1.0 (TID 146) in 1139 ms on test-runner-pkwzc (executor driver) (146/200)
21/01/19 12:47:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:21 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:21 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-82f454ad-add2-4d6d-85fa-5752a44a88c9-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:22 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:22 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:22 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:22 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:22 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:22 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:22 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:22 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:22 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:22 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:22 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:22 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000146_147' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:22 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000146_147: Committed
21/01/19 12:47:22 INFO Executor: Finished task 146.0 in stage 1.0 (TID 147). 2527 bytes result sent to driver
21/01/19 12:47:22 INFO TaskSetManager: Starting task 147.0 in stage 1.0 (TID 148, test-runner-pkwzc, executor driver, partition 147, ANY, 7815 bytes)
21/01/19 12:47:22 INFO Executor: Running task 147.0 in stage 1.0 (TID 148)
21/01/19 12:47:22 INFO TaskSetManager: Finished task 146.0 in stage 1.0 (TID 147) in 1082 ms on test-runner-pkwzc (executor driver) (147/200)
21/01/19 12:47:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:22 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:22 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-8311c08c-07ff-40fe-a317-ae2d5b1c5ac3-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:23 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:23 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:23 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:23 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:23 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:23 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:23 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:23 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:23 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:23 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:23 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:23 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000147_148' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:23 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000147_148: Committed
21/01/19 12:47:23 INFO Executor: Finished task 147.0 in stage 1.0 (TID 148). 2527 bytes result sent to driver
21/01/19 12:47:23 INFO TaskSetManager: Starting task 148.0 in stage 1.0 (TID 149, test-runner-pkwzc, executor driver, partition 148, ANY, 7815 bytes)
21/01/19 12:47:23 INFO Executor: Running task 148.0 in stage 1.0 (TID 149)
21/01/19 12:47:23 INFO TaskSetManager: Finished task 147.0 in stage 1.0 (TID 148) in 1140 ms on test-runner-pkwzc (executor driver) (148/200)
21/01/19 12:47:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:23 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:23 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-863b013a-fc4f-4b69-9d3c-92101182ede6-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:24 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:24 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:24 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:24 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:24 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:24 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:24 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:24 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:24 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:24 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:24 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:25 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000148_149' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:25 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000148_149: Committed
21/01/19 12:47:25 INFO Executor: Finished task 148.0 in stage 1.0 (TID 149). 2527 bytes result sent to driver
21/01/19 12:47:25 INFO TaskSetManager: Starting task 149.0 in stage 1.0 (TID 150, test-runner-pkwzc, executor driver, partition 149, ANY, 7815 bytes)
21/01/19 12:47:25 INFO Executor: Running task 149.0 in stage 1.0 (TID 150)
21/01/19 12:47:25 INFO TaskSetManager: Finished task 148.0 in stage 1.0 (TID 149) in 1276 ms on test-runner-pkwzc (executor driver) (149/200)
21/01/19 12:47:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:25 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:25 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-88a8e717-22d0-41b0-a7e9-fa343458ac50-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:25 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:25 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:25 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:25 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:25 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:25 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:25 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:25 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:25 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:25 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:25 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:26 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000149_150' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:26 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000149_150: Committed
21/01/19 12:47:26 INFO Executor: Finished task 149.0 in stage 1.0 (TID 150). 2527 bytes result sent to driver
21/01/19 12:47:26 INFO TaskSetManager: Starting task 150.0 in stage 1.0 (TID 151, test-runner-pkwzc, executor driver, partition 150, ANY, 7815 bytes)
21/01/19 12:47:26 INFO Executor: Running task 150.0 in stage 1.0 (TID 151)
21/01/19 12:47:26 INFO TaskSetManager: Finished task 149.0 in stage 1.0 (TID 150) in 1138 ms on test-runner-pkwzc (executor driver) (150/200)
21/01/19 12:47:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:26 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:26 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-8a8de862-e55e-4d87-8570-17309d1fd76d-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:26 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:26 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:26 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:26 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:26 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:26 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:26 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:26 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:26 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:26 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:26 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:27 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000150_151' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:27 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000150_151: Committed
21/01/19 12:47:27 INFO Executor: Finished task 150.0 in stage 1.0 (TID 151). 2527 bytes result sent to driver
21/01/19 12:47:27 INFO TaskSetManager: Starting task 151.0 in stage 1.0 (TID 152, test-runner-pkwzc, executor driver, partition 151, ANY, 7815 bytes)
21/01/19 12:47:27 INFO Executor: Running task 151.0 in stage 1.0 (TID 152)
21/01/19 12:47:27 INFO TaskSetManager: Finished task 150.0 in stage 1.0 (TID 151) in 1109 ms on test-runner-pkwzc (executor driver) (151/200)
21/01/19 12:47:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:27 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:27 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-8d237f93-a9cd-4f08-b076-e1e8c45faca4-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:27 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:27 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:27 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:27 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:27 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:27 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:27 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:27 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:27 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:27 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:27 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:28 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000151_152' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:28 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000151_152: Committed
21/01/19 12:47:28 INFO Executor: Finished task 151.0 in stage 1.0 (TID 152). 2527 bytes result sent to driver
21/01/19 12:47:28 INFO TaskSetManager: Starting task 152.0 in stage 1.0 (TID 153, test-runner-pkwzc, executor driver, partition 152, ANY, 7815 bytes)
21/01/19 12:47:28 INFO Executor: Running task 152.0 in stage 1.0 (TID 153)
21/01/19 12:47:28 INFO TaskSetManager: Finished task 151.0 in stage 1.0 (TID 152) in 1094 ms on test-runner-pkwzc (executor driver) (152/200)
21/01/19 12:47:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:28 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:28 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-90820398-99bf-4072-b00c-bcc106673179-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:29 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:29 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:29 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:29 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:29 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:29 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:29 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:29 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:29 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:29 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:29 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:29 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000152_153' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:29 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000152_153: Committed
21/01/19 12:47:29 INFO Executor: Finished task 152.0 in stage 1.0 (TID 153). 2527 bytes result sent to driver
21/01/19 12:47:29 INFO TaskSetManager: Starting task 153.0 in stage 1.0 (TID 154, test-runner-pkwzc, executor driver, partition 153, ANY, 7815 bytes)
21/01/19 12:47:29 INFO Executor: Running task 153.0 in stage 1.0 (TID 154)
21/01/19 12:47:29 INFO TaskSetManager: Finished task 152.0 in stage 1.0 (TID 153) in 1077 ms on test-runner-pkwzc (executor driver) (153/200)
21/01/19 12:47:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:29 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-91961986-d797-4e59-b10a-df419b0b0ee5-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:30 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:30 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:30 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:30 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:30 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:30 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:30 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:30 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:30 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:30 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:30 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:30 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000153_154' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:30 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000153_154: Committed
21/01/19 12:47:30 INFO Executor: Finished task 153.0 in stage 1.0 (TID 154). 2527 bytes result sent to driver
21/01/19 12:47:30 INFO TaskSetManager: Starting task 154.0 in stage 1.0 (TID 155, test-runner-pkwzc, executor driver, partition 154, ANY, 7815 bytes)
21/01/19 12:47:30 INFO Executor: Running task 154.0 in stage 1.0 (TID 155)
21/01/19 12:47:30 INFO TaskSetManager: Finished task 153.0 in stage 1.0 (TID 154) in 1268 ms on test-runner-pkwzc (executor driver) (154/200)
21/01/19 12:47:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:30 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-99619c57-f68b-4a1f-9306-9929d822ddd4-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:31 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:31 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:31 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:31 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:31 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:31 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:31 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:31 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:31 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:31 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:31 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:32 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000154_155' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:32 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000154_155: Committed
21/01/19 12:47:32 INFO Executor: Finished task 154.0 in stage 1.0 (TID 155). 2527 bytes result sent to driver
21/01/19 12:47:32 INFO TaskSetManager: Starting task 155.0 in stage 1.0 (TID 156, test-runner-pkwzc, executor driver, partition 155, ANY, 7815 bytes)
21/01/19 12:47:32 INFO Executor: Running task 155.0 in stage 1.0 (TID 156)
21/01/19 12:47:32 INFO TaskSetManager: Finished task 154.0 in stage 1.0 (TID 155) in 1091 ms on test-runner-pkwzc (executor driver) (155/200)
21/01/19 12:47:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:32 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:32 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-998e6493-7be9-4aac-8cc5-707ea3e6a2b1-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:32 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:32 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:32 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:32 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:32 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:32 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:32 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:32 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:32 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:32 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:32 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:33 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000155_156' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:33 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000155_156: Committed
21/01/19 12:47:33 INFO Executor: Finished task 155.0 in stage 1.0 (TID 156). 2527 bytes result sent to driver
21/01/19 12:47:33 INFO TaskSetManager: Starting task 156.0 in stage 1.0 (TID 157, test-runner-pkwzc, executor driver, partition 156, ANY, 7815 bytes)
21/01/19 12:47:33 INFO Executor: Running task 156.0 in stage 1.0 (TID 157)
21/01/19 12:47:33 INFO TaskSetManager: Finished task 155.0 in stage 1.0 (TID 156) in 1097 ms on test-runner-pkwzc (executor driver) (156/200)
21/01/19 12:47:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:33 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:33 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-9aef0959-9535-4d6d-aafc-17abd7db62d8-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:33 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:33 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:33 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:33 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:33 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:33 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:33 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:33 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:33 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:33 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:33 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:34 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000156_157' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:34 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000156_157: Committed
21/01/19 12:47:34 INFO Executor: Finished task 156.0 in stage 1.0 (TID 157). 2527 bytes result sent to driver
21/01/19 12:47:34 INFO TaskSetManager: Starting task 157.0 in stage 1.0 (TID 158, test-runner-pkwzc, executor driver, partition 157, ANY, 7815 bytes)
21/01/19 12:47:34 INFO Executor: Running task 157.0 in stage 1.0 (TID 158)
21/01/19 12:47:34 INFO TaskSetManager: Finished task 156.0 in stage 1.0 (TID 157) in 1068 ms on test-runner-pkwzc (executor driver) (157/200)
21/01/19 12:47:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:34 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:34 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-9be6205f-b1b1-4921-b52e-ec06dcd30242-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:34 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:34 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:34 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:34 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:34 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:34 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:34 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:34 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:34 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:34 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:34 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:34 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:34 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:35 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000157_158' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:35 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000157_158: Committed
21/01/19 12:47:35 INFO Executor: Finished task 157.0 in stage 1.0 (TID 158). 2527 bytes result sent to driver
21/01/19 12:47:35 INFO TaskSetManager: Starting task 158.0 in stage 1.0 (TID 159, test-runner-pkwzc, executor driver, partition 158, ANY, 7815 bytes)
21/01/19 12:47:35 INFO Executor: Running task 158.0 in stage 1.0 (TID 159)
21/01/19 12:47:35 INFO TaskSetManager: Finished task 157.0 in stage 1.0 (TID 158) in 1144 ms on test-runner-pkwzc (executor driver) (158/200)
21/01/19 12:47:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:35 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:35 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-9dbed8f1-40df-4500-a391-43d1cb7f2744-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:35 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:35 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:35 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:35 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:35 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:35 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:35 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:35 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:35 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:35 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:35 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:36 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000158_159' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:36 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000158_159: Committed
21/01/19 12:47:36 INFO Executor: Finished task 158.0 in stage 1.0 (TID 159). 2527 bytes result sent to driver
21/01/19 12:47:36 INFO TaskSetManager: Starting task 159.0 in stage 1.0 (TID 160, test-runner-pkwzc, executor driver, partition 159, ANY, 7815 bytes)
21/01/19 12:47:36 INFO Executor: Running task 159.0 in stage 1.0 (TID 160)
21/01/19 12:47:36 INFO TaskSetManager: Finished task 158.0 in stage 1.0 (TID 159) in 1149 ms on test-runner-pkwzc (executor driver) (159/200)
21/01/19 12:47:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:36 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:36 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-9f3e2686-e7e8-4b68-806b-7f836f7df0fd-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:36 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:36 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:36 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:36 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:36 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:36 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:36 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:36 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:36 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:36 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:36 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:36 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:36 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:37 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:37 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000159_160' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:37 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000159_160: Committed
21/01/19 12:47:37 INFO Executor: Finished task 159.0 in stage 1.0 (TID 160). 2527 bytes result sent to driver
21/01/19 12:47:37 INFO TaskSetManager: Starting task 160.0 in stage 1.0 (TID 161, test-runner-pkwzc, executor driver, partition 160, ANY, 7815 bytes)
21/01/19 12:47:37 INFO Executor: Running task 160.0 in stage 1.0 (TID 161)
21/01/19 12:47:37 INFO TaskSetManager: Finished task 159.0 in stage 1.0 (TID 160) in 1296 ms on test-runner-pkwzc (executor driver) (160/200)
21/01/19 12:47:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:37 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:37 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-9f719450-3437-497b-b90e-7e4896c12975-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:38 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:38 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:38 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:38 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:38 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:38 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:38 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:38 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:38 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:38 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:38 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:38 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000160_161' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:38 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000160_161: Committed
21/01/19 12:47:38 INFO Executor: Finished task 160.0 in stage 1.0 (TID 161). 2527 bytes result sent to driver
21/01/19 12:47:38 INFO TaskSetManager: Starting task 161.0 in stage 1.0 (TID 162, test-runner-pkwzc, executor driver, partition 161, ANY, 7815 bytes)
21/01/19 12:47:38 INFO TaskSetManager: Finished task 160.0 in stage 1.0 (TID 161) in 1113 ms on test-runner-pkwzc (executor driver) (161/200)
21/01/19 12:47:38 INFO Executor: Running task 161.0 in stage 1.0 (TID 162)
21/01/19 12:47:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:38 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:38 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-a06dcf0c-d6d0-4c40-888c-dab19e6ae9f1-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:39 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:39 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:39 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:39 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:39 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:39 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:39 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:39 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:39 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:39 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:39 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:40 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000161_162' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:40 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000161_162: Committed
21/01/19 12:47:40 INFO Executor: Finished task 161.0 in stage 1.0 (TID 162). 2527 bytes result sent to driver
21/01/19 12:47:40 INFO TaskSetManager: Starting task 162.0 in stage 1.0 (TID 163, test-runner-pkwzc, executor driver, partition 162, ANY, 7815 bytes)
21/01/19 12:47:40 INFO Executor: Running task 162.0 in stage 1.0 (TID 163)
21/01/19 12:47:40 INFO TaskSetManager: Finished task 161.0 in stage 1.0 (TID 162) in 1145 ms on test-runner-pkwzc (executor driver) (162/200)
21/01/19 12:47:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:40 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:40 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-a098bf99-810f-4b40-b6ce-979d46995aaa-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:40 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:40 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:40 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:40 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:40 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:40 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:40 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:40 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:40 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:40 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:40 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:41 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000162_163' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:41 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000162_163: Committed
21/01/19 12:47:41 INFO Executor: Finished task 162.0 in stage 1.0 (TID 163). 2527 bytes result sent to driver
21/01/19 12:47:41 INFO TaskSetManager: Starting task 163.0 in stage 1.0 (TID 164, test-runner-pkwzc, executor driver, partition 163, ANY, 7815 bytes)
21/01/19 12:47:41 INFO Executor: Running task 163.0 in stage 1.0 (TID 164)
21/01/19 12:47:41 INFO TaskSetManager: Finished task 162.0 in stage 1.0 (TID 163) in 1112 ms on test-runner-pkwzc (executor driver) (163/200)
21/01/19 12:47:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:41 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:41 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-a0f951cf-bded-47bb-9829-afe4ec0fac7b-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:41 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:41 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:41 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:41 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:41 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:41 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:41 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:41 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:41 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:41 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:41 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:42 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000163_164' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:42 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000163_164: Committed
21/01/19 12:47:42 INFO Executor: Finished task 163.0 in stage 1.0 (TID 164). 2527 bytes result sent to driver
21/01/19 12:47:42 INFO TaskSetManager: Starting task 164.0 in stage 1.0 (TID 165, test-runner-pkwzc, executor driver, partition 164, ANY, 7815 bytes)
21/01/19 12:47:42 INFO Executor: Running task 164.0 in stage 1.0 (TID 165)
21/01/19 12:47:42 INFO TaskSetManager: Finished task 163.0 in stage 1.0 (TID 164) in 1128 ms on test-runner-pkwzc (executor driver) (164/200)
21/01/19 12:47:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:42 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:42 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-a867f688-b15f-46de-bd48-95c4241a74fc-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:42 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:42 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:42 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:42 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:42 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:42 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:42 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:42 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:42 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:42 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:42 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:43 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000164_165' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:43 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000164_165: Committed
21/01/19 12:47:43 INFO Executor: Finished task 164.0 in stage 1.0 (TID 165). 2527 bytes result sent to driver
21/01/19 12:47:43 INFO TaskSetManager: Starting task 165.0 in stage 1.0 (TID 166, test-runner-pkwzc, executor driver, partition 165, ANY, 7815 bytes)
21/01/19 12:47:43 INFO Executor: Running task 165.0 in stage 1.0 (TID 166)
21/01/19 12:47:43 INFO TaskSetManager: Finished task 164.0 in stage 1.0 (TID 165) in 1164 ms on test-runner-pkwzc (executor driver) (165/200)
21/01/19 12:47:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:43 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:43 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-ad0ed8c7-73d8-45b8-93cc-5c8c201f9f8d-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:43 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:43 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:43 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:43 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:43 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:43 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:43 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:43 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:43 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:43 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:43 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:44 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000165_166' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:44 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000165_166: Committed
21/01/19 12:47:44 INFO Executor: Finished task 165.0 in stage 1.0 (TID 166). 2527 bytes result sent to driver
21/01/19 12:47:44 INFO TaskSetManager: Starting task 166.0 in stage 1.0 (TID 167, test-runner-pkwzc, executor driver, partition 166, ANY, 7815 bytes)
21/01/19 12:47:44 INFO Executor: Running task 166.0 in stage 1.0 (TID 167)
21/01/19 12:47:44 INFO TaskSetManager: Finished task 165.0 in stage 1.0 (TID 166) in 1138 ms on test-runner-pkwzc (executor driver) (166/200)
21/01/19 12:47:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:44 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:44 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-af131e46-cef7-45a6-836d-f289dee72c0a-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:45 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:45 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:45 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:45 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:45 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:45 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:45 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:45 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:45 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:45 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:45 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:45 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000166_167' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:45 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000166_167: Committed
21/01/19 12:47:45 INFO Executor: Finished task 166.0 in stage 1.0 (TID 167). 2527 bytes result sent to driver
21/01/19 12:47:45 INFO TaskSetManager: Starting task 167.0 in stage 1.0 (TID 168, test-runner-pkwzc, executor driver, partition 167, ANY, 7815 bytes)
21/01/19 12:47:45 INFO Executor: Running task 167.0 in stage 1.0 (TID 168)
21/01/19 12:47:45 INFO TaskSetManager: Finished task 166.0 in stage 1.0 (TID 167) in 1114 ms on test-runner-pkwzc (executor driver) (167/200)
21/01/19 12:47:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:45 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:45 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-b6989f56-ed72-4068-93ee-ab937a320a42-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:46 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:46 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:46 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:46 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:46 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:46 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:46 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:46 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:46 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:46 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:46 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:46 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000167_168' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:46 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000167_168: Committed
21/01/19 12:47:46 INFO Executor: Finished task 167.0 in stage 1.0 (TID 168). 2527 bytes result sent to driver
21/01/19 12:47:46 INFO TaskSetManager: Starting task 168.0 in stage 1.0 (TID 169, test-runner-pkwzc, executor driver, partition 168, ANY, 7815 bytes)
21/01/19 12:47:46 INFO Executor: Running task 168.0 in stage 1.0 (TID 169)
21/01/19 12:47:46 INFO TaskSetManager: Finished task 167.0 in stage 1.0 (TID 168) in 1102 ms on test-runner-pkwzc (executor driver) (168/200)
21/01/19 12:47:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:46 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:46 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-bb8cf56f-175f-4109-a5fc-d273dc530386-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:47 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:47 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:47 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:47 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:47 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:47 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:47 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:47 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:47 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:47 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:47 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:47 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000168_169' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:47 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000168_169: Committed
21/01/19 12:47:47 INFO Executor: Finished task 168.0 in stage 1.0 (TID 169). 2527 bytes result sent to driver
21/01/19 12:47:47 INFO TaskSetManager: Starting task 169.0 in stage 1.0 (TID 170, test-runner-pkwzc, executor driver, partition 169, ANY, 7815 bytes)
21/01/19 12:47:47 INFO Executor: Running task 169.0 in stage 1.0 (TID 170)
21/01/19 12:47:47 INFO TaskSetManager: Finished task 168.0 in stage 1.0 (TID 169) in 1194 ms on test-runner-pkwzc (executor driver) (169/200)
21/01/19 12:47:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:48 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:48 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-bebd176a-cd15-45e2-b420-7fc99bd64d26-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:48 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:48 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:48 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:48 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:48 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:48 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:48 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:48 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:48 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:48 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:48 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:49 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000169_170' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:49 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000169_170: Committed
21/01/19 12:47:49 INFO Executor: Finished task 169.0 in stage 1.0 (TID 170). 2527 bytes result sent to driver
21/01/19 12:47:49 INFO TaskSetManager: Starting task 170.0 in stage 1.0 (TID 171, test-runner-pkwzc, executor driver, partition 170, ANY, 7815 bytes)
21/01/19 12:47:49 INFO Executor: Running task 170.0 in stage 1.0 (TID 171)
21/01/19 12:47:49 INFO TaskSetManager: Finished task 169.0 in stage 1.0 (TID 170) in 1216 ms on test-runner-pkwzc (executor driver) (170/200)
21/01/19 12:47:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:49 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-c148f478-dc25-4794-ac41-5955407c7081-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:49 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:49 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:49 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:49 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:49 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:49 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:49 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:49 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:49 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:49 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:49 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:50 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000170_171' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:50 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000170_171: Committed
21/01/19 12:47:50 INFO Executor: Finished task 170.0 in stage 1.0 (TID 171). 2527 bytes result sent to driver
21/01/19 12:47:50 INFO TaskSetManager: Starting task 171.0 in stage 1.0 (TID 172, test-runner-pkwzc, executor driver, partition 171, ANY, 7815 bytes)
21/01/19 12:47:50 INFO Executor: Running task 171.0 in stage 1.0 (TID 172)
21/01/19 12:47:50 INFO TaskSetManager: Finished task 170.0 in stage 1.0 (TID 171) in 1177 ms on test-runner-pkwzc (executor driver) (171/200)
21/01/19 12:47:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:50 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:50 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-c5b0c25e-89da-4403-8565-f2881c854b0f-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:50 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:50 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:50 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:50 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:50 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:50 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:50 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:50 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:50 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:50 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:50 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:50 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:50 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:50 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:51 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000171_172' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:51 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000171_172: Committed
21/01/19 12:47:51 INFO Executor: Finished task 171.0 in stage 1.0 (TID 172). 2527 bytes result sent to driver
21/01/19 12:47:51 INFO TaskSetManager: Starting task 172.0 in stage 1.0 (TID 173, test-runner-pkwzc, executor driver, partition 172, ANY, 7815 bytes)
21/01/19 12:47:51 INFO Executor: Running task 172.0 in stage 1.0 (TID 173)
21/01/19 12:47:51 INFO TaskSetManager: Finished task 171.0 in stage 1.0 (TID 172) in 1125 ms on test-runner-pkwzc (executor driver) (172/200)
21/01/19 12:47:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:51 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-c81cea3b-259a-41b2-a7b7-a74563f3ab56-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:51 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:51 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:51 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:51 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:51 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:51 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:51 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:51 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:51 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:51 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:51 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:51 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:51 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:52 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000172_173' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:52 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000172_173: Committed
21/01/19 12:47:52 INFO Executor: Finished task 172.0 in stage 1.0 (TID 173). 2527 bytes result sent to driver
21/01/19 12:47:52 INFO TaskSetManager: Starting task 173.0 in stage 1.0 (TID 174, test-runner-pkwzc, executor driver, partition 173, ANY, 7815 bytes)
21/01/19 12:47:52 INFO Executor: Running task 173.0 in stage 1.0 (TID 174)
21/01/19 12:47:52 INFO TaskSetManager: Finished task 172.0 in stage 1.0 (TID 173) in 1155 ms on test-runner-pkwzc (executor driver) (173/200)
21/01/19 12:47:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:52 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:52 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-d30a19da-e1f6-457b-8ca4-5dc101da7f7c-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:53 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:53 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:53 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:53 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:53 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:53 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:53 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:53 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:53 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:53 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:53 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:53 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000173_174' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:53 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000173_174: Committed
21/01/19 12:47:53 INFO Executor: Finished task 173.0 in stage 1.0 (TID 174). 2527 bytes result sent to driver
21/01/19 12:47:53 INFO TaskSetManager: Starting task 174.0 in stage 1.0 (TID 175, test-runner-pkwzc, executor driver, partition 174, ANY, 7815 bytes)
21/01/19 12:47:53 INFO Executor: Running task 174.0 in stage 1.0 (TID 175)
21/01/19 12:47:53 INFO TaskSetManager: Finished task 173.0 in stage 1.0 (TID 174) in 1335 ms on test-runner-pkwzc (executor driver) (174/200)
21/01/19 12:47:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:54 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:54 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-d7d0cd5d-3de6-44c5-b18e-50e482a9a13b-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:54 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:54 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:54 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:54 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:54 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:54 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:54 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:54 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:54 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:54 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:54 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:55 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000174_175' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:55 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000174_175: Committed
21/01/19 12:47:55 INFO Executor: Finished task 174.0 in stage 1.0 (TID 175). 2527 bytes result sent to driver
21/01/19 12:47:55 INFO TaskSetManager: Starting task 175.0 in stage 1.0 (TID 176, test-runner-pkwzc, executor driver, partition 175, ANY, 7815 bytes)
21/01/19 12:47:55 INFO TaskSetManager: Finished task 174.0 in stage 1.0 (TID 175) in 1153 ms on test-runner-pkwzc (executor driver) (175/200)
21/01/19 12:47:55 INFO Executor: Running task 175.0 in stage 1.0 (TID 176)
21/01/19 12:47:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:55 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-db874692-6af7-4c0d-935d-46c12144f26f-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:55 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:55 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:55 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:55 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:55 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:55 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:55 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:55 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:55 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:55 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:55 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:56 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000175_176' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:56 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000175_176: Committed
21/01/19 12:47:56 INFO Executor: Finished task 175.0 in stage 1.0 (TID 176). 2527 bytes result sent to driver
21/01/19 12:47:56 INFO TaskSetManager: Starting task 176.0 in stage 1.0 (TID 177, test-runner-pkwzc, executor driver, partition 176, ANY, 7815 bytes)
21/01/19 12:47:56 INFO Executor: Running task 176.0 in stage 1.0 (TID 177)
21/01/19 12:47:56 INFO TaskSetManager: Finished task 175.0 in stage 1.0 (TID 176) in 1140 ms on test-runner-pkwzc (executor driver) (176/200)
21/01/19 12:47:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:56 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:56 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-de45cb6c-27fc-4571-8d56-130d9277913a-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:56 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:56 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:56 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:56 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:56 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:56 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:56 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:56 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:56 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:56 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:56 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:57 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000176_177' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:57 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000176_177: Committed
21/01/19 12:47:57 INFO Executor: Finished task 176.0 in stage 1.0 (TID 177). 2527 bytes result sent to driver
21/01/19 12:47:57 INFO TaskSetManager: Starting task 177.0 in stage 1.0 (TID 178, test-runner-pkwzc, executor driver, partition 177, ANY, 7815 bytes)
21/01/19 12:47:57 INFO Executor: Running task 177.0 in stage 1.0 (TID 178)
21/01/19 12:47:57 INFO TaskSetManager: Finished task 176.0 in stage 1.0 (TID 177) in 1116 ms on test-runner-pkwzc (executor driver) (177/200)
21/01/19 12:47:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:57 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:57 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-e34049e5-10f3-4005-aead-700376f947e0-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:57 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:57 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:57 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:57 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:57 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:57 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:57 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:57 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:57 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:57 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:57 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:58 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000177_178' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:58 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000177_178: Committed
21/01/19 12:47:58 INFO Executor: Finished task 177.0 in stage 1.0 (TID 178). 2527 bytes result sent to driver
21/01/19 12:47:58 INFO TaskSetManager: Starting task 178.0 in stage 1.0 (TID 179, test-runner-pkwzc, executor driver, partition 178, ANY, 7815 bytes)
21/01/19 12:47:58 INFO Executor: Running task 178.0 in stage 1.0 (TID 179)
21/01/19 12:47:58 INFO TaskSetManager: Finished task 177.0 in stage 1.0 (TID 178) in 1119 ms on test-runner-pkwzc (executor driver) (178/200)
21/01/19 12:47:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:58 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:58 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-e4658a7c-bbfe-40f0-a485-0c0d0520d5fe-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:47:58 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:58 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:47:58 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:47:58 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:47:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:47:58 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:47:58 INFO ParquetOutputFormat: Validation is off
21/01/19 12:47:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:47:58 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:47:58 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:47:58 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:47:58 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:47:58 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:47:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:47:59 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000178_179' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:47:59 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000178_179: Committed
21/01/19 12:47:59 INFO Executor: Finished task 178.0 in stage 1.0 (TID 179). 2527 bytes result sent to driver
21/01/19 12:47:59 INFO TaskSetManager: Starting task 179.0 in stage 1.0 (TID 180, test-runner-pkwzc, executor driver, partition 179, ANY, 7815 bytes)
21/01/19 12:47:59 INFO Executor: Running task 179.0 in stage 1.0 (TID 180)
21/01/19 12:47:59 INFO TaskSetManager: Finished task 178.0 in stage 1.0 (TID 179) in 1255 ms on test-runner-pkwzc (executor driver) (179/200)
21/01/19 12:47:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:59 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:47:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:47:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:47:59 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-e6199de0-1540-4646-bb8a-98b41e535d15-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:48:00 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:00 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:00 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:48:00 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:48:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:48:00 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:48:00 INFO ParquetOutputFormat: Validation is off
21/01/19 12:48:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:48:00 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:48:00 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:48:00 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:48:00 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:48:00 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:48:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:48:00 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000179_180' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:48:00 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000179_180: Committed
21/01/19 12:48:00 INFO Executor: Finished task 179.0 in stage 1.0 (TID 180). 2527 bytes result sent to driver
21/01/19 12:48:00 INFO TaskSetManager: Starting task 180.0 in stage 1.0 (TID 181, test-runner-pkwzc, executor driver, partition 180, ANY, 7815 bytes)
21/01/19 12:48:00 INFO Executor: Running task 180.0 in stage 1.0 (TID 181)
21/01/19 12:48:00 INFO TaskSetManager: Finished task 179.0 in stage 1.0 (TID 180) in 1079 ms on test-runner-pkwzc (executor driver) (180/200)
21/01/19 12:48:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:48:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:48:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:48:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:48:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:48:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:48:00 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-e72dbedc-20dd-4add-a38a-76f46ceb8158-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:48:01 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:01 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:01 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:48:01 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:48:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:48:01 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:48:01 INFO ParquetOutputFormat: Validation is off
21/01/19 12:48:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:48:01 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:48:01 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:48:01 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:48:01 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:48:01 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:48:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:48:01 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000180_181' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:48:01 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000180_181: Committed
21/01/19 12:48:01 INFO Executor: Finished task 180.0 in stage 1.0 (TID 181). 2527 bytes result sent to driver
21/01/19 12:48:01 INFO TaskSetManager: Starting task 181.0 in stage 1.0 (TID 182, test-runner-pkwzc, executor driver, partition 181, ANY, 7815 bytes)
21/01/19 12:48:01 INFO Executor: Running task 181.0 in stage 1.0 (TID 182)
21/01/19 12:48:01 INFO TaskSetManager: Finished task 180.0 in stage 1.0 (TID 181) in 1101 ms on test-runner-pkwzc (executor driver) (181/200)
21/01/19 12:48:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:48:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:48:01 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:48:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:48:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:48:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:48:01 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-eb5a9dbb-2872-489a-939e-e9436cf781be-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:48:02 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:02 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:02 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:48:02 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:48:02 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:48:02 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:48:02 INFO ParquetOutputFormat: Validation is off
21/01/19 12:48:02 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:48:02 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:48:02 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:48:02 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:48:02 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:48:02 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:48:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:48:03 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000181_182' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:48:03 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000181_182: Committed
21/01/19 12:48:03 INFO Executor: Finished task 181.0 in stage 1.0 (TID 182). 2527 bytes result sent to driver
21/01/19 12:48:03 INFO TaskSetManager: Starting task 182.0 in stage 1.0 (TID 183, test-runner-pkwzc, executor driver, partition 182, ANY, 7815 bytes)
21/01/19 12:48:03 INFO Executor: Running task 182.0 in stage 1.0 (TID 183)
21/01/19 12:48:03 INFO TaskSetManager: Finished task 181.0 in stage 1.0 (TID 182) in 1088 ms on test-runner-pkwzc (executor driver) (182/200)
21/01/19 12:48:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:48:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:48:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:48:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:48:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:48:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:48:03 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-ee3337dd-1f56-4661-870d-352eb754a776-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:48:03 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:03 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:03 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:48:03 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:48:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:48:03 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:48:03 INFO ParquetOutputFormat: Validation is off
21/01/19 12:48:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:48:03 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:48:03 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:48:03 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:48:03 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:48:03 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:48:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:48:04 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000182_183' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:48:04 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000182_183: Committed
21/01/19 12:48:04 INFO Executor: Finished task 182.0 in stage 1.0 (TID 183). 2527 bytes result sent to driver
21/01/19 12:48:04 INFO TaskSetManager: Starting task 183.0 in stage 1.0 (TID 184, test-runner-pkwzc, executor driver, partition 183, ANY, 7815 bytes)
21/01/19 12:48:04 INFO Executor: Running task 183.0 in stage 1.0 (TID 184)
21/01/19 12:48:04 INFO TaskSetManager: Finished task 182.0 in stage 1.0 (TID 183) in 1109 ms on test-runner-pkwzc (executor driver) (183/200)
21/01/19 12:48:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:48:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:48:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:48:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:48:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:48:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:48:04 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-f02cfea1-5d4d-457e-91bc-b7fad065106b-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:48:04 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:04 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:04 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:48:04 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:48:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:48:04 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:48:04 INFO ParquetOutputFormat: Validation is off
21/01/19 12:48:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:48:04 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:48:04 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:48:04 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:48:04 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:48:04 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:48:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:48:05 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000183_184' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:48:05 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000183_184: Committed
21/01/19 12:48:05 INFO Executor: Finished task 183.0 in stage 1.0 (TID 184). 2527 bytes result sent to driver
21/01/19 12:48:05 INFO TaskSetManager: Starting task 184.0 in stage 1.0 (TID 185, test-runner-pkwzc, executor driver, partition 184, ANY, 7815 bytes)
21/01/19 12:48:05 INFO Executor: Running task 184.0 in stage 1.0 (TID 185)
21/01/19 12:48:05 INFO TaskSetManager: Finished task 183.0 in stage 1.0 (TID 184) in 1181 ms on test-runner-pkwzc (executor driver) (184/200)
21/01/19 12:48:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:48:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:48:05 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:48:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:48:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:48:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:48:05 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-f07b30d9-8df7-4e2b-b9e7-31082de40e8a-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:48:05 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:05 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:05 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:48:05 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:48:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:48:05 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:48:05 INFO ParquetOutputFormat: Validation is off
21/01/19 12:48:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:48:05 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:48:05 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:48:05 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:48:05 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:48:05 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:48:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:48:06 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000184_185' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:48:06 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000184_185: Committed
21/01/19 12:48:06 INFO Executor: Finished task 184.0 in stage 1.0 (TID 185). 2527 bytes result sent to driver
21/01/19 12:48:06 INFO TaskSetManager: Starting task 185.0 in stage 1.0 (TID 186, test-runner-pkwzc, executor driver, partition 185, ANY, 7815 bytes)
21/01/19 12:48:06 INFO Executor: Running task 185.0 in stage 1.0 (TID 186)
21/01/19 12:48:06 INFO TaskSetManager: Finished task 184.0 in stage 1.0 (TID 185) in 1111 ms on test-runner-pkwzc (executor driver) (185/200)
21/01/19 12:48:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:48:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:48:06 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:48:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:48:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:48:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:48:06 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-f319fa94-a8df-4a82-a673-366f6959d3dc-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:48:06 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:06 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:06 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:48:06 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:48:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:48:06 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:48:06 INFO ParquetOutputFormat: Validation is off
21/01/19 12:48:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:48:06 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:48:06 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:48:06 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:48:06 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:48:06 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:48:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:48:07 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000185_186' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:48:07 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000185_186: Committed
21/01/19 12:48:07 INFO Executor: Finished task 185.0 in stage 1.0 (TID 186). 2527 bytes result sent to driver
21/01/19 12:48:07 INFO TaskSetManager: Starting task 186.0 in stage 1.0 (TID 187, test-runner-pkwzc, executor driver, partition 186, ANY, 7815 bytes)
21/01/19 12:48:07 INFO Executor: Running task 186.0 in stage 1.0 (TID 187)
21/01/19 12:48:07 INFO TaskSetManager: Finished task 185.0 in stage 1.0 (TID 186) in 1163 ms on test-runner-pkwzc (executor driver) (186/200)
21/01/19 12:48:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:48:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:48:07 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:48:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:48:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:48:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:48:07 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-f3452ff3-b7f8-4835-9b9f-3efc402b0171-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:48:08 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:08 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:08 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:48:08 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:48:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:48:08 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:48:08 INFO ParquetOutputFormat: Validation is off
21/01/19 12:48:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:48:08 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:48:08 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:48:08 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:48:08 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:48:08 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:48:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:48:08 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000186_187' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:48:08 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000186_187: Committed
21/01/19 12:48:08 INFO Executor: Finished task 186.0 in stage 1.0 (TID 187). 2527 bytes result sent to driver
21/01/19 12:48:08 INFO TaskSetManager: Starting task 187.0 in stage 1.0 (TID 188, test-runner-pkwzc, executor driver, partition 187, ANY, 7815 bytes)
21/01/19 12:48:08 INFO Executor: Running task 187.0 in stage 1.0 (TID 188)
21/01/19 12:48:08 INFO TaskSetManager: Finished task 186.0 in stage 1.0 (TID 187) in 1257 ms on test-runner-pkwzc (executor driver) (187/200)
21/01/19 12:48:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:48:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:48:08 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:48:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:48:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:48:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:48:08 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-f3ca9d4a-4fde-41f3-be16-0546484a936b-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:48:09 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:09 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:09 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:48:09 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:48:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:48:09 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:48:09 INFO ParquetOutputFormat: Validation is off
21/01/19 12:48:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:48:09 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:48:09 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:48:09 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:48:09 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:48:09 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:48:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:48:09 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000187_188' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:48:09 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000187_188: Committed
21/01/19 12:48:09 INFO Executor: Finished task 187.0 in stage 1.0 (TID 188). 2527 bytes result sent to driver
21/01/19 12:48:09 INFO TaskSetManager: Starting task 188.0 in stage 1.0 (TID 189, test-runner-pkwzc, executor driver, partition 188, ANY, 7815 bytes)
21/01/19 12:48:09 INFO Executor: Running task 188.0 in stage 1.0 (TID 189)
21/01/19 12:48:09 INFO TaskSetManager: Finished task 187.0 in stage 1.0 (TID 188) in 1115 ms on test-runner-pkwzc (executor driver) (188/200)
21/01/19 12:48:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:48:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:48:09 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:48:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:48:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:48:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:48:09 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-f5034c03-7476-4306-8297-4998e5b5cf90-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:48:10 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:10 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:10 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:48:10 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:48:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:48:10 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:48:10 INFO ParquetOutputFormat: Validation is off
21/01/19 12:48:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:48:10 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:48:10 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:48:10 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:48:10 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:48:10 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:48:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:48:11 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000188_189' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:48:11 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000188_189: Committed
21/01/19 12:48:11 INFO Executor: Finished task 188.0 in stage 1.0 (TID 189). 2527 bytes result sent to driver
21/01/19 12:48:11 INFO TaskSetManager: Starting task 189.0 in stage 1.0 (TID 190, test-runner-pkwzc, executor driver, partition 189, ANY, 7815 bytes)
21/01/19 12:48:11 INFO Executor: Running task 189.0 in stage 1.0 (TID 190)
21/01/19 12:48:11 INFO TaskSetManager: Finished task 188.0 in stage 1.0 (TID 189) in 1367 ms on test-runner-pkwzc (executor driver) (189/200)
21/01/19 12:48:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:48:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:48:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:48:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:48:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:48:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:48:11 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-f5a86e4b-57fc-4637-9357-a00df45cd04a-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:48:11 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:11 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:11 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:48:11 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:48:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:48:11 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:48:11 INFO ParquetOutputFormat: Validation is off
21/01/19 12:48:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:48:11 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:48:11 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:48:11 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:48:11 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:48:11 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:48:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:48:12 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000189_190' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:48:12 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000189_190: Committed
21/01/19 12:48:12 INFO Executor: Finished task 189.0 in stage 1.0 (TID 190). 2527 bytes result sent to driver
21/01/19 12:48:12 INFO TaskSetManager: Starting task 190.0 in stage 1.0 (TID 191, test-runner-pkwzc, executor driver, partition 190, ANY, 7815 bytes)
21/01/19 12:48:12 INFO Executor: Running task 190.0 in stage 1.0 (TID 191)
21/01/19 12:48:12 INFO TaskSetManager: Finished task 189.0 in stage 1.0 (TID 190) in 1057 ms on test-runner-pkwzc (executor driver) (190/200)
21/01/19 12:48:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:48:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:48:12 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:48:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:48:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:48:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:48:12 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-f71817ba-6ecb-43d6-b16e-1e90bf8057f2-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:48:12 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:12 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:12 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:48:12 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:48:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:48:12 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:48:12 INFO ParquetOutputFormat: Validation is off
21/01/19 12:48:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:48:12 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:48:12 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:48:12 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:48:12 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:48:12 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:48:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:48:13 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000190_191' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:48:13 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000190_191: Committed
21/01/19 12:48:13 INFO Executor: Finished task 190.0 in stage 1.0 (TID 191). 2527 bytes result sent to driver
21/01/19 12:48:13 INFO TaskSetManager: Starting task 191.0 in stage 1.0 (TID 192, test-runner-pkwzc, executor driver, partition 191, ANY, 7815 bytes)
21/01/19 12:48:13 INFO Executor: Running task 191.0 in stage 1.0 (TID 192)
21/01/19 12:48:13 INFO TaskSetManager: Finished task 190.0 in stage 1.0 (TID 191) in 1208 ms on test-runner-pkwzc (executor driver) (191/200)
21/01/19 12:48:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:48:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:48:13 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:48:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:48:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:48:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:48:13 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-f74cc5bc-7b57-491d-a896-cef19b769fbb-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:48:14 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:14 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:14 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:48:14 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:48:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:48:14 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:48:14 INFO ParquetOutputFormat: Validation is off
21/01/19 12:48:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:48:14 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:48:14 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:48:14 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:48:14 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:48:14 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:48:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:48:14 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000191_192' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:48:14 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000191_192: Committed
21/01/19 12:48:14 INFO Executor: Finished task 191.0 in stage 1.0 (TID 192). 2527 bytes result sent to driver
21/01/19 12:48:14 INFO TaskSetManager: Starting task 192.0 in stage 1.0 (TID 193, test-runner-pkwzc, executor driver, partition 192, ANY, 7815 bytes)
21/01/19 12:48:14 INFO Executor: Running task 192.0 in stage 1.0 (TID 193)
21/01/19 12:48:14 INFO TaskSetManager: Finished task 191.0 in stage 1.0 (TID 192) in 1225 ms on test-runner-pkwzc (executor driver) (192/200)
21/01/19 12:48:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:48:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:48:14 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:48:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:48:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:48:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:48:14 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-f753efdf-c067-4d05-9150-058247d55222-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:48:15 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:15 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:15 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:48:15 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:48:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:48:15 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:48:15 INFO ParquetOutputFormat: Validation is off
21/01/19 12:48:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:48:15 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:48:15 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:48:15 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:48:15 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:48:15 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:48:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:48:15 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000192_193' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:48:15 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000192_193: Committed
21/01/19 12:48:15 INFO Executor: Finished task 192.0 in stage 1.0 (TID 193). 2527 bytes result sent to driver
21/01/19 12:48:15 INFO TaskSetManager: Starting task 193.0 in stage 1.0 (TID 194, test-runner-pkwzc, executor driver, partition 193, ANY, 7815 bytes)
21/01/19 12:48:15 INFO Executor: Running task 193.0 in stage 1.0 (TID 194)
21/01/19 12:48:15 INFO TaskSetManager: Finished task 192.0 in stage 1.0 (TID 193) in 1071 ms on test-runner-pkwzc (executor driver) (193/200)
21/01/19 12:48:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:48:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:48:15 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:48:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:48:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:48:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:48:15 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-f8f9671e-ce7c-4a10-84ba-7fc21edea75b-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:48:16 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:16 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:16 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:48:16 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:48:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:48:16 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:48:16 INFO ParquetOutputFormat: Validation is off
21/01/19 12:48:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:48:16 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:48:16 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:48:16 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:48:16 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:48:16 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:48:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:48:17 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000193_194' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:48:17 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000193_194: Committed
21/01/19 12:48:17 INFO Executor: Finished task 193.0 in stage 1.0 (TID 194). 2527 bytes result sent to driver
21/01/19 12:48:17 INFO TaskSetManager: Starting task 194.0 in stage 1.0 (TID 195, test-runner-pkwzc, executor driver, partition 194, ANY, 7815 bytes)
21/01/19 12:48:17 INFO Executor: Running task 194.0 in stage 1.0 (TID 195)
21/01/19 12:48:17 INFO TaskSetManager: Finished task 193.0 in stage 1.0 (TID 194) in 1328 ms on test-runner-pkwzc (executor driver) (194/200)
21/01/19 12:48:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:48:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:48:17 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:48:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:48:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:48:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:48:17 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-fa985053-5744-4c95-bfeb-977a31c6c78f-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:48:17 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:17 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:17 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:48:17 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:48:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:48:17 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:48:17 INFO ParquetOutputFormat: Validation is off
21/01/19 12:48:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:48:17 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:48:17 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:48:17 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:48:17 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:48:17 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:48:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:48:18 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000194_195' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:48:18 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000194_195: Committed
21/01/19 12:48:18 INFO Executor: Finished task 194.0 in stage 1.0 (TID 195). 2527 bytes result sent to driver
21/01/19 12:48:18 INFO TaskSetManager: Starting task 195.0 in stage 1.0 (TID 196, test-runner-pkwzc, executor driver, partition 195, ANY, 7815 bytes)
21/01/19 12:48:18 INFO Executor: Running task 195.0 in stage 1.0 (TID 196)
21/01/19 12:48:18 INFO TaskSetManager: Finished task 194.0 in stage 1.0 (TID 195) in 1198 ms on test-runner-pkwzc (executor driver) (195/200)
21/01/19 12:48:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:48:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:48:18 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:48:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:48:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:48:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:48:18 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-fc02137c-fe8a-4db9-b00c-b7e7d4ecb557-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:48:18 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:18 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:18 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:48:18 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:48:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:48:18 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:48:18 INFO ParquetOutputFormat: Validation is off
21/01/19 12:48:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:48:18 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:48:18 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:48:18 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:48:18 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:48:18 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:48:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:48:19 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000195_196' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:48:19 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000195_196: Committed
21/01/19 12:48:19 INFO Executor: Finished task 195.0 in stage 1.0 (TID 196). 2527 bytes result sent to driver
21/01/19 12:48:19 INFO TaskSetManager: Starting task 196.0 in stage 1.0 (TID 197, test-runner-pkwzc, executor driver, partition 196, ANY, 7815 bytes)
21/01/19 12:48:19 INFO Executor: Running task 196.0 in stage 1.0 (TID 197)
21/01/19 12:48:19 INFO TaskSetManager: Finished task 195.0 in stage 1.0 (TID 196) in 1195 ms on test-runner-pkwzc (executor driver) (196/200)
21/01/19 12:48:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:48:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:48:19 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:48:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:48:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:48:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:48:19 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-ffdc3e90-8cb7-403b-835b-50c549c62e01-c000.snappy.parquet, range: 0-104863434, partition values: [empty row]
21/01/19 12:48:20 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:20 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:20 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:48:20 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:48:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:48:20 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:48:20 INFO ParquetOutputFormat: Validation is off
21/01/19 12:48:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:48:20 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:48:20 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:48:20 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:48:20 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:48:20 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:48:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:48:20 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000196_197' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:48:20 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000196_197: Committed
21/01/19 12:48:20 INFO Executor: Finished task 196.0 in stage 1.0 (TID 197). 2527 bytes result sent to driver
21/01/19 12:48:20 INFO TaskSetManager: Starting task 197.0 in stage 1.0 (TID 198, test-runner-pkwzc, executor driver, partition 197, ANY, 7815 bytes)
21/01/19 12:48:20 INFO Executor: Running task 197.0 in stage 1.0 (TID 198)
21/01/19 12:48:20 INFO TaskSetManager: Finished task 196.0 in stage 1.0 (TID 197) in 1081 ms on test-runner-pkwzc (executor driver) (197/200)
21/01/19 12:48:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:48:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:48:20 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:48:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:48:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:48:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:48:20 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-3c855803-886c-443f-ab52-1e0031c55ec6-c000.snappy.parquet, range: 0-104863433, partition values: [empty row]
21/01/19 12:48:21 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:21 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:21 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:48:21 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:48:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:48:21 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:48:21 INFO ParquetOutputFormat: Validation is off
21/01/19 12:48:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:48:21 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:48:21 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:48:21 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:48:21 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:48:21 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:48:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:48:21 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000197_198' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:48:21 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000197_198: Committed
21/01/19 12:48:21 INFO Executor: Finished task 197.0 in stage 1.0 (TID 198). 2527 bytes result sent to driver
21/01/19 12:48:21 INFO TaskSetManager: Starting task 198.0 in stage 1.0 (TID 199, test-runner-pkwzc, executor driver, partition 198, ANY, 7815 bytes)
21/01/19 12:48:21 INFO Executor: Running task 198.0 in stage 1.0 (TID 199)
21/01/19 12:48:21 INFO TaskSetManager: Finished task 197.0 in stage 1.0 (TID 198) in 1203 ms on test-runner-pkwzc (executor driver) (198/200)
21/01/19 12:48:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:48:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:48:21 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:48:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:48:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:48:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:48:21 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-e3d26a94-7dfa-4a66-9b08-eca96a14b209-c000.snappy.parquet, range: 0-104863433, partition values: [empty row]
21/01/19 12:48:22 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:22 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:22 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:48:22 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:48:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:48:22 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:48:22 INFO ParquetOutputFormat: Validation is off
21/01/19 12:48:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:48:22 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:48:22 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:48:22 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:48:22 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:48:22 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:48:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/19 12:48:23 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000198_199' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:48:23 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000198_199: Committed
21/01/19 12:48:23 INFO Executor: Finished task 198.0 in stage 1.0 (TID 199). 2527 bytes result sent to driver
21/01/19 12:48:23 INFO TaskSetManager: Starting task 199.0 in stage 1.0 (TID 200, test-runner-pkwzc, executor driver, partition 199, ANY, 7963 bytes)
21/01/19 12:48:23 INFO Executor: Running task 199.0 in stage 1.0 (TID 200)
21/01/19 12:48:23 INFO TaskSetManager: Finished task 198.0 in stage 1.0 (TID 199) in 1388 ms on test-runner-pkwzc (executor driver) (199/200)
21/01/19 12:48:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:48:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:48:23 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:48:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/19 12:48:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/19 12:48:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/19 12:48:23 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-f7f41085-4ca1-4d1a-9440-bdac7b651c2f-c000.snappy.parquet, range: 0-104863433, partition values: [empty row]
21/01/19 12:48:23 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:23 INFO CodecConfig: Compression: SNAPPY
21/01/19 12:48:23 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/19 12:48:23 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/19 12:48:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/19 12:48:23 INFO ParquetOutputFormat: Dictionary is on
21/01/19 12:48:23 INFO ParquetOutputFormat: Validation is off
21/01/19 12:48:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/19 12:48:23 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/19 12:48:23 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/19 12:48:23 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/19 12:48:23 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/19 12:48:23 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  optional int32 index;
}

       
21/01/19 12:48:23 INFO FileScanRDD: Reading File path: hdfs://hdfs-namenode-0.hdfs-namenode:9820/testdata/part-00000-fb8abe06-0bab-4661-9c4b-e6a6fd079059-c000.snappy.parquet, range: 0-608, partition values: [empty row]
21/01/19 12:48:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104863256
21/01/19 12:48:24 INFO FileOutputCommitter: Saved output of task 'attempt_20210119124356_0001_m_000199_200' to hdfs://hdfs-namenode-0.hdfs-namenode:9820/r1
21/01/19 12:48:24 INFO SparkHadoopMapRedUtil: attempt_20210119124356_0001_m_000199_200: Committed
21/01/19 12:48:24 INFO Executor: Finished task 199.0 in stage 1.0 (TID 200). 2527 bytes result sent to driver
21/01/19 12:48:24 INFO TaskSetManager: Finished task 199.0 in stage 1.0 (TID 200) in 1125 ms on test-runner-pkwzc (executor driver) (200/200)
21/01/19 12:48:24 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
21/01/19 12:48:24 INFO DAGScheduler: ResultStage 1 (parquet at Copy.java:28) finished in 268.319 s
21/01/19 12:48:24 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/19 12:48:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
21/01/19 12:48:24 INFO DAGScheduler: Job 1 finished: parquet at Copy.java:28, took 268.359438 s
21/01/19 12:48:24 INFO FileFormatWriter: Write Job ce891696-56e6-4362-83c6-b38de4f59346 committed.
21/01/19 12:48:24 INFO FileFormatWriter: Finished processing stats for write job ce891696-56e6-4362-83c6-b38de4f59346.
21/01/19 12:48:24 INFO SparkUI: Stopped Spark web UI at http://test-runner-pkwzc:4040
21/01/19 12:48:24 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
21/01/19 12:48:24 INFO MemoryStore: MemoryStore cleared
21/01/19 12:48:24 INFO BlockManager: BlockManager stopped
21/01/19 12:48:24 INFO BlockManagerMaster: BlockManagerMaster stopped
21/01/19 12:48:24 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
21/01/19 12:48:24 INFO SparkContext: Successfully stopped SparkContext
21/01/19 12:48:24 INFO ShutdownHookManager: Shutdown hook called
21/01/19 12:48:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-52ef70d3-4de2-49e2-9dfb-d4a757a5e68d
21/01/19 12:48:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-a037648d-209e-4b2c-bb6c-566370f50e22

real	4m37.587s
user	3m43.740s
sys	0m59.291s
Process exited with exit code 0
