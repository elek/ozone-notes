generate --iteration 200 o3fs://bucket1.vol1/testdata
Flokkr launcher script 1.1-18-g58a3efc

===== Plugin is activated ENVTOCONF =====
core-site.xml
File core-site.xml has been written out successfullly.
hdfs-site.xml
File hdfs-site.xml has been written out successfullly.
hadoop.conf
File hadoop.conf has been written out successfullly.
ozone-site.xml
File ozone-site.xml has been written out successfullly.
mapred-site.xml
File mapred-site.xml has been written out successfullly.
Non-spark-on-k8s command provided, proceeding in pass-through mode...
======================================
*** Launching "/opt/testscripts/parquet.sh generate --iteration 200 o3fs://bucket1.vol1/testdata"
+ : /opt/spark
+ : /opt
+ : watchforcommit.btm
+ /opt/spark/bin/spark-submit --conf spark.executor.memory=4g --jars /opt/ozonefs/hadoop-ozone-filesystem.jar /opt/spark-samples-1.0-SNAPSHOT.jar generate --iteration 200 o3fs://bucket1.vol1/testdata
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.0.0.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
21/01/20 15:35:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
21/01/20 15:35:20 INFO SparkContext: Running Spark version 3.0.0
21/01/20 15:35:20 INFO ResourceUtils: ==============================================================
21/01/20 15:35:20 INFO ResourceUtils: Resources for spark.driver:

21/01/20 15:35:20 INFO ResourceUtils: ==============================================================
21/01/20 15:35:20 INFO SparkContext: Submitted application: Generate
21/01/20 15:35:20 INFO SecurityManager: Changing view acls to: spark
21/01/20 15:35:20 INFO SecurityManager: Changing modify acls to: spark
21/01/20 15:35:20 INFO SecurityManager: Changing view acls groups to: 
21/01/20 15:35:20 INFO SecurityManager: Changing modify acls groups to: 
21/01/20 15:35:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
21/01/20 15:35:20 INFO Utils: Successfully started service 'sparkDriver' on port 35508.
21/01/20 15:35:20 INFO SparkEnv: Registering MapOutputTracker
21/01/20 15:35:20 INFO SparkEnv: Registering BlockManagerMaster
21/01/20 15:35:20 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/01/20 15:35:20 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/01/20 15:35:20 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/01/20 15:35:20 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-730e0928-e81e-4b5f-b850-0d316136dee2
21/01/20 15:35:20 INFO MemoryStore: MemoryStore started with capacity 413.9 MiB
21/01/20 15:35:20 INFO SparkEnv: Registering OutputCommitCoordinator
21/01/20 15:35:20 INFO Utils: Successfully started service 'SparkUI' on port 4040.
21/01/20 15:35:20 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://test-runner-lmfgs:4040
21/01/20 15:35:20 INFO SparkContext: Added JAR file:///opt/ozonefs/hadoop-ozone-filesystem.jar at spark://test-runner-lmfgs:35508/jars/hadoop-ozone-filesystem.jar with timestamp 1611156920819
21/01/20 15:35:20 INFO SparkContext: Added JAR file:/opt/spark-samples-1.0-SNAPSHOT.jar at spark://test-runner-lmfgs:35508/jars/spark-samples-1.0-SNAPSHOT.jar with timestamp 1611156920820
21/01/20 15:35:20 INFO Executor: Starting executor ID driver on host test-runner-lmfgs
21/01/20 15:35:20 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39761.
21/01/20 15:35:20 INFO NettyBlockTransferService: Server created on test-runner-lmfgs:39761
21/01/20 15:35:20 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/01/20 15:35:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, test-runner-lmfgs, 39761, None)
21/01/20 15:35:21 INFO BlockManagerMasterEndpoint: Registering block manager test-runner-lmfgs:39761 with 413.9 MiB RAM, BlockManagerId(driver, test-runner-lmfgs, 39761, None)
21/01/20 15:35:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, test-runner-lmfgs, 39761, None)
21/01/20 15:35:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, test-runner-lmfgs, 39761, None)
21/01/20 15:35:21 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/opt/spark/spark-warehouse').
21/01/20 15:35:21 INFO SharedState: Warehouse path is 'file:/opt/spark/spark-warehouse'.
21/01/20 15:35:24 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:35:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:35:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:35:24 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:35:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:35:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:35:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:35:24 INFO CodeGenerator: Code generated in 191.933532 ms
21/01/20 15:35:24 INFO SparkContext: Starting job: parquet at Generate.java:38
21/01/20 15:35:24 INFO DAGScheduler: Got job 0 (parquet at Generate.java:38) with 1 output partitions
21/01/20 15:35:24 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at Generate.java:38)
21/01/20 15:35:24 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:35:24 INFO DAGScheduler: Missing parents: List()
21/01/20 15:35:24 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at Generate.java:38), which has no missing parents
21/01/20 15:35:25 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:35:25 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:35:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:35:25 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1200
21/01/20 15:35:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at Generate.java:38) (first 15 tasks are for partitions Vector(0))
21/01/20 15:35:25 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
21/01/20 15:35:25 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 7541 bytes)
21/01/20 15:35:25 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
21/01/20 15:35:25 INFO Executor: Fetching spark://test-runner-lmfgs:35508/jars/spark-samples-1.0-SNAPSHOT.jar with timestamp 1611156920820
21/01/20 15:35:25 INFO TransportClientFactory: Successfully created connection to test-runner-lmfgs/10.42.4.45:35508 after 26 ms (0 ms spent in bootstraps)
21/01/20 15:35:25 INFO Utils: Fetching spark://test-runner-lmfgs:35508/jars/spark-samples-1.0-SNAPSHOT.jar to /tmp/spark-14e82dee-8c82-43bb-9e69-c24c447fc558/userFiles-f27107a4-f13c-4243-9c03-0b1571a8a6b3/fetchFileTemp6355616812446499262.tmp
21/01/20 15:35:25 INFO Executor: Adding file:/tmp/spark-14e82dee-8c82-43bb-9e69-c24c447fc558/userFiles-f27107a4-f13c-4243-9c03-0b1571a8a6b3/spark-samples-1.0-SNAPSHOT.jar to class loader
21/01/20 15:35:25 INFO Executor: Fetching spark://test-runner-lmfgs:35508/jars/hadoop-ozone-filesystem.jar with timestamp 1611156920819
21/01/20 15:35:25 INFO Utils: Fetching spark://test-runner-lmfgs:35508/jars/hadoop-ozone-filesystem.jar to /tmp/spark-14e82dee-8c82-43bb-9e69-c24c447fc558/userFiles-f27107a4-f13c-4243-9c03-0b1571a8a6b3/fetchFileTemp1285279369921023058.tmp
21/01/20 15:35:25 INFO Executor: Adding file:/tmp/spark-14e82dee-8c82-43bb-9e69-c24c447fc558/userFiles-f27107a4-f13c-4243-9c03-0b1571a8a6b3/hadoop-ozone-filesystem.jar to class loader
21/01/20 15:35:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:35:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:35:25 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:35:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:35:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:35:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:35:25 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:35:25 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:35:25 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:35:25 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:35:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:35:25 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:35:25 INFO ParquetOutputFormat: Validation is off
21/01/20 15:35:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:35:25 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:35:25 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:35:25 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:35:25 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:35:25 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:35:25 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-xceiverclientmetrics.properties,hadoop-metrics2.properties
21/01/20 15:35:25 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
21/01/20 15:35:25 INFO MetricsSystemImpl: XceiverClientMetrics metrics system started
21/01/20 15:35:25 INFO CodecPool: Got brand-new compressor [.snappy]
21/01/20 15:35:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 17
21/01/20 15:35:26 INFO MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
21/01/20 15:35:26 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-4939A81D92F5->af5c9c6a-a887-4538-ae7b-ba6d49328297
21/01/20 15:35:26 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:35:27 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153524_0000_m_000000_0' to o3fs://bucket1.vol1/testdata
21/01/20 15:35:27 INFO SparkHadoopMapRedUtil: attempt_20210120153524_0000_m_000000_0: Committed
21/01/20 15:35:27 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2198 bytes result sent to driver
21/01/20 15:35:27 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2039 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:35:27 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
21/01/20 15:35:27 INFO DAGScheduler: ResultStage 0 (parquet at Generate.java:38) finished in 2.235 s
21/01/20 15:35:27 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:35:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
21/01/20 15:35:27 INFO DAGScheduler: Job 0 finished: parquet at Generate.java:38, took 2.273192 s
21/01/20 15:35:27 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-c34d7fbb-11d8-4d01-96fb-21692222ab90
21/01/20 15:35:27 INFO FileFormatWriter: Write Job dc5f4208-9290-496b-840f-6261365d41c9 committed.
21/01/20 15:35:27 INFO FileFormatWriter: Finished processing stats for write job dc5f4208-9290-496b-840f-6261365d41c9.
21/01/20 15:35:29 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:35:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:35:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:35:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:35:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:35:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:35:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:35:29 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:35:29 INFO DAGScheduler: Got job 1 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:35:29 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at Generate.java:61)
21/01/20 15:35:29 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:35:29 INFO DAGScheduler: Missing parents: List()
21/01/20 15:35:29 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:35:29 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 205.8 KiB, free 413.5 MiB)
21/01/20 15:35:29 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.4 MiB)
21/01/20 15:35:29 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.8 MiB)
21/01/20 15:35:29 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1200
21/01/20 15:35:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:35:29 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
21/01/20 15:35:30 WARN TaskSetManager: Stage 1 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:35:30 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:35:30 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
21/01/20 15:35:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:35:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:35:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:35:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:35:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:35:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:35:30 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:35:30 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:35:30 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:35:30 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:35:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:35:30 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:35:30 INFO ParquetOutputFormat: Validation is off
21/01/20 15:35:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:35:30 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:35:30 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:35:30 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:35:30 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:35:30 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:35:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:35:30 INFO BlockManagerInfo: Removed broadcast_0_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:35:34 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-A1AF7AA35470->d3c4d3e3-944b-4528-9d82-e005ad427697
21/01/20 15:35:34 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:35:34 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153529_0001_m_000000_1' to o3fs://bucket1.vol1/testdata
21/01/20 15:35:34 INFO SparkHadoopMapRedUtil: attempt_20210120153529_0001_m_000000_1: Committed
21/01/20 15:35:34 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2155 bytes result sent to driver
21/01/20 15:35:34 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 4972 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:35:34 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
21/01/20 15:35:34 INFO DAGScheduler: ResultStage 1 (parquet at Generate.java:61) finished in 5.001 s
21/01/20 15:35:34 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:35:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
21/01/20 15:35:34 INFO DAGScheduler: Job 1 finished: parquet at Generate.java:61, took 5.006168 s
21/01/20 15:35:34 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-0521d23f-c360-438a-b238-bdcb074753dc
21/01/20 15:35:34 INFO FileFormatWriter: Write Job 19ef8f60-8c1a-4089-8d6c-412b5e9b72cc committed.
21/01/20 15:35:34 INFO FileFormatWriter: Finished processing stats for write job 19ef8f60-8c1a-4089-8d6c-412b5e9b72cc.
21/01/20 15:35:36 INFO BlockManagerInfo: Removed broadcast_1_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:35:37 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:35:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:35:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:35:37 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:35:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:35:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:35:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:35:37 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:35:37 INFO DAGScheduler: Got job 2 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:35:37 INFO DAGScheduler: Final stage: ResultStage 2 (parquet at Generate.java:61)
21/01/20 15:35:37 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:35:37 INFO DAGScheduler: Missing parents: List()
21/01/20 15:35:37 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[9] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:35:37 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:35:37 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:35:37 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:35:37 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1200
21/01/20 15:35:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:35:37 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
21/01/20 15:35:37 WARN TaskSetManager: Stage 2 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:35:37 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:35:37 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
21/01/20 15:35:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:35:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:35:37 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:35:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:35:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:35:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:35:37 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:35:37 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:35:37 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:35:37 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:35:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:35:37 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:35:37 INFO ParquetOutputFormat: Validation is off
21/01/20 15:35:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:35:37 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:35:37 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:35:37 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:35:37 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:35:37 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:35:37 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:35:38 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-D03240C7B021->af5c9c6a-a887-4538-ae7b-ba6d49328297
21/01/20 15:35:38 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:35:38 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153537_0002_m_000000_2' to o3fs://bucket1.vol1/testdata
21/01/20 15:35:38 INFO SparkHadoopMapRedUtil: attempt_20210120153537_0002_m_000000_2: Committed
21/01/20 15:35:38 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2155 bytes result sent to driver
21/01/20 15:35:38 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1218 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:35:38 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
21/01/20 15:35:38 INFO DAGScheduler: ResultStage 2 (parquet at Generate.java:61) finished in 1.245 s
21/01/20 15:35:38 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:35:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
21/01/20 15:35:38 INFO DAGScheduler: Job 2 finished: parquet at Generate.java:61, took 1.250105 s
21/01/20 15:35:38 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-76e19d0a-3594-4638-8713-c95b678ab252
21/01/20 15:35:38 INFO FileFormatWriter: Write Job 4ce30cae-ea7d-4c61-8207-1b8e246a9f82 committed.
21/01/20 15:35:38 INFO FileFormatWriter: Finished processing stats for write job 4ce30cae-ea7d-4c61-8207-1b8e246a9f82.
21/01/20 15:35:39 INFO BlockManagerInfo: Removed broadcast_2_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:35:41 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:35:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:35:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:35:41 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:35:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:35:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:35:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:35:41 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:35:41 INFO DAGScheduler: Got job 3 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:35:41 INFO DAGScheduler: Final stage: ResultStage 3 (parquet at Generate.java:61)
21/01/20 15:35:41 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:35:41 INFO DAGScheduler: Missing parents: List()
21/01/20 15:35:41 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[13] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:35:41 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:35:41 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:35:41 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:35:41 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1200
21/01/20 15:35:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:35:41 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
21/01/20 15:35:41 WARN TaskSetManager: Stage 3 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:35:41 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:35:41 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
21/01/20 15:35:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:35:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:35:41 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:35:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:35:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:35:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:35:41 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:35:41 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:35:41 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:35:41 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:35:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:35:41 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:35:41 INFO ParquetOutputFormat: Validation is off
21/01/20 15:35:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:35:41 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:35:41 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:35:41 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:35:41 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:35:41 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:35:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:35:42 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153541_0003_m_000000_3' to o3fs://bucket1.vol1/testdata
21/01/20 15:35:42 INFO SparkHadoopMapRedUtil: attempt_20210120153541_0003_m_000000_3: Committed
21/01/20 15:35:42 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2155 bytes result sent to driver
21/01/20 15:35:42 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1091 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:35:42 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
21/01/20 15:35:42 INFO DAGScheduler: ResultStage 3 (parquet at Generate.java:61) finished in 1.117 s
21/01/20 15:35:42 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:35:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
21/01/20 15:35:42 INFO DAGScheduler: Job 3 finished: parquet at Generate.java:61, took 1.120416 s
21/01/20 15:35:42 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-b10e96b5-1f19-4d62-a71f-a078d2d02398
21/01/20 15:35:42 INFO FileFormatWriter: Write Job 347f8cd7-acb7-4463-ab9e-baaa73f80a80 committed.
21/01/20 15:35:42 INFO FileFormatWriter: Finished processing stats for write job 347f8cd7-acb7-4463-ab9e-baaa73f80a80.
21/01/20 15:35:43 INFO BlockManagerInfo: Removed broadcast_3_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:35:45 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:35:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:35:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:35:45 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:35:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:35:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:35:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:35:45 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:35:45 INFO DAGScheduler: Got job 4 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:35:45 INFO DAGScheduler: Final stage: ResultStage 4 (parquet at Generate.java:61)
21/01/20 15:35:45 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:35:45 INFO DAGScheduler: Missing parents: List()
21/01/20 15:35:45 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[17] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:35:45 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:35:45 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:35:45 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:35:45 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1200
21/01/20 15:35:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[17] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:35:45 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
21/01/20 15:35:45 WARN TaskSetManager: Stage 4 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:35:45 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:35:45 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
21/01/20 15:35:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:35:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:35:45 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:35:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:35:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:35:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:35:45 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:35:45 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:35:45 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:35:45 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:35:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:35:45 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:35:45 INFO ParquetOutputFormat: Validation is off
21/01/20 15:35:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:35:45 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:35:45 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:35:45 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:35:45 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:35:45 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:35:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:35:46 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153545_0004_m_000000_4' to o3fs://bucket1.vol1/testdata
21/01/20 15:35:46 INFO SparkHadoopMapRedUtil: attempt_20210120153545_0004_m_000000_4: Committed
21/01/20 15:35:46 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 2155 bytes result sent to driver
21/01/20 15:35:46 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 997 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:35:46 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
21/01/20 15:35:46 INFO DAGScheduler: ResultStage 4 (parquet at Generate.java:61) finished in 1.027 s
21/01/20 15:35:46 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:35:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
21/01/20 15:35:46 INFO DAGScheduler: Job 4 finished: parquet at Generate.java:61, took 1.031313 s
21/01/20 15:35:46 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-f5b99f0f-5ed5-4863-bdf2-9328095bea8f
21/01/20 15:35:46 INFO FileFormatWriter: Write Job 9f66e00c-49a8-4ca3-b641-0803419ae962 committed.
21/01/20 15:35:46 INFO FileFormatWriter: Finished processing stats for write job 9f66e00c-49a8-4ca3-b641-0803419ae962.
21/01/20 15:35:46 INFO BlockManagerInfo: Removed broadcast_4_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:35:48 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:35:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:35:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:35:48 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:35:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:35:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:35:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:35:48 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:35:48 INFO DAGScheduler: Got job 5 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:35:48 INFO DAGScheduler: Final stage: ResultStage 5 (parquet at Generate.java:61)
21/01/20 15:35:48 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:35:48 INFO DAGScheduler: Missing parents: List()
21/01/20 15:35:48 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[21] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:35:48 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:35:48 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:35:48 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:35:48 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1200
21/01/20 15:35:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[21] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:35:48 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
21/01/20 15:35:48 WARN TaskSetManager: Stage 5 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:35:48 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:35:48 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
21/01/20 15:35:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:35:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:35:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:35:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:35:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:35:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:35:49 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:35:49 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:35:49 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:35:49 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:35:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:35:49 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:35:49 INFO ParquetOutputFormat: Validation is off
21/01/20 15:35:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:35:49 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:35:49 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:35:49 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:35:49 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:35:49 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:35:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:35:50 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153548_0005_m_000000_5' to o3fs://bucket1.vol1/testdata
21/01/20 15:35:50 INFO SparkHadoopMapRedUtil: attempt_20210120153548_0005_m_000000_5: Committed
21/01/20 15:35:50 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2155 bytes result sent to driver
21/01/20 15:35:50 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 1109 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:35:50 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
21/01/20 15:35:50 INFO DAGScheduler: ResultStage 5 (parquet at Generate.java:61) finished in 1.133 s
21/01/20 15:35:50 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:35:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
21/01/20 15:35:50 INFO DAGScheduler: Job 5 finished: parquet at Generate.java:61, took 1.135874 s
21/01/20 15:35:50 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-8bf8e5df-90b8-41b3-9c65-bc21327822de
21/01/20 15:35:50 INFO FileFormatWriter: Write Job 6a8423fa-2b70-4b76-8681-21b242882a28 committed.
21/01/20 15:35:50 INFO FileFormatWriter: Finished processing stats for write job 6a8423fa-2b70-4b76-8681-21b242882a28.
21/01/20 15:35:51 INFO BlockManagerInfo: Removed broadcast_5_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:35:52 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:35:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:35:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:35:52 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:35:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:35:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:35:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:35:52 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:35:52 INFO DAGScheduler: Got job 6 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:35:52 INFO DAGScheduler: Final stage: ResultStage 6 (parquet at Generate.java:61)
21/01/20 15:35:52 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:35:52 INFO DAGScheduler: Missing parents: List()
21/01/20 15:35:52 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[25] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:35:52 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:35:52 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:35:52 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:35:52 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1200
21/01/20 15:35:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[25] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:35:52 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
21/01/20 15:35:52 WARN TaskSetManager: Stage 6 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:35:52 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:35:52 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
21/01/20 15:35:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:35:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:35:52 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:35:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:35:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:35:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:35:52 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:35:52 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:35:52 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:35:52 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:35:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:35:52 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:35:52 INFO ParquetOutputFormat: Validation is off
21/01/20 15:35:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:35:52 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:35:52 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:35:52 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:35:52 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:35:52 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:35:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:35:53 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-231380C4507F->d3c4d3e3-944b-4528-9d82-e005ad427697
21/01/20 15:35:53 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:35:53 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153552_0006_m_000000_6' to o3fs://bucket1.vol1/testdata
21/01/20 15:35:53 INFO SparkHadoopMapRedUtil: attempt_20210120153552_0006_m_000000_6: Committed
21/01/20 15:35:53 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 2155 bytes result sent to driver
21/01/20 15:35:53 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 1248 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:35:53 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
21/01/20 15:35:53 INFO DAGScheduler: ResultStage 6 (parquet at Generate.java:61) finished in 1.272 s
21/01/20 15:35:53 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:35:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
21/01/20 15:35:53 INFO DAGScheduler: Job 6 finished: parquet at Generate.java:61, took 1.276262 s
21/01/20 15:35:53 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-d82d43a4-87c9-4462-8c87-3acc1b0c512d
21/01/20 15:35:53 INFO FileFormatWriter: Write Job 4b263f44-5edd-4742-9f88-c2566332dff7 committed.
21/01/20 15:35:53 INFO FileFormatWriter: Finished processing stats for write job 4b263f44-5edd-4742-9f88-c2566332dff7.
21/01/20 15:35:54 INFO BlockManagerInfo: Removed broadcast_6_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:35:56 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:35:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:35:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:35:56 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:35:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:35:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:35:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:35:56 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:35:56 INFO DAGScheduler: Got job 7 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:35:56 INFO DAGScheduler: Final stage: ResultStage 7 (parquet at Generate.java:61)
21/01/20 15:35:56 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:35:56 INFO DAGScheduler: Missing parents: List()
21/01/20 15:35:56 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[29] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:35:56 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:35:56 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:35:56 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:35:56 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1200
21/01/20 15:35:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[29] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:35:56 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks
21/01/20 15:35:56 WARN TaskSetManager: Stage 7 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:35:56 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:35:56 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
21/01/20 15:35:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:35:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:35:56 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:35:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:35:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:35:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:35:56 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:35:56 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:35:56 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:35:56 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:35:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:35:56 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:35:56 INFO ParquetOutputFormat: Validation is off
21/01/20 15:35:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:35:56 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:35:56 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:35:56 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:35:56 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:35:56 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:35:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:35:57 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153556_0007_m_000000_7' to o3fs://bucket1.vol1/testdata
21/01/20 15:35:57 INFO SparkHadoopMapRedUtil: attempt_20210120153556_0007_m_000000_7: Committed
21/01/20 15:35:57 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 2155 bytes result sent to driver
21/01/20 15:35:57 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 1044 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:35:57 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
21/01/20 15:35:57 INFO DAGScheduler: ResultStage 7 (parquet at Generate.java:61) finished in 1.067 s
21/01/20 15:35:57 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:35:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
21/01/20 15:35:57 INFO DAGScheduler: Job 7 finished: parquet at Generate.java:61, took 1.070998 s
21/01/20 15:35:57 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-04ffaf8c-c697-4279-8a26-9087b53157cf
21/01/20 15:35:57 INFO FileFormatWriter: Write Job 2e94c5f4-c101-4707-9014-1dfd3d7aa104 committed.
21/01/20 15:35:57 INFO FileFormatWriter: Finished processing stats for write job 2e94c5f4-c101-4707-9014-1dfd3d7aa104.
21/01/20 15:35:58 INFO BlockManagerInfo: Removed broadcast_7_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:36:00 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:00 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:36:00 INFO DAGScheduler: Got job 8 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:36:00 INFO DAGScheduler: Final stage: ResultStage 8 (parquet at Generate.java:61)
21/01/20 15:36:00 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:36:00 INFO DAGScheduler: Missing parents: List()
21/01/20 15:36:00 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[33] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:36:00 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:36:00 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:36:00 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:36:00 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1200
21/01/20 15:36:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[33] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:36:00 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks
21/01/20 15:36:00 WARN TaskSetManager: Stage 8 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:36:00 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:36:00 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
21/01/20 15:36:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:00 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:36:00 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:36:00 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:36:00 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:36:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:36:00 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:36:00 INFO ParquetOutputFormat: Validation is off
21/01/20 15:36:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:36:00 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:36:00 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:36:00 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:36:00 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:36:00 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:36:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:36:01 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153600_0008_m_000000_8' to o3fs://bucket1.vol1/testdata
21/01/20 15:36:01 INFO SparkHadoopMapRedUtil: attempt_20210120153600_0008_m_000000_8: Committed
21/01/20 15:36:01 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2155 bytes result sent to driver
21/01/20 15:36:01 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 902 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:36:01 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
21/01/20 15:36:01 INFO DAGScheduler: ResultStage 8 (parquet at Generate.java:61) finished in 0.948 s
21/01/20 15:36:01 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:36:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
21/01/20 15:36:01 INFO DAGScheduler: Job 8 finished: parquet at Generate.java:61, took 0.951035 s
21/01/20 15:36:01 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-7f27ad9a-9173-4d0c-bae5-0ea35ee857e6
21/01/20 15:36:01 INFO FileFormatWriter: Write Job d41dd29e-8dfb-4b94-835d-27a21d846147 committed.
21/01/20 15:36:01 INFO FileFormatWriter: Finished processing stats for write job d41dd29e-8dfb-4b94-835d-27a21d846147.
21/01/20 15:36:01 INFO BlockManagerInfo: Removed broadcast_8_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:36:03 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:03 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:36:03 INFO DAGScheduler: Got job 9 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:36:03 INFO DAGScheduler: Final stage: ResultStage 9 (parquet at Generate.java:61)
21/01/20 15:36:03 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:36:03 INFO DAGScheduler: Missing parents: List()
21/01/20 15:36:03 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[37] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:36:03 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:36:03 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 73.9 KiB, free 413.7 MiB)
21/01/20 15:36:03 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on test-runner-lmfgs:39761 (size: 73.9 KiB, free: 413.9 MiB)
21/01/20 15:36:03 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1200
21/01/20 15:36:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[37] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:36:03 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
21/01/20 15:36:03 WARN TaskSetManager: Stage 9 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:36:03 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:36:03 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)
21/01/20 15:36:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:03 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:36:03 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:36:03 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:36:03 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:36:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:36:03 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:36:03 INFO ParquetOutputFormat: Validation is off
21/01/20 15:36:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:36:03 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:36:03 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:36:03 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:36:03 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:36:03 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:36:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:36:04 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153603_0009_m_000000_9' to o3fs://bucket1.vol1/testdata
21/01/20 15:36:04 INFO SparkHadoopMapRedUtil: attempt_20210120153603_0009_m_000000_9: Committed
21/01/20 15:36:04 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 2155 bytes result sent to driver
21/01/20 15:36:04 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 1046 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:36:04 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
21/01/20 15:36:04 INFO DAGScheduler: ResultStage 9 (parquet at Generate.java:61) finished in 1.072 s
21/01/20 15:36:04 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:36:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
21/01/20 15:36:04 INFO DAGScheduler: Job 9 finished: parquet at Generate.java:61, took 1.075369 s
21/01/20 15:36:04 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-6b393170-24be-4117-8c6e-c569012d24a5
21/01/20 15:36:04 INFO FileFormatWriter: Write Job c00596ba-75f1-4c46-a3ce-998d9368016e committed.
21/01/20 15:36:04 INFO FileFormatWriter: Finished processing stats for write job c00596ba-75f1-4c46-a3ce-998d9368016e.
21/01/20 15:36:05 INFO BlockManagerInfo: Removed broadcast_9_piece0 on test-runner-lmfgs:39761 in memory (size: 73.9 KiB, free: 413.9 MiB)
21/01/20 15:36:07 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:07 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:07 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:36:07 INFO DAGScheduler: Got job 10 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:36:07 INFO DAGScheduler: Final stage: ResultStage 10 (parquet at Generate.java:61)
21/01/20 15:36:07 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:36:07 INFO DAGScheduler: Missing parents: List()
21/01/20 15:36:07 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[41] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:36:07 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:36:07 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:36:07 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:36:07 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1200
21/01/20 15:36:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[41] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:36:07 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks
21/01/20 15:36:07 WARN TaskSetManager: Stage 10 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:36:07 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:36:07 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)
21/01/20 15:36:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:07 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:07 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:36:07 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:36:07 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:36:07 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:36:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:36:07 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:36:07 INFO ParquetOutputFormat: Validation is off
21/01/20 15:36:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:36:07 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:36:07 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:36:07 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:36:07 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:36:07 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:36:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:36:08 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153607_0010_m_000000_10' to o3fs://bucket1.vol1/testdata
21/01/20 15:36:08 INFO SparkHadoopMapRedUtil: attempt_20210120153607_0010_m_000000_10: Committed
21/01/20 15:36:08 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 2155 bytes result sent to driver
21/01/20 15:36:08 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 998 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:36:08 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
21/01/20 15:36:08 INFO DAGScheduler: ResultStage 10 (parquet at Generate.java:61) finished in 1.022 s
21/01/20 15:36:08 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:36:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
21/01/20 15:36:08 INFO DAGScheduler: Job 10 finished: parquet at Generate.java:61, took 1.024513 s
21/01/20 15:36:08 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-30a42ad3-6d2d-4455-bd74-462ea7756b3e
21/01/20 15:36:08 INFO FileFormatWriter: Write Job 56ec1800-08fe-40ba-8220-b56c3487a609 committed.
21/01/20 15:36:08 INFO FileFormatWriter: Finished processing stats for write job 56ec1800-08fe-40ba-8220-b56c3487a609.
21/01/20 15:36:09 INFO BlockManagerInfo: Removed broadcast_10_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:36:11 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:11 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:36:11 INFO DAGScheduler: Got job 11 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:36:11 INFO DAGScheduler: Final stage: ResultStage 11 (parquet at Generate.java:61)
21/01/20 15:36:11 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:36:11 INFO DAGScheduler: Missing parents: List()
21/01/20 15:36:11 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[45] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:36:11 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:36:11 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:36:11 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:36:11 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1200
21/01/20 15:36:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[45] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:36:11 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks
21/01/20 15:36:11 WARN TaskSetManager: Stage 11 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:36:11 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:36:11 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)
21/01/20 15:36:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:11 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:36:11 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:36:11 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:36:11 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:36:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:36:11 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:36:11 INFO ParquetOutputFormat: Validation is off
21/01/20 15:36:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:36:11 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:36:11 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:36:11 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:36:11 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:36:11 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:36:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:36:12 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153611_0011_m_000000_11' to o3fs://bucket1.vol1/testdata
21/01/20 15:36:12 INFO SparkHadoopMapRedUtil: attempt_20210120153611_0011_m_000000_11: Committed
21/01/20 15:36:12 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 2155 bytes result sent to driver
21/01/20 15:36:12 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 891 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:36:12 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
21/01/20 15:36:12 INFO DAGScheduler: ResultStage 11 (parquet at Generate.java:61) finished in 0.938 s
21/01/20 15:36:12 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:36:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
21/01/20 15:36:12 INFO DAGScheduler: Job 11 finished: parquet at Generate.java:61, took 0.941287 s
21/01/20 15:36:12 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-e4c5b64d-4b51-4491-b061-e2f2be99c07d
21/01/20 15:36:12 INFO FileFormatWriter: Write Job 1589179d-fa55-4c7a-afee-54de021cec1c committed.
21/01/20 15:36:12 INFO FileFormatWriter: Finished processing stats for write job 1589179d-fa55-4c7a-afee-54de021cec1c.
21/01/20 15:36:12 INFO BlockManagerInfo: Removed broadcast_11_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:36:14 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:14 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:14 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:36:14 INFO DAGScheduler: Got job 12 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:36:14 INFO DAGScheduler: Final stage: ResultStage 12 (parquet at Generate.java:61)
21/01/20 15:36:14 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:36:14 INFO DAGScheduler: Missing parents: List()
21/01/20 15:36:14 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[49] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:36:14 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:36:14 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:36:14 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:36:14 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1200
21/01/20 15:36:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[49] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:36:14 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks
21/01/20 15:36:14 WARN TaskSetManager: Stage 12 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:36:14 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:36:14 INFO Executor: Running task 0.0 in stage 12.0 (TID 12)
21/01/20 15:36:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:14 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:14 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:36:14 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:36:14 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:36:14 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:36:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:36:14 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:36:14 INFO ParquetOutputFormat: Validation is off
21/01/20 15:36:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:36:14 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:36:14 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:36:14 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:36:14 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:36:14 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:36:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:36:15 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153614_0012_m_000000_12' to o3fs://bucket1.vol1/testdata
21/01/20 15:36:15 INFO SparkHadoopMapRedUtil: attempt_20210120153614_0012_m_000000_12: Committed
21/01/20 15:36:15 INFO Executor: Finished task 0.0 in stage 12.0 (TID 12). 2155 bytes result sent to driver
21/01/20 15:36:15 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 1191 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:36:15 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
21/01/20 15:36:15 INFO DAGScheduler: ResultStage 12 (parquet at Generate.java:61) finished in 1.213 s
21/01/20 15:36:15 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:36:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
21/01/20 15:36:15 INFO DAGScheduler: Job 12 finished: parquet at Generate.java:61, took 1.217033 s
21/01/20 15:36:15 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-7bb9a136-8fd8-4d88-8035-31996295093f
21/01/20 15:36:15 INFO FileFormatWriter: Write Job 7141e093-bb9c-49f8-8913-96db0d435609 committed.
21/01/20 15:36:15 INFO FileFormatWriter: Finished processing stats for write job 7141e093-bb9c-49f8-8913-96db0d435609.
21/01/20 15:36:17 INFO BlockManagerInfo: Removed broadcast_12_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:36:19 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:19 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:19 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:36:19 INFO DAGScheduler: Got job 13 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:36:19 INFO DAGScheduler: Final stage: ResultStage 13 (parquet at Generate.java:61)
21/01/20 15:36:19 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:36:19 INFO DAGScheduler: Missing parents: List()
21/01/20 15:36:19 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[53] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:36:19 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:36:19 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:36:19 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:36:19 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1200
21/01/20 15:36:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[53] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:36:19 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks
21/01/20 15:36:19 WARN TaskSetManager: Stage 13 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:36:19 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:36:19 INFO Executor: Running task 0.0 in stage 13.0 (TID 13)
21/01/20 15:36:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:19 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:19 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:36:19 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:36:19 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:36:19 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:36:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:36:19 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:36:19 INFO ParquetOutputFormat: Validation is off
21/01/20 15:36:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:36:19 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:36:19 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:36:19 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:36:19 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:36:19 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:36:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:36:20 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153619_0013_m_000000_13' to o3fs://bucket1.vol1/testdata
21/01/20 15:36:20 INFO SparkHadoopMapRedUtil: attempt_20210120153619_0013_m_000000_13: Committed
21/01/20 15:36:20 INFO Executor: Finished task 0.0 in stage 13.0 (TID 13). 2155 bytes result sent to driver
21/01/20 15:36:20 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 847 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:36:20 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
21/01/20 15:36:20 INFO DAGScheduler: ResultStage 13 (parquet at Generate.java:61) finished in 0.893 s
21/01/20 15:36:20 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:36:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
21/01/20 15:36:20 INFO DAGScheduler: Job 13 finished: parquet at Generate.java:61, took 0.895706 s
21/01/20 15:36:20 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-165eb79b-9914-4792-8b70-371ea5108ca6
21/01/20 15:36:20 INFO FileFormatWriter: Write Job 8bd6f269-fdf5-4cc0-8e14-43d71035dc56 committed.
21/01/20 15:36:20 INFO FileFormatWriter: Finished processing stats for write job 8bd6f269-fdf5-4cc0-8e14-43d71035dc56.
21/01/20 15:36:20 INFO BlockManagerInfo: Removed broadcast_13_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:36:23 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:23 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:23 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:36:23 INFO DAGScheduler: Got job 14 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:36:23 INFO DAGScheduler: Final stage: ResultStage 14 (parquet at Generate.java:61)
21/01/20 15:36:23 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:36:23 INFO DAGScheduler: Missing parents: List()
21/01/20 15:36:23 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[57] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:36:23 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:36:23 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:36:23 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:36:23 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1200
21/01/20 15:36:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[57] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:36:23 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks
21/01/20 15:36:23 WARN TaskSetManager: Stage 14 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:36:23 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:36:23 INFO Executor: Running task 0.0 in stage 14.0 (TID 14)
21/01/20 15:36:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:23 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:23 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:36:23 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:36:23 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:36:23 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:36:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:36:23 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:36:23 INFO ParquetOutputFormat: Validation is off
21/01/20 15:36:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:36:23 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:36:23 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:36:23 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:36:23 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:36:23 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:36:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:36:24 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153623_0014_m_000000_14' to o3fs://bucket1.vol1/testdata
21/01/20 15:36:24 INFO SparkHadoopMapRedUtil: attempt_20210120153623_0014_m_000000_14: Committed
21/01/20 15:36:24 INFO Executor: Finished task 0.0 in stage 14.0 (TID 14). 2155 bytes result sent to driver
21/01/20 15:36:24 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 1031 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:36:24 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
21/01/20 15:36:24 INFO DAGScheduler: ResultStage 14 (parquet at Generate.java:61) finished in 1.051 s
21/01/20 15:36:24 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:36:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
21/01/20 15:36:24 INFO DAGScheduler: Job 14 finished: parquet at Generate.java:61, took 1.053965 s
21/01/20 15:36:24 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-04ff0921-3ca2-4307-a6fb-d533c2be90ab
21/01/20 15:36:24 INFO FileFormatWriter: Write Job 9d6bf26e-283a-4270-ba3a-f230fe42185c committed.
21/01/20 15:36:24 INFO FileFormatWriter: Finished processing stats for write job 9d6bf26e-283a-4270-ba3a-f230fe42185c.
21/01/20 15:36:26 INFO BlockManagerInfo: Removed broadcast_14_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:36:27 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:27 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:27 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:36:27 INFO DAGScheduler: Got job 15 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:36:27 INFO DAGScheduler: Final stage: ResultStage 15 (parquet at Generate.java:61)
21/01/20 15:36:27 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:36:27 INFO DAGScheduler: Missing parents: List()
21/01/20 15:36:27 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[61] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:36:27 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:36:27 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:36:27 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:36:27 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1200
21/01/20 15:36:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[61] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:36:27 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks
21/01/20 15:36:27 WARN TaskSetManager: Stage 15 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:36:27 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 15, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:36:27 INFO Executor: Running task 0.0 in stage 15.0 (TID 15)
21/01/20 15:36:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:27 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:27 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:36:27 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:36:27 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:36:27 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:36:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:36:27 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:36:27 INFO ParquetOutputFormat: Validation is off
21/01/20 15:36:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:36:27 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:36:27 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:36:27 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:36:27 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:36:27 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:36:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:36:27 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-FD2854367737->d3c4d3e3-944b-4528-9d82-e005ad427697
21/01/20 15:36:27 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:36:28 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153627_0015_m_000000_15' to o3fs://bucket1.vol1/testdata
21/01/20 15:36:28 INFO SparkHadoopMapRedUtil: attempt_20210120153627_0015_m_000000_15: Committed
21/01/20 15:36:28 INFO Executor: Finished task 0.0 in stage 15.0 (TID 15). 2155 bytes result sent to driver
21/01/20 15:36:28 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 15) in 966 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:36:28 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
21/01/20 15:36:28 INFO DAGScheduler: ResultStage 15 (parquet at Generate.java:61) finished in 1.013 s
21/01/20 15:36:28 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:36:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
21/01/20 15:36:28 INFO DAGScheduler: Job 15 finished: parquet at Generate.java:61, took 1.015198 s
21/01/20 15:36:28 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-2a0d1b88-fd48-4889-bcd8-45475eed4210
21/01/20 15:36:28 INFO FileFormatWriter: Write Job e7e638ea-7286-400b-9e88-63c8fe162b06 committed.
21/01/20 15:36:28 INFO FileFormatWriter: Finished processing stats for write job e7e638ea-7286-400b-9e88-63c8fe162b06.
21/01/20 15:36:28 INFO BlockManagerInfo: Removed broadcast_15_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:36:30 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:30 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:36:30 INFO DAGScheduler: Got job 16 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:36:30 INFO DAGScheduler: Final stage: ResultStage 16 (parquet at Generate.java:61)
21/01/20 15:36:30 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:36:30 INFO DAGScheduler: Missing parents: List()
21/01/20 15:36:30 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[65] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:36:30 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:36:30 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:36:30 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:36:30 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1200
21/01/20 15:36:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[65] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:36:30 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks
21/01/20 15:36:31 WARN TaskSetManager: Stage 16 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:36:31 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 16, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:36:31 INFO Executor: Running task 0.0 in stage 16.0 (TID 16)
21/01/20 15:36:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:31 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:36:31 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:36:31 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:36:31 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:36:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:36:31 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:36:31 INFO ParquetOutputFormat: Validation is off
21/01/20 15:36:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:36:31 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:36:31 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:36:31 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:36:31 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:36:31 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:36:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:36:31 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153630_0016_m_000000_16' to o3fs://bucket1.vol1/testdata
21/01/20 15:36:31 INFO SparkHadoopMapRedUtil: attempt_20210120153630_0016_m_000000_16: Committed
21/01/20 15:36:31 INFO Executor: Finished task 0.0 in stage 16.0 (TID 16). 2155 bytes result sent to driver
21/01/20 15:36:31 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 16) in 999 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:36:31 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
21/01/20 15:36:31 INFO DAGScheduler: ResultStage 16 (parquet at Generate.java:61) finished in 1.019 s
21/01/20 15:36:31 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:36:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished
21/01/20 15:36:31 INFO DAGScheduler: Job 16 finished: parquet at Generate.java:61, took 1.021781 s
21/01/20 15:36:31 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-021b84ec-540f-480b-a3e7-cdb24e7c9718
21/01/20 15:36:31 INFO FileFormatWriter: Write Job 2366de78-1b30-4b55-a9be-a427e3b21621 committed.
21/01/20 15:36:31 INFO FileFormatWriter: Finished processing stats for write job 2366de78-1b30-4b55-a9be-a427e3b21621.
21/01/20 15:36:33 INFO BlockManagerInfo: Removed broadcast_16_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:36:34 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:34 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:34 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:36:34 INFO DAGScheduler: Got job 17 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:36:34 INFO DAGScheduler: Final stage: ResultStage 17 (parquet at Generate.java:61)
21/01/20 15:36:34 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:36:34 INFO DAGScheduler: Missing parents: List()
21/01/20 15:36:34 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[69] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:36:34 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:36:34 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:36:34 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:36:34 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1200
21/01/20 15:36:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[69] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:36:34 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks
21/01/20 15:36:34 WARN TaskSetManager: Stage 17 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:36:34 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 17, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:36:34 INFO Executor: Running task 0.0 in stage 17.0 (TID 17)
21/01/20 15:36:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:34 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:34 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:36:34 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:36:34 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:36:34 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:36:34 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:36:34 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:36:34 INFO ParquetOutputFormat: Validation is off
21/01/20 15:36:34 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:36:34 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:36:34 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:36:34 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:36:34 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:36:34 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:36:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:36:35 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153634_0017_m_000000_17' to o3fs://bucket1.vol1/testdata
21/01/20 15:36:35 INFO SparkHadoopMapRedUtil: attempt_20210120153634_0017_m_000000_17: Committed
21/01/20 15:36:35 INFO Executor: Finished task 0.0 in stage 17.0 (TID 17). 2155 bytes result sent to driver
21/01/20 15:36:35 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 17) in 952 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:36:35 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
21/01/20 15:36:35 INFO DAGScheduler: ResultStage 17 (parquet at Generate.java:61) finished in 0.974 s
21/01/20 15:36:35 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:36:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
21/01/20 15:36:35 INFO DAGScheduler: Job 17 finished: parquet at Generate.java:61, took 0.999692 s
21/01/20 15:36:35 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-90e28aeb-7637-4698-9b3d-972dcc43c9b3
21/01/20 15:36:35 INFO FileFormatWriter: Write Job 3860b967-99e6-40c5-8479-281c772dbe6f committed.
21/01/20 15:36:35 INFO FileFormatWriter: Finished processing stats for write job 3860b967-99e6-40c5-8479-281c772dbe6f.
21/01/20 15:36:35 INFO BlockManagerInfo: Removed broadcast_17_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:36:38 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:38 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:38 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:36:38 INFO DAGScheduler: Got job 18 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:36:38 INFO DAGScheduler: Final stage: ResultStage 18 (parquet at Generate.java:61)
21/01/20 15:36:38 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:36:38 INFO DAGScheduler: Missing parents: List()
21/01/20 15:36:38 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[73] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:36:38 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:36:38 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:36:38 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:36:38 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1200
21/01/20 15:36:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[73] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:36:38 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks
21/01/20 15:36:38 WARN TaskSetManager: Stage 18 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:36:38 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 18, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:36:38 INFO Executor: Running task 0.0 in stage 18.0 (TID 18)
21/01/20 15:36:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:38 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:38 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:36:38 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:36:38 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:36:38 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:36:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:36:38 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:36:38 INFO ParquetOutputFormat: Validation is off
21/01/20 15:36:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:36:38 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:36:38 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:36:38 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:36:38 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:36:38 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:36:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:36:39 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153638_0018_m_000000_18' to o3fs://bucket1.vol1/testdata
21/01/20 15:36:39 INFO SparkHadoopMapRedUtil: attempt_20210120153638_0018_m_000000_18: Committed
21/01/20 15:36:39 INFO Executor: Finished task 0.0 in stage 18.0 (TID 18). 2155 bytes result sent to driver
21/01/20 15:36:39 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 18) in 1021 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:36:39 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
21/01/20 15:36:39 INFO DAGScheduler: ResultStage 18 (parquet at Generate.java:61) finished in 1.045 s
21/01/20 15:36:39 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:36:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
21/01/20 15:36:39 INFO DAGScheduler: Job 18 finished: parquet at Generate.java:61, took 1.048028 s
21/01/20 15:36:39 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-9ef8526e-b01b-44a2-a72b-a96b88c1cf49
21/01/20 15:36:39 INFO FileFormatWriter: Write Job 4a10af8b-fb30-49f6-83ae-1029ec64b335 committed.
21/01/20 15:36:39 INFO FileFormatWriter: Finished processing stats for write job 4a10af8b-fb30-49f6-83ae-1029ec64b335.
21/01/20 15:36:40 INFO BlockManagerInfo: Removed broadcast_18_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:36:41 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:41 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:41 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:36:41 INFO DAGScheduler: Got job 19 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:36:41 INFO DAGScheduler: Final stage: ResultStage 19 (parquet at Generate.java:61)
21/01/20 15:36:41 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:36:41 INFO DAGScheduler: Missing parents: List()
21/01/20 15:36:41 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[77] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:36:41 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:36:41 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:36:41 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:36:41 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1200
21/01/20 15:36:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[77] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:36:41 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks
21/01/20 15:36:41 WARN TaskSetManager: Stage 19 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:36:41 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 19, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:36:41 INFO Executor: Running task 0.0 in stage 19.0 (TID 19)
21/01/20 15:36:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:41 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:41 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:36:41 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:36:41 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:36:41 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:36:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:36:41 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:36:41 INFO ParquetOutputFormat: Validation is off
21/01/20 15:36:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:36:41 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:36:41 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:36:41 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:36:41 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:36:41 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:36:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:36:42 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153641_0019_m_000000_19' to o3fs://bucket1.vol1/testdata
21/01/20 15:36:42 INFO SparkHadoopMapRedUtil: attempt_20210120153641_0019_m_000000_19: Committed
21/01/20 15:36:42 INFO Executor: Finished task 0.0 in stage 19.0 (TID 19). 2155 bytes result sent to driver
21/01/20 15:36:42 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 19) in 905 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:36:42 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
21/01/20 15:36:42 INFO DAGScheduler: ResultStage 19 (parquet at Generate.java:61) finished in 0.950 s
21/01/20 15:36:42 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:36:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished
21/01/20 15:36:42 INFO DAGScheduler: Job 19 finished: parquet at Generate.java:61, took 0.954337 s
21/01/20 15:36:42 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-22161f50-fc15-4c15-80b3-752c35afefb7
21/01/20 15:36:42 INFO FileFormatWriter: Write Job dd859d54-3820-4528-844c-e1e55ba2e706 committed.
21/01/20 15:36:42 INFO FileFormatWriter: Finished processing stats for write job dd859d54-3820-4528-844c-e1e55ba2e706.
21/01/20 15:36:42 INFO BlockManagerInfo: Removed broadcast_19_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:36:45 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:45 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:45 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:36:45 INFO DAGScheduler: Got job 20 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:36:45 INFO DAGScheduler: Final stage: ResultStage 20 (parquet at Generate.java:61)
21/01/20 15:36:45 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:36:45 INFO DAGScheduler: Missing parents: List()
21/01/20 15:36:45 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[81] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:36:45 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:36:45 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:36:45 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:36:45 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1200
21/01/20 15:36:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[81] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:36:45 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks
21/01/20 15:36:45 WARN TaskSetManager: Stage 20 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:36:45 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 20, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:36:45 INFO Executor: Running task 0.0 in stage 20.0 (TID 20)
21/01/20 15:36:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:45 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:45 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:36:45 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:36:45 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:36:45 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:36:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:36:45 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:36:45 INFO ParquetOutputFormat: Validation is off
21/01/20 15:36:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:36:45 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:36:45 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:36:45 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:36:45 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:36:45 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:36:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:36:46 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153645_0020_m_000000_20' to o3fs://bucket1.vol1/testdata
21/01/20 15:36:46 INFO SparkHadoopMapRedUtil: attempt_20210120153645_0020_m_000000_20: Committed
21/01/20 15:36:46 INFO Executor: Finished task 0.0 in stage 20.0 (TID 20). 2155 bytes result sent to driver
21/01/20 15:36:46 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 20) in 1049 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:36:46 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
21/01/20 15:36:46 INFO DAGScheduler: ResultStage 20 (parquet at Generate.java:61) finished in 1.070 s
21/01/20 15:36:46 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:36:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished
21/01/20 15:36:46 INFO DAGScheduler: Job 20 finished: parquet at Generate.java:61, took 1.072396 s
21/01/20 15:36:46 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-2b5c78ef-c051-4573-9b91-a41675a913e0
21/01/20 15:36:46 INFO FileFormatWriter: Write Job 58d242b1-2342-4588-a5a6-111bad359dde committed.
21/01/20 15:36:46 INFO FileFormatWriter: Finished processing stats for write job 58d242b1-2342-4588-a5a6-111bad359dde.
21/01/20 15:36:47 INFO BlockManagerInfo: Removed broadcast_20_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:36:49 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:49 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:36:49 INFO DAGScheduler: Got job 21 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:36:49 INFO DAGScheduler: Final stage: ResultStage 21 (parquet at Generate.java:61)
21/01/20 15:36:49 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:36:49 INFO DAGScheduler: Missing parents: List()
21/01/20 15:36:49 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[85] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:36:49 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:36:49 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:36:49 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:36:49 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1200
21/01/20 15:36:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[85] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:36:49 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks
21/01/20 15:36:49 WARN TaskSetManager: Stage 21 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:36:49 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 21, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:36:49 INFO Executor: Running task 0.0 in stage 21.0 (TID 21)
21/01/20 15:36:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:49 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:36:49 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:36:49 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:36:49 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:36:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:36:49 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:36:49 INFO ParquetOutputFormat: Validation is off
21/01/20 15:36:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:36:49 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:36:49 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:36:49 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:36:49 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:36:49 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:36:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:36:49 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-55C957BA1832->d3c4d3e3-944b-4528-9d82-e005ad427697
21/01/20 15:36:49 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:36:50 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153649_0021_m_000000_21' to o3fs://bucket1.vol1/testdata
21/01/20 15:36:50 INFO SparkHadoopMapRedUtil: attempt_20210120153649_0021_m_000000_21: Committed
21/01/20 15:36:50 INFO Executor: Finished task 0.0 in stage 21.0 (TID 21). 2155 bytes result sent to driver
21/01/20 15:36:50 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 21) in 1055 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:36:50 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
21/01/20 15:36:50 INFO DAGScheduler: ResultStage 21 (parquet at Generate.java:61) finished in 1.076 s
21/01/20 15:36:50 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:36:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
21/01/20 15:36:50 INFO DAGScheduler: Job 21 finished: parquet at Generate.java:61, took 1.078121 s
21/01/20 15:36:50 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-9a80b694-c8b3-422c-bbed-90e6ed15f7c6
21/01/20 15:36:50 INFO FileFormatWriter: Write Job 99ffedbd-1936-42e5-8738-8b0c6a2805fc committed.
21/01/20 15:36:50 INFO FileFormatWriter: Finished processing stats for write job 99ffedbd-1936-42e5-8738-8b0c6a2805fc.
21/01/20 15:36:51 INFO BlockManagerInfo: Removed broadcast_21_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:36:52 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:52 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:52 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:36:52 INFO DAGScheduler: Got job 22 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:36:52 INFO DAGScheduler: Final stage: ResultStage 22 (parquet at Generate.java:61)
21/01/20 15:36:52 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:36:52 INFO DAGScheduler: Missing parents: List()
21/01/20 15:36:52 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[89] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:36:52 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:36:52 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:36:52 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:36:52 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1200
21/01/20 15:36:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[89] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:36:52 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks
21/01/20 15:36:52 WARN TaskSetManager: Stage 22 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:36:52 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 22, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:36:52 INFO Executor: Running task 0.0 in stage 22.0 (TID 22)
21/01/20 15:36:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:53 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:36:53 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:36:53 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:36:53 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:36:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:36:53 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:36:53 INFO ParquetOutputFormat: Validation is off
21/01/20 15:36:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:36:53 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:36:53 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:36:53 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:36:53 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:36:53 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:36:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:36:53 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153652_0022_m_000000_22' to o3fs://bucket1.vol1/testdata
21/01/20 15:36:53 INFO SparkHadoopMapRedUtil: attempt_20210120153652_0022_m_000000_22: Committed
21/01/20 15:36:53 INFO Executor: Finished task 0.0 in stage 22.0 (TID 22). 2155 bytes result sent to driver
21/01/20 15:36:53 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 22) in 1089 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:36:53 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
21/01/20 15:36:53 INFO DAGScheduler: ResultStage 22 (parquet at Generate.java:61) finished in 1.131 s
21/01/20 15:36:53 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:36:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished
21/01/20 15:36:53 INFO DAGScheduler: Job 22 finished: parquet at Generate.java:61, took 1.133835 s
21/01/20 15:36:53 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-bdd51129-9f80-49d0-a291-3055f5fe1924
21/01/20 15:36:53 INFO FileFormatWriter: Write Job 7e4aa9c7-4073-4949-8732-13dcb3c6e974 committed.
21/01/20 15:36:53 INFO FileFormatWriter: Finished processing stats for write job 7e4aa9c7-4073-4949-8732-13dcb3c6e974.
21/01/20 15:36:54 INFO BlockManagerInfo: Removed broadcast_22_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:36:56 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:56 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:56 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:36:56 INFO DAGScheduler: Got job 23 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:36:56 INFO DAGScheduler: Final stage: ResultStage 23 (parquet at Generate.java:61)
21/01/20 15:36:56 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:36:56 INFO DAGScheduler: Missing parents: List()
21/01/20 15:36:56 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[93] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:36:56 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:36:56 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:36:56 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:36:56 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1200
21/01/20 15:36:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[93] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:36:56 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks
21/01/20 15:36:56 WARN TaskSetManager: Stage 23 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:36:56 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 23, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:36:56 INFO Executor: Running task 0.0 in stage 23.0 (TID 23)
21/01/20 15:36:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:56 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:36:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:36:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:36:56 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:36:56 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:36:56 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:36:56 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:36:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:36:56 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:36:56 INFO ParquetOutputFormat: Validation is off
21/01/20 15:36:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:36:56 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:36:56 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:36:56 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:36:56 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:36:56 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:36:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:36:57 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-9B9FBB768C1E->af5c9c6a-a887-4538-ae7b-ba6d49328297
21/01/20 15:36:57 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:36:57 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153656_0023_m_000000_23' to o3fs://bucket1.vol1/testdata
21/01/20 15:36:57 INFO SparkHadoopMapRedUtil: attempt_20210120153656_0023_m_000000_23: Committed
21/01/20 15:36:57 INFO Executor: Finished task 0.0 in stage 23.0 (TID 23). 2155 bytes result sent to driver
21/01/20 15:36:57 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 23) in 1076 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:36:57 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
21/01/20 15:36:57 INFO DAGScheduler: ResultStage 23 (parquet at Generate.java:61) finished in 1.097 s
21/01/20 15:36:57 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:36:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished
21/01/20 15:36:57 INFO DAGScheduler: Job 23 finished: parquet at Generate.java:61, took 1.099563 s
21/01/20 15:36:57 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-d431f679-a46e-4be5-899d-dffc4e786e54
21/01/20 15:36:57 INFO FileFormatWriter: Write Job ffe354a8-19c8-474a-ae33-669678d8bc5e committed.
21/01/20 15:36:57 INFO FileFormatWriter: Finished processing stats for write job ffe354a8-19c8-474a-ae33-669678d8bc5e.
21/01/20 15:36:58 INFO BlockManagerInfo: Removed broadcast_23_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:37:00 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:00 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:37:00 INFO DAGScheduler: Got job 24 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:37:00 INFO DAGScheduler: Final stage: ResultStage 24 (parquet at Generate.java:61)
21/01/20 15:37:00 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:37:00 INFO DAGScheduler: Missing parents: List()
21/01/20 15:37:00 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[97] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:37:00 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:37:00 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:37:00 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:37:00 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1200
21/01/20 15:37:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[97] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:37:00 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks
21/01/20 15:37:00 WARN TaskSetManager: Stage 24 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:37:00 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 24, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:37:00 INFO Executor: Running task 0.0 in stage 24.0 (TID 24)
21/01/20 15:37:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:00 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:37:00 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:37:00 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:37:00 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:37:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:37:00 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:37:00 INFO ParquetOutputFormat: Validation is off
21/01/20 15:37:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:37:00 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:37:00 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:37:00 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:37:00 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:37:00 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:37:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:37:01 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153700_0024_m_000000_24' to o3fs://bucket1.vol1/testdata
21/01/20 15:37:01 INFO SparkHadoopMapRedUtil: attempt_20210120153700_0024_m_000000_24: Committed
21/01/20 15:37:01 INFO Executor: Finished task 0.0 in stage 24.0 (TID 24). 2155 bytes result sent to driver
21/01/20 15:37:01 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 24) in 934 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:37:01 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
21/01/20 15:37:01 INFO DAGScheduler: ResultStage 24 (parquet at Generate.java:61) finished in 0.976 s
21/01/20 15:37:01 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:37:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished
21/01/20 15:37:01 INFO DAGScheduler: Job 24 finished: parquet at Generate.java:61, took 0.978469 s
21/01/20 15:37:01 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-43a6a931-9139-4728-8d1f-8d37fb7afa9a
21/01/20 15:37:01 INFO FileFormatWriter: Write Job 887e5114-30c8-4747-850c-d0555cbdef0a committed.
21/01/20 15:37:01 INFO FileFormatWriter: Finished processing stats for write job 887e5114-30c8-4747-850c-d0555cbdef0a.
21/01/20 15:37:01 INFO BlockManagerInfo: Removed broadcast_24_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:37:03 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:03 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:37:03 INFO DAGScheduler: Got job 25 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:37:03 INFO DAGScheduler: Final stage: ResultStage 25 (parquet at Generate.java:61)
21/01/20 15:37:03 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:37:03 INFO DAGScheduler: Missing parents: List()
21/01/20 15:37:03 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[101] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:37:03 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:37:03 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:37:03 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:37:03 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1200
21/01/20 15:37:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[101] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:37:03 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks
21/01/20 15:37:03 WARN TaskSetManager: Stage 25 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:37:03 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 25, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:37:03 INFO Executor: Running task 0.0 in stage 25.0 (TID 25)
21/01/20 15:37:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:03 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:37:03 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:37:03 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:37:03 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:37:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:37:03 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:37:03 INFO ParquetOutputFormat: Validation is off
21/01/20 15:37:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:37:03 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:37:03 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:37:03 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:37:03 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:37:03 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:37:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:37:04 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153703_0025_m_000000_25' to o3fs://bucket1.vol1/testdata
21/01/20 15:37:04 INFO SparkHadoopMapRedUtil: attempt_20210120153703_0025_m_000000_25: Committed
21/01/20 15:37:04 INFO Executor: Finished task 0.0 in stage 25.0 (TID 25). 2155 bytes result sent to driver
21/01/20 15:37:04 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 25) in 990 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:37:04 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
21/01/20 15:37:04 INFO DAGScheduler: ResultStage 25 (parquet at Generate.java:61) finished in 1.011 s
21/01/20 15:37:04 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:37:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished
21/01/20 15:37:04 INFO DAGScheduler: Job 25 finished: parquet at Generate.java:61, took 1.013481 s
21/01/20 15:37:04 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-b667193e-394d-48ba-8b00-d61871d335e1
21/01/20 15:37:04 INFO FileFormatWriter: Write Job 8f0f9a07-1de3-49da-be50-161c2d1a243b committed.
21/01/20 15:37:04 INFO FileFormatWriter: Finished processing stats for write job 8f0f9a07-1de3-49da-be50-161c2d1a243b.
21/01/20 15:37:06 INFO BlockManagerInfo: Removed broadcast_25_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:37:07 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:07 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:07 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:37:07 INFO DAGScheduler: Got job 26 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:37:07 INFO DAGScheduler: Final stage: ResultStage 26 (parquet at Generate.java:61)
21/01/20 15:37:07 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:37:07 INFO DAGScheduler: Missing parents: List()
21/01/20 15:37:07 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[105] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:37:07 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:37:07 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:37:07 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:37:07 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1200
21/01/20 15:37:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[105] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:37:07 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks
21/01/20 15:37:07 WARN TaskSetManager: Stage 26 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:37:07 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 26, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:37:07 INFO Executor: Running task 0.0 in stage 26.0 (TID 26)
21/01/20 15:37:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:07 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:07 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:37:07 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:37:07 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:37:07 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:37:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:37:07 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:37:07 INFO ParquetOutputFormat: Validation is off
21/01/20 15:37:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:37:07 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:37:07 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:37:07 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:37:07 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:37:07 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:37:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:37:08 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-CA11AC4A83F0->d3c4d3e3-944b-4528-9d82-e005ad427697
21/01/20 15:37:08 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:37:08 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153707_0026_m_000000_26' to o3fs://bucket1.vol1/testdata
21/01/20 15:37:08 INFO SparkHadoopMapRedUtil: attempt_20210120153707_0026_m_000000_26: Committed
21/01/20 15:37:08 INFO Executor: Finished task 0.0 in stage 26.0 (TID 26). 2155 bytes result sent to driver
21/01/20 15:37:08 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 26) in 968 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:37:08 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
21/01/20 15:37:08 INFO DAGScheduler: ResultStage 26 (parquet at Generate.java:61) finished in 1.012 s
21/01/20 15:37:08 INFO DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:37:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished
21/01/20 15:37:08 INFO DAGScheduler: Job 26 finished: parquet at Generate.java:61, took 1.014504 s
21/01/20 15:37:08 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-af2cfca8-0dab-443f-92ac-2a11f6ada566
21/01/20 15:37:08 INFO FileFormatWriter: Write Job 6b7c457c-3a64-45e3-9d62-47e862782017 committed.
21/01/20 15:37:08 INFO FileFormatWriter: Finished processing stats for write job 6b7c457c-3a64-45e3-9d62-47e862782017.
21/01/20 15:37:08 INFO BlockManagerInfo: Removed broadcast_26_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:37:11 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:11 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:37:11 INFO DAGScheduler: Got job 27 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:37:11 INFO DAGScheduler: Final stage: ResultStage 27 (parquet at Generate.java:61)
21/01/20 15:37:11 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:37:11 INFO DAGScheduler: Missing parents: List()
21/01/20 15:37:11 INFO DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[109] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:37:11 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:37:11 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:37:11 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:37:11 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1200
21/01/20 15:37:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[109] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:37:11 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks
21/01/20 15:37:11 WARN TaskSetManager: Stage 27 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:37:11 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 27, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:37:11 INFO Executor: Running task 0.0 in stage 27.0 (TID 27)
21/01/20 15:37:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:11 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:37:11 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:37:11 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:37:11 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:37:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:37:11 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:37:11 INFO ParquetOutputFormat: Validation is off
21/01/20 15:37:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:37:11 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:37:11 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:37:11 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:37:11 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:37:11 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:37:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:37:12 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153711_0027_m_000000_27' to o3fs://bucket1.vol1/testdata
21/01/20 15:37:12 INFO SparkHadoopMapRedUtil: attempt_20210120153711_0027_m_000000_27: Committed
21/01/20 15:37:12 INFO Executor: Finished task 0.0 in stage 27.0 (TID 27). 2155 bytes result sent to driver
21/01/20 15:37:12 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 27) in 991 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:37:12 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool 
21/01/20 15:37:12 INFO DAGScheduler: ResultStage 27 (parquet at Generate.java:61) finished in 1.010 s
21/01/20 15:37:12 INFO DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:37:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage finished
21/01/20 15:37:12 INFO DAGScheduler: Job 27 finished: parquet at Generate.java:61, took 1.012344 s
21/01/20 15:37:12 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-83a8f0cc-bda9-4fbf-beb7-43a2175cd11c
21/01/20 15:37:12 INFO FileFormatWriter: Write Job d63a85c6-60c0-438b-b3b4-5c32bc4481f1 committed.
21/01/20 15:37:12 INFO FileFormatWriter: Finished processing stats for write job d63a85c6-60c0-438b-b3b4-5c32bc4481f1.
21/01/20 15:37:13 INFO BlockManagerInfo: Removed broadcast_27_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:37:14 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:14 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:14 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:37:14 INFO DAGScheduler: Got job 28 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:37:14 INFO DAGScheduler: Final stage: ResultStage 28 (parquet at Generate.java:61)
21/01/20 15:37:14 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:37:14 INFO DAGScheduler: Missing parents: List()
21/01/20 15:37:14 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[113] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:37:14 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:37:14 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:37:14 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:37:14 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1200
21/01/20 15:37:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[113] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:37:14 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks
21/01/20 15:37:14 WARN TaskSetManager: Stage 28 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:37:14 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 28, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:37:14 INFO Executor: Running task 0.0 in stage 28.0 (TID 28)
21/01/20 15:37:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:14 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:14 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:37:14 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:37:14 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:37:14 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:37:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:37:14 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:37:14 INFO ParquetOutputFormat: Validation is off
21/01/20 15:37:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:37:14 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:37:14 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:37:14 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:37:14 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:37:14 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:37:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:37:15 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153714_0028_m_000000_28' to o3fs://bucket1.vol1/testdata
21/01/20 15:37:15 INFO SparkHadoopMapRedUtil: attempt_20210120153714_0028_m_000000_28: Committed
21/01/20 15:37:15 INFO Executor: Finished task 0.0 in stage 28.0 (TID 28). 2155 bytes result sent to driver
21/01/20 15:37:15 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 28) in 948 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:37:15 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool 
21/01/20 15:37:15 INFO DAGScheduler: ResultStage 28 (parquet at Generate.java:61) finished in 0.991 s
21/01/20 15:37:15 INFO DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:37:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage finished
21/01/20 15:37:15 INFO DAGScheduler: Job 28 finished: parquet at Generate.java:61, took 0.993871 s
21/01/20 15:37:15 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-f45ff145-18fc-47eb-b51a-5b02fbc1e0ed
21/01/20 15:37:15 INFO FileFormatWriter: Write Job 58be9fb9-c20a-4e25-ad73-5aef71bfc4ab committed.
21/01/20 15:37:15 INFO FileFormatWriter: Finished processing stats for write job 58be9fb9-c20a-4e25-ad73-5aef71bfc4ab.
21/01/20 15:37:15 INFO BlockManagerInfo: Removed broadcast_28_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:37:18 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:18 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:18 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:37:18 INFO DAGScheduler: Got job 29 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:37:18 INFO DAGScheduler: Final stage: ResultStage 29 (parquet at Generate.java:61)
21/01/20 15:37:18 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:37:18 INFO DAGScheduler: Missing parents: List()
21/01/20 15:37:18 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[117] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:37:18 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:37:18 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:37:18 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:37:18 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1200
21/01/20 15:37:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[117] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:37:18 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks
21/01/20 15:37:18 WARN TaskSetManager: Stage 29 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:37:18 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 29, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:37:18 INFO Executor: Running task 0.0 in stage 29.0 (TID 29)
21/01/20 15:37:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:18 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:18 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:37:18 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:37:18 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:37:18 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:37:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:37:18 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:37:18 INFO ParquetOutputFormat: Validation is off
21/01/20 15:37:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:37:18 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:37:18 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:37:18 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:37:18 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:37:18 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:37:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:37:19 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153718_0029_m_000000_29' to o3fs://bucket1.vol1/testdata
21/01/20 15:37:19 INFO SparkHadoopMapRedUtil: attempt_20210120153718_0029_m_000000_29: Committed
21/01/20 15:37:19 INFO Executor: Finished task 0.0 in stage 29.0 (TID 29). 2155 bytes result sent to driver
21/01/20 15:37:19 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 29) in 1121 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:37:19 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool 
21/01/20 15:37:19 INFO DAGScheduler: ResultStage 29 (parquet at Generate.java:61) finished in 1.142 s
21/01/20 15:37:19 INFO DAGScheduler: Job 29 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:37:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 29: Stage finished
21/01/20 15:37:19 INFO DAGScheduler: Job 29 finished: parquet at Generate.java:61, took 1.144802 s
21/01/20 15:37:19 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-5344bbbd-7af3-413f-892d-3f6adf375dbe
21/01/20 15:37:19 INFO FileFormatWriter: Write Job f0627a04-7203-4c6f-830e-141e811c62e5 committed.
21/01/20 15:37:19 INFO FileFormatWriter: Finished processing stats for write job f0627a04-7203-4c6f-830e-141e811c62e5.
21/01/20 15:37:20 INFO BlockManagerInfo: Removed broadcast_29_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:37:22 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:22 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:22 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:37:22 INFO DAGScheduler: Got job 30 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:37:22 INFO DAGScheduler: Final stage: ResultStage 30 (parquet at Generate.java:61)
21/01/20 15:37:22 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:37:22 INFO DAGScheduler: Missing parents: List()
21/01/20 15:37:22 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[121] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:37:22 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:37:22 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:37:22 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:37:22 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1200
21/01/20 15:37:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[121] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:37:22 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks
21/01/20 15:37:22 WARN TaskSetManager: Stage 30 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:37:22 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 30, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:37:22 INFO Executor: Running task 0.0 in stage 30.0 (TID 30)
21/01/20 15:37:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:22 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:22 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:37:22 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:37:22 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:37:22 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:37:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:37:22 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:37:22 INFO ParquetOutputFormat: Validation is off
21/01/20 15:37:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:37:22 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:37:22 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:37:22 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:37:22 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:37:22 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:37:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:37:22 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-051D9F58608D->af5c9c6a-a887-4538-ae7b-ba6d49328297
21/01/20 15:37:22 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:37:23 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153722_0030_m_000000_30' to o3fs://bucket1.vol1/testdata
21/01/20 15:37:23 INFO SparkHadoopMapRedUtil: attempt_20210120153722_0030_m_000000_30: Committed
21/01/20 15:37:23 INFO Executor: Finished task 0.0 in stage 30.0 (TID 30). 2155 bytes result sent to driver
21/01/20 15:37:23 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 30) in 1024 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:37:23 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool 
21/01/20 15:37:23 INFO DAGScheduler: ResultStage 30 (parquet at Generate.java:61) finished in 1.044 s
21/01/20 15:37:23 INFO DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:37:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished
21/01/20 15:37:23 INFO DAGScheduler: Job 30 finished: parquet at Generate.java:61, took 1.046287 s
21/01/20 15:37:23 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-84fe0632-7e9f-4f9f-8cb0-17048e735692
21/01/20 15:37:23 INFO FileFormatWriter: Write Job c034c676-c9d6-45ef-880b-4ae044d9361e committed.
21/01/20 15:37:23 INFO FileFormatWriter: Finished processing stats for write job c034c676-c9d6-45ef-880b-4ae044d9361e.
21/01/20 15:37:24 INFO BlockManagerInfo: Removed broadcast_30_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:37:25 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:25 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:25 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:37:25 INFO DAGScheduler: Got job 31 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:37:25 INFO DAGScheduler: Final stage: ResultStage 31 (parquet at Generate.java:61)
21/01/20 15:37:25 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:37:25 INFO DAGScheduler: Missing parents: List()
21/01/20 15:37:25 INFO DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[125] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:37:25 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:37:25 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:37:25 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:37:25 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1200
21/01/20 15:37:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[125] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:37:25 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks
21/01/20 15:37:25 WARN TaskSetManager: Stage 31 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:37:25 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 31, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:37:25 INFO Executor: Running task 0.0 in stage 31.0 (TID 31)
21/01/20 15:37:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:25 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:25 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:37:25 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:37:25 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:37:25 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:37:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:37:25 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:37:25 INFO ParquetOutputFormat: Validation is off
21/01/20 15:37:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:37:25 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:37:25 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:37:25 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:37:25 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:37:25 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:37:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:37:26 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153725_0031_m_000000_31' to o3fs://bucket1.vol1/testdata
21/01/20 15:37:26 INFO SparkHadoopMapRedUtil: attempt_20210120153725_0031_m_000000_31: Committed
21/01/20 15:37:26 INFO Executor: Finished task 0.0 in stage 31.0 (TID 31). 2155 bytes result sent to driver
21/01/20 15:37:26 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 31) in 997 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:37:26 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool 
21/01/20 15:37:26 INFO DAGScheduler: ResultStage 31 (parquet at Generate.java:61) finished in 1.016 s
21/01/20 15:37:26 INFO DAGScheduler: Job 31 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:37:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 31: Stage finished
21/01/20 15:37:26 INFO DAGScheduler: Job 31 finished: parquet at Generate.java:61, took 1.018348 s
21/01/20 15:37:26 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-fd1cacd2-3f1d-469a-9db6-36d4973c0324
21/01/20 15:37:26 INFO FileFormatWriter: Write Job 667a55b9-42e7-4f07-8dff-d1549030804c committed.
21/01/20 15:37:26 INFO FileFormatWriter: Finished processing stats for write job 667a55b9-42e7-4f07-8dff-d1549030804c.
21/01/20 15:37:28 INFO BlockManagerInfo: Removed broadcast_31_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:37:29 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:29 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:37:29 INFO DAGScheduler: Got job 32 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:37:29 INFO DAGScheduler: Final stage: ResultStage 32 (parquet at Generate.java:61)
21/01/20 15:37:29 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:37:29 INFO DAGScheduler: Missing parents: List()
21/01/20 15:37:29 INFO DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[129] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:37:29 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:37:29 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:37:29 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:37:29 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1200
21/01/20 15:37:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[129] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:37:29 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks
21/01/20 15:37:29 WARN TaskSetManager: Stage 32 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:37:29 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 32, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:37:29 INFO Executor: Running task 0.0 in stage 32.0 (TID 32)
21/01/20 15:37:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:29 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:37:29 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:37:29 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:37:29 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:37:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:37:29 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:37:29 INFO ParquetOutputFormat: Validation is off
21/01/20 15:37:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:37:29 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:37:29 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:37:29 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:37:29 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:37:29 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:37:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:37:29 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-5655661020FB->d3c4d3e3-944b-4528-9d82-e005ad427697
21/01/20 15:37:29 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:37:30 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153729_0032_m_000000_32' to o3fs://bucket1.vol1/testdata
21/01/20 15:37:30 INFO SparkHadoopMapRedUtil: attempt_20210120153729_0032_m_000000_32: Committed
21/01/20 15:37:30 INFO Executor: Finished task 0.0 in stage 32.0 (TID 32). 2155 bytes result sent to driver
21/01/20 15:37:30 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 32) in 920 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:37:30 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool 
21/01/20 15:37:30 INFO DAGScheduler: ResultStage 32 (parquet at Generate.java:61) finished in 0.962 s
21/01/20 15:37:30 INFO DAGScheduler: Job 32 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:37:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 32: Stage finished
21/01/20 15:37:30 INFO DAGScheduler: Job 32 finished: parquet at Generate.java:61, took 0.963791 s
21/01/20 15:37:30 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-a12a9499-523b-477b-9b58-69c4aa10b6c8
21/01/20 15:37:30 INFO FileFormatWriter: Write Job 56627db8-df9b-4e06-b217-ea612772966a committed.
21/01/20 15:37:30 INFO FileFormatWriter: Finished processing stats for write job 56627db8-df9b-4e06-b217-ea612772966a.
21/01/20 15:37:30 INFO BlockManagerInfo: Removed broadcast_32_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:37:32 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:32 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:32 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:37:32 INFO DAGScheduler: Got job 33 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:37:32 INFO DAGScheduler: Final stage: ResultStage 33 (parquet at Generate.java:61)
21/01/20 15:37:32 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:37:32 INFO DAGScheduler: Missing parents: List()
21/01/20 15:37:32 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[133] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:37:33 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:37:33 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:37:33 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:37:33 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1200
21/01/20 15:37:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[133] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:37:33 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks
21/01/20 15:37:33 WARN TaskSetManager: Stage 33 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:37:33 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 33, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:37:33 INFO Executor: Running task 0.0 in stage 33.0 (TID 33)
21/01/20 15:37:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:33 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:33 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:37:33 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:37:33 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:37:33 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:37:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:37:33 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:37:33 INFO ParquetOutputFormat: Validation is off
21/01/20 15:37:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:37:33 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:37:33 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:37:33 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:37:33 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:37:33 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:37:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:37:34 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153732_0033_m_000000_33' to o3fs://bucket1.vol1/testdata
21/01/20 15:37:34 INFO SparkHadoopMapRedUtil: attempt_20210120153732_0033_m_000000_33: Committed
21/01/20 15:37:34 INFO Executor: Finished task 0.0 in stage 33.0 (TID 33). 2155 bytes result sent to driver
21/01/20 15:37:34 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 33) in 1051 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:37:34 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool 
21/01/20 15:37:34 INFO DAGScheduler: ResultStage 33 (parquet at Generate.java:61) finished in 1.071 s
21/01/20 15:37:34 INFO DAGScheduler: Job 33 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:37:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 33: Stage finished
21/01/20 15:37:34 INFO DAGScheduler: Job 33 finished: parquet at Generate.java:61, took 1.072847 s
21/01/20 15:37:34 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-1b1fd4a9-b916-43ac-b093-32318b1fc771
21/01/20 15:37:34 INFO FileFormatWriter: Write Job bdb16104-6110-4bb4-a2ce-bb57ee34d22c committed.
21/01/20 15:37:34 INFO FileFormatWriter: Finished processing stats for write job bdb16104-6110-4bb4-a2ce-bb57ee34d22c.
21/01/20 15:37:35 INFO BlockManagerInfo: Removed broadcast_33_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:37:36 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:36 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:36 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:37:36 INFO DAGScheduler: Got job 34 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:37:36 INFO DAGScheduler: Final stage: ResultStage 34 (parquet at Generate.java:61)
21/01/20 15:37:36 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:37:36 INFO DAGScheduler: Missing parents: List()
21/01/20 15:37:36 INFO DAGScheduler: Submitting ResultStage 34 (MapPartitionsRDD[137] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:37:36 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:37:36 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:37:36 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:37:36 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1200
21/01/20 15:37:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 34 (MapPartitionsRDD[137] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:37:36 INFO TaskSchedulerImpl: Adding task set 34.0 with 1 tasks
21/01/20 15:37:36 WARN TaskSetManager: Stage 34 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:37:36 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 34, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:37:36 INFO Executor: Running task 0.0 in stage 34.0 (TID 34)
21/01/20 15:37:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:36 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:36 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:37:36 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:37:36 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:37:36 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:37:36 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:37:36 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:37:36 INFO ParquetOutputFormat: Validation is off
21/01/20 15:37:36 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:37:36 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:37:36 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:37:36 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:37:36 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:37:36 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:37:37 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:37:37 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-6963B1346288->af5c9c6a-a887-4538-ae7b-ba6d49328297
21/01/20 15:37:37 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:37:37 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153736_0034_m_000000_34' to o3fs://bucket1.vol1/testdata
21/01/20 15:37:37 INFO SparkHadoopMapRedUtil: attempt_20210120153736_0034_m_000000_34: Committed
21/01/20 15:37:37 INFO Executor: Finished task 0.0 in stage 34.0 (TID 34). 2155 bytes result sent to driver
21/01/20 15:37:37 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 34) in 978 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:37:37 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool 
21/01/20 15:37:37 INFO DAGScheduler: ResultStage 34 (parquet at Generate.java:61) finished in 0.998 s
21/01/20 15:37:37 INFO DAGScheduler: Job 34 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:37:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 34: Stage finished
21/01/20 15:37:37 INFO DAGScheduler: Job 34 finished: parquet at Generate.java:61, took 0.999980 s
21/01/20 15:37:37 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-8ac2c8fa-0932-42ff-afbf-fedc0180345f
21/01/20 15:37:37 INFO FileFormatWriter: Write Job 3483c6cb-6fc8-4359-aa80-3e9e67f8db42 committed.
21/01/20 15:37:37 INFO FileFormatWriter: Finished processing stats for write job 3483c6cb-6fc8-4359-aa80-3e9e67f8db42.
21/01/20 15:37:39 INFO BlockManagerInfo: Removed broadcast_34_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:37:40 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:40 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:40 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:37:40 INFO DAGScheduler: Got job 35 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:37:40 INFO DAGScheduler: Final stage: ResultStage 35 (parquet at Generate.java:61)
21/01/20 15:37:40 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:37:40 INFO DAGScheduler: Missing parents: List()
21/01/20 15:37:40 INFO DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[141] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:37:40 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:37:40 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:37:40 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:37:40 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1200
21/01/20 15:37:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[141] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:37:40 INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks
21/01/20 15:37:40 WARN TaskSetManager: Stage 35 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:37:40 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 35, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:37:40 INFO Executor: Running task 0.0 in stage 35.0 (TID 35)
21/01/20 15:37:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:40 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:40 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:37:40 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:37:40 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:37:40 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:37:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:37:40 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:37:40 INFO ParquetOutputFormat: Validation is off
21/01/20 15:37:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:37:40 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:37:40 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:37:40 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:37:40 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:37:40 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:37:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:37:41 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153740_0035_m_000000_35' to o3fs://bucket1.vol1/testdata
21/01/20 15:37:41 INFO SparkHadoopMapRedUtil: attempt_20210120153740_0035_m_000000_35: Committed
21/01/20 15:37:41 INFO Executor: Finished task 0.0 in stage 35.0 (TID 35). 2155 bytes result sent to driver
21/01/20 15:37:41 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 35) in 935 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:37:41 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool 
21/01/20 15:37:41 INFO DAGScheduler: ResultStage 35 (parquet at Generate.java:61) finished in 0.986 s
21/01/20 15:37:41 INFO DAGScheduler: Job 35 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:37:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished
21/01/20 15:37:41 INFO DAGScheduler: Job 35 finished: parquet at Generate.java:61, took 0.988010 s
21/01/20 15:37:41 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-85d5a283-edb0-4232-b5e5-1844a6f04559
21/01/20 15:37:41 INFO FileFormatWriter: Write Job 040b4886-3124-4875-8af1-6c87c0a158bb committed.
21/01/20 15:37:41 INFO FileFormatWriter: Finished processing stats for write job 040b4886-3124-4875-8af1-6c87c0a158bb.
21/01/20 15:37:41 INFO BlockManagerInfo: Removed broadcast_35_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:37:43 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:43 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:43 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:37:43 INFO DAGScheduler: Got job 36 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:37:43 INFO DAGScheduler: Final stage: ResultStage 36 (parquet at Generate.java:61)
21/01/20 15:37:43 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:37:43 INFO DAGScheduler: Missing parents: List()
21/01/20 15:37:43 INFO DAGScheduler: Submitting ResultStage 36 (MapPartitionsRDD[145] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:37:43 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:37:43 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:37:43 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:37:43 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1200
21/01/20 15:37:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 36 (MapPartitionsRDD[145] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:37:43 INFO TaskSchedulerImpl: Adding task set 36.0 with 1 tasks
21/01/20 15:37:44 WARN TaskSetManager: Stage 36 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:37:44 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 36, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:37:44 INFO Executor: Running task 0.0 in stage 36.0 (TID 36)
21/01/20 15:37:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:44 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:44 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:37:44 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:37:44 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:37:44 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:37:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:37:44 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:37:44 INFO ParquetOutputFormat: Validation is off
21/01/20 15:37:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:37:44 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:37:44 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:37:44 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:37:44 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:37:44 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:37:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:37:44 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153743_0036_m_000000_36' to o3fs://bucket1.vol1/testdata
21/01/20 15:37:44 INFO SparkHadoopMapRedUtil: attempt_20210120153743_0036_m_000000_36: Committed
21/01/20 15:37:44 INFO Executor: Finished task 0.0 in stage 36.0 (TID 36). 2155 bytes result sent to driver
21/01/20 15:37:44 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 36) in 1037 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:37:44 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool 
21/01/20 15:37:44 INFO DAGScheduler: ResultStage 36 (parquet at Generate.java:61) finished in 1.058 s
21/01/20 15:37:44 INFO DAGScheduler: Job 36 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:37:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 36: Stage finished
21/01/20 15:37:44 INFO DAGScheduler: Job 36 finished: parquet at Generate.java:61, took 1.059682 s
21/01/20 15:37:44 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-7a82711f-1e51-4191-81ef-b41625320bbe
21/01/20 15:37:44 INFO FileFormatWriter: Write Job 2284bf44-952d-4abc-ba68-84d84de30b1f committed.
21/01/20 15:37:44 INFO FileFormatWriter: Finished processing stats for write job 2284bf44-952d-4abc-ba68-84d84de30b1f.
21/01/20 15:37:46 INFO BlockManagerInfo: Removed broadcast_36_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:37:47 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:47 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:37:47 INFO DAGScheduler: Got job 37 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:37:47 INFO DAGScheduler: Final stage: ResultStage 37 (parquet at Generate.java:61)
21/01/20 15:37:47 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:37:47 INFO DAGScheduler: Missing parents: List()
21/01/20 15:37:47 INFO DAGScheduler: Submitting ResultStage 37 (MapPartitionsRDD[149] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:37:47 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:37:47 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:37:47 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:37:47 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1200
21/01/20 15:37:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (MapPartitionsRDD[149] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:37:47 INFO TaskSchedulerImpl: Adding task set 37.0 with 1 tasks
21/01/20 15:37:47 WARN TaskSetManager: Stage 37 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:37:47 INFO TaskSetManager: Starting task 0.0 in stage 37.0 (TID 37, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:37:47 INFO Executor: Running task 0.0 in stage 37.0 (TID 37)
21/01/20 15:37:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:47 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:37:47 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:37:47 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:37:47 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:37:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:37:47 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:37:47 INFO ParquetOutputFormat: Validation is off
21/01/20 15:37:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:37:47 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:37:47 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:37:47 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:37:47 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:37:47 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:37:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:37:48 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-01240A4E47EE->af5c9c6a-a887-4538-ae7b-ba6d49328297
21/01/20 15:37:48 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:37:48 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153747_0037_m_000000_37' to o3fs://bucket1.vol1/testdata
21/01/20 15:37:48 INFO SparkHadoopMapRedUtil: attempt_20210120153747_0037_m_000000_37: Committed
21/01/20 15:37:48 INFO Executor: Finished task 0.0 in stage 37.0 (TID 37). 2155 bytes result sent to driver
21/01/20 15:37:48 INFO TaskSetManager: Finished task 0.0 in stage 37.0 (TID 37) in 885 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:37:48 INFO TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool 
21/01/20 15:37:48 INFO DAGScheduler: ResultStage 37 (parquet at Generate.java:61) finished in 0.936 s
21/01/20 15:37:48 INFO DAGScheduler: Job 37 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:37:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 37: Stage finished
21/01/20 15:37:48 INFO DAGScheduler: Job 37 finished: parquet at Generate.java:61, took 0.938008 s
21/01/20 15:37:48 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-b08a66d7-ff5c-4307-b10e-c72cc36faba4
21/01/20 15:37:48 INFO FileFormatWriter: Write Job abc8c0df-c5d4-4c9c-942a-5e1cb13e30b3 committed.
21/01/20 15:37:48 INFO FileFormatWriter: Finished processing stats for write job abc8c0df-c5d4-4c9c-942a-5e1cb13e30b3.
21/01/20 15:37:48 INFO BlockManagerInfo: Removed broadcast_37_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:37:51 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:51 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:37:51 INFO DAGScheduler: Got job 38 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:37:51 INFO DAGScheduler: Final stage: ResultStage 38 (parquet at Generate.java:61)
21/01/20 15:37:51 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:37:51 INFO DAGScheduler: Missing parents: List()
21/01/20 15:37:51 INFO DAGScheduler: Submitting ResultStage 38 (MapPartitionsRDD[153] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:37:51 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:37:51 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:37:51 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:37:51 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1200
21/01/20 15:37:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 38 (MapPartitionsRDD[153] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:37:51 INFO TaskSchedulerImpl: Adding task set 38.0 with 1 tasks
21/01/20 15:37:51 WARN TaskSetManager: Stage 38 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:37:51 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 38, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:37:51 INFO Executor: Running task 0.0 in stage 38.0 (TID 38)
21/01/20 15:37:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:51 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:37:51 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:37:51 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:37:51 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:37:51 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:37:51 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:37:51 INFO ParquetOutputFormat: Validation is off
21/01/20 15:37:51 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:37:51 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:37:51 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:37:51 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:37:51 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:37:51 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:37:51 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:37:52 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153751_0038_m_000000_38' to o3fs://bucket1.vol1/testdata
21/01/20 15:37:52 INFO SparkHadoopMapRedUtil: attempt_20210120153751_0038_m_000000_38: Committed
21/01/20 15:37:52 INFO Executor: Finished task 0.0 in stage 38.0 (TID 38). 2155 bytes result sent to driver
21/01/20 15:37:52 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 38) in 1101 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:37:52 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool 
21/01/20 15:37:52 INFO DAGScheduler: ResultStage 38 (parquet at Generate.java:61) finished in 1.123 s
21/01/20 15:37:52 INFO DAGScheduler: Job 38 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:37:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 38: Stage finished
21/01/20 15:37:52 INFO DAGScheduler: Job 38 finished: parquet at Generate.java:61, took 1.124753 s
21/01/20 15:37:52 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-c2673120-072f-4f46-aca0-9bf23c3f024c
21/01/20 15:37:52 INFO FileFormatWriter: Write Job 439d4b55-5a3b-439d-bb5a-b53a285c4939 committed.
21/01/20 15:37:52 INFO FileFormatWriter: Finished processing stats for write job 439d4b55-5a3b-439d-bb5a-b53a285c4939.
21/01/20 15:37:53 INFO BlockManagerInfo: Removed broadcast_38_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:37:54 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:54 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:54 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:37:54 INFO DAGScheduler: Got job 39 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:37:54 INFO DAGScheduler: Final stage: ResultStage 39 (parquet at Generate.java:61)
21/01/20 15:37:54 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:37:54 INFO DAGScheduler: Missing parents: List()
21/01/20 15:37:54 INFO DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[157] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:37:54 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:37:54 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:37:54 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:37:54 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1200
21/01/20 15:37:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[157] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:37:54 INFO TaskSchedulerImpl: Adding task set 39.0 with 1 tasks
21/01/20 15:37:54 WARN TaskSetManager: Stage 39 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:37:54 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 39, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:37:54 INFO Executor: Running task 0.0 in stage 39.0 (TID 39)
21/01/20 15:37:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:54 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:54 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:37:54 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:37:54 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:37:54 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:37:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:37:54 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:37:54 INFO ParquetOutputFormat: Validation is off
21/01/20 15:37:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:37:54 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:37:54 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:37:54 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:37:54 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:37:54 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:37:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:37:55 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153754_0039_m_000000_39' to o3fs://bucket1.vol1/testdata
21/01/20 15:37:55 INFO SparkHadoopMapRedUtil: attempt_20210120153754_0039_m_000000_39: Committed
21/01/20 15:37:55 INFO Executor: Finished task 0.0 in stage 39.0 (TID 39). 2155 bytes result sent to driver
21/01/20 15:37:55 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 39) in 1019 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:37:55 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool 
21/01/20 15:37:55 INFO DAGScheduler: ResultStage 39 (parquet at Generate.java:61) finished in 1.039 s
21/01/20 15:37:55 INFO DAGScheduler: Job 39 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:37:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 39: Stage finished
21/01/20 15:37:55 INFO DAGScheduler: Job 39 finished: parquet at Generate.java:61, took 1.040262 s
21/01/20 15:37:55 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-527fd1e8-9d0d-44c4-bdf3-ad1359b9aec0
21/01/20 15:37:55 INFO FileFormatWriter: Write Job 39a03e69-86c5-413b-98bd-24dad7437d32 committed.
21/01/20 15:37:55 INFO FileFormatWriter: Finished processing stats for write job 39a03e69-86c5-413b-98bd-24dad7437d32.
21/01/20 15:37:56 INFO BlockManagerInfo: Removed broadcast_39_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:37:58 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:58 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:58 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:37:58 INFO DAGScheduler: Got job 40 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:37:58 INFO DAGScheduler: Final stage: ResultStage 40 (parquet at Generate.java:61)
21/01/20 15:37:58 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:37:58 INFO DAGScheduler: Missing parents: List()
21/01/20 15:37:58 INFO DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[161] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:37:58 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:37:58 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 73.9 KiB, free 413.7 MiB)
21/01/20 15:37:58 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on test-runner-lmfgs:39761 (size: 73.9 KiB, free: 413.9 MiB)
21/01/20 15:37:58 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1200
21/01/20 15:37:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[161] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:37:58 INFO TaskSchedulerImpl: Adding task set 40.0 with 1 tasks
21/01/20 15:37:58 WARN TaskSetManager: Stage 40 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:37:58 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 40, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:37:58 INFO Executor: Running task 0.0 in stage 40.0 (TID 40)
21/01/20 15:37:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:58 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:37:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:37:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:37:58 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:37:58 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:37:58 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:37:58 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:37:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:37:58 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:37:58 INFO ParquetOutputFormat: Validation is off
21/01/20 15:37:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:37:58 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:37:58 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:37:58 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:37:58 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:37:58 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:37:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:37:59 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153758_0040_m_000000_40' to o3fs://bucket1.vol1/testdata
21/01/20 15:37:59 INFO SparkHadoopMapRedUtil: attempt_20210120153758_0040_m_000000_40: Committed
21/01/20 15:37:59 INFO Executor: Finished task 0.0 in stage 40.0 (TID 40). 2155 bytes result sent to driver
21/01/20 15:37:59 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 40) in 1084 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:37:59 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool 
21/01/20 15:37:59 INFO DAGScheduler: ResultStage 40 (parquet at Generate.java:61) finished in 1.103 s
21/01/20 15:37:59 INFO DAGScheduler: Job 40 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:37:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 40: Stage finished
21/01/20 15:37:59 INFO DAGScheduler: Job 40 finished: parquet at Generate.java:61, took 1.104889 s
21/01/20 15:37:59 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-6fb018cf-1bc6-4906-9ebf-ff7996872b41
21/01/20 15:37:59 INFO FileFormatWriter: Write Job a2a86a43-9eb0-439a-a043-03e618230d39 committed.
21/01/20 15:37:59 INFO FileFormatWriter: Finished processing stats for write job a2a86a43-9eb0-439a-a043-03e618230d39.
21/01/20 15:38:00 INFO BlockManagerInfo: Removed broadcast_40_piece0 on test-runner-lmfgs:39761 in memory (size: 73.9 KiB, free: 413.9 MiB)
21/01/20 15:38:02 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:02 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:38:02 INFO DAGScheduler: Got job 41 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:38:02 INFO DAGScheduler: Final stage: ResultStage 41 (parquet at Generate.java:61)
21/01/20 15:38:02 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:38:02 INFO DAGScheduler: Missing parents: List()
21/01/20 15:38:02 INFO DAGScheduler: Submitting ResultStage 41 (MapPartitionsRDD[165] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:38:02 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:38:02 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:38:02 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:38:02 INFO SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1200
21/01/20 15:38:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 41 (MapPartitionsRDD[165] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:38:02 INFO TaskSchedulerImpl: Adding task set 41.0 with 1 tasks
21/01/20 15:38:02 WARN TaskSetManager: Stage 41 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:38:02 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 41, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:38:02 INFO Executor: Running task 0.0 in stage 41.0 (TID 41)
21/01/20 15:38:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:02 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:38:02 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:38:02 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:38:02 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:38:02 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:38:02 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:38:02 INFO ParquetOutputFormat: Validation is off
21/01/20 15:38:02 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:38:02 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:38:02 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:38:02 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:38:02 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:38:02 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:38:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:38:03 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153802_0041_m_000000_41' to o3fs://bucket1.vol1/testdata
21/01/20 15:38:03 INFO SparkHadoopMapRedUtil: attempt_20210120153802_0041_m_000000_41: Committed
21/01/20 15:38:03 INFO Executor: Finished task 0.0 in stage 41.0 (TID 41). 2155 bytes result sent to driver
21/01/20 15:38:03 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 41) in 1097 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:38:03 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool 
21/01/20 15:38:03 INFO DAGScheduler: ResultStage 41 (parquet at Generate.java:61) finished in 1.116 s
21/01/20 15:38:03 INFO DAGScheduler: Job 41 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:38:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 41: Stage finished
21/01/20 15:38:03 INFO DAGScheduler: Job 41 finished: parquet at Generate.java:61, took 1.118691 s
21/01/20 15:38:03 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-5dba5227-3f81-4ab1-829c-f566cedf1634
21/01/20 15:38:03 INFO FileFormatWriter: Write Job fc9901bc-221a-4d09-aadb-88669e5ff338 committed.
21/01/20 15:38:03 INFO FileFormatWriter: Finished processing stats for write job fc9901bc-221a-4d09-aadb-88669e5ff338.
21/01/20 15:38:04 INFO BlockManagerInfo: Removed broadcast_41_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:38:05 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:05 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:05 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:38:05 INFO DAGScheduler: Got job 42 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:38:05 INFO DAGScheduler: Final stage: ResultStage 42 (parquet at Generate.java:61)
21/01/20 15:38:05 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:38:05 INFO DAGScheduler: Missing parents: List()
21/01/20 15:38:05 INFO DAGScheduler: Submitting ResultStage 42 (MapPartitionsRDD[169] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:38:05 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:38:05 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:38:05 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:38:06 INFO SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1200
21/01/20 15:38:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 42 (MapPartitionsRDD[169] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:38:06 INFO TaskSchedulerImpl: Adding task set 42.0 with 1 tasks
21/01/20 15:38:06 WARN TaskSetManager: Stage 42 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:38:06 INFO TaskSetManager: Starting task 0.0 in stage 42.0 (TID 42, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:38:06 INFO Executor: Running task 0.0 in stage 42.0 (TID 42)
21/01/20 15:38:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:06 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:06 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:38:06 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:38:06 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:38:06 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:38:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:38:06 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:38:06 INFO ParquetOutputFormat: Validation is off
21/01/20 15:38:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:38:06 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:38:06 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:38:06 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:38:06 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:38:06 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:38:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:38:07 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153805_0042_m_000000_42' to o3fs://bucket1.vol1/testdata
21/01/20 15:38:07 INFO SparkHadoopMapRedUtil: attempt_20210120153805_0042_m_000000_42: Committed
21/01/20 15:38:07 INFO Executor: Finished task 0.0 in stage 42.0 (TID 42). 2155 bytes result sent to driver
21/01/20 15:38:07 INFO TaskSetManager: Finished task 0.0 in stage 42.0 (TID 42) in 1027 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:38:07 INFO TaskSchedulerImpl: Removed TaskSet 42.0, whose tasks have all completed, from pool 
21/01/20 15:38:07 INFO DAGScheduler: ResultStage 42 (parquet at Generate.java:61) finished in 1.047 s
21/01/20 15:38:07 INFO DAGScheduler: Job 42 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:38:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 42: Stage finished
21/01/20 15:38:07 INFO DAGScheduler: Job 42 finished: parquet at Generate.java:61, took 1.049085 s
21/01/20 15:38:07 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-ffbf0ec1-16e1-499c-866f-f486e91d6b6d
21/01/20 15:38:07 INFO FileFormatWriter: Write Job 68e98b35-8199-4531-8834-d8a049390d42 committed.
21/01/20 15:38:07 INFO FileFormatWriter: Finished processing stats for write job 68e98b35-8199-4531-8834-d8a049390d42.
21/01/20 15:38:08 INFO BlockManagerInfo: Removed broadcast_42_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:38:09 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:09 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:09 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:38:09 INFO DAGScheduler: Got job 43 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:38:09 INFO DAGScheduler: Final stage: ResultStage 43 (parquet at Generate.java:61)
21/01/20 15:38:09 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:38:09 INFO DAGScheduler: Missing parents: List()
21/01/20 15:38:09 INFO DAGScheduler: Submitting ResultStage 43 (MapPartitionsRDD[173] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:38:09 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:38:09 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:38:09 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:38:09 INFO SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1200
21/01/20 15:38:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 43 (MapPartitionsRDD[173] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:38:09 INFO TaskSchedulerImpl: Adding task set 43.0 with 1 tasks
21/01/20 15:38:09 WARN TaskSetManager: Stage 43 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:38:09 INFO TaskSetManager: Starting task 0.0 in stage 43.0 (TID 43, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:38:09 INFO Executor: Running task 0.0 in stage 43.0 (TID 43)
21/01/20 15:38:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:09 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:09 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:38:09 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:38:09 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:38:09 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:38:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:38:09 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:38:09 INFO ParquetOutputFormat: Validation is off
21/01/20 15:38:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:38:09 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:38:09 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:38:09 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:38:09 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:38:09 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:38:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:38:10 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-9583B0304646->af5c9c6a-a887-4538-ae7b-ba6d49328297
21/01/20 15:38:10 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:38:10 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153809_0043_m_000000_43' to o3fs://bucket1.vol1/testdata
21/01/20 15:38:10 INFO SparkHadoopMapRedUtil: attempt_20210120153809_0043_m_000000_43: Committed
21/01/20 15:38:10 INFO Executor: Finished task 0.0 in stage 43.0 (TID 43). 2155 bytes result sent to driver
21/01/20 15:38:10 INFO TaskSetManager: Finished task 0.0 in stage 43.0 (TID 43) in 776 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:38:10 INFO TaskSchedulerImpl: Removed TaskSet 43.0, whose tasks have all completed, from pool 
21/01/20 15:38:10 INFO DAGScheduler: ResultStage 43 (parquet at Generate.java:61) finished in 0.821 s
21/01/20 15:38:10 INFO DAGScheduler: Job 43 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:38:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 43: Stage finished
21/01/20 15:38:10 INFO DAGScheduler: Job 43 finished: parquet at Generate.java:61, took 0.822789 s
21/01/20 15:38:10 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-5a4d5626-6504-4e73-9c83-67539a9e5c51
21/01/20 15:38:10 INFO FileFormatWriter: Write Job d6ff1356-c4c6-44c6-b11c-b628d8510e7c committed.
21/01/20 15:38:10 INFO FileFormatWriter: Finished processing stats for write job d6ff1356-c4c6-44c6-b11c-b628d8510e7c.
21/01/20 15:38:10 INFO BlockManagerInfo: Removed broadcast_43_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:38:13 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:13 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:13 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:38:13 INFO DAGScheduler: Got job 44 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:38:13 INFO DAGScheduler: Final stage: ResultStage 44 (parquet at Generate.java:61)
21/01/20 15:38:13 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:38:13 INFO DAGScheduler: Missing parents: List()
21/01/20 15:38:13 INFO DAGScheduler: Submitting ResultStage 44 (MapPartitionsRDD[177] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:38:13 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:38:13 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:38:13 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:38:13 INFO SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1200
21/01/20 15:38:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 44 (MapPartitionsRDD[177] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:38:13 INFO TaskSchedulerImpl: Adding task set 44.0 with 1 tasks
21/01/20 15:38:13 WARN TaskSetManager: Stage 44 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:38:13 INFO TaskSetManager: Starting task 0.0 in stage 44.0 (TID 44, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:38:13 INFO Executor: Running task 0.0 in stage 44.0 (TID 44)
21/01/20 15:38:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:13 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:13 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:38:13 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:38:13 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:38:13 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:38:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:38:13 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:38:13 INFO ParquetOutputFormat: Validation is off
21/01/20 15:38:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:38:13 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:38:13 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:38:13 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:38:13 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:38:13 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:38:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:38:14 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153813_0044_m_000000_44' to o3fs://bucket1.vol1/testdata
21/01/20 15:38:14 INFO SparkHadoopMapRedUtil: attempt_20210120153813_0044_m_000000_44: Committed
21/01/20 15:38:14 INFO Executor: Finished task 0.0 in stage 44.0 (TID 44). 2155 bytes result sent to driver
21/01/20 15:38:14 INFO TaskSetManager: Finished task 0.0 in stage 44.0 (TID 44) in 997 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:38:14 INFO TaskSchedulerImpl: Removed TaskSet 44.0, whose tasks have all completed, from pool 
21/01/20 15:38:14 INFO DAGScheduler: ResultStage 44 (parquet at Generate.java:61) finished in 1.017 s
21/01/20 15:38:14 INFO DAGScheduler: Job 44 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:38:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 44: Stage finished
21/01/20 15:38:14 INFO DAGScheduler: Job 44 finished: parquet at Generate.java:61, took 1.018899 s
21/01/20 15:38:14 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-81871dad-e205-430c-b278-09d9c1b855b5
21/01/20 15:38:14 INFO FileFormatWriter: Write Job ab48b18f-10b7-41d2-8155-6cdeed3a7602 committed.
21/01/20 15:38:14 INFO FileFormatWriter: Finished processing stats for write job ab48b18f-10b7-41d2-8155-6cdeed3a7602.
21/01/20 15:38:15 INFO BlockManagerInfo: Removed broadcast_44_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:38:16 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:16 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:16 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:38:16 INFO DAGScheduler: Got job 45 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:38:16 INFO DAGScheduler: Final stage: ResultStage 45 (parquet at Generate.java:61)
21/01/20 15:38:16 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:38:16 INFO DAGScheduler: Missing parents: List()
21/01/20 15:38:16 INFO DAGScheduler: Submitting ResultStage 45 (MapPartitionsRDD[181] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:38:16 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:38:16 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:38:16 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:38:16 INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1200
21/01/20 15:38:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 45 (MapPartitionsRDD[181] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:38:16 INFO TaskSchedulerImpl: Adding task set 45.0 with 1 tasks
21/01/20 15:38:16 WARN TaskSetManager: Stage 45 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:38:16 INFO TaskSetManager: Starting task 0.0 in stage 45.0 (TID 45, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:38:16 INFO Executor: Running task 0.0 in stage 45.0 (TID 45)
21/01/20 15:38:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:16 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:16 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:38:16 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:38:16 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:38:16 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:38:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:38:16 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:38:16 INFO ParquetOutputFormat: Validation is off
21/01/20 15:38:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:38:16 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:38:16 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:38:16 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:38:16 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:38:16 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:38:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:38:17 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153816_0045_m_000000_45' to o3fs://bucket1.vol1/testdata
21/01/20 15:38:17 INFO SparkHadoopMapRedUtil: attempt_20210120153816_0045_m_000000_45: Committed
21/01/20 15:38:17 INFO Executor: Finished task 0.0 in stage 45.0 (TID 45). 2155 bytes result sent to driver
21/01/20 15:38:17 INFO TaskSetManager: Finished task 0.0 in stage 45.0 (TID 45) in 868 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:38:17 INFO TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool 
21/01/20 15:38:17 INFO DAGScheduler: ResultStage 45 (parquet at Generate.java:61) finished in 0.886 s
21/01/20 15:38:17 INFO DAGScheduler: Job 45 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:38:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 45: Stage finished
21/01/20 15:38:17 INFO DAGScheduler: Job 45 finished: parquet at Generate.java:61, took 0.916002 s
21/01/20 15:38:17 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-6df44162-474e-4875-b3ac-685ee1f94b18
21/01/20 15:38:17 INFO FileFormatWriter: Write Job 1a1e339d-a4eb-4794-8ef5-742d41a56e40 committed.
21/01/20 15:38:17 INFO FileFormatWriter: Finished processing stats for write job 1a1e339d-a4eb-4794-8ef5-742d41a56e40.
21/01/20 15:38:17 INFO BlockManagerInfo: Removed broadcast_45_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:38:20 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:20 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:20 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:38:20 INFO DAGScheduler: Got job 46 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:38:20 INFO DAGScheduler: Final stage: ResultStage 46 (parquet at Generate.java:61)
21/01/20 15:38:20 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:38:20 INFO DAGScheduler: Missing parents: List()
21/01/20 15:38:20 INFO DAGScheduler: Submitting ResultStage 46 (MapPartitionsRDD[185] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:38:20 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:38:20 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:38:20 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:38:20 INFO SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1200
21/01/20 15:38:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 46 (MapPartitionsRDD[185] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:38:20 INFO TaskSchedulerImpl: Adding task set 46.0 with 1 tasks
21/01/20 15:38:20 WARN TaskSetManager: Stage 46 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:38:20 INFO TaskSetManager: Starting task 0.0 in stage 46.0 (TID 46, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:38:20 INFO Executor: Running task 0.0 in stage 46.0 (TID 46)
21/01/20 15:38:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:20 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:20 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:38:20 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:38:20 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:38:20 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:38:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:38:20 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:38:20 INFO ParquetOutputFormat: Validation is off
21/01/20 15:38:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:38:20 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:38:20 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:38:20 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:38:20 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:38:20 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:38:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:38:20 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-A581E8CF5B64->d3c4d3e3-944b-4528-9d82-e005ad427697
21/01/20 15:38:20 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:38:21 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153820_0046_m_000000_46' to o3fs://bucket1.vol1/testdata
21/01/20 15:38:21 INFO SparkHadoopMapRedUtil: attempt_20210120153820_0046_m_000000_46: Committed
21/01/20 15:38:21 INFO Executor: Finished task 0.0 in stage 46.0 (TID 46). 2155 bytes result sent to driver
21/01/20 15:38:21 INFO TaskSetManager: Finished task 0.0 in stage 46.0 (TID 46) in 1126 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:38:21 INFO TaskSchedulerImpl: Removed TaskSet 46.0, whose tasks have all completed, from pool 
21/01/20 15:38:21 INFO DAGScheduler: ResultStage 46 (parquet at Generate.java:61) finished in 1.147 s
21/01/20 15:38:21 INFO DAGScheduler: Job 46 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:38:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 46: Stage finished
21/01/20 15:38:21 INFO DAGScheduler: Job 46 finished: parquet at Generate.java:61, took 1.148174 s
21/01/20 15:38:21 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-5448fb9a-e3e0-447d-aa94-dfada8fa64e4
21/01/20 15:38:21 INFO FileFormatWriter: Write Job e62d48be-f8f4-4e20-b5af-12a43b586170 committed.
21/01/20 15:38:21 INFO FileFormatWriter: Finished processing stats for write job e62d48be-f8f4-4e20-b5af-12a43b586170.
21/01/20 15:38:22 INFO BlockManagerInfo: Removed broadcast_46_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:38:23 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:23 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:23 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:38:23 INFO DAGScheduler: Got job 47 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:38:23 INFO DAGScheduler: Final stage: ResultStage 47 (parquet at Generate.java:61)
21/01/20 15:38:23 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:38:23 INFO DAGScheduler: Missing parents: List()
21/01/20 15:38:23 INFO DAGScheduler: Submitting ResultStage 47 (MapPartitionsRDD[189] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:38:24 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:38:24 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:38:24 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:38:24 INFO SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1200
21/01/20 15:38:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 47 (MapPartitionsRDD[189] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:38:24 INFO TaskSchedulerImpl: Adding task set 47.0 with 1 tasks
21/01/20 15:38:24 WARN TaskSetManager: Stage 47 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:38:24 INFO TaskSetManager: Starting task 0.0 in stage 47.0 (TID 47, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:38:24 INFO Executor: Running task 0.0 in stage 47.0 (TID 47)
21/01/20 15:38:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:24 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:24 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:38:24 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:38:24 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:38:24 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:38:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:38:24 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:38:24 INFO ParquetOutputFormat: Validation is off
21/01/20 15:38:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:38:24 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:38:24 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:38:24 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:38:24 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:38:24 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:38:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:38:25 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153823_0047_m_000000_47' to o3fs://bucket1.vol1/testdata
21/01/20 15:38:25 INFO SparkHadoopMapRedUtil: attempt_20210120153823_0047_m_000000_47: Committed
21/01/20 15:38:25 INFO Executor: Finished task 0.0 in stage 47.0 (TID 47). 2155 bytes result sent to driver
21/01/20 15:38:25 INFO TaskSetManager: Finished task 0.0 in stage 47.0 (TID 47) in 1099 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:38:25 INFO TaskSchedulerImpl: Removed TaskSet 47.0, whose tasks have all completed, from pool 
21/01/20 15:38:25 INFO DAGScheduler: ResultStage 47 (parquet at Generate.java:61) finished in 1.117 s
21/01/20 15:38:25 INFO DAGScheduler: Job 47 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:38:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 47: Stage finished
21/01/20 15:38:25 INFO DAGScheduler: Job 47 finished: parquet at Generate.java:61, took 1.119721 s
21/01/20 15:38:25 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-f0f2e52d-70a0-4faf-b090-8129c7b1d1b1
21/01/20 15:38:25 INFO FileFormatWriter: Write Job db950cc9-d0b2-44b6-aea1-20299234f444 committed.
21/01/20 15:38:25 INFO FileFormatWriter: Finished processing stats for write job db950cc9-d0b2-44b6-aea1-20299234f444.
21/01/20 15:38:26 INFO BlockManagerInfo: Removed broadcast_47_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:38:27 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:27 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:27 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:38:27 INFO DAGScheduler: Got job 48 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:38:27 INFO DAGScheduler: Final stage: ResultStage 48 (parquet at Generate.java:61)
21/01/20 15:38:27 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:38:27 INFO DAGScheduler: Missing parents: List()
21/01/20 15:38:27 INFO DAGScheduler: Submitting ResultStage 48 (MapPartitionsRDD[193] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:38:27 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:38:27 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:38:27 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:38:27 INFO SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1200
21/01/20 15:38:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 48 (MapPartitionsRDD[193] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:38:27 INFO TaskSchedulerImpl: Adding task set 48.0 with 1 tasks
21/01/20 15:38:27 WARN TaskSetManager: Stage 48 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:38:27 INFO TaskSetManager: Starting task 0.0 in stage 48.0 (TID 48, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:38:27 INFO Executor: Running task 0.0 in stage 48.0 (TID 48)
21/01/20 15:38:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:27 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:27 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:38:27 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:38:27 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:38:27 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:38:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:38:27 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:38:27 INFO ParquetOutputFormat: Validation is off
21/01/20 15:38:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:38:27 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:38:27 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:38:27 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:38:27 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:38:27 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:38:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:38:28 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153827_0048_m_000000_48' to o3fs://bucket1.vol1/testdata
21/01/20 15:38:28 INFO SparkHadoopMapRedUtil: attempt_20210120153827_0048_m_000000_48: Committed
21/01/20 15:38:28 INFO Executor: Finished task 0.0 in stage 48.0 (TID 48). 2155 bytes result sent to driver
21/01/20 15:38:28 INFO TaskSetManager: Finished task 0.0 in stage 48.0 (TID 48) in 1111 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:38:28 INFO TaskSchedulerImpl: Removed TaskSet 48.0, whose tasks have all completed, from pool 
21/01/20 15:38:28 INFO DAGScheduler: ResultStage 48 (parquet at Generate.java:61) finished in 1.131 s
21/01/20 15:38:28 INFO DAGScheduler: Job 48 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:38:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 48: Stage finished
21/01/20 15:38:28 INFO DAGScheduler: Job 48 finished: parquet at Generate.java:61, took 1.132298 s
21/01/20 15:38:28 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-cd810331-3f40-4daa-933a-8de6b836651c
21/01/20 15:38:28 INFO FileFormatWriter: Write Job e643514c-b39a-442e-a21c-7181f3630494 committed.
21/01/20 15:38:28 INFO FileFormatWriter: Finished processing stats for write job e643514c-b39a-442e-a21c-7181f3630494.
21/01/20 15:38:29 INFO BlockManagerInfo: Removed broadcast_48_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:38:31 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:31 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:38:31 INFO DAGScheduler: Got job 49 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:38:31 INFO DAGScheduler: Final stage: ResultStage 49 (parquet at Generate.java:61)
21/01/20 15:38:31 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:38:31 INFO DAGScheduler: Missing parents: List()
21/01/20 15:38:31 INFO DAGScheduler: Submitting ResultStage 49 (MapPartitionsRDD[197] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:38:31 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:38:31 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:38:31 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:38:31 INFO SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1200
21/01/20 15:38:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 49 (MapPartitionsRDD[197] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:38:31 INFO TaskSchedulerImpl: Adding task set 49.0 with 1 tasks
21/01/20 15:38:31 WARN TaskSetManager: Stage 49 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:38:31 INFO TaskSetManager: Starting task 0.0 in stage 49.0 (TID 49, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:38:31 INFO Executor: Running task 0.0 in stage 49.0 (TID 49)
21/01/20 15:38:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:31 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:38:31 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:38:31 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:38:31 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:38:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:38:31 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:38:31 INFO ParquetOutputFormat: Validation is off
21/01/20 15:38:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:38:31 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:38:31 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:38:31 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:38:31 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:38:31 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:38:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:38:32 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-42BC48071BCC->af5c9c6a-a887-4538-ae7b-ba6d49328297
21/01/20 15:38:32 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:38:32 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153831_0049_m_000000_49' to o3fs://bucket1.vol1/testdata
21/01/20 15:38:32 INFO SparkHadoopMapRedUtil: attempt_20210120153831_0049_m_000000_49: Committed
21/01/20 15:38:32 INFO Executor: Finished task 0.0 in stage 49.0 (TID 49). 2155 bytes result sent to driver
21/01/20 15:38:32 INFO TaskSetManager: Finished task 0.0 in stage 49.0 (TID 49) in 1034 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:38:32 INFO TaskSchedulerImpl: Removed TaskSet 49.0, whose tasks have all completed, from pool 
21/01/20 15:38:32 INFO DAGScheduler: ResultStage 49 (parquet at Generate.java:61) finished in 1.052 s
21/01/20 15:38:32 INFO DAGScheduler: Job 49 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:38:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 49: Stage finished
21/01/20 15:38:32 INFO DAGScheduler: Job 49 finished: parquet at Generate.java:61, took 1.054309 s
21/01/20 15:38:32 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-003d4d2b-d471-42d1-8c69-3b4de84823ac
21/01/20 15:38:32 INFO FileFormatWriter: Write Job c85425f7-d1de-4c40-b68a-9c74fb4601cd committed.
21/01/20 15:38:32 INFO FileFormatWriter: Finished processing stats for write job c85425f7-d1de-4c40-b68a-9c74fb4601cd.
21/01/20 15:38:33 INFO BlockManagerInfo: Removed broadcast_49_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:38:35 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:35 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:35 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:38:35 INFO DAGScheduler: Got job 50 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:38:35 INFO DAGScheduler: Final stage: ResultStage 50 (parquet at Generate.java:61)
21/01/20 15:38:35 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:38:35 INFO DAGScheduler: Missing parents: List()
21/01/20 15:38:35 INFO DAGScheduler: Submitting ResultStage 50 (MapPartitionsRDD[201] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:38:35 INFO MemoryStore: Block broadcast_50 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:38:35 INFO MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:38:35 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:38:35 INFO SparkContext: Created broadcast 50 from broadcast at DAGScheduler.scala:1200
21/01/20 15:38:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 50 (MapPartitionsRDD[201] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:38:35 INFO TaskSchedulerImpl: Adding task set 50.0 with 1 tasks
21/01/20 15:38:35 WARN TaskSetManager: Stage 50 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:38:35 INFO TaskSetManager: Starting task 0.0 in stage 50.0 (TID 50, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:38:35 INFO Executor: Running task 0.0 in stage 50.0 (TID 50)
21/01/20 15:38:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:35 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:35 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:38:35 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:38:35 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:38:35 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:38:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:38:35 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:38:35 INFO ParquetOutputFormat: Validation is off
21/01/20 15:38:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:38:35 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:38:35 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:38:35 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:38:35 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:38:35 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:38:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:38:36 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153835_0050_m_000000_50' to o3fs://bucket1.vol1/testdata
21/01/20 15:38:36 INFO SparkHadoopMapRedUtil: attempt_20210120153835_0050_m_000000_50: Committed
21/01/20 15:38:36 INFO Executor: Finished task 0.0 in stage 50.0 (TID 50). 2155 bytes result sent to driver
21/01/20 15:38:36 INFO TaskSetManager: Finished task 0.0 in stage 50.0 (TID 50) in 1095 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:38:36 INFO TaskSchedulerImpl: Removed TaskSet 50.0, whose tasks have all completed, from pool 
21/01/20 15:38:36 INFO DAGScheduler: ResultStage 50 (parquet at Generate.java:61) finished in 1.113 s
21/01/20 15:38:36 INFO DAGScheduler: Job 50 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:38:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 50: Stage finished
21/01/20 15:38:36 INFO DAGScheduler: Job 50 finished: parquet at Generate.java:61, took 1.115315 s
21/01/20 15:38:36 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-91414436-0484-442c-8a4e-0f90fa408c0b
21/01/20 15:38:36 INFO FileFormatWriter: Write Job 928e6cbf-4fa2-46b6-8774-17b50a3c98be committed.
21/01/20 15:38:36 INFO FileFormatWriter: Finished processing stats for write job 928e6cbf-4fa2-46b6-8774-17b50a3c98be.
21/01/20 15:38:37 INFO BlockManagerInfo: Removed broadcast_50_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:38:38 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:38 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:39 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:38:39 INFO DAGScheduler: Got job 51 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:38:39 INFO DAGScheduler: Final stage: ResultStage 51 (parquet at Generate.java:61)
21/01/20 15:38:39 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:38:39 INFO DAGScheduler: Missing parents: List()
21/01/20 15:38:39 INFO DAGScheduler: Submitting ResultStage 51 (MapPartitionsRDD[205] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:38:39 INFO MemoryStore: Block broadcast_51 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:38:39 INFO MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:38:39 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:38:39 INFO SparkContext: Created broadcast 51 from broadcast at DAGScheduler.scala:1200
21/01/20 15:38:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 51 (MapPartitionsRDD[205] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:38:39 INFO TaskSchedulerImpl: Adding task set 51.0 with 1 tasks
21/01/20 15:38:39 WARN TaskSetManager: Stage 51 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:38:39 INFO TaskSetManager: Starting task 0.0 in stage 51.0 (TID 51, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:38:39 INFO Executor: Running task 0.0 in stage 51.0 (TID 51)
21/01/20 15:38:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:39 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:39 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:38:39 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:38:39 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:38:39 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:38:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:38:39 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:38:39 INFO ParquetOutputFormat: Validation is off
21/01/20 15:38:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:38:39 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:38:39 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:38:39 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:38:39 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:38:39 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:38:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:38:40 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153839_0051_m_000000_51' to o3fs://bucket1.vol1/testdata
21/01/20 15:38:40 INFO SparkHadoopMapRedUtil: attempt_20210120153839_0051_m_000000_51: Committed
21/01/20 15:38:40 INFO Executor: Finished task 0.0 in stage 51.0 (TID 51). 2155 bytes result sent to driver
21/01/20 15:38:40 INFO TaskSetManager: Finished task 0.0 in stage 51.0 (TID 51) in 1047 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:38:40 INFO TaskSchedulerImpl: Removed TaskSet 51.0, whose tasks have all completed, from pool 
21/01/20 15:38:40 INFO DAGScheduler: ResultStage 51 (parquet at Generate.java:61) finished in 1.066 s
21/01/20 15:38:40 INFO DAGScheduler: Job 51 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:38:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 51: Stage finished
21/01/20 15:38:40 INFO DAGScheduler: Job 51 finished: parquet at Generate.java:61, took 1.068893 s
21/01/20 15:38:40 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-10b0b4b4-3e8a-433a-828c-2a5d9aac0a7f
21/01/20 15:38:40 INFO FileFormatWriter: Write Job 72d3c84d-c1b3-4d82-af48-ec17f020b1e8 committed.
21/01/20 15:38:40 INFO FileFormatWriter: Finished processing stats for write job 72d3c84d-c1b3-4d82-af48-ec17f020b1e8.
21/01/20 15:38:41 INFO BlockManagerInfo: Removed broadcast_51_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:38:42 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:42 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:42 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:38:42 INFO DAGScheduler: Got job 52 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:38:42 INFO DAGScheduler: Final stage: ResultStage 52 (parquet at Generate.java:61)
21/01/20 15:38:42 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:38:42 INFO DAGScheduler: Missing parents: List()
21/01/20 15:38:42 INFO DAGScheduler: Submitting ResultStage 52 (MapPartitionsRDD[209] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:38:42 INFO MemoryStore: Block broadcast_52 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:38:42 INFO MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:38:42 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:38:42 INFO SparkContext: Created broadcast 52 from broadcast at DAGScheduler.scala:1200
21/01/20 15:38:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 52 (MapPartitionsRDD[209] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:38:42 INFO TaskSchedulerImpl: Adding task set 52.0 with 1 tasks
21/01/20 15:38:42 WARN TaskSetManager: Stage 52 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:38:42 INFO TaskSetManager: Starting task 0.0 in stage 52.0 (TID 52, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:38:42 INFO Executor: Running task 0.0 in stage 52.0 (TID 52)
21/01/20 15:38:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:42 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:42 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:38:42 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:38:42 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:38:42 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:38:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:38:42 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:38:42 INFO ParquetOutputFormat: Validation is off
21/01/20 15:38:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:38:42 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:38:42 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:38:42 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:38:42 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:38:42 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:38:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:38:43 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153842_0052_m_000000_52' to o3fs://bucket1.vol1/testdata
21/01/20 15:38:43 INFO SparkHadoopMapRedUtil: attempt_20210120153842_0052_m_000000_52: Committed
21/01/20 15:38:43 INFO Executor: Finished task 0.0 in stage 52.0 (TID 52). 2155 bytes result sent to driver
21/01/20 15:38:43 INFO TaskSetManager: Finished task 0.0 in stage 52.0 (TID 52) in 951 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:38:43 INFO TaskSchedulerImpl: Removed TaskSet 52.0, whose tasks have all completed, from pool 
21/01/20 15:38:43 INFO DAGScheduler: ResultStage 52 (parquet at Generate.java:61) finished in 0.993 s
21/01/20 15:38:43 INFO DAGScheduler: Job 52 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:38:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 52: Stage finished
21/01/20 15:38:43 INFO DAGScheduler: Job 52 finished: parquet at Generate.java:61, took 0.994633 s
21/01/20 15:38:43 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-86d9d66c-4b51-4d63-9b3d-c54899ce7ad7
21/01/20 15:38:43 INFO FileFormatWriter: Write Job c39229b3-cfcd-4eee-8ed8-98d2331dd622 committed.
21/01/20 15:38:43 INFO FileFormatWriter: Finished processing stats for write job c39229b3-cfcd-4eee-8ed8-98d2331dd622.
21/01/20 15:38:43 INFO BlockManagerInfo: Removed broadcast_52_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:38:46 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:46 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:46 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:38:46 INFO DAGScheduler: Got job 53 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:38:46 INFO DAGScheduler: Final stage: ResultStage 53 (parquet at Generate.java:61)
21/01/20 15:38:46 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:38:46 INFO DAGScheduler: Missing parents: List()
21/01/20 15:38:46 INFO DAGScheduler: Submitting ResultStage 53 (MapPartitionsRDD[213] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:38:46 INFO MemoryStore: Block broadcast_53 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:38:46 INFO MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:38:46 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:38:46 INFO SparkContext: Created broadcast 53 from broadcast at DAGScheduler.scala:1200
21/01/20 15:38:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 53 (MapPartitionsRDD[213] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:38:46 INFO TaskSchedulerImpl: Adding task set 53.0 with 1 tasks
21/01/20 15:38:46 WARN TaskSetManager: Stage 53 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:38:46 INFO TaskSetManager: Starting task 0.0 in stage 53.0 (TID 53, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:38:46 INFO Executor: Running task 0.0 in stage 53.0 (TID 53)
21/01/20 15:38:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:46 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:46 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:38:46 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:38:46 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:38:46 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:38:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:38:46 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:38:46 INFO ParquetOutputFormat: Validation is off
21/01/20 15:38:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:38:46 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:38:46 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:38:46 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:38:46 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:38:46 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:38:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:38:47 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-8480A16B22D6->af5c9c6a-a887-4538-ae7b-ba6d49328297
21/01/20 15:38:47 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:38:47 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153846_0053_m_000000_53' to o3fs://bucket1.vol1/testdata
21/01/20 15:38:47 INFO SparkHadoopMapRedUtil: attempt_20210120153846_0053_m_000000_53: Committed
21/01/20 15:38:47 INFO Executor: Finished task 0.0 in stage 53.0 (TID 53). 2155 bytes result sent to driver
21/01/20 15:38:47 INFO TaskSetManager: Finished task 0.0 in stage 53.0 (TID 53) in 987 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:38:47 INFO TaskSchedulerImpl: Removed TaskSet 53.0, whose tasks have all completed, from pool 
21/01/20 15:38:47 INFO DAGScheduler: ResultStage 53 (parquet at Generate.java:61) finished in 1.008 s
21/01/20 15:38:47 INFO DAGScheduler: Job 53 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:38:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 53: Stage finished
21/01/20 15:38:47 INFO DAGScheduler: Job 53 finished: parquet at Generate.java:61, took 1.009552 s
21/01/20 15:38:47 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-8205c247-45d1-40fc-bb63-73b0b640478f
21/01/20 15:38:47 INFO FileFormatWriter: Write Job 344f0e19-cc0b-413f-a710-6adaae2cf557 committed.
21/01/20 15:38:47 INFO FileFormatWriter: Finished processing stats for write job 344f0e19-cc0b-413f-a710-6adaae2cf557.
21/01/20 15:38:48 INFO BlockManagerInfo: Removed broadcast_53_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:38:49 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:50 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:38:50 INFO DAGScheduler: Got job 54 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:38:50 INFO DAGScheduler: Final stage: ResultStage 54 (parquet at Generate.java:61)
21/01/20 15:38:50 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:38:50 INFO DAGScheduler: Missing parents: List()
21/01/20 15:38:50 INFO DAGScheduler: Submitting ResultStage 54 (MapPartitionsRDD[217] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:38:50 INFO MemoryStore: Block broadcast_54 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:38:50 INFO MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:38:50 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:38:50 INFO SparkContext: Created broadcast 54 from broadcast at DAGScheduler.scala:1200
21/01/20 15:38:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 54 (MapPartitionsRDD[217] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:38:50 INFO TaskSchedulerImpl: Adding task set 54.0 with 1 tasks
21/01/20 15:38:50 WARN TaskSetManager: Stage 54 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:38:50 INFO TaskSetManager: Starting task 0.0 in stage 54.0 (TID 54, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:38:50 INFO Executor: Running task 0.0 in stage 54.0 (TID 54)
21/01/20 15:38:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:50 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:50 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:38:50 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:38:50 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:38:50 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:38:50 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:38:50 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:38:50 INFO ParquetOutputFormat: Validation is off
21/01/20 15:38:50 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:38:50 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:38:50 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:38:50 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:38:50 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:38:50 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:38:50 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:38:50 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153850_0054_m_000000_54' to o3fs://bucket1.vol1/testdata
21/01/20 15:38:50 INFO SparkHadoopMapRedUtil: attempt_20210120153850_0054_m_000000_54: Committed
21/01/20 15:38:50 INFO Executor: Finished task 0.0 in stage 54.0 (TID 54). 2155 bytes result sent to driver
21/01/20 15:38:50 INFO TaskSetManager: Finished task 0.0 in stage 54.0 (TID 54) in 877 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:38:50 INFO TaskSchedulerImpl: Removed TaskSet 54.0, whose tasks have all completed, from pool 
21/01/20 15:38:50 INFO DAGScheduler: ResultStage 54 (parquet at Generate.java:61) finished in 0.920 s
21/01/20 15:38:50 INFO DAGScheduler: Job 54 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:38:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 54: Stage finished
21/01/20 15:38:50 INFO DAGScheduler: Job 54 finished: parquet at Generate.java:61, took 0.922115 s
21/01/20 15:38:50 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-5164815a-119e-43fd-9685-a7b383a904f3
21/01/20 15:38:50 INFO FileFormatWriter: Write Job f3de151d-34da-4129-8e4a-3a6dad47ef36 committed.
21/01/20 15:38:50 INFO FileFormatWriter: Finished processing stats for write job f3de151d-34da-4129-8e4a-3a6dad47ef36.
21/01/20 15:38:51 INFO BlockManagerInfo: Removed broadcast_54_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:38:53 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:53 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:38:53 INFO DAGScheduler: Got job 55 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:38:53 INFO DAGScheduler: Final stage: ResultStage 55 (parquet at Generate.java:61)
21/01/20 15:38:53 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:38:53 INFO DAGScheduler: Missing parents: List()
21/01/20 15:38:53 INFO DAGScheduler: Submitting ResultStage 55 (MapPartitionsRDD[221] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:38:53 INFO MemoryStore: Block broadcast_55 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:38:53 INFO MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:38:53 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:38:53 INFO SparkContext: Created broadcast 55 from broadcast at DAGScheduler.scala:1200
21/01/20 15:38:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 55 (MapPartitionsRDD[221] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:38:53 INFO TaskSchedulerImpl: Adding task set 55.0 with 1 tasks
21/01/20 15:38:53 WARN TaskSetManager: Stage 55 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:38:53 INFO TaskSetManager: Starting task 0.0 in stage 55.0 (TID 55, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:38:53 INFO Executor: Running task 0.0 in stage 55.0 (TID 55)
21/01/20 15:38:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:53 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:38:53 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:38:53 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:38:53 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:38:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:38:53 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:38:53 INFO ParquetOutputFormat: Validation is off
21/01/20 15:38:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:38:53 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:38:53 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:38:53 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:38:53 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:38:53 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:38:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:38:54 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-F0445BE44436->d3c4d3e3-944b-4528-9d82-e005ad427697
21/01/20 15:38:54 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:38:54 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153853_0055_m_000000_55' to o3fs://bucket1.vol1/testdata
21/01/20 15:38:54 INFO SparkHadoopMapRedUtil: attempt_20210120153853_0055_m_000000_55: Committed
21/01/20 15:38:54 INFO Executor: Finished task 0.0 in stage 55.0 (TID 55). 2155 bytes result sent to driver
21/01/20 15:38:54 INFO TaskSetManager: Finished task 0.0 in stage 55.0 (TID 55) in 1082 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:38:54 INFO TaskSchedulerImpl: Removed TaskSet 55.0, whose tasks have all completed, from pool 
21/01/20 15:38:54 INFO DAGScheduler: ResultStage 55 (parquet at Generate.java:61) finished in 1.101 s
21/01/20 15:38:54 INFO DAGScheduler: Job 55 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:38:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 55: Stage finished
21/01/20 15:38:54 INFO DAGScheduler: Job 55 finished: parquet at Generate.java:61, took 1.102992 s
21/01/20 15:38:54 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-95b5f964-567d-451b-8a15-f090ac87bb60
21/01/20 15:38:54 INFO FileFormatWriter: Write Job 11bd4c33-3c29-4e7b-a6c9-955001068726 committed.
21/01/20 15:38:54 INFO FileFormatWriter: Finished processing stats for write job 11bd4c33-3c29-4e7b-a6c9-955001068726.
21/01/20 15:38:55 INFO BlockManagerInfo: Removed broadcast_55_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:38:57 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:57 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:57 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:38:57 INFO DAGScheduler: Got job 56 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:38:57 INFO DAGScheduler: Final stage: ResultStage 56 (parquet at Generate.java:61)
21/01/20 15:38:57 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:38:57 INFO DAGScheduler: Missing parents: List()
21/01/20 15:38:57 INFO DAGScheduler: Submitting ResultStage 56 (MapPartitionsRDD[225] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:38:57 INFO MemoryStore: Block broadcast_56 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:38:57 INFO MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:38:57 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:38:57 INFO SparkContext: Created broadcast 56 from broadcast at DAGScheduler.scala:1200
21/01/20 15:38:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 56 (MapPartitionsRDD[225] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:38:57 INFO TaskSchedulerImpl: Adding task set 56.0 with 1 tasks
21/01/20 15:38:57 WARN TaskSetManager: Stage 56 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:38:57 INFO TaskSetManager: Starting task 0.0 in stage 56.0 (TID 56, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:38:57 INFO Executor: Running task 0.0 in stage 56.0 (TID 56)
21/01/20 15:38:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:57 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:38:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:38:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:38:57 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:38:57 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:38:57 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:38:57 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:38:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:38:57 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:38:57 INFO ParquetOutputFormat: Validation is off
21/01/20 15:38:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:38:57 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:38:57 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:38:57 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:38:57 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:38:57 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:38:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:38:58 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153857_0056_m_000000_56' to o3fs://bucket1.vol1/testdata
21/01/20 15:38:58 INFO SparkHadoopMapRedUtil: attempt_20210120153857_0056_m_000000_56: Committed
21/01/20 15:38:58 INFO Executor: Finished task 0.0 in stage 56.0 (TID 56). 2155 bytes result sent to driver
21/01/20 15:38:58 INFO TaskSetManager: Finished task 0.0 in stage 56.0 (TID 56) in 861 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:38:58 INFO TaskSchedulerImpl: Removed TaskSet 56.0, whose tasks have all completed, from pool 
21/01/20 15:38:58 INFO DAGScheduler: ResultStage 56 (parquet at Generate.java:61) finished in 0.904 s
21/01/20 15:38:58 INFO DAGScheduler: Job 56 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:38:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 56: Stage finished
21/01/20 15:38:58 INFO DAGScheduler: Job 56 finished: parquet at Generate.java:61, took 0.905991 s
21/01/20 15:38:58 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-9c5541bc-d9b2-4fb2-b6e2-d2ec6ec35351
21/01/20 15:38:58 INFO FileFormatWriter: Write Job ed2ca955-2c54-4845-9f61-369cfad0b77b committed.
21/01/20 15:38:58 INFO FileFormatWriter: Finished processing stats for write job ed2ca955-2c54-4845-9f61-369cfad0b77b.
21/01/20 15:38:58 INFO BlockManagerInfo: Removed broadcast_56_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:39:00 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:00 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:39:00 INFO DAGScheduler: Got job 57 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:39:00 INFO DAGScheduler: Final stage: ResultStage 57 (parquet at Generate.java:61)
21/01/20 15:39:00 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:39:00 INFO DAGScheduler: Missing parents: List()
21/01/20 15:39:00 INFO DAGScheduler: Submitting ResultStage 57 (MapPartitionsRDD[229] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:39:00 INFO MemoryStore: Block broadcast_57 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:39:00 INFO MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:39:00 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:39:00 INFO SparkContext: Created broadcast 57 from broadcast at DAGScheduler.scala:1200
21/01/20 15:39:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 57 (MapPartitionsRDD[229] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:39:00 INFO TaskSchedulerImpl: Adding task set 57.0 with 1 tasks
21/01/20 15:39:00 WARN TaskSetManager: Stage 57 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:39:00 INFO TaskSetManager: Starting task 0.0 in stage 57.0 (TID 57, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:39:00 INFO Executor: Running task 0.0 in stage 57.0 (TID 57)
21/01/20 15:39:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:00 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:39:00 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:39:00 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:39:00 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:39:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:39:00 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:39:00 INFO ParquetOutputFormat: Validation is off
21/01/20 15:39:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:39:00 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:39:00 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:39:00 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:39:00 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:39:00 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:39:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:39:01 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153900_0057_m_000000_57' to o3fs://bucket1.vol1/testdata
21/01/20 15:39:01 INFO SparkHadoopMapRedUtil: attempt_20210120153900_0057_m_000000_57: Committed
21/01/20 15:39:01 INFO Executor: Finished task 0.0 in stage 57.0 (TID 57). 2155 bytes result sent to driver
21/01/20 15:39:01 INFO TaskSetManager: Finished task 0.0 in stage 57.0 (TID 57) in 1033 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:39:01 INFO TaskSchedulerImpl: Removed TaskSet 57.0, whose tasks have all completed, from pool 
21/01/20 15:39:01 INFO DAGScheduler: ResultStage 57 (parquet at Generate.java:61) finished in 1.052 s
21/01/20 15:39:01 INFO DAGScheduler: Job 57 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:39:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 57: Stage finished
21/01/20 15:39:01 INFO DAGScheduler: Job 57 finished: parquet at Generate.java:61, took 1.054210 s
21/01/20 15:39:01 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-b27fac6d-f57b-47e5-b97e-e5af4745886a
21/01/20 15:39:01 INFO FileFormatWriter: Write Job b957ffe3-63a2-4570-b3a3-ce73bf166ee6 committed.
21/01/20 15:39:01 INFO FileFormatWriter: Finished processing stats for write job b957ffe3-63a2-4570-b3a3-ce73bf166ee6.
21/01/20 15:39:02 INFO BlockManagerInfo: Removed broadcast_57_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:39:04 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:04 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:39:04 INFO DAGScheduler: Got job 58 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:39:04 INFO DAGScheduler: Final stage: ResultStage 58 (parquet at Generate.java:61)
21/01/20 15:39:04 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:39:04 INFO DAGScheduler: Missing parents: List()
21/01/20 15:39:04 INFO DAGScheduler: Submitting ResultStage 58 (MapPartitionsRDD[233] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:39:04 INFO MemoryStore: Block broadcast_58 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:39:04 INFO MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:39:04 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:39:04 INFO SparkContext: Created broadcast 58 from broadcast at DAGScheduler.scala:1200
21/01/20 15:39:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 58 (MapPartitionsRDD[233] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:39:04 INFO TaskSchedulerImpl: Adding task set 58.0 with 1 tasks
21/01/20 15:39:04 WARN TaskSetManager: Stage 58 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:39:04 INFO TaskSetManager: Starting task 0.0 in stage 58.0 (TID 58, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:39:04 INFO Executor: Running task 0.0 in stage 58.0 (TID 58)
21/01/20 15:39:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:04 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:39:04 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:39:04 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:39:04 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:39:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:39:04 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:39:04 INFO ParquetOutputFormat: Validation is off
21/01/20 15:39:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:39:04 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:39:04 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:39:04 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:39:04 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:39:04 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:39:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:39:05 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-CAE69B460CA1->d3c4d3e3-944b-4528-9d82-e005ad427697
21/01/20 15:39:05 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:39:05 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153904_0058_m_000000_58' to o3fs://bucket1.vol1/testdata
21/01/20 15:39:05 INFO SparkHadoopMapRedUtil: attempt_20210120153904_0058_m_000000_58: Committed
21/01/20 15:39:05 INFO Executor: Finished task 0.0 in stage 58.0 (TID 58). 2155 bytes result sent to driver
21/01/20 15:39:05 INFO TaskSetManager: Finished task 0.0 in stage 58.0 (TID 58) in 1108 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:39:05 INFO TaskSchedulerImpl: Removed TaskSet 58.0, whose tasks have all completed, from pool 
21/01/20 15:39:05 INFO DAGScheduler: ResultStage 58 (parquet at Generate.java:61) finished in 1.127 s
21/01/20 15:39:05 INFO DAGScheduler: Job 58 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:39:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 58: Stage finished
21/01/20 15:39:05 INFO DAGScheduler: Job 58 finished: parquet at Generate.java:61, took 1.128132 s
21/01/20 15:39:05 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-c59604a5-d7e7-4d52-9982-01eda0516b0e
21/01/20 15:39:05 INFO FileFormatWriter: Write Job 8be04900-7239-4a3c-91e2-02e1925a5889 committed.
21/01/20 15:39:05 INFO FileFormatWriter: Finished processing stats for write job 8be04900-7239-4a3c-91e2-02e1925a5889.
21/01/20 15:39:06 INFO BlockManagerInfo: Removed broadcast_58_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:39:08 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:08 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:08 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:39:08 INFO DAGScheduler: Got job 59 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:39:08 INFO DAGScheduler: Final stage: ResultStage 59 (parquet at Generate.java:61)
21/01/20 15:39:08 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:39:08 INFO DAGScheduler: Missing parents: List()
21/01/20 15:39:08 INFO DAGScheduler: Submitting ResultStage 59 (MapPartitionsRDD[237] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:39:08 INFO MemoryStore: Block broadcast_59 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:39:08 INFO MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:39:08 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:39:08 INFO SparkContext: Created broadcast 59 from broadcast at DAGScheduler.scala:1200
21/01/20 15:39:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 59 (MapPartitionsRDD[237] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:39:08 INFO TaskSchedulerImpl: Adding task set 59.0 with 1 tasks
21/01/20 15:39:08 WARN TaskSetManager: Stage 59 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:39:08 INFO TaskSetManager: Starting task 0.0 in stage 59.0 (TID 59, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:39:08 INFO Executor: Running task 0.0 in stage 59.0 (TID 59)
21/01/20 15:39:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:08 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:08 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:39:08 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:39:08 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:39:08 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:39:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:39:08 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:39:08 INFO ParquetOutputFormat: Validation is off
21/01/20 15:39:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:39:08 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:39:08 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:39:08 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:39:08 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:39:08 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:39:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:39:09 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153908_0059_m_000000_59' to o3fs://bucket1.vol1/testdata
21/01/20 15:39:09 INFO SparkHadoopMapRedUtil: attempt_20210120153908_0059_m_000000_59: Committed
21/01/20 15:39:09 INFO Executor: Finished task 0.0 in stage 59.0 (TID 59). 2155 bytes result sent to driver
21/01/20 15:39:09 INFO TaskSetManager: Finished task 0.0 in stage 59.0 (TID 59) in 1110 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:39:09 INFO TaskSchedulerImpl: Removed TaskSet 59.0, whose tasks have all completed, from pool 
21/01/20 15:39:09 INFO DAGScheduler: ResultStage 59 (parquet at Generate.java:61) finished in 1.130 s
21/01/20 15:39:09 INFO DAGScheduler: Job 59 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:39:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 59: Stage finished
21/01/20 15:39:09 INFO DAGScheduler: Job 59 finished: parquet at Generate.java:61, took 1.131825 s
21/01/20 15:39:09 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-d8f31f15-a670-4adc-80e4-ccfc54b3a8db
21/01/20 15:39:09 INFO FileFormatWriter: Write Job 1bb76f04-acee-4d3e-8ef0-8d22b9c93bed committed.
21/01/20 15:39:09 INFO FileFormatWriter: Finished processing stats for write job 1bb76f04-acee-4d3e-8ef0-8d22b9c93bed.
21/01/20 15:39:10 INFO BlockManagerInfo: Removed broadcast_59_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:39:11 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:11 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:39:11 INFO DAGScheduler: Got job 60 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:39:11 INFO DAGScheduler: Final stage: ResultStage 60 (parquet at Generate.java:61)
21/01/20 15:39:11 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:39:11 INFO DAGScheduler: Missing parents: List()
21/01/20 15:39:11 INFO DAGScheduler: Submitting ResultStage 60 (MapPartitionsRDD[241] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:39:11 INFO MemoryStore: Block broadcast_60 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:39:11 INFO MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:39:11 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:39:11 INFO SparkContext: Created broadcast 60 from broadcast at DAGScheduler.scala:1200
21/01/20 15:39:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 60 (MapPartitionsRDD[241] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:39:11 INFO TaskSchedulerImpl: Adding task set 60.0 with 1 tasks
21/01/20 15:39:12 WARN TaskSetManager: Stage 60 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:39:12 INFO TaskSetManager: Starting task 0.0 in stage 60.0 (TID 60, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:39:12 INFO Executor: Running task 0.0 in stage 60.0 (TID 60)
21/01/20 15:39:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:12 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:12 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:39:12 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:39:12 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:39:12 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:39:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:39:12 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:39:12 INFO ParquetOutputFormat: Validation is off
21/01/20 15:39:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:39:12 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:39:12 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:39:12 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:39:12 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:39:12 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:39:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:39:12 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-68788E559BE7->af5c9c6a-a887-4538-ae7b-ba6d49328297
21/01/20 15:39:12 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:39:13 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153911_0060_m_000000_60' to o3fs://bucket1.vol1/testdata
21/01/20 15:39:13 INFO SparkHadoopMapRedUtil: attempt_20210120153911_0060_m_000000_60: Committed
21/01/20 15:39:13 INFO Executor: Finished task 0.0 in stage 60.0 (TID 60). 2155 bytes result sent to driver
21/01/20 15:39:13 INFO TaskSetManager: Finished task 0.0 in stage 60.0 (TID 60) in 1047 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:39:13 INFO TaskSchedulerImpl: Removed TaskSet 60.0, whose tasks have all completed, from pool 
21/01/20 15:39:13 INFO DAGScheduler: ResultStage 60 (parquet at Generate.java:61) finished in 1.065 s
21/01/20 15:39:13 INFO DAGScheduler: Job 60 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:39:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 60: Stage finished
21/01/20 15:39:13 INFO DAGScheduler: Job 60 finished: parquet at Generate.java:61, took 1.066386 s
21/01/20 15:39:13 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-72495445-7472-4baa-8711-77f3a8207f3c
21/01/20 15:39:13 INFO FileFormatWriter: Write Job 3a458e42-d830-4bad-a748-d6746c82ab06 committed.
21/01/20 15:39:13 INFO FileFormatWriter: Finished processing stats for write job 3a458e42-d830-4bad-a748-d6746c82ab06.
21/01/20 15:39:14 INFO BlockManagerInfo: Removed broadcast_60_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:39:15 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:15 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:15 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:39:15 INFO DAGScheduler: Got job 61 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:39:15 INFO DAGScheduler: Final stage: ResultStage 61 (parquet at Generate.java:61)
21/01/20 15:39:15 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:39:15 INFO DAGScheduler: Missing parents: List()
21/01/20 15:39:15 INFO DAGScheduler: Submitting ResultStage 61 (MapPartitionsRDD[245] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:39:15 INFO MemoryStore: Block broadcast_61 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:39:15 INFO MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:39:15 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:39:15 INFO SparkContext: Created broadcast 61 from broadcast at DAGScheduler.scala:1200
21/01/20 15:39:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 61 (MapPartitionsRDD[245] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:39:15 INFO TaskSchedulerImpl: Adding task set 61.0 with 1 tasks
21/01/20 15:39:15 WARN TaskSetManager: Stage 61 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:39:15 INFO TaskSetManager: Starting task 0.0 in stage 61.0 (TID 61, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:39:15 INFO Executor: Running task 0.0 in stage 61.0 (TID 61)
21/01/20 15:39:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:15 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:15 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:39:15 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:39:15 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:39:15 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:39:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:39:15 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:39:15 INFO ParquetOutputFormat: Validation is off
21/01/20 15:39:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:39:15 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:39:15 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:39:15 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:39:15 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:39:15 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:39:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:39:16 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153915_0061_m_000000_61' to o3fs://bucket1.vol1/testdata
21/01/20 15:39:16 INFO SparkHadoopMapRedUtil: attempt_20210120153915_0061_m_000000_61: Committed
21/01/20 15:39:16 INFO Executor: Finished task 0.0 in stage 61.0 (TID 61). 2155 bytes result sent to driver
21/01/20 15:39:16 INFO TaskSetManager: Finished task 0.0 in stage 61.0 (TID 61) in 1085 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:39:16 INFO TaskSchedulerImpl: Removed TaskSet 61.0, whose tasks have all completed, from pool 
21/01/20 15:39:16 INFO DAGScheduler: ResultStage 61 (parquet at Generate.java:61) finished in 1.106 s
21/01/20 15:39:16 INFO DAGScheduler: Job 61 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:39:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 61: Stage finished
21/01/20 15:39:16 INFO DAGScheduler: Job 61 finished: parquet at Generate.java:61, took 1.107697 s
21/01/20 15:39:16 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-29e2fcd5-f1ff-48fe-a5f2-a8f110ca2f0d
21/01/20 15:39:16 INFO FileFormatWriter: Write Job 2393e7f7-0444-4ae1-a152-9f91e1120ebc committed.
21/01/20 15:39:16 INFO FileFormatWriter: Finished processing stats for write job 2393e7f7-0444-4ae1-a152-9f91e1120ebc.
21/01/20 15:39:17 INFO BlockManagerInfo: Removed broadcast_61_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:39:19 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:19 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:19 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:39:19 INFO DAGScheduler: Got job 62 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:39:19 INFO DAGScheduler: Final stage: ResultStage 62 (parquet at Generate.java:61)
21/01/20 15:39:19 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:39:19 INFO DAGScheduler: Missing parents: List()
21/01/20 15:39:19 INFO DAGScheduler: Submitting ResultStage 62 (MapPartitionsRDD[249] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:39:19 INFO MemoryStore: Block broadcast_62 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:39:19 INFO MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:39:19 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:39:19 INFO SparkContext: Created broadcast 62 from broadcast at DAGScheduler.scala:1200
21/01/20 15:39:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 62 (MapPartitionsRDD[249] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:39:19 INFO TaskSchedulerImpl: Adding task set 62.0 with 1 tasks
21/01/20 15:39:19 WARN TaskSetManager: Stage 62 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:39:19 INFO TaskSetManager: Starting task 0.0 in stage 62.0 (TID 62, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:39:19 INFO Executor: Running task 0.0 in stage 62.0 (TID 62)
21/01/20 15:39:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:19 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:19 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:39:19 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:39:19 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:39:19 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:39:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:39:19 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:39:19 INFO ParquetOutputFormat: Validation is off
21/01/20 15:39:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:39:19 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:39:19 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:39:19 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:39:19 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:39:19 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:39:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:39:20 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153919_0062_m_000000_62' to o3fs://bucket1.vol1/testdata
21/01/20 15:39:20 INFO SparkHadoopMapRedUtil: attempt_20210120153919_0062_m_000000_62: Committed
21/01/20 15:39:20 INFO Executor: Finished task 0.0 in stage 62.0 (TID 62). 2155 bytes result sent to driver
21/01/20 15:39:20 INFO TaskSetManager: Finished task 0.0 in stage 62.0 (TID 62) in 965 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:39:20 INFO TaskSchedulerImpl: Removed TaskSet 62.0, whose tasks have all completed, from pool 
21/01/20 15:39:20 INFO DAGScheduler: ResultStage 62 (parquet at Generate.java:61) finished in 0.984 s
21/01/20 15:39:20 INFO DAGScheduler: Job 62 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:39:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 62: Stage finished
21/01/20 15:39:20 INFO DAGScheduler: Job 62 finished: parquet at Generate.java:61, took 0.985450 s
21/01/20 15:39:20 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-5f5d9ec5-49ba-4b9b-af16-bc378da01d35
21/01/20 15:39:20 INFO FileFormatWriter: Write Job 17d02ddb-c392-4ee7-a814-97efca23e648 committed.
21/01/20 15:39:20 INFO FileFormatWriter: Finished processing stats for write job 17d02ddb-c392-4ee7-a814-97efca23e648.
21/01/20 15:39:21 INFO BlockManagerInfo: Removed broadcast_62_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:39:22 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:22 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:22 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:39:22 INFO DAGScheduler: Got job 63 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:39:22 INFO DAGScheduler: Final stage: ResultStage 63 (parquet at Generate.java:61)
21/01/20 15:39:22 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:39:22 INFO DAGScheduler: Missing parents: List()
21/01/20 15:39:22 INFO DAGScheduler: Submitting ResultStage 63 (MapPartitionsRDD[253] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:39:23 INFO MemoryStore: Block broadcast_63 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:39:23 INFO MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:39:23 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:39:23 INFO SparkContext: Created broadcast 63 from broadcast at DAGScheduler.scala:1200
21/01/20 15:39:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 63 (MapPartitionsRDD[253] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:39:23 INFO TaskSchedulerImpl: Adding task set 63.0 with 1 tasks
21/01/20 15:39:23 WARN TaskSetManager: Stage 63 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:39:23 INFO TaskSetManager: Starting task 0.0 in stage 63.0 (TID 63, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:39:23 INFO Executor: Running task 0.0 in stage 63.0 (TID 63)
21/01/20 15:39:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:23 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:23 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:39:23 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:39:23 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:39:23 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:39:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:39:23 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:39:23 INFO ParquetOutputFormat: Validation is off
21/01/20 15:39:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:39:23 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:39:23 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:39:23 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:39:23 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:39:23 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:39:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:39:23 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153922_0063_m_000000_63' to o3fs://bucket1.vol1/testdata
21/01/20 15:39:23 INFO SparkHadoopMapRedUtil: attempt_20210120153922_0063_m_000000_63: Committed
21/01/20 15:39:23 INFO Executor: Finished task 0.0 in stage 63.0 (TID 63). 2155 bytes result sent to driver
21/01/20 15:39:23 INFO TaskSetManager: Finished task 0.0 in stage 63.0 (TID 63) in 938 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:39:23 INFO TaskSchedulerImpl: Removed TaskSet 63.0, whose tasks have all completed, from pool 
21/01/20 15:39:23 INFO DAGScheduler: ResultStage 63 (parquet at Generate.java:61) finished in 0.980 s
21/01/20 15:39:23 INFO DAGScheduler: Job 63 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:39:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 63: Stage finished
21/01/20 15:39:23 INFO DAGScheduler: Job 63 finished: parquet at Generate.java:61, took 0.982290 s
21/01/20 15:39:23 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-1d8a6c14-6645-48ac-ace2-0f5f9b036b9c
21/01/20 15:39:23 INFO FileFormatWriter: Write Job e706041b-4810-4c4b-b29d-4cb339816e90 committed.
21/01/20 15:39:23 INFO FileFormatWriter: Finished processing stats for write job e706041b-4810-4c4b-b29d-4cb339816e90.
21/01/20 15:39:24 INFO BlockManagerInfo: Removed broadcast_63_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:39:26 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:26 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:26 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:39:26 INFO DAGScheduler: Got job 64 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:39:26 INFO DAGScheduler: Final stage: ResultStage 64 (parquet at Generate.java:61)
21/01/20 15:39:26 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:39:26 INFO DAGScheduler: Missing parents: List()
21/01/20 15:39:26 INFO DAGScheduler: Submitting ResultStage 64 (MapPartitionsRDD[257] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:39:26 INFO MemoryStore: Block broadcast_64 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:39:26 INFO MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:39:26 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:39:26 INFO SparkContext: Created broadcast 64 from broadcast at DAGScheduler.scala:1200
21/01/20 15:39:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 64 (MapPartitionsRDD[257] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:39:26 INFO TaskSchedulerImpl: Adding task set 64.0 with 1 tasks
21/01/20 15:39:26 WARN TaskSetManager: Stage 64 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:39:26 INFO TaskSetManager: Starting task 0.0 in stage 64.0 (TID 64, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:39:26 INFO Executor: Running task 0.0 in stage 64.0 (TID 64)
21/01/20 15:39:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:26 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:26 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:39:26 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:39:26 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:39:26 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:39:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:39:26 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:39:26 INFO ParquetOutputFormat: Validation is off
21/01/20 15:39:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:39:26 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:39:26 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:39:26 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:39:26 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:39:26 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:39:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:39:27 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153926_0064_m_000000_64' to o3fs://bucket1.vol1/testdata
21/01/20 15:39:27 INFO SparkHadoopMapRedUtil: attempt_20210120153926_0064_m_000000_64: Committed
21/01/20 15:39:27 INFO Executor: Finished task 0.0 in stage 64.0 (TID 64). 2155 bytes result sent to driver
21/01/20 15:39:27 INFO TaskSetManager: Finished task 0.0 in stage 64.0 (TID 64) in 1044 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:39:27 INFO TaskSchedulerImpl: Removed TaskSet 64.0, whose tasks have all completed, from pool 
21/01/20 15:39:27 INFO DAGScheduler: ResultStage 64 (parquet at Generate.java:61) finished in 1.062 s
21/01/20 15:39:27 INFO DAGScheduler: Job 64 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:39:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 64: Stage finished
21/01/20 15:39:27 INFO DAGScheduler: Job 64 finished: parquet at Generate.java:61, took 1.063935 s
21/01/20 15:39:27 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-6246bc53-471c-4656-a986-83c94882cf7c
21/01/20 15:39:27 INFO FileFormatWriter: Write Job b6adb906-71eb-4066-a518-f7b55b3f27b0 committed.
21/01/20 15:39:27 INFO FileFormatWriter: Finished processing stats for write job b6adb906-71eb-4066-a518-f7b55b3f27b0.
21/01/20 15:39:28 INFO BlockManagerInfo: Removed broadcast_64_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:39:30 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:30 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:39:30 INFO DAGScheduler: Got job 65 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:39:30 INFO DAGScheduler: Final stage: ResultStage 65 (parquet at Generate.java:61)
21/01/20 15:39:30 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:39:30 INFO DAGScheduler: Missing parents: List()
21/01/20 15:39:30 INFO DAGScheduler: Submitting ResultStage 65 (MapPartitionsRDD[261] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:39:30 INFO MemoryStore: Block broadcast_65 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:39:30 INFO MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:39:30 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:39:30 INFO SparkContext: Created broadcast 65 from broadcast at DAGScheduler.scala:1200
21/01/20 15:39:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 65 (MapPartitionsRDD[261] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:39:30 INFO TaskSchedulerImpl: Adding task set 65.0 with 1 tasks
21/01/20 15:39:30 WARN TaskSetManager: Stage 65 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:39:30 INFO TaskSetManager: Starting task 0.0 in stage 65.0 (TID 65, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:39:30 INFO Executor: Running task 0.0 in stage 65.0 (TID 65)
21/01/20 15:39:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:30 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:39:30 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:39:30 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:39:30 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:39:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:39:30 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:39:30 INFO ParquetOutputFormat: Validation is off
21/01/20 15:39:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:39:30 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:39:30 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:39:30 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:39:30 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:39:30 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:39:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:39:30 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-D5FA3D69F166->af5c9c6a-a887-4538-ae7b-ba6d49328297
21/01/20 15:39:30 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:39:31 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153930_0065_m_000000_65' to o3fs://bucket1.vol1/testdata
21/01/20 15:39:31 INFO SparkHadoopMapRedUtil: attempt_20210120153930_0065_m_000000_65: Committed
21/01/20 15:39:31 INFO Executor: Finished task 0.0 in stage 65.0 (TID 65). 2155 bytes result sent to driver
21/01/20 15:39:31 INFO TaskSetManager: Finished task 0.0 in stage 65.0 (TID 65) in 870 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:39:31 INFO TaskSchedulerImpl: Removed TaskSet 65.0, whose tasks have all completed, from pool 
21/01/20 15:39:31 INFO DAGScheduler: ResultStage 65 (parquet at Generate.java:61) finished in 0.888 s
21/01/20 15:39:31 INFO DAGScheduler: Job 65 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:39:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 65: Stage finished
21/01/20 15:39:31 INFO DAGScheduler: Job 65 finished: parquet at Generate.java:61, took 0.913483 s
21/01/20 15:39:31 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-ad5f7479-b758-4c70-8b68-cb002cff4a03
21/01/20 15:39:31 INFO FileFormatWriter: Write Job 353434cc-0300-425d-a702-36ab83295eb6 committed.
21/01/20 15:39:31 INFO FileFormatWriter: Finished processing stats for write job 353434cc-0300-425d-a702-36ab83295eb6.
21/01/20 15:39:31 INFO BlockManagerInfo: Removed broadcast_65_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:39:33 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:33 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:33 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:39:33 INFO DAGScheduler: Got job 66 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:39:33 INFO DAGScheduler: Final stage: ResultStage 66 (parquet at Generate.java:61)
21/01/20 15:39:33 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:39:33 INFO DAGScheduler: Missing parents: List()
21/01/20 15:39:33 INFO DAGScheduler: Submitting ResultStage 66 (MapPartitionsRDD[265] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:39:33 INFO MemoryStore: Block broadcast_66 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:39:33 INFO MemoryStore: Block broadcast_66_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:39:33 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:39:33 INFO SparkContext: Created broadcast 66 from broadcast at DAGScheduler.scala:1200
21/01/20 15:39:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 66 (MapPartitionsRDD[265] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:39:33 INFO TaskSchedulerImpl: Adding task set 66.0 with 1 tasks
21/01/20 15:39:33 WARN TaskSetManager: Stage 66 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:39:33 INFO TaskSetManager: Starting task 0.0 in stage 66.0 (TID 66, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:39:33 INFO Executor: Running task 0.0 in stage 66.0 (TID 66)
21/01/20 15:39:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:33 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:33 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:39:33 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:39:33 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:39:33 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:39:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:39:33 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:39:33 INFO ParquetOutputFormat: Validation is off
21/01/20 15:39:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:39:33 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:39:33 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:39:33 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:39:33 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:39:33 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:39:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:39:34 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153933_0066_m_000000_66' to o3fs://bucket1.vol1/testdata
21/01/20 15:39:34 INFO SparkHadoopMapRedUtil: attempt_20210120153933_0066_m_000000_66: Committed
21/01/20 15:39:34 INFO Executor: Finished task 0.0 in stage 66.0 (TID 66). 2155 bytes result sent to driver
21/01/20 15:39:34 INFO TaskSetManager: Finished task 0.0 in stage 66.0 (TID 66) in 982 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:39:34 INFO TaskSchedulerImpl: Removed TaskSet 66.0, whose tasks have all completed, from pool 
21/01/20 15:39:34 INFO DAGScheduler: ResultStage 66 (parquet at Generate.java:61) finished in 1.000 s
21/01/20 15:39:34 INFO DAGScheduler: Job 66 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:39:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 66: Stage finished
21/01/20 15:39:34 INFO DAGScheduler: Job 66 finished: parquet at Generate.java:61, took 1.001459 s
21/01/20 15:39:34 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-8cb1d759-a1f4-4481-a075-996267f2f51c
21/01/20 15:39:34 INFO FileFormatWriter: Write Job 11cdbeb6-b3ea-4f24-a6ed-079625670492 committed.
21/01/20 15:39:34 INFO FileFormatWriter: Finished processing stats for write job 11cdbeb6-b3ea-4f24-a6ed-079625670492.
21/01/20 15:39:36 INFO BlockManagerInfo: Removed broadcast_66_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:39:37 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:37 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:37 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:39:37 INFO DAGScheduler: Got job 67 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:39:37 INFO DAGScheduler: Final stage: ResultStage 67 (parquet at Generate.java:61)
21/01/20 15:39:37 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:39:37 INFO DAGScheduler: Missing parents: List()
21/01/20 15:39:37 INFO DAGScheduler: Submitting ResultStage 67 (MapPartitionsRDD[269] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:39:37 INFO MemoryStore: Block broadcast_67 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:39:37 INFO MemoryStore: Block broadcast_67_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:39:37 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:39:37 INFO SparkContext: Created broadcast 67 from broadcast at DAGScheduler.scala:1200
21/01/20 15:39:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 67 (MapPartitionsRDD[269] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:39:37 INFO TaskSchedulerImpl: Adding task set 67.0 with 1 tasks
21/01/20 15:39:37 WARN TaskSetManager: Stage 67 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:39:37 INFO TaskSetManager: Starting task 0.0 in stage 67.0 (TID 67, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:39:37 INFO Executor: Running task 0.0 in stage 67.0 (TID 67)
21/01/20 15:39:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:37 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:37 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:39:37 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:39:37 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:39:37 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:39:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:39:37 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:39:37 INFO ParquetOutputFormat: Validation is off
21/01/20 15:39:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:39:37 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:39:37 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:39:37 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:39:37 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:39:37 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:39:37 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:39:38 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153937_0067_m_000000_67' to o3fs://bucket1.vol1/testdata
21/01/20 15:39:38 INFO SparkHadoopMapRedUtil: attempt_20210120153937_0067_m_000000_67: Committed
21/01/20 15:39:38 INFO Executor: Finished task 0.0 in stage 67.0 (TID 67). 2155 bytes result sent to driver
21/01/20 15:39:38 INFO TaskSetManager: Finished task 0.0 in stage 67.0 (TID 67) in 846 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:39:38 INFO TaskSchedulerImpl: Removed TaskSet 67.0, whose tasks have all completed, from pool 
21/01/20 15:39:38 INFO DAGScheduler: ResultStage 67 (parquet at Generate.java:61) finished in 0.865 s
21/01/20 15:39:38 INFO DAGScheduler: Job 67 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:39:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 67: Stage finished
21/01/20 15:39:38 INFO DAGScheduler: Job 67 finished: parquet at Generate.java:61, took 0.890311 s
21/01/20 15:39:38 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-48d9f64f-933d-45c6-b3a1-63bc1bc8a4fa
21/01/20 15:39:38 INFO FileFormatWriter: Write Job 47751efb-db41-45c2-a0db-e8a6a2a65a84 committed.
21/01/20 15:39:38 INFO FileFormatWriter: Finished processing stats for write job 47751efb-db41-45c2-a0db-e8a6a2a65a84.
21/01/20 15:39:38 INFO BlockManagerInfo: Removed broadcast_67_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:39:40 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:40 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:40 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:39:40 INFO DAGScheduler: Got job 68 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:39:40 INFO DAGScheduler: Final stage: ResultStage 68 (parquet at Generate.java:61)
21/01/20 15:39:40 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:39:40 INFO DAGScheduler: Missing parents: List()
21/01/20 15:39:40 INFO DAGScheduler: Submitting ResultStage 68 (MapPartitionsRDD[273] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:39:40 INFO MemoryStore: Block broadcast_68 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:39:40 INFO MemoryStore: Block broadcast_68_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:39:40 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:39:40 INFO SparkContext: Created broadcast 68 from broadcast at DAGScheduler.scala:1200
21/01/20 15:39:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 68 (MapPartitionsRDD[273] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:39:40 INFO TaskSchedulerImpl: Adding task set 68.0 with 1 tasks
21/01/20 15:39:40 WARN TaskSetManager: Stage 68 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:39:40 INFO TaskSetManager: Starting task 0.0 in stage 68.0 (TID 68, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:39:40 INFO Executor: Running task 0.0 in stage 68.0 (TID 68)
21/01/20 15:39:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:41 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:41 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:39:41 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:39:41 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:39:41 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:39:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:39:41 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:39:41 INFO ParquetOutputFormat: Validation is off
21/01/20 15:39:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:39:41 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:39:41 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:39:41 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:39:41 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:39:41 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:39:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:39:41 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153940_0068_m_000000_68' to o3fs://bucket1.vol1/testdata
21/01/20 15:39:41 INFO SparkHadoopMapRedUtil: attempt_20210120153940_0068_m_000000_68: Committed
21/01/20 15:39:41 INFO Executor: Finished task 0.0 in stage 68.0 (TID 68). 2155 bytes result sent to driver
21/01/20 15:39:41 INFO TaskSetManager: Finished task 0.0 in stage 68.0 (TID 68) in 1039 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:39:41 INFO TaskSchedulerImpl: Removed TaskSet 68.0, whose tasks have all completed, from pool 
21/01/20 15:39:41 INFO DAGScheduler: ResultStage 68 (parquet at Generate.java:61) finished in 1.058 s
21/01/20 15:39:41 INFO DAGScheduler: Job 68 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:39:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 68: Stage finished
21/01/20 15:39:41 INFO DAGScheduler: Job 68 finished: parquet at Generate.java:61, took 1.059684 s
21/01/20 15:39:41 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-abbe22cf-c968-4fe6-8450-9b1948a1b7ea
21/01/20 15:39:41 INFO FileFormatWriter: Write Job 561ff38b-baed-49c7-ac6d-8b53583a0223 committed.
21/01/20 15:39:41 INFO FileFormatWriter: Finished processing stats for write job 561ff38b-baed-49c7-ac6d-8b53583a0223.
21/01/20 15:39:43 INFO BlockManagerInfo: Removed broadcast_68_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:39:44 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:44 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:44 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:39:44 INFO DAGScheduler: Got job 69 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:39:44 INFO DAGScheduler: Final stage: ResultStage 69 (parquet at Generate.java:61)
21/01/20 15:39:44 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:39:44 INFO DAGScheduler: Missing parents: List()
21/01/20 15:39:44 INFO DAGScheduler: Submitting ResultStage 69 (MapPartitionsRDD[277] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:39:44 INFO MemoryStore: Block broadcast_69 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:39:44 INFO MemoryStore: Block broadcast_69_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:39:44 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:39:44 INFO SparkContext: Created broadcast 69 from broadcast at DAGScheduler.scala:1200
21/01/20 15:39:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 69 (MapPartitionsRDD[277] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:39:44 INFO TaskSchedulerImpl: Adding task set 69.0 with 1 tasks
21/01/20 15:39:44 WARN TaskSetManager: Stage 69 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:39:44 INFO TaskSetManager: Starting task 0.0 in stage 69.0 (TID 69, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:39:44 INFO Executor: Running task 0.0 in stage 69.0 (TID 69)
21/01/20 15:39:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:44 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:44 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:39:44 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:39:44 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:39:44 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:39:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:39:44 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:39:44 INFO ParquetOutputFormat: Validation is off
21/01/20 15:39:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:39:44 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:39:44 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:39:44 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:39:44 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:39:44 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:39:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:39:45 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153944_0069_m_000000_69' to o3fs://bucket1.vol1/testdata
21/01/20 15:39:45 INFO SparkHadoopMapRedUtil: attempt_20210120153944_0069_m_000000_69: Committed
21/01/20 15:39:45 INFO Executor: Finished task 0.0 in stage 69.0 (TID 69). 2155 bytes result sent to driver
21/01/20 15:39:45 INFO TaskSetManager: Finished task 0.0 in stage 69.0 (TID 69) in 1026 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:39:45 INFO TaskSchedulerImpl: Removed TaskSet 69.0, whose tasks have all completed, from pool 
21/01/20 15:39:45 INFO DAGScheduler: ResultStage 69 (parquet at Generate.java:61) finished in 1.045 s
21/01/20 15:39:45 INFO DAGScheduler: Job 69 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:39:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 69: Stage finished
21/01/20 15:39:45 INFO DAGScheduler: Job 69 finished: parquet at Generate.java:61, took 1.046685 s
21/01/20 15:39:45 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-8e8bb2f8-2a6e-4c7c-a3f3-a14d4ab6f3d9
21/01/20 15:39:45 INFO FileFormatWriter: Write Job 3b3762a6-b92a-42a0-914e-b1728c2752b3 committed.
21/01/20 15:39:45 INFO FileFormatWriter: Finished processing stats for write job 3b3762a6-b92a-42a0-914e-b1728c2752b3.
21/01/20 15:39:46 INFO BlockManagerInfo: Removed broadcast_69_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:39:48 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:48 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:48 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:39:48 INFO DAGScheduler: Got job 70 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:39:48 INFO DAGScheduler: Final stage: ResultStage 70 (parquet at Generate.java:61)
21/01/20 15:39:48 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:39:48 INFO DAGScheduler: Missing parents: List()
21/01/20 15:39:48 INFO DAGScheduler: Submitting ResultStage 70 (MapPartitionsRDD[281] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:39:48 INFO MemoryStore: Block broadcast_70 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:39:48 INFO MemoryStore: Block broadcast_70_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:39:48 INFO BlockManagerInfo: Added broadcast_70_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:39:48 INFO SparkContext: Created broadcast 70 from broadcast at DAGScheduler.scala:1200
21/01/20 15:39:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 70 (MapPartitionsRDD[281] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:39:48 INFO TaskSchedulerImpl: Adding task set 70.0 with 1 tasks
21/01/20 15:39:48 WARN TaskSetManager: Stage 70 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:39:48 INFO TaskSetManager: Starting task 0.0 in stage 70.0 (TID 70, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:39:48 INFO Executor: Running task 0.0 in stage 70.0 (TID 70)
21/01/20 15:39:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:48 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:48 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:39:48 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:39:48 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:39:48 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:39:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:39:48 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:39:48 INFO ParquetOutputFormat: Validation is off
21/01/20 15:39:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:39:48 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:39:48 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:39:48 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:39:48 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:39:48 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:39:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:39:49 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153948_0070_m_000000_70' to o3fs://bucket1.vol1/testdata
21/01/20 15:39:49 INFO SparkHadoopMapRedUtil: attempt_20210120153948_0070_m_000000_70: Committed
21/01/20 15:39:49 INFO Executor: Finished task 0.0 in stage 70.0 (TID 70). 2155 bytes result sent to driver
21/01/20 15:39:49 INFO TaskSetManager: Finished task 0.0 in stage 70.0 (TID 70) in 1019 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:39:49 INFO TaskSchedulerImpl: Removed TaskSet 70.0, whose tasks have all completed, from pool 
21/01/20 15:39:49 INFO DAGScheduler: ResultStage 70 (parquet at Generate.java:61) finished in 1.039 s
21/01/20 15:39:49 INFO DAGScheduler: Job 70 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:39:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 70: Stage finished
21/01/20 15:39:49 INFO DAGScheduler: Job 70 finished: parquet at Generate.java:61, took 1.040189 s
21/01/20 15:39:49 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-2d6dca43-5e31-4378-a26d-aa9766340981
21/01/20 15:39:49 INFO FileFormatWriter: Write Job 687da91a-bf8c-4c74-88b8-cc6603db56ab committed.
21/01/20 15:39:49 INFO FileFormatWriter: Finished processing stats for write job 687da91a-bf8c-4c74-88b8-cc6603db56ab.
21/01/20 15:39:50 INFO BlockManagerInfo: Removed broadcast_70_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:39:51 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:51 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:39:51 INFO DAGScheduler: Got job 71 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:39:51 INFO DAGScheduler: Final stage: ResultStage 71 (parquet at Generate.java:61)
21/01/20 15:39:51 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:39:51 INFO DAGScheduler: Missing parents: List()
21/01/20 15:39:51 INFO DAGScheduler: Submitting ResultStage 71 (MapPartitionsRDD[285] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:39:51 INFO MemoryStore: Block broadcast_71 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:39:51 INFO MemoryStore: Block broadcast_71_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:39:51 INFO BlockManagerInfo: Added broadcast_71_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:39:51 INFO SparkContext: Created broadcast 71 from broadcast at DAGScheduler.scala:1200
21/01/20 15:39:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 71 (MapPartitionsRDD[285] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:39:51 INFO TaskSchedulerImpl: Adding task set 71.0 with 1 tasks
21/01/20 15:39:52 WARN TaskSetManager: Stage 71 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:39:52 INFO TaskSetManager: Starting task 0.0 in stage 71.0 (TID 71, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:39:52 INFO Executor: Running task 0.0 in stage 71.0 (TID 71)
21/01/20 15:39:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:52 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:52 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:39:52 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:39:52 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:39:52 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:39:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:39:52 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:39:52 INFO ParquetOutputFormat: Validation is off
21/01/20 15:39:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:39:52 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:39:52 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:39:52 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:39:52 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:39:52 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:39:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:39:52 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-1DF669D9AC71->d3c4d3e3-944b-4528-9d82-e005ad427697
21/01/20 15:39:52 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:39:53 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153951_0071_m_000000_71' to o3fs://bucket1.vol1/testdata
21/01/20 15:39:53 INFO SparkHadoopMapRedUtil: attempt_20210120153951_0071_m_000000_71: Committed
21/01/20 15:39:53 INFO Executor: Finished task 0.0 in stage 71.0 (TID 71). 2155 bytes result sent to driver
21/01/20 15:39:53 INFO TaskSetManager: Finished task 0.0 in stage 71.0 (TID 71) in 1113 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:39:53 INFO TaskSchedulerImpl: Removed TaskSet 71.0, whose tasks have all completed, from pool 
21/01/20 15:39:53 INFO DAGScheduler: ResultStage 71 (parquet at Generate.java:61) finished in 1.130 s
21/01/20 15:39:53 INFO DAGScheduler: Job 71 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:39:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 71: Stage finished
21/01/20 15:39:53 INFO DAGScheduler: Job 71 finished: parquet at Generate.java:61, took 1.134522 s
21/01/20 15:39:53 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-e3b6e321-c8f7-419c-a206-13109abedafa
21/01/20 15:39:53 INFO FileFormatWriter: Write Job fd045a4c-042e-423d-ad0f-b2c7fc1006f1 committed.
21/01/20 15:39:53 INFO FileFormatWriter: Finished processing stats for write job fd045a4c-042e-423d-ad0f-b2c7fc1006f1.
21/01/20 15:39:54 INFO BlockManagerInfo: Removed broadcast_71_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:39:55 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:55 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:39:55 INFO DAGScheduler: Got job 72 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:39:55 INFO DAGScheduler: Final stage: ResultStage 72 (parquet at Generate.java:61)
21/01/20 15:39:55 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:39:55 INFO DAGScheduler: Missing parents: List()
21/01/20 15:39:55 INFO DAGScheduler: Submitting ResultStage 72 (MapPartitionsRDD[289] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:39:55 INFO MemoryStore: Block broadcast_72 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:39:55 INFO MemoryStore: Block broadcast_72_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:39:55 INFO BlockManagerInfo: Added broadcast_72_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:39:55 INFO SparkContext: Created broadcast 72 from broadcast at DAGScheduler.scala:1200
21/01/20 15:39:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 72 (MapPartitionsRDD[289] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:39:55 INFO TaskSchedulerImpl: Adding task set 72.0 with 1 tasks
21/01/20 15:39:55 WARN TaskSetManager: Stage 72 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:39:55 INFO TaskSetManager: Starting task 0.0 in stage 72.0 (TID 72, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:39:55 INFO Executor: Running task 0.0 in stage 72.0 (TID 72)
21/01/20 15:39:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:55 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:39:55 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:39:55 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:39:55 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:39:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:39:55 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:39:55 INFO ParquetOutputFormat: Validation is off
21/01/20 15:39:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:39:55 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:39:55 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:39:55 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:39:55 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:39:55 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:39:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:39:56 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153955_0072_m_000000_72' to o3fs://bucket1.vol1/testdata
21/01/20 15:39:56 INFO SparkHadoopMapRedUtil: attempt_20210120153955_0072_m_000000_72: Committed
21/01/20 15:39:56 INFO Executor: Finished task 0.0 in stage 72.0 (TID 72). 2155 bytes result sent to driver
21/01/20 15:39:56 INFO TaskSetManager: Finished task 0.0 in stage 72.0 (TID 72) in 959 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:39:56 INFO TaskSchedulerImpl: Removed TaskSet 72.0, whose tasks have all completed, from pool 
21/01/20 15:39:56 INFO DAGScheduler: ResultStage 72 (parquet at Generate.java:61) finished in 0.977 s
21/01/20 15:39:56 INFO DAGScheduler: Job 72 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:39:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 72: Stage finished
21/01/20 15:39:56 INFO DAGScheduler: Job 72 finished: parquet at Generate.java:61, took 0.978621 s
21/01/20 15:39:56 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-634edaaa-61d1-414b-b9d1-3a813582df1a
21/01/20 15:39:56 INFO FileFormatWriter: Write Job 434e409e-5d79-4aae-9eef-1973ba395263 committed.
21/01/20 15:39:56 INFO FileFormatWriter: Finished processing stats for write job 434e409e-5d79-4aae-9eef-1973ba395263.
21/01/20 15:39:57 INFO BlockManagerInfo: Removed broadcast_72_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:39:59 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:59 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:59 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:39:59 INFO DAGScheduler: Got job 73 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:39:59 INFO DAGScheduler: Final stage: ResultStage 73 (parquet at Generate.java:61)
21/01/20 15:39:59 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:39:59 INFO DAGScheduler: Missing parents: List()
21/01/20 15:39:59 INFO DAGScheduler: Submitting ResultStage 73 (MapPartitionsRDD[293] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:39:59 INFO MemoryStore: Block broadcast_73 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:39:59 INFO MemoryStore: Block broadcast_73_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:39:59 INFO BlockManagerInfo: Added broadcast_73_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:39:59 INFO SparkContext: Created broadcast 73 from broadcast at DAGScheduler.scala:1200
21/01/20 15:39:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 73 (MapPartitionsRDD[293] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:39:59 INFO TaskSchedulerImpl: Adding task set 73.0 with 1 tasks
21/01/20 15:39:59 WARN TaskSetManager: Stage 73 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:39:59 INFO TaskSetManager: Starting task 0.0 in stage 73.0 (TID 73, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:39:59 INFO Executor: Running task 0.0 in stage 73.0 (TID 73)
21/01/20 15:39:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:59 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:39:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:39:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:39:59 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:39:59 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:39:59 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:39:59 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:39:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:39:59 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:39:59 INFO ParquetOutputFormat: Validation is off
21/01/20 15:39:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:39:59 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:39:59 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:39:59 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:39:59 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:39:59 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:39:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:40:00 INFO FileOutputCommitter: Saved output of task 'attempt_20210120153959_0073_m_000000_73' to o3fs://bucket1.vol1/testdata
21/01/20 15:40:00 INFO SparkHadoopMapRedUtil: attempt_20210120153959_0073_m_000000_73: Committed
21/01/20 15:40:00 INFO Executor: Finished task 0.0 in stage 73.0 (TID 73). 2155 bytes result sent to driver
21/01/20 15:40:00 INFO TaskSetManager: Finished task 0.0 in stage 73.0 (TID 73) in 748 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:40:00 INFO TaskSchedulerImpl: Removed TaskSet 73.0, whose tasks have all completed, from pool 
21/01/20 15:40:00 INFO DAGScheduler: ResultStage 73 (parquet at Generate.java:61) finished in 0.791 s
21/01/20 15:40:00 INFO DAGScheduler: Job 73 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:40:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 73: Stage finished
21/01/20 15:40:00 INFO DAGScheduler: Job 73 finished: parquet at Generate.java:61, took 0.792827 s
21/01/20 15:40:00 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-ab983836-3ded-4c47-9b0e-8866fe14e834
21/01/20 15:40:00 INFO FileFormatWriter: Write Job db519338-af0e-49cc-a48e-c4f8f0c4e0cb committed.
21/01/20 15:40:00 INFO FileFormatWriter: Finished processing stats for write job db519338-af0e-49cc-a48e-c4f8f0c4e0cb.
21/01/20 15:40:00 INFO BlockManagerInfo: Removed broadcast_73_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:40:02 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:02 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:40:02 INFO DAGScheduler: Got job 74 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:40:02 INFO DAGScheduler: Final stage: ResultStage 74 (parquet at Generate.java:61)
21/01/20 15:40:02 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:40:02 INFO DAGScheduler: Missing parents: List()
21/01/20 15:40:02 INFO DAGScheduler: Submitting ResultStage 74 (MapPartitionsRDD[297] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:40:02 INFO MemoryStore: Block broadcast_74 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:40:02 INFO MemoryStore: Block broadcast_74_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:40:02 INFO BlockManagerInfo: Added broadcast_74_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:40:02 INFO SparkContext: Created broadcast 74 from broadcast at DAGScheduler.scala:1200
21/01/20 15:40:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 74 (MapPartitionsRDD[297] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:40:02 INFO TaskSchedulerImpl: Adding task set 74.0 with 1 tasks
21/01/20 15:40:02 WARN TaskSetManager: Stage 74 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:40:02 INFO TaskSetManager: Starting task 0.0 in stage 74.0 (TID 74, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:40:02 INFO Executor: Running task 0.0 in stage 74.0 (TID 74)
21/01/20 15:40:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:02 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:40:02 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:40:02 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:40:02 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:40:02 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:40:02 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:40:02 INFO ParquetOutputFormat: Validation is off
21/01/20 15:40:02 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:40:02 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:40:02 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:40:02 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:40:02 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:40:02 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:40:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:40:03 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-3436A5FF727E->d3c4d3e3-944b-4528-9d82-e005ad427697
21/01/20 15:40:03 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:40:03 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154002_0074_m_000000_74' to o3fs://bucket1.vol1/testdata
21/01/20 15:40:03 INFO SparkHadoopMapRedUtil: attempt_20210120154002_0074_m_000000_74: Committed
21/01/20 15:40:03 INFO Executor: Finished task 0.0 in stage 74.0 (TID 74). 2155 bytes result sent to driver
21/01/20 15:40:03 INFO TaskSetManager: Finished task 0.0 in stage 74.0 (TID 74) in 1089 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:40:03 INFO TaskSchedulerImpl: Removed TaskSet 74.0, whose tasks have all completed, from pool 
21/01/20 15:40:03 INFO DAGScheduler: ResultStage 74 (parquet at Generate.java:61) finished in 1.108 s
21/01/20 15:40:03 INFO DAGScheduler: Job 74 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:40:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 74: Stage finished
21/01/20 15:40:03 INFO DAGScheduler: Job 74 finished: parquet at Generate.java:61, took 1.109633 s
21/01/20 15:40:03 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-60c8f65b-75c9-4745-9d13-0914c5ccb3a7
21/01/20 15:40:03 INFO FileFormatWriter: Write Job af82f882-eebd-4605-8df1-964276b82ec0 committed.
21/01/20 15:40:03 INFO FileFormatWriter: Finished processing stats for write job af82f882-eebd-4605-8df1-964276b82ec0.
21/01/20 15:40:05 INFO BlockManagerInfo: Removed broadcast_74_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:40:06 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:06 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:06 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:40:06 INFO DAGScheduler: Got job 75 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:40:06 INFO DAGScheduler: Final stage: ResultStage 75 (parquet at Generate.java:61)
21/01/20 15:40:06 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:40:06 INFO DAGScheduler: Missing parents: List()
21/01/20 15:40:06 INFO DAGScheduler: Submitting ResultStage 75 (MapPartitionsRDD[301] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:40:06 INFO MemoryStore: Block broadcast_75 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:40:06 INFO MemoryStore: Block broadcast_75_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:40:06 INFO BlockManagerInfo: Added broadcast_75_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:40:06 INFO SparkContext: Created broadcast 75 from broadcast at DAGScheduler.scala:1200
21/01/20 15:40:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 75 (MapPartitionsRDD[301] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:40:06 INFO TaskSchedulerImpl: Adding task set 75.0 with 1 tasks
21/01/20 15:40:06 WARN TaskSetManager: Stage 75 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:40:06 INFO TaskSetManager: Starting task 0.0 in stage 75.0 (TID 75, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:40:06 INFO Executor: Running task 0.0 in stage 75.0 (TID 75)
21/01/20 15:40:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:06 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:06 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:40:06 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:40:06 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:40:06 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:40:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:40:06 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:40:06 INFO ParquetOutputFormat: Validation is off
21/01/20 15:40:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:40:06 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:40:06 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:40:06 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:40:06 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:40:06 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:40:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:40:07 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154006_0075_m_000000_75' to o3fs://bucket1.vol1/testdata
21/01/20 15:40:07 INFO SparkHadoopMapRedUtil: attempt_20210120154006_0075_m_000000_75: Committed
21/01/20 15:40:07 INFO Executor: Finished task 0.0 in stage 75.0 (TID 75). 2155 bytes result sent to driver
21/01/20 15:40:07 INFO TaskSetManager: Finished task 0.0 in stage 75.0 (TID 75) in 799 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:40:07 INFO TaskSchedulerImpl: Removed TaskSet 75.0, whose tasks have all completed, from pool 
21/01/20 15:40:07 INFO DAGScheduler: ResultStage 75 (parquet at Generate.java:61) finished in 0.819 s
21/01/20 15:40:07 INFO DAGScheduler: Job 75 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:40:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 75: Stage finished
21/01/20 15:40:07 INFO DAGScheduler: Job 75 finished: parquet at Generate.java:61, took 0.843811 s
21/01/20 15:40:07 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-915160d7-e0e3-4afe-8963-338bfd3452c1
21/01/20 15:40:07 INFO FileFormatWriter: Write Job 17129ced-cfe7-4108-8fb1-504e6d7ef477 committed.
21/01/20 15:40:07 INFO FileFormatWriter: Finished processing stats for write job 17129ced-cfe7-4108-8fb1-504e6d7ef477.
21/01/20 15:40:07 INFO BlockManagerInfo: Removed broadcast_75_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:40:09 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:09 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:09 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:40:09 INFO DAGScheduler: Got job 76 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:40:09 INFO DAGScheduler: Final stage: ResultStage 76 (parquet at Generate.java:61)
21/01/20 15:40:09 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:40:09 INFO DAGScheduler: Missing parents: List()
21/01/20 15:40:09 INFO DAGScheduler: Submitting ResultStage 76 (MapPartitionsRDD[305] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:40:09 INFO MemoryStore: Block broadcast_76 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:40:09 INFO MemoryStore: Block broadcast_76_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:40:09 INFO BlockManagerInfo: Added broadcast_76_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:40:09 INFO SparkContext: Created broadcast 76 from broadcast at DAGScheduler.scala:1200
21/01/20 15:40:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 76 (MapPartitionsRDD[305] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:40:09 INFO TaskSchedulerImpl: Adding task set 76.0 with 1 tasks
21/01/20 15:40:09 WARN TaskSetManager: Stage 76 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:40:09 INFO TaskSetManager: Starting task 0.0 in stage 76.0 (TID 76, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:40:09 INFO Executor: Running task 0.0 in stage 76.0 (TID 76)
21/01/20 15:40:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:09 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:09 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:40:09 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:40:09 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:40:09 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:40:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:40:09 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:40:09 INFO ParquetOutputFormat: Validation is off
21/01/20 15:40:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:40:09 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:40:09 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:40:09 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:40:09 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:40:09 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:40:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:40:10 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154009_0076_m_000000_76' to o3fs://bucket1.vol1/testdata
21/01/20 15:40:10 INFO SparkHadoopMapRedUtil: attempt_20210120154009_0076_m_000000_76: Committed
21/01/20 15:40:10 INFO Executor: Finished task 0.0 in stage 76.0 (TID 76). 2155 bytes result sent to driver
21/01/20 15:40:10 INFO TaskSetManager: Finished task 0.0 in stage 76.0 (TID 76) in 1029 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:40:10 INFO TaskSchedulerImpl: Removed TaskSet 76.0, whose tasks have all completed, from pool 
21/01/20 15:40:10 INFO DAGScheduler: ResultStage 76 (parquet at Generate.java:61) finished in 1.048 s
21/01/20 15:40:10 INFO DAGScheduler: Job 76 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:40:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 76: Stage finished
21/01/20 15:40:10 INFO DAGScheduler: Job 76 finished: parquet at Generate.java:61, took 1.050047 s
21/01/20 15:40:10 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-cba0f093-a60f-416e-aca3-febdc05684eb
21/01/20 15:40:10 INFO FileFormatWriter: Write Job 8c148092-2af7-4aa1-9b15-ddd95dceb582 committed.
21/01/20 15:40:10 INFO FileFormatWriter: Finished processing stats for write job 8c148092-2af7-4aa1-9b15-ddd95dceb582.
21/01/20 15:40:12 INFO BlockManagerInfo: Removed broadcast_76_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:40:13 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:13 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:13 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:40:13 INFO DAGScheduler: Got job 77 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:40:13 INFO DAGScheduler: Final stage: ResultStage 77 (parquet at Generate.java:61)
21/01/20 15:40:13 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:40:13 INFO DAGScheduler: Missing parents: List()
21/01/20 15:40:13 INFO DAGScheduler: Submitting ResultStage 77 (MapPartitionsRDD[309] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:40:13 INFO MemoryStore: Block broadcast_77 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:40:13 INFO MemoryStore: Block broadcast_77_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:40:13 INFO BlockManagerInfo: Added broadcast_77_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:40:13 INFO SparkContext: Created broadcast 77 from broadcast at DAGScheduler.scala:1200
21/01/20 15:40:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 77 (MapPartitionsRDD[309] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:40:13 INFO TaskSchedulerImpl: Adding task set 77.0 with 1 tasks
21/01/20 15:40:13 WARN TaskSetManager: Stage 77 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:40:13 INFO TaskSetManager: Starting task 0.0 in stage 77.0 (TID 77, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:40:13 INFO Executor: Running task 0.0 in stage 77.0 (TID 77)
21/01/20 15:40:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:13 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:13 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:40:13 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:40:13 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:40:13 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:40:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:40:13 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:40:13 INFO ParquetOutputFormat: Validation is off
21/01/20 15:40:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:40:13 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:40:13 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:40:13 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:40:13 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:40:13 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:40:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:40:14 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154013_0077_m_000000_77' to o3fs://bucket1.vol1/testdata
21/01/20 15:40:14 INFO SparkHadoopMapRedUtil: attempt_20210120154013_0077_m_000000_77: Committed
21/01/20 15:40:14 INFO Executor: Finished task 0.0 in stage 77.0 (TID 77). 2155 bytes result sent to driver
21/01/20 15:40:14 INFO TaskSetManager: Finished task 0.0 in stage 77.0 (TID 77) in 875 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:40:14 INFO TaskSchedulerImpl: Removed TaskSet 77.0, whose tasks have all completed, from pool 
21/01/20 15:40:14 INFO DAGScheduler: ResultStage 77 (parquet at Generate.java:61) finished in 0.915 s
21/01/20 15:40:14 INFO DAGScheduler: Job 77 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:40:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 77: Stage finished
21/01/20 15:40:14 INFO DAGScheduler: Job 77 finished: parquet at Generate.java:61, took 0.917480 s
21/01/20 15:40:14 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-f01da3d7-a6ce-4024-a819-4c03fb6d59b6
21/01/20 15:40:14 INFO FileFormatWriter: Write Job cb59b8f4-4b56-42e7-b01d-c2370cb5b1e1 committed.
21/01/20 15:40:14 INFO FileFormatWriter: Finished processing stats for write job cb59b8f4-4b56-42e7-b01d-c2370cb5b1e1.
21/01/20 15:40:14 INFO BlockManagerInfo: Removed broadcast_77_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:40:17 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:17 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:17 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:40:17 INFO DAGScheduler: Got job 78 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:40:17 INFO DAGScheduler: Final stage: ResultStage 78 (parquet at Generate.java:61)
21/01/20 15:40:17 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:40:17 INFO DAGScheduler: Missing parents: List()
21/01/20 15:40:17 INFO DAGScheduler: Submitting ResultStage 78 (MapPartitionsRDD[313] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:40:17 INFO MemoryStore: Block broadcast_78 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:40:17 INFO MemoryStore: Block broadcast_78_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:40:17 INFO BlockManagerInfo: Added broadcast_78_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:40:17 INFO SparkContext: Created broadcast 78 from broadcast at DAGScheduler.scala:1200
21/01/20 15:40:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 78 (MapPartitionsRDD[313] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:40:17 INFO TaskSchedulerImpl: Adding task set 78.0 with 1 tasks
21/01/20 15:40:17 WARN TaskSetManager: Stage 78 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:40:17 INFO TaskSetManager: Starting task 0.0 in stage 78.0 (TID 78, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:40:17 INFO Executor: Running task 0.0 in stage 78.0 (TID 78)
21/01/20 15:40:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:17 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:17 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:40:17 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:40:17 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:40:17 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:40:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:40:17 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:40:17 INFO ParquetOutputFormat: Validation is off
21/01/20 15:40:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:40:17 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:40:17 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:40:17 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:40:17 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:40:17 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:40:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:40:17 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-F35A9777E9A1->d3c4d3e3-944b-4528-9d82-e005ad427697
21/01/20 15:40:17 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:40:18 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154017_0078_m_000000_78' to o3fs://bucket1.vol1/testdata
21/01/20 15:40:18 INFO SparkHadoopMapRedUtil: attempt_20210120154017_0078_m_000000_78: Committed
21/01/20 15:40:18 INFO Executor: Finished task 0.0 in stage 78.0 (TID 78). 2155 bytes result sent to driver
21/01/20 15:40:18 INFO TaskSetManager: Finished task 0.0 in stage 78.0 (TID 78) in 1060 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:40:18 INFO TaskSchedulerImpl: Removed TaskSet 78.0, whose tasks have all completed, from pool 
21/01/20 15:40:18 INFO DAGScheduler: ResultStage 78 (parquet at Generate.java:61) finished in 1.078 s
21/01/20 15:40:18 INFO DAGScheduler: Job 78 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:40:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 78: Stage finished
21/01/20 15:40:18 INFO DAGScheduler: Job 78 finished: parquet at Generate.java:61, took 1.079612 s
21/01/20 15:40:18 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-4eb5b55f-909d-48a3-b4ae-45f680adbb76
21/01/20 15:40:18 INFO FileFormatWriter: Write Job 1104aa89-8709-4362-ba91-0b2166544a66 committed.
21/01/20 15:40:18 INFO FileFormatWriter: Finished processing stats for write job 1104aa89-8709-4362-ba91-0b2166544a66.
21/01/20 15:40:19 INFO BlockManagerInfo: Removed broadcast_78_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:40:20 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:20 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:20 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:40:20 INFO DAGScheduler: Got job 79 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:40:20 INFO DAGScheduler: Final stage: ResultStage 79 (parquet at Generate.java:61)
21/01/20 15:40:20 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:40:20 INFO DAGScheduler: Missing parents: List()
21/01/20 15:40:20 INFO DAGScheduler: Submitting ResultStage 79 (MapPartitionsRDD[317] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:40:20 INFO MemoryStore: Block broadcast_79 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:40:20 INFO MemoryStore: Block broadcast_79_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:40:20 INFO BlockManagerInfo: Added broadcast_79_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:40:20 INFO SparkContext: Created broadcast 79 from broadcast at DAGScheduler.scala:1200
21/01/20 15:40:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 79 (MapPartitionsRDD[317] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:40:20 INFO TaskSchedulerImpl: Adding task set 79.0 with 1 tasks
21/01/20 15:40:20 WARN TaskSetManager: Stage 79 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:40:20 INFO TaskSetManager: Starting task 0.0 in stage 79.0 (TID 79, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:40:20 INFO Executor: Running task 0.0 in stage 79.0 (TID 79)
21/01/20 15:40:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:20 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:20 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:40:20 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:40:20 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:40:20 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:40:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:40:20 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:40:20 INFO ParquetOutputFormat: Validation is off
21/01/20 15:40:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:40:20 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:40:20 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:40:20 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:40:20 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:40:20 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:40:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:40:21 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154020_0079_m_000000_79' to o3fs://bucket1.vol1/testdata
21/01/20 15:40:21 INFO SparkHadoopMapRedUtil: attempt_20210120154020_0079_m_000000_79: Committed
21/01/20 15:40:21 INFO Executor: Finished task 0.0 in stage 79.0 (TID 79). 2155 bytes result sent to driver
21/01/20 15:40:21 INFO TaskSetManager: Finished task 0.0 in stage 79.0 (TID 79) in 930 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:40:21 INFO TaskSchedulerImpl: Removed TaskSet 79.0, whose tasks have all completed, from pool 
21/01/20 15:40:21 INFO DAGScheduler: ResultStage 79 (parquet at Generate.java:61) finished in 0.949 s
21/01/20 15:40:21 INFO DAGScheduler: Job 79 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:40:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 79: Stage finished
21/01/20 15:40:21 INFO DAGScheduler: Job 79 finished: parquet at Generate.java:61, took 0.949989 s
21/01/20 15:40:21 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-bca26cb2-4270-4d46-8d2d-97744a3372d0
21/01/20 15:40:21 INFO FileFormatWriter: Write Job 4c11eb1b-f34d-476b-be80-8e31b85f615d committed.
21/01/20 15:40:21 INFO FileFormatWriter: Finished processing stats for write job 4c11eb1b-f34d-476b-be80-8e31b85f615d.
21/01/20 15:40:21 INFO BlockManagerInfo: Removed broadcast_79_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:40:24 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:24 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:24 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:40:24 INFO DAGScheduler: Got job 80 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:40:24 INFO DAGScheduler: Final stage: ResultStage 80 (parquet at Generate.java:61)
21/01/20 15:40:24 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:40:24 INFO DAGScheduler: Missing parents: List()
21/01/20 15:40:24 INFO DAGScheduler: Submitting ResultStage 80 (MapPartitionsRDD[321] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:40:24 INFO MemoryStore: Block broadcast_80 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:40:24 INFO MemoryStore: Block broadcast_80_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:40:24 INFO BlockManagerInfo: Added broadcast_80_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:40:24 INFO SparkContext: Created broadcast 80 from broadcast at DAGScheduler.scala:1200
21/01/20 15:40:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 80 (MapPartitionsRDD[321] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:40:24 INFO TaskSchedulerImpl: Adding task set 80.0 with 1 tasks
21/01/20 15:40:24 WARN TaskSetManager: Stage 80 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:40:24 INFO TaskSetManager: Starting task 0.0 in stage 80.0 (TID 80, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:40:24 INFO Executor: Running task 0.0 in stage 80.0 (TID 80)
21/01/20 15:40:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:24 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:24 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:40:24 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:40:24 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:40:24 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:40:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:40:24 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:40:24 INFO ParquetOutputFormat: Validation is off
21/01/20 15:40:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:40:24 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:40:24 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:40:24 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:40:24 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:40:24 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:40:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:40:25 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154024_0080_m_000000_80' to o3fs://bucket1.vol1/testdata
21/01/20 15:40:25 INFO SparkHadoopMapRedUtil: attempt_20210120154024_0080_m_000000_80: Committed
21/01/20 15:40:25 INFO Executor: Finished task 0.0 in stage 80.0 (TID 80). 2155 bytes result sent to driver
21/01/20 15:40:25 INFO TaskSetManager: Finished task 0.0 in stage 80.0 (TID 80) in 1077 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:40:25 INFO TaskSchedulerImpl: Removed TaskSet 80.0, whose tasks have all completed, from pool 
21/01/20 15:40:25 INFO DAGScheduler: ResultStage 80 (parquet at Generate.java:61) finished in 1.095 s
21/01/20 15:40:25 INFO DAGScheduler: Job 80 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:40:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 80: Stage finished
21/01/20 15:40:25 INFO DAGScheduler: Job 80 finished: parquet at Generate.java:61, took 1.096544 s
21/01/20 15:40:25 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-d9591a67-3def-41b6-831c-47a9842691c1
21/01/20 15:40:25 INFO FileFormatWriter: Write Job 3e831c1a-342b-4f07-abc8-d7dfcd3935ce committed.
21/01/20 15:40:25 INFO FileFormatWriter: Finished processing stats for write job 3e831c1a-342b-4f07-abc8-d7dfcd3935ce.
21/01/20 15:40:26 INFO BlockManagerInfo: Removed broadcast_80_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:40:28 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:28 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:28 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:40:28 INFO DAGScheduler: Got job 81 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:40:28 INFO DAGScheduler: Final stage: ResultStage 81 (parquet at Generate.java:61)
21/01/20 15:40:28 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:40:28 INFO DAGScheduler: Missing parents: List()
21/01/20 15:40:28 INFO DAGScheduler: Submitting ResultStage 81 (MapPartitionsRDD[325] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:40:28 INFO MemoryStore: Block broadcast_81 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:40:28 INFO MemoryStore: Block broadcast_81_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:40:28 INFO BlockManagerInfo: Added broadcast_81_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:40:28 INFO SparkContext: Created broadcast 81 from broadcast at DAGScheduler.scala:1200
21/01/20 15:40:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 81 (MapPartitionsRDD[325] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:40:28 INFO TaskSchedulerImpl: Adding task set 81.0 with 1 tasks
21/01/20 15:40:28 WARN TaskSetManager: Stage 81 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:40:28 INFO TaskSetManager: Starting task 0.0 in stage 81.0 (TID 81, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:40:28 INFO Executor: Running task 0.0 in stage 81.0 (TID 81)
21/01/20 15:40:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:28 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:28 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:40:28 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:40:28 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:40:28 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:40:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:40:28 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:40:28 INFO ParquetOutputFormat: Validation is off
21/01/20 15:40:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:40:28 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:40:28 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:40:28 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:40:28 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:40:28 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:40:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:40:28 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-31C5C082E932->af5c9c6a-a887-4538-ae7b-ba6d49328297
21/01/20 15:40:28 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:40:29 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154028_0081_m_000000_81' to o3fs://bucket1.vol1/testdata
21/01/20 15:40:29 INFO SparkHadoopMapRedUtil: attempt_20210120154028_0081_m_000000_81: Committed
21/01/20 15:40:29 INFO Executor: Finished task 0.0 in stage 81.0 (TID 81). 2155 bytes result sent to driver
21/01/20 15:40:29 INFO TaskSetManager: Finished task 0.0 in stage 81.0 (TID 81) in 879 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:40:29 INFO TaskSchedulerImpl: Removed TaskSet 81.0, whose tasks have all completed, from pool 
21/01/20 15:40:29 INFO DAGScheduler: ResultStage 81 (parquet at Generate.java:61) finished in 0.922 s
21/01/20 15:40:29 INFO DAGScheduler: Job 81 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:40:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 81: Stage finished
21/01/20 15:40:29 INFO DAGScheduler: Job 81 finished: parquet at Generate.java:61, took 0.924289 s
21/01/20 15:40:29 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-c6f72fa7-4528-460e-b7a8-ea55419283a0
21/01/20 15:40:29 INFO FileFormatWriter: Write Job bf47bd78-9268-4d60-a23c-e519000f275f committed.
21/01/20 15:40:29 INFO FileFormatWriter: Finished processing stats for write job bf47bd78-9268-4d60-a23c-e519000f275f.
21/01/20 15:40:29 INFO BlockManagerInfo: Removed broadcast_81_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:40:31 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:31 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:40:31 INFO DAGScheduler: Got job 82 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:40:31 INFO DAGScheduler: Final stage: ResultStage 82 (parquet at Generate.java:61)
21/01/20 15:40:31 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:40:31 INFO DAGScheduler: Missing parents: List()
21/01/20 15:40:31 INFO DAGScheduler: Submitting ResultStage 82 (MapPartitionsRDD[329] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:40:31 INFO MemoryStore: Block broadcast_82 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:40:31 INFO MemoryStore: Block broadcast_82_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:40:31 INFO BlockManagerInfo: Added broadcast_82_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:40:31 INFO SparkContext: Created broadcast 82 from broadcast at DAGScheduler.scala:1200
21/01/20 15:40:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 82 (MapPartitionsRDD[329] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:40:31 INFO TaskSchedulerImpl: Adding task set 82.0 with 1 tasks
21/01/20 15:40:31 WARN TaskSetManager: Stage 82 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:40:31 INFO TaskSetManager: Starting task 0.0 in stage 82.0 (TID 82, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:40:31 INFO Executor: Running task 0.0 in stage 82.0 (TID 82)
21/01/20 15:40:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:31 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:40:31 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:40:31 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:40:31 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:40:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:40:31 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:40:31 INFO ParquetOutputFormat: Validation is off
21/01/20 15:40:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:40:31 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:40:31 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:40:31 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:40:31 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:40:31 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:40:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:40:32 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154031_0082_m_000000_82' to o3fs://bucket1.vol1/testdata
21/01/20 15:40:32 INFO SparkHadoopMapRedUtil: attempt_20210120154031_0082_m_000000_82: Committed
21/01/20 15:40:32 INFO Executor: Finished task 0.0 in stage 82.0 (TID 82). 2155 bytes result sent to driver
21/01/20 15:40:32 INFO TaskSetManager: Finished task 0.0 in stage 82.0 (TID 82) in 1089 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:40:32 INFO TaskSchedulerImpl: Removed TaskSet 82.0, whose tasks have all completed, from pool 
21/01/20 15:40:32 INFO DAGScheduler: ResultStage 82 (parquet at Generate.java:61) finished in 1.108 s
21/01/20 15:40:32 INFO DAGScheduler: Job 82 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:40:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 82: Stage finished
21/01/20 15:40:32 INFO DAGScheduler: Job 82 finished: parquet at Generate.java:61, took 1.110037 s
21/01/20 15:40:32 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-b2cefa6a-04ba-400b-b9a1-5a35f9be4235
21/01/20 15:40:32 INFO FileFormatWriter: Write Job d87fc7de-c5a2-4122-b81f-a023343802f9 committed.
21/01/20 15:40:32 INFO FileFormatWriter: Finished processing stats for write job d87fc7de-c5a2-4122-b81f-a023343802f9.
21/01/20 15:40:33 INFO BlockManagerInfo: Removed broadcast_82_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:40:35 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:35 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:35 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:40:35 INFO DAGScheduler: Got job 83 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:40:35 INFO DAGScheduler: Final stage: ResultStage 83 (parquet at Generate.java:61)
21/01/20 15:40:35 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:40:35 INFO DAGScheduler: Missing parents: List()
21/01/20 15:40:35 INFO DAGScheduler: Submitting ResultStage 83 (MapPartitionsRDD[333] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:40:35 INFO MemoryStore: Block broadcast_83 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:40:35 INFO MemoryStore: Block broadcast_83_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:40:35 INFO BlockManagerInfo: Added broadcast_83_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:40:35 INFO SparkContext: Created broadcast 83 from broadcast at DAGScheduler.scala:1200
21/01/20 15:40:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 83 (MapPartitionsRDD[333] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:40:35 INFO TaskSchedulerImpl: Adding task set 83.0 with 1 tasks
21/01/20 15:40:35 WARN TaskSetManager: Stage 83 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:40:35 INFO TaskSetManager: Starting task 0.0 in stage 83.0 (TID 83, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:40:35 INFO Executor: Running task 0.0 in stage 83.0 (TID 83)
21/01/20 15:40:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:35 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:35 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:40:35 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:40:35 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:40:35 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:40:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:40:35 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:40:35 INFO ParquetOutputFormat: Validation is off
21/01/20 15:40:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:40:35 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:40:35 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:40:35 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:40:35 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:40:35 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:40:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:40:36 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154035_0083_m_000000_83' to o3fs://bucket1.vol1/testdata
21/01/20 15:40:36 INFO SparkHadoopMapRedUtil: attempt_20210120154035_0083_m_000000_83: Committed
21/01/20 15:40:36 INFO Executor: Finished task 0.0 in stage 83.0 (TID 83). 2155 bytes result sent to driver
21/01/20 15:40:36 INFO TaskSetManager: Finished task 0.0 in stage 83.0 (TID 83) in 1019 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:40:36 INFO TaskSchedulerImpl: Removed TaskSet 83.0, whose tasks have all completed, from pool 
21/01/20 15:40:36 INFO DAGScheduler: ResultStage 83 (parquet at Generate.java:61) finished in 1.038 s
21/01/20 15:40:36 INFO DAGScheduler: Job 83 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:40:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 83: Stage finished
21/01/20 15:40:36 INFO DAGScheduler: Job 83 finished: parquet at Generate.java:61, took 1.038979 s
21/01/20 15:40:36 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-e2e02445-f546-4c36-886f-52620076bfeb
21/01/20 15:40:36 INFO FileFormatWriter: Write Job 58ca798c-1d4d-476d-b25d-3a56e447f53f committed.
21/01/20 15:40:36 INFO FileFormatWriter: Finished processing stats for write job 58ca798c-1d4d-476d-b25d-3a56e447f53f.
21/01/20 15:40:37 INFO BlockManagerInfo: Removed broadcast_83_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:40:38 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:38 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:39 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:40:39 INFO DAGScheduler: Got job 84 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:40:39 INFO DAGScheduler: Final stage: ResultStage 84 (parquet at Generate.java:61)
21/01/20 15:40:39 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:40:39 INFO DAGScheduler: Missing parents: List()
21/01/20 15:40:39 INFO DAGScheduler: Submitting ResultStage 84 (MapPartitionsRDD[337] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:40:39 INFO MemoryStore: Block broadcast_84 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:40:39 INFO MemoryStore: Block broadcast_84_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:40:39 INFO BlockManagerInfo: Added broadcast_84_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:40:39 INFO SparkContext: Created broadcast 84 from broadcast at DAGScheduler.scala:1200
21/01/20 15:40:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 84 (MapPartitionsRDD[337] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:40:39 INFO TaskSchedulerImpl: Adding task set 84.0 with 1 tasks
21/01/20 15:40:39 WARN TaskSetManager: Stage 84 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:40:39 INFO TaskSetManager: Starting task 0.0 in stage 84.0 (TID 84, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:40:39 INFO Executor: Running task 0.0 in stage 84.0 (TID 84)
21/01/20 15:40:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:39 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:39 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:40:39 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:40:39 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:40:39 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:40:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:40:39 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:40:39 INFO ParquetOutputFormat: Validation is off
21/01/20 15:40:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:40:39 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:40:39 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:40:39 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:40:39 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:40:39 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:40:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:40:40 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154039_0084_m_000000_84' to o3fs://bucket1.vol1/testdata
21/01/20 15:40:40 INFO SparkHadoopMapRedUtil: attempt_20210120154039_0084_m_000000_84: Committed
21/01/20 15:40:40 INFO Executor: Finished task 0.0 in stage 84.0 (TID 84). 2155 bytes result sent to driver
21/01/20 15:40:40 INFO TaskSetManager: Finished task 0.0 in stage 84.0 (TID 84) in 939 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:40:40 INFO TaskSchedulerImpl: Removed TaskSet 84.0, whose tasks have all completed, from pool 
21/01/20 15:40:40 INFO DAGScheduler: ResultStage 84 (parquet at Generate.java:61) finished in 0.956 s
21/01/20 15:40:40 INFO DAGScheduler: Job 84 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:40:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 84: Stage finished
21/01/20 15:40:40 INFO DAGScheduler: Job 84 finished: parquet at Generate.java:61, took 0.958189 s
21/01/20 15:40:40 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-0c531c12-a257-46f0-8e9e-e85fd52bb8fa
21/01/20 15:40:40 INFO FileFormatWriter: Write Job e6b42f41-a038-43d4-b180-7c86f87e0fff committed.
21/01/20 15:40:40 INFO FileFormatWriter: Finished processing stats for write job e6b42f41-a038-43d4-b180-7c86f87e0fff.
21/01/20 15:40:41 INFO BlockManagerInfo: Removed broadcast_84_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:40:42 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:42 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:42 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:40:42 INFO DAGScheduler: Got job 85 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:40:42 INFO DAGScheduler: Final stage: ResultStage 85 (parquet at Generate.java:61)
21/01/20 15:40:42 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:40:42 INFO DAGScheduler: Missing parents: List()
21/01/20 15:40:42 INFO DAGScheduler: Submitting ResultStage 85 (MapPartitionsRDD[341] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:40:42 INFO MemoryStore: Block broadcast_85 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:40:42 INFO MemoryStore: Block broadcast_85_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:40:42 INFO BlockManagerInfo: Added broadcast_85_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:40:42 INFO SparkContext: Created broadcast 85 from broadcast at DAGScheduler.scala:1200
21/01/20 15:40:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 85 (MapPartitionsRDD[341] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:40:42 INFO TaskSchedulerImpl: Adding task set 85.0 with 1 tasks
21/01/20 15:40:42 WARN TaskSetManager: Stage 85 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:40:42 INFO TaskSetManager: Starting task 0.0 in stage 85.0 (TID 85, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:40:42 INFO Executor: Running task 0.0 in stage 85.0 (TID 85)
21/01/20 15:40:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:42 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:42 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:40:42 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:40:42 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:40:42 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:40:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:40:42 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:40:42 INFO ParquetOutputFormat: Validation is off
21/01/20 15:40:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:40:42 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:40:42 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:40:42 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:40:42 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:40:42 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:40:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:40:43 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154042_0085_m_000000_85' to o3fs://bucket1.vol1/testdata
21/01/20 15:40:43 INFO SparkHadoopMapRedUtil: attempt_20210120154042_0085_m_000000_85: Committed
21/01/20 15:40:43 INFO Executor: Finished task 0.0 in stage 85.0 (TID 85). 2155 bytes result sent to driver
21/01/20 15:40:43 INFO TaskSetManager: Finished task 0.0 in stage 85.0 (TID 85) in 997 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:40:43 INFO TaskSchedulerImpl: Removed TaskSet 85.0, whose tasks have all completed, from pool 
21/01/20 15:40:43 INFO DAGScheduler: ResultStage 85 (parquet at Generate.java:61) finished in 1.038 s
21/01/20 15:40:43 INFO DAGScheduler: Job 85 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:40:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 85: Stage finished
21/01/20 15:40:43 INFO DAGScheduler: Job 85 finished: parquet at Generate.java:61, took 1.040518 s
21/01/20 15:40:43 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-1ae9c9a9-294b-4526-a64f-bbcab0826a66
21/01/20 15:40:43 INFO FileFormatWriter: Write Job c3b14767-bf62-4c36-bac2-b46ecc6d4aaa committed.
21/01/20 15:40:43 INFO FileFormatWriter: Finished processing stats for write job c3b14767-bf62-4c36-bac2-b46ecc6d4aaa.
21/01/20 15:40:43 INFO BlockManagerInfo: Removed broadcast_85_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:40:46 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:46 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:46 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:40:46 INFO DAGScheduler: Got job 86 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:40:46 INFO DAGScheduler: Final stage: ResultStage 86 (parquet at Generate.java:61)
21/01/20 15:40:46 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:40:46 INFO DAGScheduler: Missing parents: List()
21/01/20 15:40:46 INFO DAGScheduler: Submitting ResultStage 86 (MapPartitionsRDD[345] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:40:46 INFO MemoryStore: Block broadcast_86 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:40:46 INFO MemoryStore: Block broadcast_86_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:40:46 INFO BlockManagerInfo: Added broadcast_86_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:40:46 INFO SparkContext: Created broadcast 86 from broadcast at DAGScheduler.scala:1200
21/01/20 15:40:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 86 (MapPartitionsRDD[345] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:40:46 INFO TaskSchedulerImpl: Adding task set 86.0 with 1 tasks
21/01/20 15:40:46 WARN TaskSetManager: Stage 86 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:40:46 INFO TaskSetManager: Starting task 0.0 in stage 86.0 (TID 86, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:40:46 INFO Executor: Running task 0.0 in stage 86.0 (TID 86)
21/01/20 15:40:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:46 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:46 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:40:46 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:40:46 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:40:46 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:40:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:40:46 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:40:46 INFO ParquetOutputFormat: Validation is off
21/01/20 15:40:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:40:46 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:40:46 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:40:46 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:40:46 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:40:46 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:40:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:40:46 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-ACF4FC41FA3D->d3c4d3e3-944b-4528-9d82-e005ad427697
21/01/20 15:40:46 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:40:47 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154046_0086_m_000000_86' to o3fs://bucket1.vol1/testdata
21/01/20 15:40:47 INFO SparkHadoopMapRedUtil: attempt_20210120154046_0086_m_000000_86: Committed
21/01/20 15:40:47 INFO Executor: Finished task 0.0 in stage 86.0 (TID 86). 2155 bytes result sent to driver
21/01/20 15:40:47 INFO TaskSetManager: Finished task 0.0 in stage 86.0 (TID 86) in 1149 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:40:47 INFO TaskSchedulerImpl: Removed TaskSet 86.0, whose tasks have all completed, from pool 
21/01/20 15:40:47 INFO DAGScheduler: ResultStage 86 (parquet at Generate.java:61) finished in 1.166 s
21/01/20 15:40:47 INFO DAGScheduler: Job 86 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:40:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 86: Stage finished
21/01/20 15:40:47 INFO DAGScheduler: Job 86 finished: parquet at Generate.java:61, took 1.168033 s
21/01/20 15:40:47 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-41af6832-6997-4811-98b0-917e4fc02b01
21/01/20 15:40:47 INFO FileFormatWriter: Write Job 7f274216-d861-4dda-88e6-345443ffafe1 committed.
21/01/20 15:40:47 INFO FileFormatWriter: Finished processing stats for write job 7f274216-d861-4dda-88e6-345443ffafe1.
21/01/20 15:40:48 INFO BlockManagerInfo: Removed broadcast_86_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:40:50 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:50 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:50 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:40:50 INFO DAGScheduler: Got job 87 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:40:50 INFO DAGScheduler: Final stage: ResultStage 87 (parquet at Generate.java:61)
21/01/20 15:40:50 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:40:50 INFO DAGScheduler: Missing parents: List()
21/01/20 15:40:50 INFO DAGScheduler: Submitting ResultStage 87 (MapPartitionsRDD[349] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:40:50 INFO MemoryStore: Block broadcast_87 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:40:50 INFO MemoryStore: Block broadcast_87_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:40:50 INFO BlockManagerInfo: Added broadcast_87_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:40:50 INFO SparkContext: Created broadcast 87 from broadcast at DAGScheduler.scala:1200
21/01/20 15:40:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 87 (MapPartitionsRDD[349] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:40:50 INFO TaskSchedulerImpl: Adding task set 87.0 with 1 tasks
21/01/20 15:40:50 WARN TaskSetManager: Stage 87 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:40:50 INFO TaskSetManager: Starting task 0.0 in stage 87.0 (TID 87, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:40:50 INFO Executor: Running task 0.0 in stage 87.0 (TID 87)
21/01/20 15:40:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:50 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:50 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:40:50 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:40:50 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:40:50 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:40:50 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:40:50 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:40:50 INFO ParquetOutputFormat: Validation is off
21/01/20 15:40:50 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:40:50 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:40:50 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:40:50 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:40:50 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:40:50 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:40:50 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:40:51 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154050_0087_m_000000_87' to o3fs://bucket1.vol1/testdata
21/01/20 15:40:51 INFO SparkHadoopMapRedUtil: attempt_20210120154050_0087_m_000000_87: Committed
21/01/20 15:40:51 INFO Executor: Finished task 0.0 in stage 87.0 (TID 87). 2155 bytes result sent to driver
21/01/20 15:40:51 INFO TaskSetManager: Finished task 0.0 in stage 87.0 (TID 87) in 924 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:40:51 INFO TaskSchedulerImpl: Removed TaskSet 87.0, whose tasks have all completed, from pool 
21/01/20 15:40:51 INFO DAGScheduler: ResultStage 87 (parquet at Generate.java:61) finished in 0.965 s
21/01/20 15:40:51 INFO DAGScheduler: Job 87 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:40:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 87: Stage finished
21/01/20 15:40:51 INFO DAGScheduler: Job 87 finished: parquet at Generate.java:61, took 0.966812 s
21/01/20 15:40:51 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-d3adb86c-3bb6-475e-abda-732efcf608f9
21/01/20 15:40:51 INFO FileFormatWriter: Write Job 0093915b-6245-41f0-b170-96daf851f52e committed.
21/01/20 15:40:51 INFO FileFormatWriter: Finished processing stats for write job 0093915b-6245-41f0-b170-96daf851f52e.
21/01/20 15:40:51 INFO BlockManagerInfo: Removed broadcast_87_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:40:53 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:53 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:40:53 INFO DAGScheduler: Got job 88 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:40:53 INFO DAGScheduler: Final stage: ResultStage 88 (parquet at Generate.java:61)
21/01/20 15:40:53 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:40:53 INFO DAGScheduler: Missing parents: List()
21/01/20 15:40:53 INFO DAGScheduler: Submitting ResultStage 88 (MapPartitionsRDD[353] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:40:53 INFO MemoryStore: Block broadcast_88 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:40:53 INFO MemoryStore: Block broadcast_88_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:40:53 INFO BlockManagerInfo: Added broadcast_88_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:40:53 INFO SparkContext: Created broadcast 88 from broadcast at DAGScheduler.scala:1200
21/01/20 15:40:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 88 (MapPartitionsRDD[353] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:40:53 INFO TaskSchedulerImpl: Adding task set 88.0 with 1 tasks
21/01/20 15:40:53 WARN TaskSetManager: Stage 88 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:40:53 INFO TaskSetManager: Starting task 0.0 in stage 88.0 (TID 88, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:40:53 INFO Executor: Running task 0.0 in stage 88.0 (TID 88)
21/01/20 15:40:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:53 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:40:53 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:40:53 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:40:53 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:40:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:40:53 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:40:53 INFO ParquetOutputFormat: Validation is off
21/01/20 15:40:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:40:53 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:40:53 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:40:53 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:40:53 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:40:53 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:40:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:40:54 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154053_0088_m_000000_88' to o3fs://bucket1.vol1/testdata
21/01/20 15:40:54 INFO SparkHadoopMapRedUtil: attempt_20210120154053_0088_m_000000_88: Committed
21/01/20 15:40:54 INFO Executor: Finished task 0.0 in stage 88.0 (TID 88). 2155 bytes result sent to driver
21/01/20 15:40:54 INFO TaskSetManager: Finished task 0.0 in stage 88.0 (TID 88) in 1053 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:40:54 INFO TaskSchedulerImpl: Removed TaskSet 88.0, whose tasks have all completed, from pool 
21/01/20 15:40:54 INFO DAGScheduler: ResultStage 88 (parquet at Generate.java:61) finished in 1.070 s
21/01/20 15:40:54 INFO DAGScheduler: Job 88 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:40:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 88: Stage finished
21/01/20 15:40:54 INFO DAGScheduler: Job 88 finished: parquet at Generate.java:61, took 1.071892 s
21/01/20 15:40:54 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-41338a30-73eb-4c18-9456-487561dc680a
21/01/20 15:40:54 INFO FileFormatWriter: Write Job d7eced89-0ea3-45e7-aaef-f7cc5732b66a committed.
21/01/20 15:40:54 INFO FileFormatWriter: Finished processing stats for write job d7eced89-0ea3-45e7-aaef-f7cc5732b66a.
21/01/20 15:40:56 INFO BlockManagerInfo: Removed broadcast_88_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:40:57 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:57 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:57 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:40:57 INFO DAGScheduler: Got job 89 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:40:57 INFO DAGScheduler: Final stage: ResultStage 89 (parquet at Generate.java:61)
21/01/20 15:40:57 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:40:57 INFO DAGScheduler: Missing parents: List()
21/01/20 15:40:57 INFO DAGScheduler: Submitting ResultStage 89 (MapPartitionsRDD[357] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:40:57 INFO MemoryStore: Block broadcast_89 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:40:57 INFO MemoryStore: Block broadcast_89_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:40:57 INFO BlockManagerInfo: Added broadcast_89_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:40:57 INFO SparkContext: Created broadcast 89 from broadcast at DAGScheduler.scala:1200
21/01/20 15:40:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 89 (MapPartitionsRDD[357] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:40:57 INFO TaskSchedulerImpl: Adding task set 89.0 with 1 tasks
21/01/20 15:40:57 WARN TaskSetManager: Stage 89 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:40:57 INFO TaskSetManager: Starting task 0.0 in stage 89.0 (TID 89, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:40:57 INFO Executor: Running task 0.0 in stage 89.0 (TID 89)
21/01/20 15:40:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:57 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:40:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:40:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:40:57 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:40:57 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:40:57 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:40:57 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:40:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:40:57 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:40:57 INFO ParquetOutputFormat: Validation is off
21/01/20 15:40:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:40:57 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:40:57 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:40:57 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:40:57 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:40:57 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:40:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:40:57 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-4288B9986DE7->af5c9c6a-a887-4538-ae7b-ba6d49328297
21/01/20 15:40:57 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:40:58 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154057_0089_m_000000_89' to o3fs://bucket1.vol1/testdata
21/01/20 15:40:58 INFO SparkHadoopMapRedUtil: attempt_20210120154057_0089_m_000000_89: Committed
21/01/20 15:40:58 INFO Executor: Finished task 0.0 in stage 89.0 (TID 89). 2155 bytes result sent to driver
21/01/20 15:40:58 INFO TaskSetManager: Finished task 0.0 in stage 89.0 (TID 89) in 872 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:40:58 INFO TaskSchedulerImpl: Removed TaskSet 89.0, whose tasks have all completed, from pool 
21/01/20 15:40:58 INFO DAGScheduler: ResultStage 89 (parquet at Generate.java:61) finished in 0.915 s
21/01/20 15:40:58 INFO DAGScheduler: Job 89 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:40:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 89: Stage finished
21/01/20 15:40:58 INFO DAGScheduler: Job 89 finished: parquet at Generate.java:61, took 0.916493 s
21/01/20 15:40:58 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-e484e3b3-58ea-44d8-8b3a-0214064c71b0
21/01/20 15:40:58 INFO FileFormatWriter: Write Job a2d8c471-9c52-4e1d-b9b2-ae820404e6df committed.
21/01/20 15:40:58 INFO FileFormatWriter: Finished processing stats for write job a2d8c471-9c52-4e1d-b9b2-ae820404e6df.
21/01/20 15:40:58 INFO BlockManagerInfo: Removed broadcast_89_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:41:00 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:00 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:41:00 INFO DAGScheduler: Got job 90 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:41:00 INFO DAGScheduler: Final stage: ResultStage 90 (parquet at Generate.java:61)
21/01/20 15:41:00 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:41:00 INFO DAGScheduler: Missing parents: List()
21/01/20 15:41:00 INFO DAGScheduler: Submitting ResultStage 90 (MapPartitionsRDD[361] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:41:00 INFO MemoryStore: Block broadcast_90 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:41:00 INFO MemoryStore: Block broadcast_90_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:41:00 INFO BlockManagerInfo: Added broadcast_90_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:41:00 INFO SparkContext: Created broadcast 90 from broadcast at DAGScheduler.scala:1200
21/01/20 15:41:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 90 (MapPartitionsRDD[361] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:41:00 INFO TaskSchedulerImpl: Adding task set 90.0 with 1 tasks
21/01/20 15:41:00 WARN TaskSetManager: Stage 90 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:41:00 INFO TaskSetManager: Starting task 0.0 in stage 90.0 (TID 90, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:41:00 INFO Executor: Running task 0.0 in stage 90.0 (TID 90)
21/01/20 15:41:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:01 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:01 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:41:01 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:41:01 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:41:01 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:41:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:41:01 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:41:01 INFO ParquetOutputFormat: Validation is off
21/01/20 15:41:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:41:01 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:41:01 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:41:01 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:41:01 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:41:01 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:41:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:41:01 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154100_0090_m_000000_90' to o3fs://bucket1.vol1/testdata
21/01/20 15:41:01 INFO SparkHadoopMapRedUtil: attempt_20210120154100_0090_m_000000_90: Committed
21/01/20 15:41:01 INFO Executor: Finished task 0.0 in stage 90.0 (TID 90). 2155 bytes result sent to driver
21/01/20 15:41:01 INFO TaskSetManager: Finished task 0.0 in stage 90.0 (TID 90) in 1034 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:41:01 INFO TaskSchedulerImpl: Removed TaskSet 90.0, whose tasks have all completed, from pool 
21/01/20 15:41:01 INFO DAGScheduler: ResultStage 90 (parquet at Generate.java:61) finished in 1.052 s
21/01/20 15:41:01 INFO DAGScheduler: Job 90 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:41:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 90: Stage finished
21/01/20 15:41:01 INFO DAGScheduler: Job 90 finished: parquet at Generate.java:61, took 1.053317 s
21/01/20 15:41:01 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-c25d4b56-6519-44d6-8c1b-ae7105cdc2e6
21/01/20 15:41:01 INFO FileFormatWriter: Write Job 77c58b1b-c40b-4c7e-9cae-511683aaa9b8 committed.
21/01/20 15:41:01 INFO FileFormatWriter: Finished processing stats for write job 77c58b1b-c40b-4c7e-9cae-511683aaa9b8.
21/01/20 15:41:03 INFO BlockManagerInfo: Removed broadcast_90_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:41:04 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:04 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:41:04 INFO DAGScheduler: Got job 91 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:41:04 INFO DAGScheduler: Final stage: ResultStage 91 (parquet at Generate.java:61)
21/01/20 15:41:04 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:41:04 INFO DAGScheduler: Missing parents: List()
21/01/20 15:41:04 INFO DAGScheduler: Submitting ResultStage 91 (MapPartitionsRDD[365] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:41:04 INFO MemoryStore: Block broadcast_91 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:41:04 INFO MemoryStore: Block broadcast_91_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:41:04 INFO BlockManagerInfo: Added broadcast_91_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:41:04 INFO SparkContext: Created broadcast 91 from broadcast at DAGScheduler.scala:1200
21/01/20 15:41:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 91 (MapPartitionsRDD[365] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:41:04 INFO TaskSchedulerImpl: Adding task set 91.0 with 1 tasks
21/01/20 15:41:04 WARN TaskSetManager: Stage 91 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:41:04 INFO TaskSetManager: Starting task 0.0 in stage 91.0 (TID 91, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:41:04 INFO Executor: Running task 0.0 in stage 91.0 (TID 91)
21/01/20 15:41:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:04 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:41:04 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:41:04 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:41:04 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:41:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:41:04 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:41:04 INFO ParquetOutputFormat: Validation is off
21/01/20 15:41:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:41:04 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:41:04 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:41:04 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:41:04 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:41:04 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:41:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:41:05 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154104_0091_m_000000_91' to o3fs://bucket1.vol1/testdata
21/01/20 15:41:05 INFO SparkHadoopMapRedUtil: attempt_20210120154104_0091_m_000000_91: Committed
21/01/20 15:41:05 INFO Executor: Finished task 0.0 in stage 91.0 (TID 91). 2155 bytes result sent to driver
21/01/20 15:41:05 INFO TaskSetManager: Finished task 0.0 in stage 91.0 (TID 91) in 930 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:41:05 INFO TaskSchedulerImpl: Removed TaskSet 91.0, whose tasks have all completed, from pool 
21/01/20 15:41:05 INFO DAGScheduler: ResultStage 91 (parquet at Generate.java:61) finished in 0.948 s
21/01/20 15:41:05 INFO DAGScheduler: Job 91 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:41:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 91: Stage finished
21/01/20 15:41:05 INFO DAGScheduler: Job 91 finished: parquet at Generate.java:61, took 0.973019 s
21/01/20 15:41:05 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-663867e1-0143-4f87-aa8f-b19e59715808
21/01/20 15:41:05 INFO FileFormatWriter: Write Job ffb4e219-0f8c-4028-9d32-62649d20b4d6 committed.
21/01/20 15:41:05 INFO FileFormatWriter: Finished processing stats for write job ffb4e219-0f8c-4028-9d32-62649d20b4d6.
21/01/20 15:41:05 INFO BlockManagerInfo: Removed broadcast_91_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:41:08 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:08 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:08 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:41:08 INFO DAGScheduler: Got job 92 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:41:08 INFO DAGScheduler: Final stage: ResultStage 92 (parquet at Generate.java:61)
21/01/20 15:41:08 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:41:08 INFO DAGScheduler: Missing parents: List()
21/01/20 15:41:08 INFO DAGScheduler: Submitting ResultStage 92 (MapPartitionsRDD[369] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:41:08 INFO MemoryStore: Block broadcast_92 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:41:08 INFO MemoryStore: Block broadcast_92_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:41:08 INFO BlockManagerInfo: Added broadcast_92_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:41:08 INFO SparkContext: Created broadcast 92 from broadcast at DAGScheduler.scala:1200
21/01/20 15:41:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 92 (MapPartitionsRDD[369] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:41:08 INFO TaskSchedulerImpl: Adding task set 92.0 with 1 tasks
21/01/20 15:41:08 WARN TaskSetManager: Stage 92 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:41:08 INFO TaskSetManager: Starting task 0.0 in stage 92.0 (TID 92, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:41:08 INFO Executor: Running task 0.0 in stage 92.0 (TID 92)
21/01/20 15:41:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:08 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:08 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:41:08 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:41:08 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:41:08 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:41:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:41:08 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:41:08 INFO ParquetOutputFormat: Validation is off
21/01/20 15:41:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:41:08 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:41:08 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:41:08 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:41:08 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:41:08 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:41:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:41:09 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154108_0092_m_000000_92' to o3fs://bucket1.vol1/testdata
21/01/20 15:41:09 INFO SparkHadoopMapRedUtil: attempt_20210120154108_0092_m_000000_92: Committed
21/01/20 15:41:09 INFO Executor: Finished task 0.0 in stage 92.0 (TID 92). 2155 bytes result sent to driver
21/01/20 15:41:09 INFO TaskSetManager: Finished task 0.0 in stage 92.0 (TID 92) in 1087 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:41:09 INFO TaskSchedulerImpl: Removed TaskSet 92.0, whose tasks have all completed, from pool 
21/01/20 15:41:09 INFO DAGScheduler: ResultStage 92 (parquet at Generate.java:61) finished in 1.104 s
21/01/20 15:41:09 INFO DAGScheduler: Job 92 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:41:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 92: Stage finished
21/01/20 15:41:09 INFO DAGScheduler: Job 92 finished: parquet at Generate.java:61, took 1.105963 s
21/01/20 15:41:09 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-2ebf7d2f-ad7d-4f4a-bb2d-41ccf826e9e4
21/01/20 15:41:09 INFO FileFormatWriter: Write Job a5f5483d-3e52-4b6a-b8ee-286e7b8fcfc1 committed.
21/01/20 15:41:09 INFO FileFormatWriter: Finished processing stats for write job a5f5483d-3e52-4b6a-b8ee-286e7b8fcfc1.
21/01/20 15:41:10 INFO BlockManagerInfo: Removed broadcast_92_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:41:11 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:11 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:41:11 INFO DAGScheduler: Got job 93 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:41:11 INFO DAGScheduler: Final stage: ResultStage 93 (parquet at Generate.java:61)
21/01/20 15:41:11 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:41:11 INFO DAGScheduler: Missing parents: List()
21/01/20 15:41:11 INFO DAGScheduler: Submitting ResultStage 93 (MapPartitionsRDD[373] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:41:11 INFO MemoryStore: Block broadcast_93 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:41:11 INFO MemoryStore: Block broadcast_93_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:41:11 INFO BlockManagerInfo: Added broadcast_93_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:41:11 INFO SparkContext: Created broadcast 93 from broadcast at DAGScheduler.scala:1200
21/01/20 15:41:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 93 (MapPartitionsRDD[373] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:41:11 INFO TaskSchedulerImpl: Adding task set 93.0 with 1 tasks
21/01/20 15:41:11 WARN TaskSetManager: Stage 93 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:41:11 INFO TaskSetManager: Starting task 0.0 in stage 93.0 (TID 93, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:41:11 INFO Executor: Running task 0.0 in stage 93.0 (TID 93)
21/01/20 15:41:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:11 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:41:11 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:41:11 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:41:11 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:41:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:41:11 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:41:11 INFO ParquetOutputFormat: Validation is off
21/01/20 15:41:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:41:11 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:41:11 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:41:11 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:41:11 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:41:11 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:41:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:41:12 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154111_0093_m_000000_93' to o3fs://bucket1.vol1/testdata
21/01/20 15:41:12 INFO SparkHadoopMapRedUtil: attempt_20210120154111_0093_m_000000_93: Committed
21/01/20 15:41:12 INFO Executor: Finished task 0.0 in stage 93.0 (TID 93). 2155 bytes result sent to driver
21/01/20 15:41:12 INFO TaskSetManager: Finished task 0.0 in stage 93.0 (TID 93) in 1076 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:41:12 INFO TaskSchedulerImpl: Removed TaskSet 93.0, whose tasks have all completed, from pool 
21/01/20 15:41:12 INFO DAGScheduler: ResultStage 93 (parquet at Generate.java:61) finished in 1.094 s
21/01/20 15:41:12 INFO DAGScheduler: Job 93 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:41:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 93: Stage finished
21/01/20 15:41:12 INFO DAGScheduler: Job 93 finished: parquet at Generate.java:61, took 1.095303 s
21/01/20 15:41:12 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-827d5dcc-69fb-432a-9f41-42df12ec4852
21/01/20 15:41:12 INFO FileFormatWriter: Write Job cb498c22-77f5-4fa5-bd5a-118f59f5e213 committed.
21/01/20 15:41:12 INFO FileFormatWriter: Finished processing stats for write job cb498c22-77f5-4fa5-bd5a-118f59f5e213.
21/01/20 15:41:14 INFO BlockManagerInfo: Removed broadcast_93_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:41:15 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:15 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:15 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:41:15 INFO DAGScheduler: Got job 94 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:41:15 INFO DAGScheduler: Final stage: ResultStage 94 (parquet at Generate.java:61)
21/01/20 15:41:15 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:41:15 INFO DAGScheduler: Missing parents: List()
21/01/20 15:41:15 INFO DAGScheduler: Submitting ResultStage 94 (MapPartitionsRDD[377] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:41:15 INFO MemoryStore: Block broadcast_94 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:41:15 INFO MemoryStore: Block broadcast_94_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:41:15 INFO BlockManagerInfo: Added broadcast_94_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:41:15 INFO SparkContext: Created broadcast 94 from broadcast at DAGScheduler.scala:1200
21/01/20 15:41:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 94 (MapPartitionsRDD[377] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:41:15 INFO TaskSchedulerImpl: Adding task set 94.0 with 1 tasks
21/01/20 15:41:15 WARN TaskSetManager: Stage 94 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:41:15 INFO TaskSetManager: Starting task 0.0 in stage 94.0 (TID 94, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:41:15 INFO Executor: Running task 0.0 in stage 94.0 (TID 94)
21/01/20 15:41:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:15 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:15 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:41:15 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:41:15 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:41:15 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:41:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:41:15 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:41:15 INFO ParquetOutputFormat: Validation is off
21/01/20 15:41:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:41:15 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:41:15 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:41:15 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:41:15 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:41:15 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:41:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:41:16 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154115_0094_m_000000_94' to o3fs://bucket1.vol1/testdata
21/01/20 15:41:16 INFO SparkHadoopMapRedUtil: attempt_20210120154115_0094_m_000000_94: Committed
21/01/20 15:41:16 INFO Executor: Finished task 0.0 in stage 94.0 (TID 94). 2155 bytes result sent to driver
21/01/20 15:41:16 INFO TaskSetManager: Finished task 0.0 in stage 94.0 (TID 94) in 1096 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:41:16 INFO TaskSchedulerImpl: Removed TaskSet 94.0, whose tasks have all completed, from pool 
21/01/20 15:41:16 INFO DAGScheduler: ResultStage 94 (parquet at Generate.java:61) finished in 1.114 s
21/01/20 15:41:16 INFO DAGScheduler: Job 94 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:41:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 94: Stage finished
21/01/20 15:41:16 INFO DAGScheduler: Job 94 finished: parquet at Generate.java:61, took 1.116603 s
21/01/20 15:41:16 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-b3c98a3c-fe72-4f3b-8329-dc029af569d1
21/01/20 15:41:16 INFO FileFormatWriter: Write Job dd1b4619-261f-45f0-bef3-56a012ae9329 committed.
21/01/20 15:41:16 INFO FileFormatWriter: Finished processing stats for write job dd1b4619-261f-45f0-bef3-56a012ae9329.
21/01/20 15:41:17 INFO BlockManagerInfo: Removed broadcast_94_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:41:19 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:19 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:19 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:41:19 INFO DAGScheduler: Got job 95 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:41:19 INFO DAGScheduler: Final stage: ResultStage 95 (parquet at Generate.java:61)
21/01/20 15:41:19 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:41:19 INFO DAGScheduler: Missing parents: List()
21/01/20 15:41:19 INFO DAGScheduler: Submitting ResultStage 95 (MapPartitionsRDD[381] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:41:19 INFO MemoryStore: Block broadcast_95 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:41:19 INFO MemoryStore: Block broadcast_95_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:41:19 INFO BlockManagerInfo: Added broadcast_95_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:41:19 INFO SparkContext: Created broadcast 95 from broadcast at DAGScheduler.scala:1200
21/01/20 15:41:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 95 (MapPartitionsRDD[381] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:41:19 INFO TaskSchedulerImpl: Adding task set 95.0 with 1 tasks
21/01/20 15:41:19 WARN TaskSetManager: Stage 95 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:41:19 INFO TaskSetManager: Starting task 0.0 in stage 95.0 (TID 95, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:41:19 INFO Executor: Running task 0.0 in stage 95.0 (TID 95)
21/01/20 15:41:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:19 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:19 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:41:19 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:41:19 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:41:19 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:41:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:41:19 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:41:19 INFO ParquetOutputFormat: Validation is off
21/01/20 15:41:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:41:19 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:41:19 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:41:19 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:41:19 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:41:19 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:41:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:41:19 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-24BD61CE3BFA->af5c9c6a-a887-4538-ae7b-ba6d49328297
21/01/20 15:41:19 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:41:20 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154119_0095_m_000000_95' to o3fs://bucket1.vol1/testdata
21/01/20 15:41:20 INFO SparkHadoopMapRedUtil: attempt_20210120154119_0095_m_000000_95: Committed
21/01/20 15:41:20 INFO Executor: Finished task 0.0 in stage 95.0 (TID 95). 2155 bytes result sent to driver
21/01/20 15:41:20 INFO TaskSetManager: Finished task 0.0 in stage 95.0 (TID 95) in 969 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:41:20 INFO TaskSchedulerImpl: Removed TaskSet 95.0, whose tasks have all completed, from pool 
21/01/20 15:41:20 INFO DAGScheduler: ResultStage 95 (parquet at Generate.java:61) finished in 0.987 s
21/01/20 15:41:20 INFO DAGScheduler: Job 95 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:41:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 95: Stage finished
21/01/20 15:41:20 INFO DAGScheduler: Job 95 finished: parquet at Generate.java:61, took 0.989400 s
21/01/20 15:41:20 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-9403df49-4e7f-4a92-99e4-beae6b14535c
21/01/20 15:41:20 INFO FileFormatWriter: Write Job 444e9adc-3f35-4b17-92f2-9e81d9aa2d51 committed.
21/01/20 15:41:20 INFO FileFormatWriter: Finished processing stats for write job 444e9adc-3f35-4b17-92f2-9e81d9aa2d51.
21/01/20 15:41:21 INFO BlockManagerInfo: Removed broadcast_95_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:41:22 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:22 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:22 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:41:22 INFO DAGScheduler: Got job 96 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:41:22 INFO DAGScheduler: Final stage: ResultStage 96 (parquet at Generate.java:61)
21/01/20 15:41:22 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:41:22 INFO DAGScheduler: Missing parents: List()
21/01/20 15:41:22 INFO DAGScheduler: Submitting ResultStage 96 (MapPartitionsRDD[385] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:41:22 INFO MemoryStore: Block broadcast_96 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:41:22 INFO MemoryStore: Block broadcast_96_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:41:22 INFO BlockManagerInfo: Added broadcast_96_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:41:22 INFO SparkContext: Created broadcast 96 from broadcast at DAGScheduler.scala:1200
21/01/20 15:41:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 96 (MapPartitionsRDD[385] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:41:22 INFO TaskSchedulerImpl: Adding task set 96.0 with 1 tasks
21/01/20 15:41:23 WARN TaskSetManager: Stage 96 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:41:23 INFO TaskSetManager: Starting task 0.0 in stage 96.0 (TID 96, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:41:23 INFO Executor: Running task 0.0 in stage 96.0 (TID 96)
21/01/20 15:41:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:23 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:23 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:41:23 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:41:23 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:41:23 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:41:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:41:23 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:41:23 INFO ParquetOutputFormat: Validation is off
21/01/20 15:41:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:41:23 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:41:23 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:41:23 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:41:23 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:41:23 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:41:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:41:23 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154122_0096_m_000000_96' to o3fs://bucket1.vol1/testdata
21/01/20 15:41:23 INFO SparkHadoopMapRedUtil: attempt_20210120154122_0096_m_000000_96: Committed
21/01/20 15:41:23 INFO Executor: Finished task 0.0 in stage 96.0 (TID 96). 2155 bytes result sent to driver
21/01/20 15:41:23 INFO TaskSetManager: Finished task 0.0 in stage 96.0 (TID 96) in 996 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:41:23 INFO TaskSchedulerImpl: Removed TaskSet 96.0, whose tasks have all completed, from pool 
21/01/20 15:41:23 INFO DAGScheduler: ResultStage 96 (parquet at Generate.java:61) finished in 1.040 s
21/01/20 15:41:23 INFO DAGScheduler: Job 96 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:41:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 96: Stage finished
21/01/20 15:41:23 INFO DAGScheduler: Job 96 finished: parquet at Generate.java:61, took 1.042563 s
21/01/20 15:41:24 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-f79e7b03-803f-408e-bf5a-65bb19412017
21/01/20 15:41:24 INFO FileFormatWriter: Write Job bf116088-33eb-4e9b-94f6-d20a819bdcaf committed.
21/01/20 15:41:24 INFO FileFormatWriter: Finished processing stats for write job bf116088-33eb-4e9b-94f6-d20a819bdcaf.
21/01/20 15:41:24 INFO BlockManagerInfo: Removed broadcast_96_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:41:26 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:26 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:26 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:41:26 INFO DAGScheduler: Got job 97 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:41:26 INFO DAGScheduler: Final stage: ResultStage 97 (parquet at Generate.java:61)
21/01/20 15:41:26 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:41:26 INFO DAGScheduler: Missing parents: List()
21/01/20 15:41:26 INFO DAGScheduler: Submitting ResultStage 97 (MapPartitionsRDD[389] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:41:26 INFO MemoryStore: Block broadcast_97 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:41:26 INFO MemoryStore: Block broadcast_97_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:41:26 INFO BlockManagerInfo: Added broadcast_97_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:41:26 INFO SparkContext: Created broadcast 97 from broadcast at DAGScheduler.scala:1200
21/01/20 15:41:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 97 (MapPartitionsRDD[389] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:41:26 INFO TaskSchedulerImpl: Adding task set 97.0 with 1 tasks
21/01/20 15:41:26 WARN TaskSetManager: Stage 97 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:41:26 INFO TaskSetManager: Starting task 0.0 in stage 97.0 (TID 97, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:41:26 INFO Executor: Running task 0.0 in stage 97.0 (TID 97)
21/01/20 15:41:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:26 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:26 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:41:26 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:41:26 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:41:26 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:41:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:41:26 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:41:26 INFO ParquetOutputFormat: Validation is off
21/01/20 15:41:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:41:26 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:41:26 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:41:26 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:41:26 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:41:26 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:41:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:41:27 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154126_0097_m_000000_97' to o3fs://bucket1.vol1/testdata
21/01/20 15:41:27 INFO SparkHadoopMapRedUtil: attempt_20210120154126_0097_m_000000_97: Committed
21/01/20 15:41:27 INFO Executor: Finished task 0.0 in stage 97.0 (TID 97). 2155 bytes result sent to driver
21/01/20 15:41:27 INFO TaskSetManager: Finished task 0.0 in stage 97.0 (TID 97) in 913 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:41:27 INFO TaskSchedulerImpl: Removed TaskSet 97.0, whose tasks have all completed, from pool 
21/01/20 15:41:27 INFO DAGScheduler: ResultStage 97 (parquet at Generate.java:61) finished in 0.930 s
21/01/20 15:41:27 INFO DAGScheduler: Job 97 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:41:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 97: Stage finished
21/01/20 15:41:27 INFO DAGScheduler: Job 97 finished: parquet at Generate.java:61, took 0.934496 s
21/01/20 15:41:27 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-1925a172-eaba-4bf7-aafd-eb14c8ef9ea6
21/01/20 15:41:27 INFO FileFormatWriter: Write Job f4ae78f7-9709-4680-a83f-bea4f1fa70ea committed.
21/01/20 15:41:27 INFO FileFormatWriter: Finished processing stats for write job f4ae78f7-9709-4680-a83f-bea4f1fa70ea.
21/01/20 15:41:28 INFO BlockManagerInfo: Removed broadcast_97_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:41:30 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:30 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:41:30 INFO DAGScheduler: Got job 98 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:41:30 INFO DAGScheduler: Final stage: ResultStage 98 (parquet at Generate.java:61)
21/01/20 15:41:30 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:41:30 INFO DAGScheduler: Missing parents: List()
21/01/20 15:41:30 INFO DAGScheduler: Submitting ResultStage 98 (MapPartitionsRDD[393] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:41:30 INFO MemoryStore: Block broadcast_98 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:41:30 INFO MemoryStore: Block broadcast_98_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:41:30 INFO BlockManagerInfo: Added broadcast_98_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:41:30 INFO SparkContext: Created broadcast 98 from broadcast at DAGScheduler.scala:1200
21/01/20 15:41:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 98 (MapPartitionsRDD[393] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:41:30 INFO TaskSchedulerImpl: Adding task set 98.0 with 1 tasks
21/01/20 15:41:30 WARN TaskSetManager: Stage 98 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:41:30 INFO TaskSetManager: Starting task 0.0 in stage 98.0 (TID 98, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:41:30 INFO Executor: Running task 0.0 in stage 98.0 (TID 98)
21/01/20 15:41:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:30 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:41:30 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:41:30 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:41:30 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:41:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:41:30 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:41:30 INFO ParquetOutputFormat: Validation is off
21/01/20 15:41:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:41:30 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:41:30 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:41:30 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:41:30 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:41:30 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:41:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:41:30 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-D236A00BCAF5->d3c4d3e3-944b-4528-9d82-e005ad427697
21/01/20 15:41:30 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:41:31 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154130_0098_m_000000_98' to o3fs://bucket1.vol1/testdata
21/01/20 15:41:31 INFO SparkHadoopMapRedUtil: attempt_20210120154130_0098_m_000000_98: Committed
21/01/20 15:41:31 INFO Executor: Finished task 0.0 in stage 98.0 (TID 98). 2155 bytes result sent to driver
21/01/20 15:41:31 INFO TaskSetManager: Finished task 0.0 in stage 98.0 (TID 98) in 1040 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:41:31 INFO TaskSchedulerImpl: Removed TaskSet 98.0, whose tasks have all completed, from pool 
21/01/20 15:41:31 INFO DAGScheduler: ResultStage 98 (parquet at Generate.java:61) finished in 1.057 s
21/01/20 15:41:31 INFO DAGScheduler: Job 98 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:41:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 98: Stage finished
21/01/20 15:41:31 INFO DAGScheduler: Job 98 finished: parquet at Generate.java:61, took 1.058673 s
21/01/20 15:41:31 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-00e2bcbd-502c-46cb-9922-f031b408f347
21/01/20 15:41:31 INFO FileFormatWriter: Write Job cd161bdd-2b34-4345-a118-1694e78dd0d0 committed.
21/01/20 15:41:31 INFO FileFormatWriter: Finished processing stats for write job cd161bdd-2b34-4345-a118-1694e78dd0d0.
21/01/20 15:41:32 INFO BlockManagerInfo: Removed broadcast_98_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:41:33 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:33 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:33 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:41:33 INFO DAGScheduler: Got job 99 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:41:33 INFO DAGScheduler: Final stage: ResultStage 99 (parquet at Generate.java:61)
21/01/20 15:41:33 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:41:33 INFO DAGScheduler: Missing parents: List()
21/01/20 15:41:33 INFO DAGScheduler: Submitting ResultStage 99 (MapPartitionsRDD[397] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:41:33 INFO MemoryStore: Block broadcast_99 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:41:33 INFO MemoryStore: Block broadcast_99_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:41:33 INFO BlockManagerInfo: Added broadcast_99_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:41:33 INFO SparkContext: Created broadcast 99 from broadcast at DAGScheduler.scala:1200
21/01/20 15:41:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 99 (MapPartitionsRDD[397] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:41:33 INFO TaskSchedulerImpl: Adding task set 99.0 with 1 tasks
21/01/20 15:41:33 WARN TaskSetManager: Stage 99 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:41:33 INFO TaskSetManager: Starting task 0.0 in stage 99.0 (TID 99, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:41:33 INFO Executor: Running task 0.0 in stage 99.0 (TID 99)
21/01/20 15:41:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:34 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:34 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:41:34 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:41:34 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:41:34 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:41:34 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:41:34 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:41:34 INFO ParquetOutputFormat: Validation is off
21/01/20 15:41:34 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:41:34 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:41:34 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:41:34 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:41:34 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:41:34 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:41:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:41:34 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154133_0099_m_000000_99' to o3fs://bucket1.vol1/testdata
21/01/20 15:41:34 INFO SparkHadoopMapRedUtil: attempt_20210120154133_0099_m_000000_99: Committed
21/01/20 15:41:34 INFO Executor: Finished task 0.0 in stage 99.0 (TID 99). 2155 bytes result sent to driver
21/01/20 15:41:34 INFO TaskSetManager: Finished task 0.0 in stage 99.0 (TID 99) in 840 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:41:34 INFO TaskSchedulerImpl: Removed TaskSet 99.0, whose tasks have all completed, from pool 
21/01/20 15:41:34 INFO DAGScheduler: ResultStage 99 (parquet at Generate.java:61) finished in 0.880 s
21/01/20 15:41:34 INFO DAGScheduler: Job 99 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:41:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 99: Stage finished
21/01/20 15:41:34 INFO DAGScheduler: Job 99 finished: parquet at Generate.java:61, took 0.882349 s
21/01/20 15:41:34 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-16537052-17c3-4f0e-9866-780394cc4d10
21/01/20 15:41:34 INFO FileFormatWriter: Write Job 5af1d35d-ea4a-490f-a6f1-2a943505f42f committed.
21/01/20 15:41:34 INFO FileFormatWriter: Finished processing stats for write job 5af1d35d-ea4a-490f-a6f1-2a943505f42f.
21/01/20 15:41:34 INFO BlockManagerInfo: Removed broadcast_99_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:41:37 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:37 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:37 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:41:37 INFO DAGScheduler: Got job 100 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:41:37 INFO DAGScheduler: Final stage: ResultStage 100 (parquet at Generate.java:61)
21/01/20 15:41:37 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:41:37 INFO DAGScheduler: Missing parents: List()
21/01/20 15:41:37 INFO DAGScheduler: Submitting ResultStage 100 (MapPartitionsRDD[401] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:41:37 INFO MemoryStore: Block broadcast_100 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:41:37 INFO MemoryStore: Block broadcast_100_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:41:37 INFO BlockManagerInfo: Added broadcast_100_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:41:37 INFO SparkContext: Created broadcast 100 from broadcast at DAGScheduler.scala:1200
21/01/20 15:41:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 100 (MapPartitionsRDD[401] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:41:37 INFO TaskSchedulerImpl: Adding task set 100.0 with 1 tasks
21/01/20 15:41:37 WARN TaskSetManager: Stage 100 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:41:37 INFO TaskSetManager: Starting task 0.0 in stage 100.0 (TID 100, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:41:37 INFO Executor: Running task 0.0 in stage 100.0 (TID 100)
21/01/20 15:41:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:37 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:37 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:41:37 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:41:37 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:41:37 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:41:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:41:37 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:41:37 INFO ParquetOutputFormat: Validation is off
21/01/20 15:41:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:41:37 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:41:37 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:41:37 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:41:37 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:41:37 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:41:37 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:41:38 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154137_0100_m_000000_100' to o3fs://bucket1.vol1/testdata
21/01/20 15:41:38 INFO SparkHadoopMapRedUtil: attempt_20210120154137_0100_m_000000_100: Committed
21/01/20 15:41:38 INFO Executor: Finished task 0.0 in stage 100.0 (TID 100). 2155 bytes result sent to driver
21/01/20 15:41:38 INFO TaskSetManager: Finished task 0.0 in stage 100.0 (TID 100) in 976 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:41:38 INFO TaskSchedulerImpl: Removed TaskSet 100.0, whose tasks have all completed, from pool 
21/01/20 15:41:38 INFO DAGScheduler: ResultStage 100 (parquet at Generate.java:61) finished in 0.995 s
21/01/20 15:41:38 INFO DAGScheduler: Job 100 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:41:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 100: Stage finished
21/01/20 15:41:38 INFO DAGScheduler: Job 100 finished: parquet at Generate.java:61, took 0.996510 s
21/01/20 15:41:38 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-6ba03499-4400-4636-8831-62e3e19d93ce
21/01/20 15:41:38 INFO FileFormatWriter: Write Job 83f4b693-d9a6-4a73-8fb8-610d2706ed37 committed.
21/01/20 15:41:38 INFO FileFormatWriter: Finished processing stats for write job 83f4b693-d9a6-4a73-8fb8-610d2706ed37.
21/01/20 15:41:39 INFO BlockManagerInfo: Removed broadcast_100_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:41:40 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:40 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:40 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:41:40 INFO DAGScheduler: Got job 101 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:41:40 INFO DAGScheduler: Final stage: ResultStage 101 (parquet at Generate.java:61)
21/01/20 15:41:40 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:41:40 INFO DAGScheduler: Missing parents: List()
21/01/20 15:41:40 INFO DAGScheduler: Submitting ResultStage 101 (MapPartitionsRDD[405] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:41:41 INFO MemoryStore: Block broadcast_101 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:41:41 INFO MemoryStore: Block broadcast_101_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:41:41 INFO BlockManagerInfo: Added broadcast_101_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:41:41 INFO SparkContext: Created broadcast 101 from broadcast at DAGScheduler.scala:1200
21/01/20 15:41:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 101 (MapPartitionsRDD[405] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:41:41 INFO TaskSchedulerImpl: Adding task set 101.0 with 1 tasks
21/01/20 15:41:41 WARN TaskSetManager: Stage 101 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:41:41 INFO TaskSetManager: Starting task 0.0 in stage 101.0 (TID 101, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:41:41 INFO Executor: Running task 0.0 in stage 101.0 (TID 101)
21/01/20 15:41:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:41 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:41 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:41:41 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:41:41 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:41:41 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:41:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:41:41 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:41:41 INFO ParquetOutputFormat: Validation is off
21/01/20 15:41:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:41:41 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:41:41 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:41:41 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:41:41 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:41:41 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:41:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:41:41 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154140_0101_m_000000_101' to o3fs://bucket1.vol1/testdata
21/01/20 15:41:41 INFO SparkHadoopMapRedUtil: attempt_20210120154140_0101_m_000000_101: Committed
21/01/20 15:41:41 INFO Executor: Finished task 0.0 in stage 101.0 (TID 101). 2155 bytes result sent to driver
21/01/20 15:41:41 INFO TaskSetManager: Finished task 0.0 in stage 101.0 (TID 101) in 869 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:41:41 INFO TaskSchedulerImpl: Removed TaskSet 101.0, whose tasks have all completed, from pool 
21/01/20 15:41:41 INFO DAGScheduler: ResultStage 101 (parquet at Generate.java:61) finished in 0.911 s
21/01/20 15:41:41 INFO DAGScheduler: Job 101 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:41:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 101: Stage finished
21/01/20 15:41:41 INFO DAGScheduler: Job 101 finished: parquet at Generate.java:61, took 0.912908 s
21/01/20 15:41:41 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-786844fa-136b-4c92-a6ec-7e55493b91de
21/01/20 15:41:41 INFO FileFormatWriter: Write Job 37c5bbf5-87b4-4540-8eb4-9581a56d3785 committed.
21/01/20 15:41:41 INFO FileFormatWriter: Finished processing stats for write job 37c5bbf5-87b4-4540-8eb4-9581a56d3785.
21/01/20 15:41:42 INFO BlockManagerInfo: Removed broadcast_101_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:41:44 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:44 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:44 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:41:44 INFO DAGScheduler: Got job 102 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:41:44 INFO DAGScheduler: Final stage: ResultStage 102 (parquet at Generate.java:61)
21/01/20 15:41:44 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:41:44 INFO DAGScheduler: Missing parents: List()
21/01/20 15:41:44 INFO DAGScheduler: Submitting ResultStage 102 (MapPartitionsRDD[409] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:41:44 INFO MemoryStore: Block broadcast_102 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:41:44 INFO MemoryStore: Block broadcast_102_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:41:44 INFO BlockManagerInfo: Added broadcast_102_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:41:44 INFO SparkContext: Created broadcast 102 from broadcast at DAGScheduler.scala:1200
21/01/20 15:41:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 102 (MapPartitionsRDD[409] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:41:44 INFO TaskSchedulerImpl: Adding task set 102.0 with 1 tasks
21/01/20 15:41:44 WARN TaskSetManager: Stage 102 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:41:44 INFO TaskSetManager: Starting task 0.0 in stage 102.0 (TID 102, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:41:44 INFO Executor: Running task 0.0 in stage 102.0 (TID 102)
21/01/20 15:41:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:44 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:44 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:41:44 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:41:44 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:41:44 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:41:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:41:44 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:41:44 INFO ParquetOutputFormat: Validation is off
21/01/20 15:41:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:41:44 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:41:44 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:41:44 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:41:44 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:41:44 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:41:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:41:45 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-36EABA5D188D->d3c4d3e3-944b-4528-9d82-e005ad427697
21/01/20 15:41:45 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:41:45 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154144_0102_m_000000_102' to o3fs://bucket1.vol1/testdata
21/01/20 15:41:45 INFO SparkHadoopMapRedUtil: attempt_20210120154144_0102_m_000000_102: Committed
21/01/20 15:41:45 INFO Executor: Finished task 0.0 in stage 102.0 (TID 102). 2155 bytes result sent to driver
21/01/20 15:41:45 INFO TaskSetManager: Finished task 0.0 in stage 102.0 (TID 102) in 1111 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:41:45 INFO TaskSchedulerImpl: Removed TaskSet 102.0, whose tasks have all completed, from pool 
21/01/20 15:41:45 INFO DAGScheduler: ResultStage 102 (parquet at Generate.java:61) finished in 1.129 s
21/01/20 15:41:45 INFO DAGScheduler: Job 102 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:41:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 102: Stage finished
21/01/20 15:41:45 INFO DAGScheduler: Job 102 finished: parquet at Generate.java:61, took 1.131010 s
21/01/20 15:41:45 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-df887ccc-4cf7-4451-8a72-1a5f97b8424e
21/01/20 15:41:45 INFO FileFormatWriter: Write Job afb10293-25d4-471c-acd2-1b044e1e2097 committed.
21/01/20 15:41:45 INFO FileFormatWriter: Finished processing stats for write job afb10293-25d4-471c-acd2-1b044e1e2097.
21/01/20 15:41:46 INFO BlockManagerInfo: Removed broadcast_102_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:41:48 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:48 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:48 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:41:48 INFO DAGScheduler: Got job 103 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:41:48 INFO DAGScheduler: Final stage: ResultStage 103 (parquet at Generate.java:61)
21/01/20 15:41:48 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:41:48 INFO DAGScheduler: Missing parents: List()
21/01/20 15:41:48 INFO DAGScheduler: Submitting ResultStage 103 (MapPartitionsRDD[413] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:41:48 INFO MemoryStore: Block broadcast_103 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:41:48 INFO MemoryStore: Block broadcast_103_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:41:48 INFO BlockManagerInfo: Added broadcast_103_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:41:48 INFO SparkContext: Created broadcast 103 from broadcast at DAGScheduler.scala:1200
21/01/20 15:41:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 103 (MapPartitionsRDD[413] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:41:48 INFO TaskSchedulerImpl: Adding task set 103.0 with 1 tasks
21/01/20 15:41:48 WARN TaskSetManager: Stage 103 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:41:48 INFO TaskSetManager: Starting task 0.0 in stage 103.0 (TID 103, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:41:48 INFO Executor: Running task 0.0 in stage 103.0 (TID 103)
21/01/20 15:41:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:48 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:48 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:41:48 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:41:48 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:41:48 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:41:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:41:48 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:41:48 INFO ParquetOutputFormat: Validation is off
21/01/20 15:41:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:41:48 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:41:48 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:41:48 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:41:48 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:41:48 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:41:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:41:49 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154148_0103_m_000000_103' to o3fs://bucket1.vol1/testdata
21/01/20 15:41:49 INFO SparkHadoopMapRedUtil: attempt_20210120154148_0103_m_000000_103: Committed
21/01/20 15:41:49 INFO Executor: Finished task 0.0 in stage 103.0 (TID 103). 2155 bytes result sent to driver
21/01/20 15:41:49 INFO TaskSetManager: Finished task 0.0 in stage 103.0 (TID 103) in 1074 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:41:49 INFO TaskSchedulerImpl: Removed TaskSet 103.0, whose tasks have all completed, from pool 
21/01/20 15:41:49 INFO DAGScheduler: ResultStage 103 (parquet at Generate.java:61) finished in 1.093 s
21/01/20 15:41:49 INFO DAGScheduler: Job 103 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:41:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 103: Stage finished
21/01/20 15:41:49 INFO DAGScheduler: Job 103 finished: parquet at Generate.java:61, took 1.094981 s
21/01/20 15:41:49 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-df2e3457-bed7-4d3c-917d-54962f2517d6
21/01/20 15:41:49 INFO FileFormatWriter: Write Job f505705c-2eba-4ef5-9145-7d03b03d620e committed.
21/01/20 15:41:49 INFO FileFormatWriter: Finished processing stats for write job f505705c-2eba-4ef5-9145-7d03b03d620e.
21/01/20 15:41:50 INFO BlockManagerInfo: Removed broadcast_103_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:41:51 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:51 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:41:51 INFO DAGScheduler: Got job 104 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:41:51 INFO DAGScheduler: Final stage: ResultStage 104 (parquet at Generate.java:61)
21/01/20 15:41:51 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:41:51 INFO DAGScheduler: Missing parents: List()
21/01/20 15:41:51 INFO DAGScheduler: Submitting ResultStage 104 (MapPartitionsRDD[417] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:41:51 INFO MemoryStore: Block broadcast_104 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:41:51 INFO MemoryStore: Block broadcast_104_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:41:51 INFO BlockManagerInfo: Added broadcast_104_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:41:51 INFO SparkContext: Created broadcast 104 from broadcast at DAGScheduler.scala:1200
21/01/20 15:41:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 104 (MapPartitionsRDD[417] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:41:51 INFO TaskSchedulerImpl: Adding task set 104.0 with 1 tasks
21/01/20 15:41:52 WARN TaskSetManager: Stage 104 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:41:52 INFO TaskSetManager: Starting task 0.0 in stage 104.0 (TID 104, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:41:52 INFO Executor: Running task 0.0 in stage 104.0 (TID 104)
21/01/20 15:41:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:52 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:52 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:41:52 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:41:52 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:41:52 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:41:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:41:52 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:41:52 INFO ParquetOutputFormat: Validation is off
21/01/20 15:41:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:41:52 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:41:52 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:41:52 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:41:52 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:41:52 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:41:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:41:52 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-313307FC1532->af5c9c6a-a887-4538-ae7b-ba6d49328297
21/01/20 15:41:52 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:41:53 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154151_0104_m_000000_104' to o3fs://bucket1.vol1/testdata
21/01/20 15:41:53 INFO SparkHadoopMapRedUtil: attempt_20210120154151_0104_m_000000_104: Committed
21/01/20 15:41:53 INFO Executor: Finished task 0.0 in stage 104.0 (TID 104). 2155 bytes result sent to driver
21/01/20 15:41:53 INFO TaskSetManager: Finished task 0.0 in stage 104.0 (TID 104) in 1030 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:41:53 INFO TaskSchedulerImpl: Removed TaskSet 104.0, whose tasks have all completed, from pool 
21/01/20 15:41:53 INFO DAGScheduler: ResultStage 104 (parquet at Generate.java:61) finished in 1.047 s
21/01/20 15:41:53 INFO DAGScheduler: Job 104 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:41:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 104: Stage finished
21/01/20 15:41:53 INFO DAGScheduler: Job 104 finished: parquet at Generate.java:61, took 1.049011 s
21/01/20 15:41:53 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-ae9d633f-8bba-48e2-914c-706b666ad1c3
21/01/20 15:41:53 INFO FileFormatWriter: Write Job 5270d732-4a10-4dc8-8759-db250ebb0856 committed.
21/01/20 15:41:53 INFO FileFormatWriter: Finished processing stats for write job 5270d732-4a10-4dc8-8759-db250ebb0856.
21/01/20 15:41:54 INFO BlockManagerInfo: Removed broadcast_104_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:41:55 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:55 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:41:55 INFO DAGScheduler: Got job 105 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:41:55 INFO DAGScheduler: Final stage: ResultStage 105 (parquet at Generate.java:61)
21/01/20 15:41:55 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:41:55 INFO DAGScheduler: Missing parents: List()
21/01/20 15:41:55 INFO DAGScheduler: Submitting ResultStage 105 (MapPartitionsRDD[421] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:41:55 INFO MemoryStore: Block broadcast_105 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:41:55 INFO MemoryStore: Block broadcast_105_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:41:55 INFO BlockManagerInfo: Added broadcast_105_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:41:55 INFO SparkContext: Created broadcast 105 from broadcast at DAGScheduler.scala:1200
21/01/20 15:41:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 105 (MapPartitionsRDD[421] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:41:55 INFO TaskSchedulerImpl: Adding task set 105.0 with 1 tasks
21/01/20 15:41:55 WARN TaskSetManager: Stage 105 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:41:55 INFO TaskSetManager: Starting task 0.0 in stage 105.0 (TID 105, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:41:55 INFO Executor: Running task 0.0 in stage 105.0 (TID 105)
21/01/20 15:41:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:55 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:41:55 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:41:55 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:41:55 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:41:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:41:55 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:41:55 INFO ParquetOutputFormat: Validation is off
21/01/20 15:41:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:41:55 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:41:55 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:41:55 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:41:55 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:41:55 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:41:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:41:56 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154155_0105_m_000000_105' to o3fs://bucket1.vol1/testdata
21/01/20 15:41:56 INFO SparkHadoopMapRedUtil: attempt_20210120154155_0105_m_000000_105: Committed
21/01/20 15:41:56 INFO Executor: Finished task 0.0 in stage 105.0 (TID 105). 2155 bytes result sent to driver
21/01/20 15:41:56 INFO TaskSetManager: Finished task 0.0 in stage 105.0 (TID 105) in 1044 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:41:56 INFO TaskSchedulerImpl: Removed TaskSet 105.0, whose tasks have all completed, from pool 
21/01/20 15:41:56 INFO DAGScheduler: ResultStage 105 (parquet at Generate.java:61) finished in 1.064 s
21/01/20 15:41:56 INFO DAGScheduler: Job 105 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:41:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 105: Stage finished
21/01/20 15:41:56 INFO DAGScheduler: Job 105 finished: parquet at Generate.java:61, took 1.065756 s
21/01/20 15:41:56 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-68dd10a8-6543-4eb1-aa5e-39fe468344ee
21/01/20 15:41:56 INFO FileFormatWriter: Write Job 5c4e78ee-f49f-4cae-b734-e7a54b5af1db committed.
21/01/20 15:41:56 INFO FileFormatWriter: Finished processing stats for write job 5c4e78ee-f49f-4cae-b734-e7a54b5af1db.
21/01/20 15:41:57 INFO BlockManagerInfo: Removed broadcast_105_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:41:59 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:59 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:59 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:41:59 INFO DAGScheduler: Got job 106 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:41:59 INFO DAGScheduler: Final stage: ResultStage 106 (parquet at Generate.java:61)
21/01/20 15:41:59 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:41:59 INFO DAGScheduler: Missing parents: List()
21/01/20 15:41:59 INFO DAGScheduler: Submitting ResultStage 106 (MapPartitionsRDD[425] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:41:59 INFO MemoryStore: Block broadcast_106 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:41:59 INFO MemoryStore: Block broadcast_106_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:41:59 INFO BlockManagerInfo: Added broadcast_106_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:41:59 INFO SparkContext: Created broadcast 106 from broadcast at DAGScheduler.scala:1200
21/01/20 15:41:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 106 (MapPartitionsRDD[425] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:41:59 INFO TaskSchedulerImpl: Adding task set 106.0 with 1 tasks
21/01/20 15:41:59 WARN TaskSetManager: Stage 106 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:41:59 INFO TaskSetManager: Starting task 0.0 in stage 106.0 (TID 106, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:41:59 INFO Executor: Running task 0.0 in stage 106.0 (TID 106)
21/01/20 15:41:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:59 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:41:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:41:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:41:59 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:41:59 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:41:59 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:41:59 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:41:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:41:59 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:41:59 INFO ParquetOutputFormat: Validation is off
21/01/20 15:41:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:41:59 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:41:59 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:41:59 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:41:59 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:41:59 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:41:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:42:00 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154159_0106_m_000000_106' to o3fs://bucket1.vol1/testdata
21/01/20 15:42:00 INFO SparkHadoopMapRedUtil: attempt_20210120154159_0106_m_000000_106: Committed
21/01/20 15:42:00 INFO Executor: Finished task 0.0 in stage 106.0 (TID 106). 2155 bytes result sent to driver
21/01/20 15:42:00 INFO TaskSetManager: Finished task 0.0 in stage 106.0 (TID 106) in 943 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:42:00 INFO TaskSchedulerImpl: Removed TaskSet 106.0, whose tasks have all completed, from pool 
21/01/20 15:42:00 INFO DAGScheduler: ResultStage 106 (parquet at Generate.java:61) finished in 0.962 s
21/01/20 15:42:00 INFO DAGScheduler: Job 106 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:42:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 106: Stage finished
21/01/20 15:42:00 INFO DAGScheduler: Job 106 finished: parquet at Generate.java:61, took 0.963668 s
21/01/20 15:42:00 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-039ab836-142d-49c3-abd4-695bd26cb7e5
21/01/20 15:42:00 INFO FileFormatWriter: Write Job 4a7fd717-4893-4021-ba63-2b8c7f78ccb4 committed.
21/01/20 15:42:00 INFO FileFormatWriter: Finished processing stats for write job 4a7fd717-4893-4021-ba63-2b8c7f78ccb4.
21/01/20 15:42:01 INFO BlockManagerInfo: Removed broadcast_106_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:42:02 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:02 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:42:02 INFO DAGScheduler: Got job 107 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:42:02 INFO DAGScheduler: Final stage: ResultStage 107 (parquet at Generate.java:61)
21/01/20 15:42:02 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:42:02 INFO DAGScheduler: Missing parents: List()
21/01/20 15:42:02 INFO DAGScheduler: Submitting ResultStage 107 (MapPartitionsRDD[429] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:42:02 INFO MemoryStore: Block broadcast_107 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:42:02 INFO MemoryStore: Block broadcast_107_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:42:02 INFO BlockManagerInfo: Added broadcast_107_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:42:02 INFO SparkContext: Created broadcast 107 from broadcast at DAGScheduler.scala:1200
21/01/20 15:42:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 107 (MapPartitionsRDD[429] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:42:02 INFO TaskSchedulerImpl: Adding task set 107.0 with 1 tasks
21/01/20 15:42:02 WARN TaskSetManager: Stage 107 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:42:02 INFO TaskSetManager: Starting task 0.0 in stage 107.0 (TID 107, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:42:02 INFO Executor: Running task 0.0 in stage 107.0 (TID 107)
21/01/20 15:42:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:03 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:42:03 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:42:03 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:42:03 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:42:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:42:03 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:42:03 INFO ParquetOutputFormat: Validation is off
21/01/20 15:42:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:42:03 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:42:03 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:42:03 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:42:03 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:42:03 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:42:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:42:03 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-3222025F3264->d3c4d3e3-944b-4528-9d82-e005ad427697
21/01/20 15:42:03 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:42:03 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154202_0107_m_000000_107' to o3fs://bucket1.vol1/testdata
21/01/20 15:42:03 INFO SparkHadoopMapRedUtil: attempt_20210120154202_0107_m_000000_107: Committed
21/01/20 15:42:03 INFO Executor: Finished task 0.0 in stage 107.0 (TID 107). 2155 bytes result sent to driver
21/01/20 15:42:03 INFO TaskSetManager: Finished task 0.0 in stage 107.0 (TID 107) in 936 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:42:03 INFO TaskSchedulerImpl: Removed TaskSet 107.0, whose tasks have all completed, from pool 
21/01/20 15:42:03 INFO DAGScheduler: ResultStage 107 (parquet at Generate.java:61) finished in 0.976 s
21/01/20 15:42:03 INFO DAGScheduler: Job 107 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:42:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 107: Stage finished
21/01/20 15:42:03 INFO DAGScheduler: Job 107 finished: parquet at Generate.java:61, took 0.977535 s
21/01/20 15:42:03 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-9a5fcf2d-90ee-4875-b4d8-bdbe4a4d496c
21/01/20 15:42:03 INFO FileFormatWriter: Write Job 35739dc1-4b12-4569-93fc-b2f3c1200c2a committed.
21/01/20 15:42:03 INFO FileFormatWriter: Finished processing stats for write job 35739dc1-4b12-4569-93fc-b2f3c1200c2a.
21/01/20 15:42:04 INFO BlockManagerInfo: Removed broadcast_107_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:42:06 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:06 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:06 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:42:06 INFO DAGScheduler: Got job 108 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:42:06 INFO DAGScheduler: Final stage: ResultStage 108 (parquet at Generate.java:61)
21/01/20 15:42:06 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:42:06 INFO DAGScheduler: Missing parents: List()
21/01/20 15:42:06 INFO DAGScheduler: Submitting ResultStage 108 (MapPartitionsRDD[433] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:42:06 INFO MemoryStore: Block broadcast_108 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:42:06 INFO MemoryStore: Block broadcast_108_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:42:06 INFO BlockManagerInfo: Added broadcast_108_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:42:06 INFO SparkContext: Created broadcast 108 from broadcast at DAGScheduler.scala:1200
21/01/20 15:42:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 108 (MapPartitionsRDD[433] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:42:06 INFO TaskSchedulerImpl: Adding task set 108.0 with 1 tasks
21/01/20 15:42:06 WARN TaskSetManager: Stage 108 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:42:06 INFO TaskSetManager: Starting task 0.0 in stage 108.0 (TID 108, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:42:06 INFO Executor: Running task 0.0 in stage 108.0 (TID 108)
21/01/20 15:42:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:06 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:06 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:42:06 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:42:06 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:42:06 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:42:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:42:06 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:42:06 INFO ParquetOutputFormat: Validation is off
21/01/20 15:42:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:42:06 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:42:06 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:42:06 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:42:06 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:42:06 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:42:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:42:07 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154206_0108_m_000000_108' to o3fs://bucket1.vol1/testdata
21/01/20 15:42:07 INFO SparkHadoopMapRedUtil: attempt_20210120154206_0108_m_000000_108: Committed
21/01/20 15:42:07 INFO Executor: Finished task 0.0 in stage 108.0 (TID 108). 2155 bytes result sent to driver
21/01/20 15:42:07 INFO TaskSetManager: Finished task 0.0 in stage 108.0 (TID 108) in 1028 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:42:07 INFO TaskSchedulerImpl: Removed TaskSet 108.0, whose tasks have all completed, from pool 
21/01/20 15:42:07 INFO DAGScheduler: ResultStage 108 (parquet at Generate.java:61) finished in 1.046 s
21/01/20 15:42:07 INFO DAGScheduler: Job 108 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:42:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 108: Stage finished
21/01/20 15:42:07 INFO DAGScheduler: Job 108 finished: parquet at Generate.java:61, took 1.047913 s
21/01/20 15:42:07 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-733ce672-1484-40ec-be8e-e6d22f2dcf0d
21/01/20 15:42:07 INFO FileFormatWriter: Write Job 54bdeb7f-970f-49e1-9de5-2df8a73d0c6d committed.
21/01/20 15:42:07 INFO FileFormatWriter: Finished processing stats for write job 54bdeb7f-970f-49e1-9de5-2df8a73d0c6d.
21/01/20 15:42:08 INFO BlockManagerInfo: Removed broadcast_108_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:42:10 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:10 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:10 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:42:10 INFO DAGScheduler: Got job 109 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:42:10 INFO DAGScheduler: Final stage: ResultStage 109 (parquet at Generate.java:61)
21/01/20 15:42:10 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:42:10 INFO DAGScheduler: Missing parents: List()
21/01/20 15:42:10 INFO DAGScheduler: Submitting ResultStage 109 (MapPartitionsRDD[437] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:42:10 INFO MemoryStore: Block broadcast_109 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:42:10 INFO MemoryStore: Block broadcast_109_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:42:10 INFO BlockManagerInfo: Added broadcast_109_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:42:10 INFO SparkContext: Created broadcast 109 from broadcast at DAGScheduler.scala:1200
21/01/20 15:42:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 109 (MapPartitionsRDD[437] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:42:10 INFO TaskSchedulerImpl: Adding task set 109.0 with 1 tasks
21/01/20 15:42:10 WARN TaskSetManager: Stage 109 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:42:10 INFO TaskSetManager: Starting task 0.0 in stage 109.0 (TID 109, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:42:10 INFO Executor: Running task 0.0 in stage 109.0 (TID 109)
21/01/20 15:42:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:10 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:10 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:42:10 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:42:10 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:42:10 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:42:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:42:10 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:42:10 INFO ParquetOutputFormat: Validation is off
21/01/20 15:42:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:42:10 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:42:10 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:42:10 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:42:10 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:42:10 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:42:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:42:11 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154210_0109_m_000000_109' to o3fs://bucket1.vol1/testdata
21/01/20 15:42:11 INFO SparkHadoopMapRedUtil: attempt_20210120154210_0109_m_000000_109: Committed
21/01/20 15:42:11 INFO Executor: Finished task 0.0 in stage 109.0 (TID 109). 2155 bytes result sent to driver
21/01/20 15:42:11 INFO TaskSetManager: Finished task 0.0 in stage 109.0 (TID 109) in 928 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:42:11 INFO TaskSchedulerImpl: Removed TaskSet 109.0, whose tasks have all completed, from pool 
21/01/20 15:42:11 INFO DAGScheduler: ResultStage 109 (parquet at Generate.java:61) finished in 0.971 s
21/01/20 15:42:11 INFO DAGScheduler: Job 109 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:42:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 109: Stage finished
21/01/20 15:42:11 INFO DAGScheduler: Job 109 finished: parquet at Generate.java:61, took 0.972460 s
21/01/20 15:42:11 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-8ddef49f-b5b9-4f76-9213-7204b07ddb37
21/01/20 15:42:11 INFO FileFormatWriter: Write Job 6b83bd5a-09cc-4264-ad95-78006eb938dd committed.
21/01/20 15:42:11 INFO FileFormatWriter: Finished processing stats for write job 6b83bd5a-09cc-4264-ad95-78006eb938dd.
21/01/20 15:42:11 INFO BlockManagerInfo: Removed broadcast_109_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:42:13 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:13 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:13 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:42:13 INFO DAGScheduler: Got job 110 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:42:13 INFO DAGScheduler: Final stage: ResultStage 110 (parquet at Generate.java:61)
21/01/20 15:42:13 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:42:13 INFO DAGScheduler: Missing parents: List()
21/01/20 15:42:13 INFO DAGScheduler: Submitting ResultStage 110 (MapPartitionsRDD[441] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:42:13 INFO MemoryStore: Block broadcast_110 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:42:13 INFO MemoryStore: Block broadcast_110_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:42:13 INFO BlockManagerInfo: Added broadcast_110_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:42:13 INFO SparkContext: Created broadcast 110 from broadcast at DAGScheduler.scala:1200
21/01/20 15:42:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 110 (MapPartitionsRDD[441] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:42:13 INFO TaskSchedulerImpl: Adding task set 110.0 with 1 tasks
21/01/20 15:42:13 WARN TaskSetManager: Stage 110 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:42:13 INFO TaskSetManager: Starting task 0.0 in stage 110.0 (TID 110, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:42:13 INFO Executor: Running task 0.0 in stage 110.0 (TID 110)
21/01/20 15:42:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:13 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:13 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:42:13 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:42:13 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:42:13 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:42:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:42:13 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:42:13 INFO ParquetOutputFormat: Validation is off
21/01/20 15:42:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:42:13 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:42:13 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:42:13 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:42:13 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:42:13 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:42:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:42:14 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154213_0110_m_000000_110' to o3fs://bucket1.vol1/testdata
21/01/20 15:42:14 INFO SparkHadoopMapRedUtil: attempt_20210120154213_0110_m_000000_110: Committed
21/01/20 15:42:14 INFO Executor: Finished task 0.0 in stage 110.0 (TID 110). 2155 bytes result sent to driver
21/01/20 15:42:14 INFO TaskSetManager: Finished task 0.0 in stage 110.0 (TID 110) in 1034 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:42:14 INFO TaskSchedulerImpl: Removed TaskSet 110.0, whose tasks have all completed, from pool 
21/01/20 15:42:14 INFO DAGScheduler: ResultStage 110 (parquet at Generate.java:61) finished in 1.051 s
21/01/20 15:42:14 INFO DAGScheduler: Job 110 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:42:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 110: Stage finished
21/01/20 15:42:14 INFO DAGScheduler: Job 110 finished: parquet at Generate.java:61, took 1.052911 s
21/01/20 15:42:14 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-78db5e92-6533-4355-979d-f36cd43abba2
21/01/20 15:42:14 INFO FileFormatWriter: Write Job 80bdecfb-5c06-468b-b9c5-1b4edd2143d2 committed.
21/01/20 15:42:14 INFO FileFormatWriter: Finished processing stats for write job 80bdecfb-5c06-468b-b9c5-1b4edd2143d2.
21/01/20 15:42:16 INFO BlockManagerInfo: Removed broadcast_110_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:42:17 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:17 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:17 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:42:17 INFO DAGScheduler: Got job 111 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:42:17 INFO DAGScheduler: Final stage: ResultStage 111 (parquet at Generate.java:61)
21/01/20 15:42:17 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:42:17 INFO DAGScheduler: Missing parents: List()
21/01/20 15:42:17 INFO DAGScheduler: Submitting ResultStage 111 (MapPartitionsRDD[445] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:42:17 INFO MemoryStore: Block broadcast_111 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:42:17 INFO MemoryStore: Block broadcast_111_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:42:17 INFO BlockManagerInfo: Added broadcast_111_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:42:17 INFO SparkContext: Created broadcast 111 from broadcast at DAGScheduler.scala:1200
21/01/20 15:42:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 111 (MapPartitionsRDD[445] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:42:17 INFO TaskSchedulerImpl: Adding task set 111.0 with 1 tasks
21/01/20 15:42:17 WARN TaskSetManager: Stage 111 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:42:17 INFO TaskSetManager: Starting task 0.0 in stage 111.0 (TID 111, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:42:17 INFO Executor: Running task 0.0 in stage 111.0 (TID 111)
21/01/20 15:42:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:17 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:17 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:42:17 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:42:17 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:42:17 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:42:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:42:17 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:42:17 INFO ParquetOutputFormat: Validation is off
21/01/20 15:42:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:42:17 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:42:17 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:42:17 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:42:17 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:42:17 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:42:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:42:18 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154217_0111_m_000000_111' to o3fs://bucket1.vol1/testdata
21/01/20 15:42:18 INFO SparkHadoopMapRedUtil: attempt_20210120154217_0111_m_000000_111: Committed
21/01/20 15:42:18 INFO Executor: Finished task 0.0 in stage 111.0 (TID 111). 2155 bytes result sent to driver
21/01/20 15:42:18 INFO TaskSetManager: Finished task 0.0 in stage 111.0 (TID 111) in 911 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:42:18 INFO TaskSchedulerImpl: Removed TaskSet 111.0, whose tasks have all completed, from pool 
21/01/20 15:42:18 INFO DAGScheduler: ResultStage 111 (parquet at Generate.java:61) finished in 0.952 s
21/01/20 15:42:18 INFO DAGScheduler: Job 111 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:42:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 111: Stage finished
21/01/20 15:42:18 INFO DAGScheduler: Job 111 finished: parquet at Generate.java:61, took 0.954037 s
21/01/20 15:42:18 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-09ce4cd5-149b-4c5c-96da-9588f8250306
21/01/20 15:42:18 INFO FileFormatWriter: Write Job e8e01358-dc8d-4d17-8614-d3d1c3df12b3 committed.
21/01/20 15:42:18 INFO FileFormatWriter: Finished processing stats for write job e8e01358-dc8d-4d17-8614-d3d1c3df12b3.
21/01/20 15:42:18 INFO BlockManagerInfo: Removed broadcast_111_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:42:20 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:20 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:20 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:42:20 INFO DAGScheduler: Got job 112 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:42:20 INFO DAGScheduler: Final stage: ResultStage 112 (parquet at Generate.java:61)
21/01/20 15:42:20 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:42:20 INFO DAGScheduler: Missing parents: List()
21/01/20 15:42:20 INFO DAGScheduler: Submitting ResultStage 112 (MapPartitionsRDD[449] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:42:20 INFO MemoryStore: Block broadcast_112 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:42:20 INFO MemoryStore: Block broadcast_112_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:42:20 INFO BlockManagerInfo: Added broadcast_112_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:42:20 INFO SparkContext: Created broadcast 112 from broadcast at DAGScheduler.scala:1200
21/01/20 15:42:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 112 (MapPartitionsRDD[449] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:42:20 INFO TaskSchedulerImpl: Adding task set 112.0 with 1 tasks
21/01/20 15:42:21 WARN TaskSetManager: Stage 112 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:42:21 INFO TaskSetManager: Starting task 0.0 in stage 112.0 (TID 112, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:42:21 INFO Executor: Running task 0.0 in stage 112.0 (TID 112)
21/01/20 15:42:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:21 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:21 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:42:21 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:42:21 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:42:21 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:42:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:42:21 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:42:21 INFO ParquetOutputFormat: Validation is off
21/01/20 15:42:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:42:21 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:42:21 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:42:21 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:42:21 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:42:21 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:42:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:42:22 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154220_0112_m_000000_112' to o3fs://bucket1.vol1/testdata
21/01/20 15:42:22 INFO SparkHadoopMapRedUtil: attempt_20210120154220_0112_m_000000_112: Committed
21/01/20 15:42:22 INFO Executor: Finished task 0.0 in stage 112.0 (TID 112). 2155 bytes result sent to driver
21/01/20 15:42:22 INFO TaskSetManager: Finished task 0.0 in stage 112.0 (TID 112) in 1094 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:42:22 INFO TaskSchedulerImpl: Removed TaskSet 112.0, whose tasks have all completed, from pool 
21/01/20 15:42:22 INFO DAGScheduler: ResultStage 112 (parquet at Generate.java:61) finished in 1.113 s
21/01/20 15:42:22 INFO DAGScheduler: Job 112 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:42:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 112: Stage finished
21/01/20 15:42:22 INFO DAGScheduler: Job 112 finished: parquet at Generate.java:61, took 1.115050 s
21/01/20 15:42:22 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-daa1b97b-0145-460b-ab30-c0eb25815ca2
21/01/20 15:42:22 INFO FileFormatWriter: Write Job 5ddd2a90-2be3-409d-a182-5bb29f28f5dd committed.
21/01/20 15:42:22 INFO FileFormatWriter: Finished processing stats for write job 5ddd2a90-2be3-409d-a182-5bb29f28f5dd.
21/01/20 15:42:23 INFO BlockManagerInfo: Removed broadcast_112_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:42:24 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:24 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:24 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:42:24 INFO DAGScheduler: Got job 113 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:42:24 INFO DAGScheduler: Final stage: ResultStage 113 (parquet at Generate.java:61)
21/01/20 15:42:24 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:42:24 INFO DAGScheduler: Missing parents: List()
21/01/20 15:42:24 INFO DAGScheduler: Submitting ResultStage 113 (MapPartitionsRDD[453] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:42:24 INFO MemoryStore: Block broadcast_113 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:42:24 INFO MemoryStore: Block broadcast_113_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:42:24 INFO BlockManagerInfo: Added broadcast_113_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:42:24 INFO SparkContext: Created broadcast 113 from broadcast at DAGScheduler.scala:1200
21/01/20 15:42:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 113 (MapPartitionsRDD[453] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:42:24 INFO TaskSchedulerImpl: Adding task set 113.0 with 1 tasks
21/01/20 15:42:24 WARN TaskSetManager: Stage 113 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:42:24 INFO TaskSetManager: Starting task 0.0 in stage 113.0 (TID 113, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:42:24 INFO Executor: Running task 0.0 in stage 113.0 (TID 113)
21/01/20 15:42:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:24 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:24 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:42:24 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:42:24 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:42:24 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:42:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:42:24 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:42:24 INFO ParquetOutputFormat: Validation is off
21/01/20 15:42:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:42:24 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:42:24 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:42:24 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:42:24 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:42:24 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:42:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:42:25 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-C891F5CEE82B->af5c9c6a-a887-4538-ae7b-ba6d49328297
21/01/20 15:42:25 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:42:25 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154224_0113_m_000000_113' to o3fs://bucket1.vol1/testdata
21/01/20 15:42:25 INFO SparkHadoopMapRedUtil: attempt_20210120154224_0113_m_000000_113: Committed
21/01/20 15:42:25 INFO Executor: Finished task 0.0 in stage 113.0 (TID 113). 2155 bytes result sent to driver
21/01/20 15:42:25 INFO TaskSetManager: Finished task 0.0 in stage 113.0 (TID 113) in 1028 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:42:25 INFO TaskSchedulerImpl: Removed TaskSet 113.0, whose tasks have all completed, from pool 
21/01/20 15:42:25 INFO DAGScheduler: ResultStage 113 (parquet at Generate.java:61) finished in 1.046 s
21/01/20 15:42:25 INFO DAGScheduler: Job 113 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:42:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 113: Stage finished
21/01/20 15:42:25 INFO DAGScheduler: Job 113 finished: parquet at Generate.java:61, took 1.047412 s
21/01/20 15:42:25 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-712758a4-bf69-4a80-a528-6dc5d1386583
21/01/20 15:42:25 INFO FileFormatWriter: Write Job b77e866d-0e49-405d-b668-4b52743e3fdb committed.
21/01/20 15:42:25 INFO FileFormatWriter: Finished processing stats for write job b77e866d-0e49-405d-b668-4b52743e3fdb.
21/01/20 15:42:26 INFO BlockManagerInfo: Removed broadcast_113_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:42:28 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:28 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:28 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:42:28 INFO DAGScheduler: Got job 114 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:42:28 INFO DAGScheduler: Final stage: ResultStage 114 (parquet at Generate.java:61)
21/01/20 15:42:28 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:42:28 INFO DAGScheduler: Missing parents: List()
21/01/20 15:42:28 INFO DAGScheduler: Submitting ResultStage 114 (MapPartitionsRDD[457] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:42:28 INFO MemoryStore: Block broadcast_114 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:42:28 INFO MemoryStore: Block broadcast_114_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:42:28 INFO BlockManagerInfo: Added broadcast_114_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:42:28 INFO SparkContext: Created broadcast 114 from broadcast at DAGScheduler.scala:1200
21/01/20 15:42:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 114 (MapPartitionsRDD[457] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:42:28 INFO TaskSchedulerImpl: Adding task set 114.0 with 1 tasks
21/01/20 15:42:28 WARN TaskSetManager: Stage 114 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:42:28 INFO TaskSetManager: Starting task 0.0 in stage 114.0 (TID 114, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:42:28 INFO Executor: Running task 0.0 in stage 114.0 (TID 114)
21/01/20 15:42:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:28 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:28 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:42:28 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:42:28 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:42:28 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:42:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:42:28 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:42:28 INFO ParquetOutputFormat: Validation is off
21/01/20 15:42:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:42:28 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:42:28 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:42:28 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:42:28 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:42:28 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:42:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:42:29 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154228_0114_m_000000_114' to o3fs://bucket1.vol1/testdata
21/01/20 15:42:29 INFO SparkHadoopMapRedUtil: attempt_20210120154228_0114_m_000000_114: Committed
21/01/20 15:42:29 INFO Executor: Finished task 0.0 in stage 114.0 (TID 114). 2155 bytes result sent to driver
21/01/20 15:42:29 INFO TaskSetManager: Finished task 0.0 in stage 114.0 (TID 114) in 1025 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:42:29 INFO TaskSchedulerImpl: Removed TaskSet 114.0, whose tasks have all completed, from pool 
21/01/20 15:42:29 INFO DAGScheduler: ResultStage 114 (parquet at Generate.java:61) finished in 1.042 s
21/01/20 15:42:29 INFO DAGScheduler: Job 114 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:42:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 114: Stage finished
21/01/20 15:42:29 INFO DAGScheduler: Job 114 finished: parquet at Generate.java:61, took 1.044027 s
21/01/20 15:42:29 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-1c42599f-e017-4c78-9b9d-b91890b8d660
21/01/20 15:42:29 INFO FileFormatWriter: Write Job d8906cfa-f218-476f-a5ac-a14011b8a237 committed.
21/01/20 15:42:29 INFO FileFormatWriter: Finished processing stats for write job d8906cfa-f218-476f-a5ac-a14011b8a237.
21/01/20 15:42:30 INFO BlockManagerInfo: Removed broadcast_114_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:42:31 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:32 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:42:32 INFO DAGScheduler: Got job 115 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:42:32 INFO DAGScheduler: Final stage: ResultStage 115 (parquet at Generate.java:61)
21/01/20 15:42:32 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:42:32 INFO DAGScheduler: Missing parents: List()
21/01/20 15:42:32 INFO DAGScheduler: Submitting ResultStage 115 (MapPartitionsRDD[461] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:42:32 INFO MemoryStore: Block broadcast_115 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:42:32 INFO MemoryStore: Block broadcast_115_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:42:32 INFO BlockManagerInfo: Added broadcast_115_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:42:32 INFO SparkContext: Created broadcast 115 from broadcast at DAGScheduler.scala:1200
21/01/20 15:42:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 115 (MapPartitionsRDD[461] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:42:32 INFO TaskSchedulerImpl: Adding task set 115.0 with 1 tasks
21/01/20 15:42:32 WARN TaskSetManager: Stage 115 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:42:32 INFO TaskSetManager: Starting task 0.0 in stage 115.0 (TID 115, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:42:32 INFO Executor: Running task 0.0 in stage 115.0 (TID 115)
21/01/20 15:42:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:32 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:32 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:42:32 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:42:32 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:42:32 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:42:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:42:32 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:42:32 INFO ParquetOutputFormat: Validation is off
21/01/20 15:42:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:42:32 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:42:32 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:42:32 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:42:32 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:42:32 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:42:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:42:33 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154232_0115_m_000000_115' to o3fs://bucket1.vol1/testdata
21/01/20 15:42:33 INFO SparkHadoopMapRedUtil: attempt_20210120154232_0115_m_000000_115: Committed
21/01/20 15:42:33 INFO Executor: Finished task 0.0 in stage 115.0 (TID 115). 2155 bytes result sent to driver
21/01/20 15:42:33 INFO TaskSetManager: Finished task 0.0 in stage 115.0 (TID 115) in 1025 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:42:33 INFO TaskSchedulerImpl: Removed TaskSet 115.0, whose tasks have all completed, from pool 
21/01/20 15:42:33 INFO DAGScheduler: ResultStage 115 (parquet at Generate.java:61) finished in 1.044 s
21/01/20 15:42:33 INFO DAGScheduler: Job 115 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:42:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 115: Stage finished
21/01/20 15:42:33 INFO DAGScheduler: Job 115 finished: parquet at Generate.java:61, took 1.044758 s
21/01/20 15:42:33 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-ecc99fd5-64c6-4ccd-8814-471222a3a6aa
21/01/20 15:42:33 INFO FileFormatWriter: Write Job d2f4d94a-452b-4512-b2fd-513da59d30b9 committed.
21/01/20 15:42:33 INFO FileFormatWriter: Finished processing stats for write job d2f4d94a-452b-4512-b2fd-513da59d30b9.
21/01/20 15:42:34 INFO BlockManagerInfo: Removed broadcast_115_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:42:35 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:35 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:35 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:42:35 INFO DAGScheduler: Got job 116 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:42:35 INFO DAGScheduler: Final stage: ResultStage 116 (parquet at Generate.java:61)
21/01/20 15:42:35 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:42:35 INFO DAGScheduler: Missing parents: List()
21/01/20 15:42:35 INFO DAGScheduler: Submitting ResultStage 116 (MapPartitionsRDD[465] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:42:35 INFO MemoryStore: Block broadcast_116 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:42:35 INFO MemoryStore: Block broadcast_116_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:42:35 INFO BlockManagerInfo: Added broadcast_116_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:42:35 INFO SparkContext: Created broadcast 116 from broadcast at DAGScheduler.scala:1200
21/01/20 15:42:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 116 (MapPartitionsRDD[465] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:42:35 INFO TaskSchedulerImpl: Adding task set 116.0 with 1 tasks
21/01/20 15:42:35 WARN TaskSetManager: Stage 116 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:42:35 INFO TaskSetManager: Starting task 0.0 in stage 116.0 (TID 116, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:42:35 INFO Executor: Running task 0.0 in stage 116.0 (TID 116)
21/01/20 15:42:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:35 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:35 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:42:35 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:42:35 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:42:35 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:42:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:42:35 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:42:35 INFO ParquetOutputFormat: Validation is off
21/01/20 15:42:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:42:35 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:42:35 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:42:35 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:42:35 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:42:35 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:42:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:42:36 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-33903C642C4F->d3c4d3e3-944b-4528-9d82-e005ad427697
21/01/20 15:42:36 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:42:36 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154235_0116_m_000000_116' to o3fs://bucket1.vol1/testdata
21/01/20 15:42:36 INFO SparkHadoopMapRedUtil: attempt_20210120154235_0116_m_000000_116: Committed
21/01/20 15:42:36 INFO Executor: Finished task 0.0 in stage 116.0 (TID 116). 2155 bytes result sent to driver
21/01/20 15:42:36 INFO TaskSetManager: Finished task 0.0 in stage 116.0 (TID 116) in 1042 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:42:36 INFO TaskSchedulerImpl: Removed TaskSet 116.0, whose tasks have all completed, from pool 
21/01/20 15:42:36 INFO DAGScheduler: ResultStage 116 (parquet at Generate.java:61) finished in 1.060 s
21/01/20 15:42:36 INFO DAGScheduler: Job 116 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:42:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 116: Stage finished
21/01/20 15:42:36 INFO DAGScheduler: Job 116 finished: parquet at Generate.java:61, took 1.062210 s
21/01/20 15:42:36 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-592ca38f-0c84-4f67-b434-f53c2f80f790
21/01/20 15:42:36 INFO FileFormatWriter: Write Job f05d5ae1-bbea-47bd-b963-e4cb058d80e8 committed.
21/01/20 15:42:36 INFO FileFormatWriter: Finished processing stats for write job f05d5ae1-bbea-47bd-b963-e4cb058d80e8.
21/01/20 15:42:38 INFO BlockManagerInfo: Removed broadcast_116_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:42:39 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:39 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:39 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:42:39 INFO DAGScheduler: Got job 117 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:42:39 INFO DAGScheduler: Final stage: ResultStage 117 (parquet at Generate.java:61)
21/01/20 15:42:39 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:42:39 INFO DAGScheduler: Missing parents: List()
21/01/20 15:42:39 INFO DAGScheduler: Submitting ResultStage 117 (MapPartitionsRDD[469] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:42:39 INFO MemoryStore: Block broadcast_117 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:42:39 INFO MemoryStore: Block broadcast_117_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:42:39 INFO BlockManagerInfo: Added broadcast_117_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:42:39 INFO SparkContext: Created broadcast 117 from broadcast at DAGScheduler.scala:1200
21/01/20 15:42:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 117 (MapPartitionsRDD[469] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:42:39 INFO TaskSchedulerImpl: Adding task set 117.0 with 1 tasks
21/01/20 15:42:39 WARN TaskSetManager: Stage 117 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:42:39 INFO TaskSetManager: Starting task 0.0 in stage 117.0 (TID 117, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:42:39 INFO Executor: Running task 0.0 in stage 117.0 (TID 117)
21/01/20 15:42:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:39 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:39 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:42:39 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:42:39 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:42:39 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:42:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:42:39 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:42:39 INFO ParquetOutputFormat: Validation is off
21/01/20 15:42:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:42:39 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:42:39 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:42:39 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:42:39 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:42:39 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:42:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:42:40 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154239_0117_m_000000_117' to o3fs://bucket1.vol1/testdata
21/01/20 15:42:40 INFO SparkHadoopMapRedUtil: attempt_20210120154239_0117_m_000000_117: Committed
21/01/20 15:42:40 INFO Executor: Finished task 0.0 in stage 117.0 (TID 117). 2155 bytes result sent to driver
21/01/20 15:42:40 INFO TaskSetManager: Finished task 0.0 in stage 117.0 (TID 117) in 741 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:42:40 INFO TaskSchedulerImpl: Removed TaskSet 117.0, whose tasks have all completed, from pool 
21/01/20 15:42:40 INFO DAGScheduler: ResultStage 117 (parquet at Generate.java:61) finished in 0.781 s
21/01/20 15:42:40 INFO DAGScheduler: Job 117 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:42:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 117: Stage finished
21/01/20 15:42:40 INFO DAGScheduler: Job 117 finished: parquet at Generate.java:61, took 0.783078 s
21/01/20 15:42:40 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-c3e66a1d-1b34-4bb9-996d-74438e8a639a
21/01/20 15:42:40 INFO FileFormatWriter: Write Job 8536db65-eb11-4e44-8ffc-579536ee1507 committed.
21/01/20 15:42:40 INFO FileFormatWriter: Finished processing stats for write job 8536db65-eb11-4e44-8ffc-579536ee1507.
21/01/20 15:42:40 INFO BlockManagerInfo: Removed broadcast_117_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:42:42 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:42 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:42 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:42:42 INFO DAGScheduler: Got job 118 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:42:42 INFO DAGScheduler: Final stage: ResultStage 118 (parquet at Generate.java:61)
21/01/20 15:42:42 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:42:42 INFO DAGScheduler: Missing parents: List()
21/01/20 15:42:42 INFO DAGScheduler: Submitting ResultStage 118 (MapPartitionsRDD[473] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:42:42 INFO MemoryStore: Block broadcast_118 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:42:42 INFO MemoryStore: Block broadcast_118_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:42:42 INFO BlockManagerInfo: Added broadcast_118_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:42:42 INFO SparkContext: Created broadcast 118 from broadcast at DAGScheduler.scala:1200
21/01/20 15:42:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 118 (MapPartitionsRDD[473] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:42:42 INFO TaskSchedulerImpl: Adding task set 118.0 with 1 tasks
21/01/20 15:42:42 WARN TaskSetManager: Stage 118 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:42:42 INFO TaskSetManager: Starting task 0.0 in stage 118.0 (TID 118, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:42:42 INFO Executor: Running task 0.0 in stage 118.0 (TID 118)
21/01/20 15:42:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:42 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:42 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:42:42 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:42:42 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:42:42 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:42:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:42:42 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:42:42 INFO ParquetOutputFormat: Validation is off
21/01/20 15:42:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:42:42 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:42:42 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:42:42 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:42:42 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:42:42 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:42:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:42:43 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154242_0118_m_000000_118' to o3fs://bucket1.vol1/testdata
21/01/20 15:42:43 INFO SparkHadoopMapRedUtil: attempt_20210120154242_0118_m_000000_118: Committed
21/01/20 15:42:43 INFO Executor: Finished task 0.0 in stage 118.0 (TID 118). 2155 bytes result sent to driver
21/01/20 15:42:43 INFO TaskSetManager: Finished task 0.0 in stage 118.0 (TID 118) in 968 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:42:43 INFO TaskSchedulerImpl: Removed TaskSet 118.0, whose tasks have all completed, from pool 
21/01/20 15:42:43 INFO DAGScheduler: ResultStage 118 (parquet at Generate.java:61) finished in 0.986 s
21/01/20 15:42:43 INFO DAGScheduler: Job 118 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:42:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 118: Stage finished
21/01/20 15:42:43 INFO DAGScheduler: Job 118 finished: parquet at Generate.java:61, took 0.987723 s
21/01/20 15:42:43 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-5585f937-5d5f-4d27-a265-2303e9925cf0
21/01/20 15:42:43 INFO FileFormatWriter: Write Job 6e80f441-9509-413e-971a-4849af60735e committed.
21/01/20 15:42:43 INFO FileFormatWriter: Finished processing stats for write job 6e80f441-9509-413e-971a-4849af60735e.
21/01/20 15:42:44 INFO BlockManagerInfo: Removed broadcast_118_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:42:46 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:46 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:46 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:42:46 INFO DAGScheduler: Got job 119 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:42:46 INFO DAGScheduler: Final stage: ResultStage 119 (parquet at Generate.java:61)
21/01/20 15:42:46 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:42:46 INFO DAGScheduler: Missing parents: List()
21/01/20 15:42:46 INFO DAGScheduler: Submitting ResultStage 119 (MapPartitionsRDD[477] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:42:46 INFO MemoryStore: Block broadcast_119 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:42:46 INFO MemoryStore: Block broadcast_119_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:42:46 INFO BlockManagerInfo: Added broadcast_119_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:42:46 INFO SparkContext: Created broadcast 119 from broadcast at DAGScheduler.scala:1200
21/01/20 15:42:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 119 (MapPartitionsRDD[477] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:42:46 INFO TaskSchedulerImpl: Adding task set 119.0 with 1 tasks
21/01/20 15:42:46 WARN TaskSetManager: Stage 119 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:42:46 INFO TaskSetManager: Starting task 0.0 in stage 119.0 (TID 119, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:42:46 INFO Executor: Running task 0.0 in stage 119.0 (TID 119)
21/01/20 15:42:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:46 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:46 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:42:46 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:42:46 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:42:46 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:42:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:42:46 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:42:46 INFO ParquetOutputFormat: Validation is off
21/01/20 15:42:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:42:46 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:42:46 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:42:46 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:42:46 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:42:46 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:42:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:42:47 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154246_0119_m_000000_119' to o3fs://bucket1.vol1/testdata
21/01/20 15:42:47 INFO SparkHadoopMapRedUtil: attempt_20210120154246_0119_m_000000_119: Committed
21/01/20 15:42:47 INFO Executor: Finished task 0.0 in stage 119.0 (TID 119). 2155 bytes result sent to driver
21/01/20 15:42:47 INFO TaskSetManager: Finished task 0.0 in stage 119.0 (TID 119) in 784 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:42:47 INFO TaskSchedulerImpl: Removed TaskSet 119.0, whose tasks have all completed, from pool 
21/01/20 15:42:47 INFO DAGScheduler: ResultStage 119 (parquet at Generate.java:61) finished in 0.826 s
21/01/20 15:42:47 INFO DAGScheduler: Job 119 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:42:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 119: Stage finished
21/01/20 15:42:47 INFO DAGScheduler: Job 119 finished: parquet at Generate.java:61, took 0.827155 s
21/01/20 15:42:47 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-6fc252d4-e0ce-469a-8794-50e5dffac15c
21/01/20 15:42:47 INFO FileFormatWriter: Write Job a1fba925-0aa3-4956-ad9c-5017c2d8688c committed.
21/01/20 15:42:47 INFO FileFormatWriter: Finished processing stats for write job a1fba925-0aa3-4956-ad9c-5017c2d8688c.
21/01/20 15:42:47 INFO BlockManagerInfo: Removed broadcast_119_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:42:49 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:49 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:42:49 INFO DAGScheduler: Got job 120 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:42:49 INFO DAGScheduler: Final stage: ResultStage 120 (parquet at Generate.java:61)
21/01/20 15:42:49 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:42:49 INFO DAGScheduler: Missing parents: List()
21/01/20 15:42:49 INFO DAGScheduler: Submitting ResultStage 120 (MapPartitionsRDD[481] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:42:49 INFO MemoryStore: Block broadcast_120 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:42:49 INFO MemoryStore: Block broadcast_120_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:42:49 INFO BlockManagerInfo: Added broadcast_120_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:42:49 INFO SparkContext: Created broadcast 120 from broadcast at DAGScheduler.scala:1200
21/01/20 15:42:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 120 (MapPartitionsRDD[481] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:42:49 INFO TaskSchedulerImpl: Adding task set 120.0 with 1 tasks
21/01/20 15:42:49 WARN TaskSetManager: Stage 120 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:42:49 INFO TaskSetManager: Starting task 0.0 in stage 120.0 (TID 120, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:42:49 INFO Executor: Running task 0.0 in stage 120.0 (TID 120)
21/01/20 15:42:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:49 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:42:49 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:42:49 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:42:49 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:42:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:42:49 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:42:49 INFO ParquetOutputFormat: Validation is off
21/01/20 15:42:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:42:49 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:42:49 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:42:49 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:42:49 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:42:49 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:42:50 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:42:50 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-12612D4EBCAE->d3c4d3e3-944b-4528-9d82-e005ad427697
21/01/20 15:42:50 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:42:50 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154249_0120_m_000000_120' to o3fs://bucket1.vol1/testdata
21/01/20 15:42:50 INFO SparkHadoopMapRedUtil: attempt_20210120154249_0120_m_000000_120: Committed
21/01/20 15:42:50 INFO Executor: Finished task 0.0 in stage 120.0 (TID 120). 2155 bytes result sent to driver
21/01/20 15:42:50 INFO TaskSetManager: Finished task 0.0 in stage 120.0 (TID 120) in 1137 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:42:50 INFO TaskSchedulerImpl: Removed TaskSet 120.0, whose tasks have all completed, from pool 
21/01/20 15:42:50 INFO DAGScheduler: ResultStage 120 (parquet at Generate.java:61) finished in 1.158 s
21/01/20 15:42:50 INFO DAGScheduler: Job 120 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:42:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 120: Stage finished
21/01/20 15:42:50 INFO DAGScheduler: Job 120 finished: parquet at Generate.java:61, took 1.158968 s
21/01/20 15:42:50 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-10c09a62-66b3-43ff-b66e-f16a63e2b2d7
21/01/20 15:42:50 INFO FileFormatWriter: Write Job 9441a543-56e7-43c4-acf8-800c9075a86c committed.
21/01/20 15:42:50 INFO FileFormatWriter: Finished processing stats for write job 9441a543-56e7-43c4-acf8-800c9075a86c.
21/01/20 15:42:52 INFO BlockManagerInfo: Removed broadcast_120_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:42:53 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:53 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:42:53 INFO DAGScheduler: Got job 121 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:42:53 INFO DAGScheduler: Final stage: ResultStage 121 (parquet at Generate.java:61)
21/01/20 15:42:53 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:42:53 INFO DAGScheduler: Missing parents: List()
21/01/20 15:42:53 INFO DAGScheduler: Submitting ResultStage 121 (MapPartitionsRDD[485] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:42:53 INFO MemoryStore: Block broadcast_121 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:42:53 INFO MemoryStore: Block broadcast_121_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:42:53 INFO BlockManagerInfo: Added broadcast_121_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:42:53 INFO SparkContext: Created broadcast 121 from broadcast at DAGScheduler.scala:1200
21/01/20 15:42:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 121 (MapPartitionsRDD[485] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:42:53 INFO TaskSchedulerImpl: Adding task set 121.0 with 1 tasks
21/01/20 15:42:53 WARN TaskSetManager: Stage 121 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:42:53 INFO TaskSetManager: Starting task 0.0 in stage 121.0 (TID 121, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:42:53 INFO Executor: Running task 0.0 in stage 121.0 (TID 121)
21/01/20 15:42:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:53 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:42:53 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:42:53 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:42:53 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:42:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:42:53 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:42:53 INFO ParquetOutputFormat: Validation is off
21/01/20 15:42:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:42:53 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:42:53 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:42:53 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:42:53 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:42:53 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:42:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:42:54 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154253_0121_m_000000_121' to o3fs://bucket1.vol1/testdata
21/01/20 15:42:54 INFO SparkHadoopMapRedUtil: attempt_20210120154253_0121_m_000000_121: Committed
21/01/20 15:42:54 INFO Executor: Finished task 0.0 in stage 121.0 (TID 121). 2155 bytes result sent to driver
21/01/20 15:42:54 INFO TaskSetManager: Finished task 0.0 in stage 121.0 (TID 121) in 872 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:42:54 INFO TaskSchedulerImpl: Removed TaskSet 121.0, whose tasks have all completed, from pool 
21/01/20 15:42:54 INFO DAGScheduler: ResultStage 121 (parquet at Generate.java:61) finished in 0.914 s
21/01/20 15:42:54 INFO DAGScheduler: Job 121 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:42:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 121: Stage finished
21/01/20 15:42:54 INFO DAGScheduler: Job 121 finished: parquet at Generate.java:61, took 0.915570 s
21/01/20 15:42:54 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-1f0f0cc0-4130-452e-9802-0368918a4947
21/01/20 15:42:54 INFO FileFormatWriter: Write Job 0490feb9-f4dc-40fa-82c5-8b3818c55280 committed.
21/01/20 15:42:54 INFO FileFormatWriter: Finished processing stats for write job 0490feb9-f4dc-40fa-82c5-8b3818c55280.
21/01/20 15:42:54 INFO BlockManagerInfo: Removed broadcast_121_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:42:57 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:57 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:57 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:42:57 INFO DAGScheduler: Got job 122 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:42:57 INFO DAGScheduler: Final stage: ResultStage 122 (parquet at Generate.java:61)
21/01/20 15:42:57 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:42:57 INFO DAGScheduler: Missing parents: List()
21/01/20 15:42:57 INFO DAGScheduler: Submitting ResultStage 122 (MapPartitionsRDD[489] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:42:57 INFO MemoryStore: Block broadcast_122 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:42:57 INFO MemoryStore: Block broadcast_122_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:42:57 INFO BlockManagerInfo: Added broadcast_122_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:42:57 INFO SparkContext: Created broadcast 122 from broadcast at DAGScheduler.scala:1200
21/01/20 15:42:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 122 (MapPartitionsRDD[489] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:42:57 INFO TaskSchedulerImpl: Adding task set 122.0 with 1 tasks
21/01/20 15:42:57 WARN TaskSetManager: Stage 122 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:42:57 INFO TaskSetManager: Starting task 0.0 in stage 122.0 (TID 122, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:42:57 INFO Executor: Running task 0.0 in stage 122.0 (TID 122)
21/01/20 15:42:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:57 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:42:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:42:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:42:57 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:42:57 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:42:57 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:42:57 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:42:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:42:57 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:42:57 INFO ParquetOutputFormat: Validation is off
21/01/20 15:42:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:42:57 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:42:57 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:42:57 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:42:57 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:42:57 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:42:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:42:58 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154257_0122_m_000000_122' to o3fs://bucket1.vol1/testdata
21/01/20 15:42:58 INFO SparkHadoopMapRedUtil: attempt_20210120154257_0122_m_000000_122: Committed
21/01/20 15:42:58 INFO Executor: Finished task 0.0 in stage 122.0 (TID 122). 2155 bytes result sent to driver
21/01/20 15:42:58 INFO TaskSetManager: Finished task 0.0 in stage 122.0 (TID 122) in 962 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:42:58 INFO TaskSchedulerImpl: Removed TaskSet 122.0, whose tasks have all completed, from pool 
21/01/20 15:42:58 INFO DAGScheduler: ResultStage 122 (parquet at Generate.java:61) finished in 0.981 s
21/01/20 15:42:58 INFO DAGScheduler: Job 122 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:42:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 122: Stage finished
21/01/20 15:42:58 INFO DAGScheduler: Job 122 finished: parquet at Generate.java:61, took 0.982828 s
21/01/20 15:42:58 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-475d23ae-925e-4951-87f1-3bf17caa3906
21/01/20 15:42:58 INFO FileFormatWriter: Write Job 4b0ac178-cb99-4ec5-8eff-f7aaa2a47d41 committed.
21/01/20 15:42:58 INFO FileFormatWriter: Finished processing stats for write job 4b0ac178-cb99-4ec5-8eff-f7aaa2a47d41.
21/01/20 15:42:59 INFO BlockManagerInfo: Removed broadcast_122_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:43:00 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:00 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:43:00 INFO DAGScheduler: Got job 123 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:43:00 INFO DAGScheduler: Final stage: ResultStage 123 (parquet at Generate.java:61)
21/01/20 15:43:00 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:43:00 INFO DAGScheduler: Missing parents: List()
21/01/20 15:43:00 INFO DAGScheduler: Submitting ResultStage 123 (MapPartitionsRDD[493] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:43:00 INFO MemoryStore: Block broadcast_123 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:43:00 INFO MemoryStore: Block broadcast_123_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:43:00 INFO BlockManagerInfo: Added broadcast_123_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:43:00 INFO SparkContext: Created broadcast 123 from broadcast at DAGScheduler.scala:1200
21/01/20 15:43:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 123 (MapPartitionsRDD[493] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:43:00 INFO TaskSchedulerImpl: Adding task set 123.0 with 1 tasks
21/01/20 15:43:00 WARN TaskSetManager: Stage 123 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:43:00 INFO TaskSetManager: Starting task 0.0 in stage 123.0 (TID 123, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:43:00 INFO Executor: Running task 0.0 in stage 123.0 (TID 123)
21/01/20 15:43:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:00 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:43:00 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:43:00 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:43:00 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:43:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:43:00 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:43:00 INFO ParquetOutputFormat: Validation is off
21/01/20 15:43:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:43:00 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:43:00 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:43:00 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:43:00 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:43:00 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:43:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:43:01 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154300_0123_m_000000_123' to o3fs://bucket1.vol1/testdata
21/01/20 15:43:01 INFO SparkHadoopMapRedUtil: attempt_20210120154300_0123_m_000000_123: Committed
21/01/20 15:43:01 INFO Executor: Finished task 0.0 in stage 123.0 (TID 123). 2155 bytes result sent to driver
21/01/20 15:43:01 INFO TaskSetManager: Finished task 0.0 in stage 123.0 (TID 123) in 848 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:43:01 INFO TaskSchedulerImpl: Removed TaskSet 123.0, whose tasks have all completed, from pool 
21/01/20 15:43:01 INFO DAGScheduler: ResultStage 123 (parquet at Generate.java:61) finished in 0.888 s
21/01/20 15:43:01 INFO DAGScheduler: Job 123 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:43:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 123: Stage finished
21/01/20 15:43:01 INFO DAGScheduler: Job 123 finished: parquet at Generate.java:61, took 0.892574 s
21/01/20 15:43:01 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-974fba23-c727-4956-b111-7bd53ce3b38f
21/01/20 15:43:01 INFO FileFormatWriter: Write Job 13161789-5288-421f-9cb0-8e347864e316 committed.
21/01/20 15:43:01 INFO FileFormatWriter: Finished processing stats for write job 13161789-5288-421f-9cb0-8e347864e316.
21/01/20 15:43:01 INFO BlockManagerInfo: Removed broadcast_123_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:43:04 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:04 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:43:04 INFO DAGScheduler: Got job 124 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:43:04 INFO DAGScheduler: Final stage: ResultStage 124 (parquet at Generate.java:61)
21/01/20 15:43:04 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:43:04 INFO DAGScheduler: Missing parents: List()
21/01/20 15:43:04 INFO DAGScheduler: Submitting ResultStage 124 (MapPartitionsRDD[497] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:43:04 INFO MemoryStore: Block broadcast_124 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:43:04 INFO MemoryStore: Block broadcast_124_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:43:04 INFO BlockManagerInfo: Added broadcast_124_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:43:04 INFO SparkContext: Created broadcast 124 from broadcast at DAGScheduler.scala:1200
21/01/20 15:43:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 124 (MapPartitionsRDD[497] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:43:04 INFO TaskSchedulerImpl: Adding task set 124.0 with 1 tasks
21/01/20 15:43:04 WARN TaskSetManager: Stage 124 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:43:04 INFO TaskSetManager: Starting task 0.0 in stage 124.0 (TID 124, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:43:04 INFO Executor: Running task 0.0 in stage 124.0 (TID 124)
21/01/20 15:43:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:04 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:43:04 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:43:04 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:43:04 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:43:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:43:04 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:43:04 INFO ParquetOutputFormat: Validation is off
21/01/20 15:43:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:43:04 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:43:04 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:43:04 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:43:04 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:43:04 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:43:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:43:04 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-D2415283241B->d3c4d3e3-944b-4528-9d82-e005ad427697
21/01/20 15:43:04 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:43:05 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154304_0124_m_000000_124' to o3fs://bucket1.vol1/testdata
21/01/20 15:43:05 INFO SparkHadoopMapRedUtil: attempt_20210120154304_0124_m_000000_124: Committed
21/01/20 15:43:05 INFO Executor: Finished task 0.0 in stage 124.0 (TID 124). 2155 bytes result sent to driver
21/01/20 15:43:05 INFO TaskSetManager: Finished task 0.0 in stage 124.0 (TID 124) in 1050 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:43:05 INFO TaskSchedulerImpl: Removed TaskSet 124.0, whose tasks have all completed, from pool 
21/01/20 15:43:05 INFO DAGScheduler: ResultStage 124 (parquet at Generate.java:61) finished in 1.068 s
21/01/20 15:43:05 INFO DAGScheduler: Job 124 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:43:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 124: Stage finished
21/01/20 15:43:05 INFO DAGScheduler: Job 124 finished: parquet at Generate.java:61, took 1.069477 s
21/01/20 15:43:05 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-ccfb0d44-548c-42aa-b699-1adfa58b1aa1
21/01/20 15:43:05 INFO FileFormatWriter: Write Job 4239f16f-b29c-466f-857d-53a64ada0793 committed.
21/01/20 15:43:05 INFO FileFormatWriter: Finished processing stats for write job 4239f16f-b29c-466f-857d-53a64ada0793.
21/01/20 15:43:06 INFO BlockManagerInfo: Removed broadcast_124_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:43:07 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:07 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:07 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:43:07 INFO DAGScheduler: Got job 125 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:43:07 INFO DAGScheduler: Final stage: ResultStage 125 (parquet at Generate.java:61)
21/01/20 15:43:07 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:43:07 INFO DAGScheduler: Missing parents: List()
21/01/20 15:43:07 INFO DAGScheduler: Submitting ResultStage 125 (MapPartitionsRDD[501] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:43:07 INFO MemoryStore: Block broadcast_125 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:43:07 INFO MemoryStore: Block broadcast_125_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:43:07 INFO BlockManagerInfo: Added broadcast_125_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:43:07 INFO SparkContext: Created broadcast 125 from broadcast at DAGScheduler.scala:1200
21/01/20 15:43:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 125 (MapPartitionsRDD[501] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:43:07 INFO TaskSchedulerImpl: Adding task set 125.0 with 1 tasks
21/01/20 15:43:07 WARN TaskSetManager: Stage 125 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:43:07 INFO TaskSetManager: Starting task 0.0 in stage 125.0 (TID 125, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:43:07 INFO Executor: Running task 0.0 in stage 125.0 (TID 125)
21/01/20 15:43:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:07 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:07 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:43:07 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:43:07 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:43:07 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:43:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:43:07 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:43:07 INFO ParquetOutputFormat: Validation is off
21/01/20 15:43:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:43:07 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:43:07 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:43:07 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:43:07 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:43:07 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:43:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:43:08 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154307_0125_m_000000_125' to o3fs://bucket1.vol1/testdata
21/01/20 15:43:08 INFO SparkHadoopMapRedUtil: attempt_20210120154307_0125_m_000000_125: Committed
21/01/20 15:43:08 INFO Executor: Finished task 0.0 in stage 125.0 (TID 125). 2155 bytes result sent to driver
21/01/20 15:43:08 INFO TaskSetManager: Finished task 0.0 in stage 125.0 (TID 125) in 876 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:43:08 INFO TaskSchedulerImpl: Removed TaskSet 125.0, whose tasks have all completed, from pool 
21/01/20 15:43:08 INFO DAGScheduler: ResultStage 125 (parquet at Generate.java:61) finished in 0.917 s
21/01/20 15:43:08 INFO DAGScheduler: Job 125 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:43:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 125: Stage finished
21/01/20 15:43:08 INFO DAGScheduler: Job 125 finished: parquet at Generate.java:61, took 0.919072 s
21/01/20 15:43:08 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-8ff6c3f4-f0f2-4df2-8b0a-651bfc5ed403
21/01/20 15:43:08 INFO FileFormatWriter: Write Job dd9a5b73-dc8d-4a51-a51b-77d4b6596d13 committed.
21/01/20 15:43:08 INFO FileFormatWriter: Finished processing stats for write job dd9a5b73-dc8d-4a51-a51b-77d4b6596d13.
21/01/20 15:43:08 INFO BlockManagerInfo: Removed broadcast_125_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:43:11 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:11 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:43:11 INFO DAGScheduler: Got job 126 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:43:11 INFO DAGScheduler: Final stage: ResultStage 126 (parquet at Generate.java:61)
21/01/20 15:43:11 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:43:11 INFO DAGScheduler: Missing parents: List()
21/01/20 15:43:11 INFO DAGScheduler: Submitting ResultStage 126 (MapPartitionsRDD[505] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:43:11 INFO MemoryStore: Block broadcast_126 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:43:11 INFO MemoryStore: Block broadcast_126_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:43:11 INFO BlockManagerInfo: Added broadcast_126_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:43:11 INFO SparkContext: Created broadcast 126 from broadcast at DAGScheduler.scala:1200
21/01/20 15:43:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 126 (MapPartitionsRDD[505] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:43:11 INFO TaskSchedulerImpl: Adding task set 126.0 with 1 tasks
21/01/20 15:43:11 WARN TaskSetManager: Stage 126 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:43:11 INFO TaskSetManager: Starting task 0.0 in stage 126.0 (TID 126, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:43:11 INFO Executor: Running task 0.0 in stage 126.0 (TID 126)
21/01/20 15:43:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:11 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:43:11 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:43:11 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:43:11 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:43:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:43:11 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:43:11 INFO ParquetOutputFormat: Validation is off
21/01/20 15:43:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:43:11 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:43:11 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:43:11 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:43:11 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:43:11 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:43:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:43:12 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154311_0126_m_000000_126' to o3fs://bucket1.vol1/testdata
21/01/20 15:43:12 INFO SparkHadoopMapRedUtil: attempt_20210120154311_0126_m_000000_126: Committed
21/01/20 15:43:12 INFO Executor: Finished task 0.0 in stage 126.0 (TID 126). 2155 bytes result sent to driver
21/01/20 15:43:12 INFO TaskSetManager: Finished task 0.0 in stage 126.0 (TID 126) in 1044 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:43:12 INFO TaskSchedulerImpl: Removed TaskSet 126.0, whose tasks have all completed, from pool 
21/01/20 15:43:12 INFO DAGScheduler: ResultStage 126 (parquet at Generate.java:61) finished in 1.061 s
21/01/20 15:43:12 INFO DAGScheduler: Job 126 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:43:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 126: Stage finished
21/01/20 15:43:12 INFO DAGScheduler: Job 126 finished: parquet at Generate.java:61, took 1.063034 s
21/01/20 15:43:12 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-e3ebfab5-e410-4945-a6eb-532050d0743b
21/01/20 15:43:12 INFO FileFormatWriter: Write Job d7da9756-c494-480f-b4fa-93cd91e67a9f committed.
21/01/20 15:43:12 INFO FileFormatWriter: Finished processing stats for write job d7da9756-c494-480f-b4fa-93cd91e67a9f.
21/01/20 15:43:13 INFO BlockManagerInfo: Removed broadcast_126_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:43:15 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:15 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:15 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:43:15 INFO DAGScheduler: Got job 127 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:43:15 INFO DAGScheduler: Final stage: ResultStage 127 (parquet at Generate.java:61)
21/01/20 15:43:15 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:43:15 INFO DAGScheduler: Missing parents: List()
21/01/20 15:43:15 INFO DAGScheduler: Submitting ResultStage 127 (MapPartitionsRDD[509] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:43:15 INFO MemoryStore: Block broadcast_127 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:43:15 INFO MemoryStore: Block broadcast_127_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:43:15 INFO BlockManagerInfo: Added broadcast_127_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:43:15 INFO SparkContext: Created broadcast 127 from broadcast at DAGScheduler.scala:1200
21/01/20 15:43:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 127 (MapPartitionsRDD[509] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:43:15 INFO TaskSchedulerImpl: Adding task set 127.0 with 1 tasks
21/01/20 15:43:15 WARN TaskSetManager: Stage 127 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:43:15 INFO TaskSetManager: Starting task 0.0 in stage 127.0 (TID 127, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:43:15 INFO Executor: Running task 0.0 in stage 127.0 (TID 127)
21/01/20 15:43:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:15 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:15 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:43:15 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:43:15 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:43:15 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:43:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:43:15 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:43:15 INFO ParquetOutputFormat: Validation is off
21/01/20 15:43:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:43:15 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:43:15 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:43:15 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:43:15 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:43:15 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:43:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:43:15 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-387C309163D6->d3c4d3e3-944b-4528-9d82-e005ad427697
21/01/20 15:43:15 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:43:16 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154315_0127_m_000000_127' to o3fs://bucket1.vol1/testdata
21/01/20 15:43:16 INFO SparkHadoopMapRedUtil: attempt_20210120154315_0127_m_000000_127: Committed
21/01/20 15:43:16 INFO Executor: Finished task 0.0 in stage 127.0 (TID 127). 2155 bytes result sent to driver
21/01/20 15:43:16 INFO TaskSetManager: Finished task 0.0 in stage 127.0 (TID 127) in 1100 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:43:16 INFO TaskSchedulerImpl: Removed TaskSet 127.0, whose tasks have all completed, from pool 
21/01/20 15:43:16 INFO DAGScheduler: ResultStage 127 (parquet at Generate.java:61) finished in 1.120 s
21/01/20 15:43:16 INFO DAGScheduler: Job 127 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:43:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 127: Stage finished
21/01/20 15:43:16 INFO DAGScheduler: Job 127 finished: parquet at Generate.java:61, took 1.122477 s
21/01/20 15:43:16 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-f144c842-046f-4a6b-aab3-7e9c429a4b55
21/01/20 15:43:16 INFO FileFormatWriter: Write Job 56b10ff4-5f56-4954-95bb-c1faad831d7e committed.
21/01/20 15:43:16 INFO FileFormatWriter: Finished processing stats for write job 56b10ff4-5f56-4954-95bb-c1faad831d7e.
21/01/20 15:43:17 INFO BlockManagerInfo: Removed broadcast_127_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:43:18 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:18 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:18 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:43:18 INFO DAGScheduler: Got job 128 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:43:18 INFO DAGScheduler: Final stage: ResultStage 128 (parquet at Generate.java:61)
21/01/20 15:43:18 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:43:18 INFO DAGScheduler: Missing parents: List()
21/01/20 15:43:18 INFO DAGScheduler: Submitting ResultStage 128 (MapPartitionsRDD[513] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:43:18 INFO MemoryStore: Block broadcast_128 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:43:18 INFO MemoryStore: Block broadcast_128_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:43:18 INFO BlockManagerInfo: Added broadcast_128_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:43:18 INFO SparkContext: Created broadcast 128 from broadcast at DAGScheduler.scala:1200
21/01/20 15:43:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 128 (MapPartitionsRDD[513] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:43:18 INFO TaskSchedulerImpl: Adding task set 128.0 with 1 tasks
21/01/20 15:43:18 WARN TaskSetManager: Stage 128 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:43:18 INFO TaskSetManager: Starting task 0.0 in stage 128.0 (TID 128, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:43:18 INFO Executor: Running task 0.0 in stage 128.0 (TID 128)
21/01/20 15:43:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:18 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:18 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:43:18 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:43:18 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:43:18 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:43:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:43:18 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:43:18 INFO ParquetOutputFormat: Validation is off
21/01/20 15:43:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:43:18 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:43:18 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:43:18 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:43:18 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:43:18 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:43:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:43:19 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154318_0128_m_000000_128' to o3fs://bucket1.vol1/testdata
21/01/20 15:43:19 INFO SparkHadoopMapRedUtil: attempt_20210120154318_0128_m_000000_128: Committed
21/01/20 15:43:19 INFO Executor: Finished task 0.0 in stage 128.0 (TID 128). 2155 bytes result sent to driver
21/01/20 15:43:19 INFO TaskSetManager: Finished task 0.0 in stage 128.0 (TID 128) in 1030 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:43:19 INFO TaskSchedulerImpl: Removed TaskSet 128.0, whose tasks have all completed, from pool 
21/01/20 15:43:19 INFO DAGScheduler: ResultStage 128 (parquet at Generate.java:61) finished in 1.048 s
21/01/20 15:43:19 INFO DAGScheduler: Job 128 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:43:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 128: Stage finished
21/01/20 15:43:19 INFO DAGScheduler: Job 128 finished: parquet at Generate.java:61, took 1.049658 s
21/01/20 15:43:19 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-187cef72-ef28-4a80-a470-5be68efaebe6
21/01/20 15:43:19 INFO FileFormatWriter: Write Job d2080a02-552c-492c-b006-9a80ec5bd010 committed.
21/01/20 15:43:19 INFO FileFormatWriter: Finished processing stats for write job d2080a02-552c-492c-b006-9a80ec5bd010.
21/01/20 15:43:20 INFO BlockManagerInfo: Removed broadcast_128_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:43:22 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:22 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:22 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:43:22 INFO DAGScheduler: Got job 129 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:43:22 INFO DAGScheduler: Final stage: ResultStage 129 (parquet at Generate.java:61)
21/01/20 15:43:22 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:43:22 INFO DAGScheduler: Missing parents: List()
21/01/20 15:43:22 INFO DAGScheduler: Submitting ResultStage 129 (MapPartitionsRDD[517] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:43:22 INFO MemoryStore: Block broadcast_129 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:43:22 INFO MemoryStore: Block broadcast_129_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:43:22 INFO BlockManagerInfo: Added broadcast_129_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:43:22 INFO SparkContext: Created broadcast 129 from broadcast at DAGScheduler.scala:1200
21/01/20 15:43:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 129 (MapPartitionsRDD[517] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:43:22 INFO TaskSchedulerImpl: Adding task set 129.0 with 1 tasks
21/01/20 15:43:22 WARN TaskSetManager: Stage 129 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:43:22 INFO TaskSetManager: Starting task 0.0 in stage 129.0 (TID 129, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:43:22 INFO Executor: Running task 0.0 in stage 129.0 (TID 129)
21/01/20 15:43:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:22 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:22 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:43:22 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:43:22 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:43:22 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:43:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:43:22 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:43:22 INFO ParquetOutputFormat: Validation is off
21/01/20 15:43:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:43:22 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:43:22 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:43:22 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:43:22 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:43:22 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:43:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:43:23 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154322_0129_m_000000_129' to o3fs://bucket1.vol1/testdata
21/01/20 15:43:23 INFO SparkHadoopMapRedUtil: attempt_20210120154322_0129_m_000000_129: Committed
21/01/20 15:43:23 INFO Executor: Finished task 0.0 in stage 129.0 (TID 129). 2155 bytes result sent to driver
21/01/20 15:43:23 INFO TaskSetManager: Finished task 0.0 in stage 129.0 (TID 129) in 1093 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:43:23 INFO TaskSchedulerImpl: Removed TaskSet 129.0, whose tasks have all completed, from pool 
21/01/20 15:43:23 INFO DAGScheduler: ResultStage 129 (parquet at Generate.java:61) finished in 1.110 s
21/01/20 15:43:23 INFO DAGScheduler: Job 129 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:43:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 129: Stage finished
21/01/20 15:43:23 INFO DAGScheduler: Job 129 finished: parquet at Generate.java:61, took 1.111316 s
21/01/20 15:43:23 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-15052357-7bd8-44ff-85ed-417a9bc2d620
21/01/20 15:43:23 INFO FileFormatWriter: Write Job 80b1b061-20f0-4b86-9b6f-6d9778f8ad09 committed.
21/01/20 15:43:23 INFO FileFormatWriter: Finished processing stats for write job 80b1b061-20f0-4b86-9b6f-6d9778f8ad09.
21/01/20 15:43:24 INFO BlockManagerInfo: Removed broadcast_129_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:43:26 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:26 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:26 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:43:26 INFO DAGScheduler: Got job 130 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:43:26 INFO DAGScheduler: Final stage: ResultStage 130 (parquet at Generate.java:61)
21/01/20 15:43:26 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:43:26 INFO DAGScheduler: Missing parents: List()
21/01/20 15:43:26 INFO DAGScheduler: Submitting ResultStage 130 (MapPartitionsRDD[521] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:43:26 INFO MemoryStore: Block broadcast_130 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:43:26 INFO MemoryStore: Block broadcast_130_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:43:26 INFO BlockManagerInfo: Added broadcast_130_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:43:26 INFO SparkContext: Created broadcast 130 from broadcast at DAGScheduler.scala:1200
21/01/20 15:43:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 130 (MapPartitionsRDD[521] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:43:26 INFO TaskSchedulerImpl: Adding task set 130.0 with 1 tasks
21/01/20 15:43:26 WARN TaskSetManager: Stage 130 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:43:26 INFO TaskSetManager: Starting task 0.0 in stage 130.0 (TID 130, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:43:26 INFO Executor: Running task 0.0 in stage 130.0 (TID 130)
21/01/20 15:43:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:26 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:26 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:43:26 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:43:26 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:43:26 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:43:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:43:26 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:43:26 INFO ParquetOutputFormat: Validation is off
21/01/20 15:43:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:43:26 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:43:26 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:43:26 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:43:26 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:43:26 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:43:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:43:27 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154326_0130_m_000000_130' to o3fs://bucket1.vol1/testdata
21/01/20 15:43:27 INFO SparkHadoopMapRedUtil: attempt_20210120154326_0130_m_000000_130: Committed
21/01/20 15:43:27 INFO Executor: Finished task 0.0 in stage 130.0 (TID 130). 2155 bytes result sent to driver
21/01/20 15:43:27 INFO TaskSetManager: Finished task 0.0 in stage 130.0 (TID 130) in 1010 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:43:27 INFO TaskSchedulerImpl: Removed TaskSet 130.0, whose tasks have all completed, from pool 
21/01/20 15:43:27 INFO DAGScheduler: ResultStage 130 (parquet at Generate.java:61) finished in 1.028 s
21/01/20 15:43:27 INFO DAGScheduler: Job 130 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:43:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 130: Stage finished
21/01/20 15:43:27 INFO DAGScheduler: Job 130 finished: parquet at Generate.java:61, took 1.029885 s
21/01/20 15:43:27 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-b49a57a9-0a8d-45c0-be80-a87cdafde5be
21/01/20 15:43:27 INFO FileFormatWriter: Write Job 250538ab-d662-448d-a3da-f973cfeedf84 committed.
21/01/20 15:43:27 INFO FileFormatWriter: Finished processing stats for write job 250538ab-d662-448d-a3da-f973cfeedf84.
21/01/20 15:43:28 INFO BlockManagerInfo: Removed broadcast_130_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:43:29 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:29 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:43:29 INFO DAGScheduler: Got job 131 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:43:29 INFO DAGScheduler: Final stage: ResultStage 131 (parquet at Generate.java:61)
21/01/20 15:43:29 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:43:29 INFO DAGScheduler: Missing parents: List()
21/01/20 15:43:29 INFO DAGScheduler: Submitting ResultStage 131 (MapPartitionsRDD[525] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:43:29 INFO MemoryStore: Block broadcast_131 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:43:29 INFO MemoryStore: Block broadcast_131_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:43:29 INFO BlockManagerInfo: Added broadcast_131_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:43:29 INFO SparkContext: Created broadcast 131 from broadcast at DAGScheduler.scala:1200
21/01/20 15:43:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 131 (MapPartitionsRDD[525] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:43:29 INFO TaskSchedulerImpl: Adding task set 131.0 with 1 tasks
21/01/20 15:43:29 WARN TaskSetManager: Stage 131 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:43:29 INFO TaskSetManager: Starting task 0.0 in stage 131.0 (TID 131, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:43:29 INFO Executor: Running task 0.0 in stage 131.0 (TID 131)
21/01/20 15:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:29 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:43:29 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:43:29 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:43:29 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:43:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:43:29 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:43:29 INFO ParquetOutputFormat: Validation is off
21/01/20 15:43:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:43:29 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:43:29 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:43:29 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:43:29 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:43:29 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:43:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154329_0131_m_000000_131' to o3fs://bucket1.vol1/testdata
21/01/20 15:43:30 INFO SparkHadoopMapRedUtil: attempt_20210120154329_0131_m_000000_131: Committed
21/01/20 15:43:30 INFO Executor: Finished task 0.0 in stage 131.0 (TID 131). 2155 bytes result sent to driver
21/01/20 15:43:30 INFO TaskSetManager: Finished task 0.0 in stage 131.0 (TID 131) in 793 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:43:30 INFO TaskSchedulerImpl: Removed TaskSet 131.0, whose tasks have all completed, from pool 
21/01/20 15:43:30 INFO DAGScheduler: ResultStage 131 (parquet at Generate.java:61) finished in 0.833 s
21/01/20 15:43:30 INFO DAGScheduler: Job 131 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:43:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 131: Stage finished
21/01/20 15:43:30 INFO DAGScheduler: Job 131 finished: parquet at Generate.java:61, took 0.834597 s
21/01/20 15:43:30 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-bcc06d21-5022-4957-906c-b8b561e0868c
21/01/20 15:43:30 INFO FileFormatWriter: Write Job fbc13b43-e40e-4171-9354-13d8b899864d committed.
21/01/20 15:43:30 INFO FileFormatWriter: Finished processing stats for write job fbc13b43-e40e-4171-9354-13d8b899864d.
21/01/20 15:43:30 INFO BlockManagerInfo: Removed broadcast_131_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:43:33 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:33 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:33 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:43:33 INFO DAGScheduler: Got job 132 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:43:33 INFO DAGScheduler: Final stage: ResultStage 132 (parquet at Generate.java:61)
21/01/20 15:43:33 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:43:33 INFO DAGScheduler: Missing parents: List()
21/01/20 15:43:33 INFO DAGScheduler: Submitting ResultStage 132 (MapPartitionsRDD[529] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:43:33 INFO MemoryStore: Block broadcast_132 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:43:33 INFO MemoryStore: Block broadcast_132_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:43:33 INFO BlockManagerInfo: Added broadcast_132_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:43:33 INFO SparkContext: Created broadcast 132 from broadcast at DAGScheduler.scala:1200
21/01/20 15:43:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 132 (MapPartitionsRDD[529] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:43:33 INFO TaskSchedulerImpl: Adding task set 132.0 with 1 tasks
21/01/20 15:43:33 WARN TaskSetManager: Stage 132 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:43:33 INFO TaskSetManager: Starting task 0.0 in stage 132.0 (TID 132, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:43:33 INFO Executor: Running task 0.0 in stage 132.0 (TID 132)
21/01/20 15:43:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:33 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:33 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:43:33 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:43:33 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:43:33 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:43:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:43:33 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:43:33 INFO ParquetOutputFormat: Validation is off
21/01/20 15:43:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:43:33 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:43:33 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:43:33 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:43:33 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:43:33 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:43:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:43:33 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-131DDC599F5E->af5c9c6a-a887-4538-ae7b-ba6d49328297
21/01/20 15:43:33 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:43:34 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154333_0132_m_000000_132' to o3fs://bucket1.vol1/testdata
21/01/20 15:43:34 INFO SparkHadoopMapRedUtil: attempt_20210120154333_0132_m_000000_132: Committed
21/01/20 15:43:34 INFO Executor: Finished task 0.0 in stage 132.0 (TID 132). 2155 bytes result sent to driver
21/01/20 15:43:34 INFO TaskSetManager: Finished task 0.0 in stage 132.0 (TID 132) in 986 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:43:34 INFO TaskSchedulerImpl: Removed TaskSet 132.0, whose tasks have all completed, from pool 
21/01/20 15:43:34 INFO DAGScheduler: ResultStage 132 (parquet at Generate.java:61) finished in 1.003 s
21/01/20 15:43:34 INFO DAGScheduler: Job 132 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:43:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 132: Stage finished
21/01/20 15:43:34 INFO DAGScheduler: Job 132 finished: parquet at Generate.java:61, took 1.005045 s
21/01/20 15:43:34 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-33dd9112-f381-4bd1-9d2c-cd18b4ef556f
21/01/20 15:43:34 INFO FileFormatWriter: Write Job b5709f47-41b9-40da-8faa-ece0c7eea4e3 committed.
21/01/20 15:43:34 INFO FileFormatWriter: Finished processing stats for write job b5709f47-41b9-40da-8faa-ece0c7eea4e3.
21/01/20 15:43:35 INFO BlockManagerInfo: Removed broadcast_132_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:43:36 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:36 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:36 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:43:36 INFO DAGScheduler: Got job 133 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:43:36 INFO DAGScheduler: Final stage: ResultStage 133 (parquet at Generate.java:61)
21/01/20 15:43:36 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:43:36 INFO DAGScheduler: Missing parents: List()
21/01/20 15:43:36 INFO DAGScheduler: Submitting ResultStage 133 (MapPartitionsRDD[533] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:43:36 INFO MemoryStore: Block broadcast_133 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:43:36 INFO MemoryStore: Block broadcast_133_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:43:36 INFO BlockManagerInfo: Added broadcast_133_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:43:36 INFO SparkContext: Created broadcast 133 from broadcast at DAGScheduler.scala:1200
21/01/20 15:43:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 133 (MapPartitionsRDD[533] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:43:36 INFO TaskSchedulerImpl: Adding task set 133.0 with 1 tasks
21/01/20 15:43:36 WARN TaskSetManager: Stage 133 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:43:36 INFO TaskSetManager: Starting task 0.0 in stage 133.0 (TID 133, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:43:36 INFO Executor: Running task 0.0 in stage 133.0 (TID 133)
21/01/20 15:43:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:37 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:37 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:43:37 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:43:37 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:43:37 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:43:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:43:37 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:43:37 INFO ParquetOutputFormat: Validation is off
21/01/20 15:43:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:43:37 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:43:37 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:43:37 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:43:37 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:43:37 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:43:37 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:43:37 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154336_0133_m_000000_133' to o3fs://bucket1.vol1/testdata
21/01/20 15:43:37 INFO SparkHadoopMapRedUtil: attempt_20210120154336_0133_m_000000_133: Committed
21/01/20 15:43:37 INFO Executor: Finished task 0.0 in stage 133.0 (TID 133). 2155 bytes result sent to driver
21/01/20 15:43:37 INFO TaskSetManager: Finished task 0.0 in stage 133.0 (TID 133) in 860 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:43:37 INFO TaskSchedulerImpl: Removed TaskSet 133.0, whose tasks have all completed, from pool 
21/01/20 15:43:37 INFO DAGScheduler: ResultStage 133 (parquet at Generate.java:61) finished in 0.901 s
21/01/20 15:43:37 INFO DAGScheduler: Job 133 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:43:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 133: Stage finished
21/01/20 15:43:37 INFO DAGScheduler: Job 133 finished: parquet at Generate.java:61, took 0.902432 s
21/01/20 15:43:37 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-517ed194-f337-4ff1-bbe6-5694d8bfea6f
21/01/20 15:43:37 INFO FileFormatWriter: Write Job d7102e8e-7d74-4cd5-a50c-6c2897251f56 committed.
21/01/20 15:43:37 INFO FileFormatWriter: Finished processing stats for write job d7102e8e-7d74-4cd5-a50c-6c2897251f56.
21/01/20 15:43:37 INFO BlockManagerInfo: Removed broadcast_133_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:43:40 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:40 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:40 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:43:40 INFO DAGScheduler: Got job 134 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:43:40 INFO DAGScheduler: Final stage: ResultStage 134 (parquet at Generate.java:61)
21/01/20 15:43:40 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:43:40 INFO DAGScheduler: Missing parents: List()
21/01/20 15:43:40 INFO DAGScheduler: Submitting ResultStage 134 (MapPartitionsRDD[537] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:43:40 INFO MemoryStore: Block broadcast_134 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:43:40 INFO MemoryStore: Block broadcast_134_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:43:40 INFO BlockManagerInfo: Added broadcast_134_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:43:40 INFO SparkContext: Created broadcast 134 from broadcast at DAGScheduler.scala:1200
21/01/20 15:43:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 134 (MapPartitionsRDD[537] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:43:40 INFO TaskSchedulerImpl: Adding task set 134.0 with 1 tasks
21/01/20 15:43:40 WARN TaskSetManager: Stage 134 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:43:40 INFO TaskSetManager: Starting task 0.0 in stage 134.0 (TID 134, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:43:40 INFO Executor: Running task 0.0 in stage 134.0 (TID 134)
21/01/20 15:43:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:40 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:40 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:43:40 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:43:40 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:43:40 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:43:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:43:40 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:43:40 INFO ParquetOutputFormat: Validation is off
21/01/20 15:43:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:43:40 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:43:40 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:43:40 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:43:40 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:43:40 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:43:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:43:41 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-4336A1B1579C->d3c4d3e3-944b-4528-9d82-e005ad427697
21/01/20 15:43:41 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:43:41 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154340_0134_m_000000_134' to o3fs://bucket1.vol1/testdata
21/01/20 15:43:41 INFO SparkHadoopMapRedUtil: attempt_20210120154340_0134_m_000000_134: Committed
21/01/20 15:43:41 INFO Executor: Finished task 0.0 in stage 134.0 (TID 134). 2155 bytes result sent to driver
21/01/20 15:43:41 INFO TaskSetManager: Finished task 0.0 in stage 134.0 (TID 134) in 1143 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:43:41 INFO TaskSchedulerImpl: Removed TaskSet 134.0, whose tasks have all completed, from pool 
21/01/20 15:43:41 INFO DAGScheduler: ResultStage 134 (parquet at Generate.java:61) finished in 1.161 s
21/01/20 15:43:41 INFO DAGScheduler: Job 134 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:43:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 134: Stage finished
21/01/20 15:43:41 INFO DAGScheduler: Job 134 finished: parquet at Generate.java:61, took 1.162416 s
21/01/20 15:43:41 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-29d81645-4d05-4ab9-a89b-b0747a6c20ca
21/01/20 15:43:41 INFO FileFormatWriter: Write Job 4aa1a25f-b428-4344-9d91-d1c1578e9ed0 committed.
21/01/20 15:43:41 INFO FileFormatWriter: Finished processing stats for write job 4aa1a25f-b428-4344-9d91-d1c1578e9ed0.
21/01/20 15:43:42 INFO BlockManagerInfo: Removed broadcast_134_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:43:44 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:44 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:44 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:43:44 INFO DAGScheduler: Got job 135 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:43:44 INFO DAGScheduler: Final stage: ResultStage 135 (parquet at Generate.java:61)
21/01/20 15:43:44 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:43:44 INFO DAGScheduler: Missing parents: List()
21/01/20 15:43:44 INFO DAGScheduler: Submitting ResultStage 135 (MapPartitionsRDD[541] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:43:44 INFO MemoryStore: Block broadcast_135 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:43:44 INFO MemoryStore: Block broadcast_135_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:43:44 INFO BlockManagerInfo: Added broadcast_135_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:43:44 INFO SparkContext: Created broadcast 135 from broadcast at DAGScheduler.scala:1200
21/01/20 15:43:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 135 (MapPartitionsRDD[541] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:43:44 INFO TaskSchedulerImpl: Adding task set 135.0 with 1 tasks
21/01/20 15:43:44 WARN TaskSetManager: Stage 135 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:43:44 INFO TaskSetManager: Starting task 0.0 in stage 135.0 (TID 135, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:43:44 INFO Executor: Running task 0.0 in stage 135.0 (TID 135)
21/01/20 15:43:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:44 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:44 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:43:44 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:43:44 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:43:44 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:43:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:43:44 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:43:44 INFO ParquetOutputFormat: Validation is off
21/01/20 15:43:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:43:44 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:43:44 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:43:44 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:43:44 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:43:44 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:43:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:43:45 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154344_0135_m_000000_135' to o3fs://bucket1.vol1/testdata
21/01/20 15:43:45 INFO SparkHadoopMapRedUtil: attempt_20210120154344_0135_m_000000_135: Committed
21/01/20 15:43:45 INFO Executor: Finished task 0.0 in stage 135.0 (TID 135). 2155 bytes result sent to driver
21/01/20 15:43:45 INFO TaskSetManager: Finished task 0.0 in stage 135.0 (TID 135) in 1018 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:43:45 INFO TaskSchedulerImpl: Removed TaskSet 135.0, whose tasks have all completed, from pool 
21/01/20 15:43:45 INFO DAGScheduler: ResultStage 135 (parquet at Generate.java:61) finished in 1.036 s
21/01/20 15:43:45 INFO DAGScheduler: Job 135 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:43:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 135: Stage finished
21/01/20 15:43:45 INFO DAGScheduler: Job 135 finished: parquet at Generate.java:61, took 1.037227 s
21/01/20 15:43:45 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-e667e6ac-652d-44b5-91c6-3b39d74bdc17
21/01/20 15:43:45 INFO FileFormatWriter: Write Job 304c4c93-e47b-46e4-b7d6-da19040ffc7c committed.
21/01/20 15:43:45 INFO FileFormatWriter: Finished processing stats for write job 304c4c93-e47b-46e4-b7d6-da19040ffc7c.
21/01/20 15:43:46 INFO BlockManagerInfo: Removed broadcast_135_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:43:47 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:47 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:43:47 INFO DAGScheduler: Got job 136 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:43:47 INFO DAGScheduler: Final stage: ResultStage 136 (parquet at Generate.java:61)
21/01/20 15:43:47 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:43:47 INFO DAGScheduler: Missing parents: List()
21/01/20 15:43:47 INFO DAGScheduler: Submitting ResultStage 136 (MapPartitionsRDD[545] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:43:47 INFO MemoryStore: Block broadcast_136 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:43:47 INFO MemoryStore: Block broadcast_136_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:43:47 INFO BlockManagerInfo: Added broadcast_136_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:43:47 INFO SparkContext: Created broadcast 136 from broadcast at DAGScheduler.scala:1200
21/01/20 15:43:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 136 (MapPartitionsRDD[545] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:43:47 INFO TaskSchedulerImpl: Adding task set 136.0 with 1 tasks
21/01/20 15:43:47 WARN TaskSetManager: Stage 136 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:43:47 INFO TaskSetManager: Starting task 0.0 in stage 136.0 (TID 136, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:43:47 INFO Executor: Running task 0.0 in stage 136.0 (TID 136)
21/01/20 15:43:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:47 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:43:47 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:43:47 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:43:47 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:43:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:43:47 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:43:47 INFO ParquetOutputFormat: Validation is off
21/01/20 15:43:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:43:47 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:43:47 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:43:47 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:43:47 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:43:47 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:43:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:43:48 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154347_0136_m_000000_136' to o3fs://bucket1.vol1/testdata
21/01/20 15:43:48 INFO SparkHadoopMapRedUtil: attempt_20210120154347_0136_m_000000_136: Committed
21/01/20 15:43:48 INFO Executor: Finished task 0.0 in stage 136.0 (TID 136). 2155 bytes result sent to driver
21/01/20 15:43:48 INFO TaskSetManager: Finished task 0.0 in stage 136.0 (TID 136) in 1105 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:43:48 INFO TaskSchedulerImpl: Removed TaskSet 136.0, whose tasks have all completed, from pool 
21/01/20 15:43:48 INFO DAGScheduler: ResultStage 136 (parquet at Generate.java:61) finished in 1.122 s
21/01/20 15:43:48 INFO DAGScheduler: Job 136 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:43:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 136: Stage finished
21/01/20 15:43:48 INFO DAGScheduler: Job 136 finished: parquet at Generate.java:61, took 1.124075 s
21/01/20 15:43:48 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-16157182-bb7c-4c69-8e55-ca30b09ddc3a
21/01/20 15:43:48 INFO FileFormatWriter: Write Job 654db97d-aac5-44d5-9796-1f787cfaa653 committed.
21/01/20 15:43:48 INFO FileFormatWriter: Finished processing stats for write job 654db97d-aac5-44d5-9796-1f787cfaa653.
21/01/20 15:43:49 INFO BlockManagerInfo: Removed broadcast_136_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:43:51 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:51 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:43:51 INFO DAGScheduler: Got job 137 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:43:51 INFO DAGScheduler: Final stage: ResultStage 137 (parquet at Generate.java:61)
21/01/20 15:43:51 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:43:51 INFO DAGScheduler: Missing parents: List()
21/01/20 15:43:51 INFO DAGScheduler: Submitting ResultStage 137 (MapPartitionsRDD[549] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:43:51 INFO MemoryStore: Block broadcast_137 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:43:51 INFO MemoryStore: Block broadcast_137_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:43:51 INFO BlockManagerInfo: Added broadcast_137_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:43:51 INFO SparkContext: Created broadcast 137 from broadcast at DAGScheduler.scala:1200
21/01/20 15:43:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 137 (MapPartitionsRDD[549] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:43:51 INFO TaskSchedulerImpl: Adding task set 137.0 with 1 tasks
21/01/20 15:43:51 WARN TaskSetManager: Stage 137 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:43:51 INFO TaskSetManager: Starting task 0.0 in stage 137.0 (TID 137, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:43:51 INFO Executor: Running task 0.0 in stage 137.0 (TID 137)
21/01/20 15:43:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:51 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:43:51 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:43:51 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:43:51 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:43:51 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:43:51 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:43:51 INFO ParquetOutputFormat: Validation is off
21/01/20 15:43:51 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:43:51 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:43:51 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:43:51 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:43:51 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:43:51 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:43:51 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:43:52 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154351_0137_m_000000_137' to o3fs://bucket1.vol1/testdata
21/01/20 15:43:52 INFO SparkHadoopMapRedUtil: attempt_20210120154351_0137_m_000000_137: Committed
21/01/20 15:43:52 INFO Executor: Finished task 0.0 in stage 137.0 (TID 137). 2155 bytes result sent to driver
21/01/20 15:43:52 INFO TaskSetManager: Finished task 0.0 in stage 137.0 (TID 137) in 1105 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:43:52 INFO TaskSchedulerImpl: Removed TaskSet 137.0, whose tasks have all completed, from pool 
21/01/20 15:43:52 INFO DAGScheduler: ResultStage 137 (parquet at Generate.java:61) finished in 1.125 s
21/01/20 15:43:52 INFO DAGScheduler: Job 137 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:43:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 137: Stage finished
21/01/20 15:43:52 INFO DAGScheduler: Job 137 finished: parquet at Generate.java:61, took 1.125924 s
21/01/20 15:43:52 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-00a5e21b-dc8d-4b46-879f-3b5bd6971bff
21/01/20 15:43:52 INFO FileFormatWriter: Write Job 0d18b85b-37f0-4a82-a296-7b2fabd3d278 committed.
21/01/20 15:43:52 INFO FileFormatWriter: Finished processing stats for write job 0d18b85b-37f0-4a82-a296-7b2fabd3d278.
21/01/20 15:43:53 INFO BlockManagerInfo: Removed broadcast_137_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:43:55 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:55 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:43:55 INFO DAGScheduler: Got job 138 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:43:55 INFO DAGScheduler: Final stage: ResultStage 138 (parquet at Generate.java:61)
21/01/20 15:43:55 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:43:55 INFO DAGScheduler: Missing parents: List()
21/01/20 15:43:55 INFO DAGScheduler: Submitting ResultStage 138 (MapPartitionsRDD[553] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:43:55 INFO MemoryStore: Block broadcast_138 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:43:55 INFO MemoryStore: Block broadcast_138_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:43:55 INFO BlockManagerInfo: Added broadcast_138_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:43:55 INFO SparkContext: Created broadcast 138 from broadcast at DAGScheduler.scala:1200
21/01/20 15:43:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 138 (MapPartitionsRDD[553] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:43:55 INFO TaskSchedulerImpl: Adding task set 138.0 with 1 tasks
21/01/20 15:43:55 WARN TaskSetManager: Stage 138 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:43:55 INFO TaskSetManager: Starting task 0.0 in stage 138.0 (TID 138, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:43:55 INFO Executor: Running task 0.0 in stage 138.0 (TID 138)
21/01/20 15:43:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:55 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:43:55 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:43:55 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:43:55 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:43:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:43:55 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:43:55 INFO ParquetOutputFormat: Validation is off
21/01/20 15:43:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:43:55 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:43:55 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:43:55 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:43:55 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:43:55 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:43:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:43:55 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-807C27D4092C->af5c9c6a-a887-4538-ae7b-ba6d49328297
21/01/20 15:43:55 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:43:56 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154355_0138_m_000000_138' to o3fs://bucket1.vol1/testdata
21/01/20 15:43:56 INFO SparkHadoopMapRedUtil: attempt_20210120154355_0138_m_000000_138: Committed
21/01/20 15:43:56 INFO Executor: Finished task 0.0 in stage 138.0 (TID 138). 2155 bytes result sent to driver
21/01/20 15:43:56 INFO TaskSetManager: Finished task 0.0 in stage 138.0 (TID 138) in 1034 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:43:56 INFO TaskSchedulerImpl: Removed TaskSet 138.0, whose tasks have all completed, from pool 
21/01/20 15:43:56 INFO DAGScheduler: ResultStage 138 (parquet at Generate.java:61) finished in 1.052 s
21/01/20 15:43:56 INFO DAGScheduler: Job 138 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:43:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 138: Stage finished
21/01/20 15:43:56 INFO DAGScheduler: Job 138 finished: parquet at Generate.java:61, took 1.052904 s
21/01/20 15:43:56 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-91285b31-c952-4d12-9b4d-deb33025d7a4
21/01/20 15:43:56 INFO FileFormatWriter: Write Job a6d77b81-97e8-4882-b0aa-83904510baeb committed.
21/01/20 15:43:56 INFO FileFormatWriter: Finished processing stats for write job a6d77b81-97e8-4882-b0aa-83904510baeb.
21/01/20 15:43:57 INFO BlockManagerInfo: Removed broadcast_138_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:43:58 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:58 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:58 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:43:58 INFO DAGScheduler: Got job 139 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:43:58 INFO DAGScheduler: Final stage: ResultStage 139 (parquet at Generate.java:61)
21/01/20 15:43:58 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:43:58 INFO DAGScheduler: Missing parents: List()
21/01/20 15:43:58 INFO DAGScheduler: Submitting ResultStage 139 (MapPartitionsRDD[557] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:43:58 INFO MemoryStore: Block broadcast_139 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:43:58 INFO MemoryStore: Block broadcast_139_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:43:58 INFO BlockManagerInfo: Added broadcast_139_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:43:58 INFO SparkContext: Created broadcast 139 from broadcast at DAGScheduler.scala:1200
21/01/20 15:43:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 139 (MapPartitionsRDD[557] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:43:58 INFO TaskSchedulerImpl: Adding task set 139.0 with 1 tasks
21/01/20 15:43:59 WARN TaskSetManager: Stage 139 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:43:59 INFO TaskSetManager: Starting task 0.0 in stage 139.0 (TID 139, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:43:59 INFO Executor: Running task 0.0 in stage 139.0 (TID 139)
21/01/20 15:43:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:59 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:43:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:43:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:43:59 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:43:59 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:43:59 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:43:59 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:43:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:43:59 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:43:59 INFO ParquetOutputFormat: Validation is off
21/01/20 15:43:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:43:59 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:43:59 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:43:59 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:43:59 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:43:59 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:43:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:44:00 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154358_0139_m_000000_139' to o3fs://bucket1.vol1/testdata
21/01/20 15:44:00 INFO SparkHadoopMapRedUtil: attempt_20210120154358_0139_m_000000_139: Committed
21/01/20 15:44:00 INFO Executor: Finished task 0.0 in stage 139.0 (TID 139). 2155 bytes result sent to driver
21/01/20 15:44:00 INFO TaskSetManager: Finished task 0.0 in stage 139.0 (TID 139) in 1096 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:44:00 INFO TaskSchedulerImpl: Removed TaskSet 139.0, whose tasks have all completed, from pool 
21/01/20 15:44:00 INFO DAGScheduler: ResultStage 139 (parquet at Generate.java:61) finished in 1.113 s
21/01/20 15:44:00 INFO DAGScheduler: Job 139 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:44:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 139: Stage finished
21/01/20 15:44:00 INFO DAGScheduler: Job 139 finished: parquet at Generate.java:61, took 1.114903 s
21/01/20 15:44:00 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-38092132-e1a2-42af-9c34-11d3c1e95e80
21/01/20 15:44:00 INFO FileFormatWriter: Write Job bd9fd004-909d-4080-acf9-2c20c1dafe07 committed.
21/01/20 15:44:00 INFO FileFormatWriter: Finished processing stats for write job bd9fd004-909d-4080-acf9-2c20c1dafe07.
21/01/20 15:44:01 INFO BlockManagerInfo: Removed broadcast_139_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:44:02 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:02 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:44:02 INFO DAGScheduler: Got job 140 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:44:02 INFO DAGScheduler: Final stage: ResultStage 140 (parquet at Generate.java:61)
21/01/20 15:44:02 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:44:02 INFO DAGScheduler: Missing parents: List()
21/01/20 15:44:02 INFO DAGScheduler: Submitting ResultStage 140 (MapPartitionsRDD[561] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:44:02 INFO MemoryStore: Block broadcast_140 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:44:02 INFO MemoryStore: Block broadcast_140_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:44:02 INFO BlockManagerInfo: Added broadcast_140_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:44:02 INFO SparkContext: Created broadcast 140 from broadcast at DAGScheduler.scala:1200
21/01/20 15:44:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 140 (MapPartitionsRDD[561] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:44:02 INFO TaskSchedulerImpl: Adding task set 140.0 with 1 tasks
21/01/20 15:44:02 WARN TaskSetManager: Stage 140 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:44:02 INFO TaskSetManager: Starting task 0.0 in stage 140.0 (TID 140, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:44:02 INFO Executor: Running task 0.0 in stage 140.0 (TID 140)
21/01/20 15:44:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:02 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:44:02 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:44:02 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:44:02 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:44:02 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:44:02 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:44:02 INFO ParquetOutputFormat: Validation is off
21/01/20 15:44:02 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:44:02 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:44:02 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:44:02 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:44:02 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:44:02 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:44:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:44:03 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154402_0140_m_000000_140' to o3fs://bucket1.vol1/testdata
21/01/20 15:44:03 INFO SparkHadoopMapRedUtil: attempt_20210120154402_0140_m_000000_140: Committed
21/01/20 15:44:03 INFO Executor: Finished task 0.0 in stage 140.0 (TID 140). 2155 bytes result sent to driver
21/01/20 15:44:03 INFO TaskSetManager: Finished task 0.0 in stage 140.0 (TID 140) in 1121 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:44:03 INFO TaskSchedulerImpl: Removed TaskSet 140.0, whose tasks have all completed, from pool 
21/01/20 15:44:03 INFO DAGScheduler: ResultStage 140 (parquet at Generate.java:61) finished in 1.138 s
21/01/20 15:44:03 INFO DAGScheduler: Job 140 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:44:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 140: Stage finished
21/01/20 15:44:03 INFO DAGScheduler: Job 140 finished: parquet at Generate.java:61, took 1.140017 s
21/01/20 15:44:03 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-80e5d32a-63d5-4013-970d-46c5fbd90488
21/01/20 15:44:03 INFO FileFormatWriter: Write Job 6c09d714-2344-473b-818c-5c04a830d89c committed.
21/01/20 15:44:03 INFO FileFormatWriter: Finished processing stats for write job 6c09d714-2344-473b-818c-5c04a830d89c.
21/01/20 15:44:04 INFO BlockManagerInfo: Removed broadcast_140_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:44:06 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:06 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:06 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:44:06 INFO DAGScheduler: Got job 141 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:44:06 INFO DAGScheduler: Final stage: ResultStage 141 (parquet at Generate.java:61)
21/01/20 15:44:06 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:44:06 INFO DAGScheduler: Missing parents: List()
21/01/20 15:44:06 INFO DAGScheduler: Submitting ResultStage 141 (MapPartitionsRDD[565] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:44:06 INFO MemoryStore: Block broadcast_141 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:44:06 INFO MemoryStore: Block broadcast_141_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:44:06 INFO BlockManagerInfo: Added broadcast_141_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:44:06 INFO SparkContext: Created broadcast 141 from broadcast at DAGScheduler.scala:1200
21/01/20 15:44:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 141 (MapPartitionsRDD[565] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:44:06 INFO TaskSchedulerImpl: Adding task set 141.0 with 1 tasks
21/01/20 15:44:06 WARN TaskSetManager: Stage 141 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:44:06 INFO TaskSetManager: Starting task 0.0 in stage 141.0 (TID 141, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:44:06 INFO Executor: Running task 0.0 in stage 141.0 (TID 141)
21/01/20 15:44:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:06 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:06 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:44:06 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:44:06 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:44:06 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:44:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:44:06 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:44:06 INFO ParquetOutputFormat: Validation is off
21/01/20 15:44:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:44:06 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:44:06 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:44:06 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:44:06 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:44:06 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:44:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:44:07 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154406_0141_m_000000_141' to o3fs://bucket1.vol1/testdata
21/01/20 15:44:07 INFO SparkHadoopMapRedUtil: attempt_20210120154406_0141_m_000000_141: Committed
21/01/20 15:44:07 INFO Executor: Finished task 0.0 in stage 141.0 (TID 141). 2155 bytes result sent to driver
21/01/20 15:44:07 INFO TaskSetManager: Finished task 0.0 in stage 141.0 (TID 141) in 1043 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:44:07 INFO TaskSchedulerImpl: Removed TaskSet 141.0, whose tasks have all completed, from pool 
21/01/20 15:44:07 INFO DAGScheduler: ResultStage 141 (parquet at Generate.java:61) finished in 1.061 s
21/01/20 15:44:07 INFO DAGScheduler: Job 141 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:44:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 141: Stage finished
21/01/20 15:44:07 INFO DAGScheduler: Job 141 finished: parquet at Generate.java:61, took 1.062415 s
21/01/20 15:44:07 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-05dc57b7-6d76-40d5-8fdf-f210d53c7ead
21/01/20 15:44:07 INFO FileFormatWriter: Write Job 8340c372-82b9-4101-acc2-8ad8a03eb0d4 committed.
21/01/20 15:44:07 INFO FileFormatWriter: Finished processing stats for write job 8340c372-82b9-4101-acc2-8ad8a03eb0d4.
21/01/20 15:44:08 INFO BlockManagerInfo: Removed broadcast_141_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:44:10 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:10 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:10 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:44:10 INFO DAGScheduler: Got job 142 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:44:10 INFO DAGScheduler: Final stage: ResultStage 142 (parquet at Generate.java:61)
21/01/20 15:44:10 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:44:10 INFO DAGScheduler: Missing parents: List()
21/01/20 15:44:10 INFO DAGScheduler: Submitting ResultStage 142 (MapPartitionsRDD[569] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:44:10 INFO MemoryStore: Block broadcast_142 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:44:10 INFO MemoryStore: Block broadcast_142_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:44:10 INFO BlockManagerInfo: Added broadcast_142_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:44:10 INFO SparkContext: Created broadcast 142 from broadcast at DAGScheduler.scala:1200
21/01/20 15:44:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 142 (MapPartitionsRDD[569] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:44:10 INFO TaskSchedulerImpl: Adding task set 142.0 with 1 tasks
21/01/20 15:44:10 WARN TaskSetManager: Stage 142 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:44:10 INFO TaskSetManager: Starting task 0.0 in stage 142.0 (TID 142, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:44:10 INFO Executor: Running task 0.0 in stage 142.0 (TID 142)
21/01/20 15:44:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:10 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:10 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:44:10 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:44:10 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:44:10 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:44:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:44:10 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:44:10 INFO ParquetOutputFormat: Validation is off
21/01/20 15:44:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:44:10 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:44:10 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:44:10 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:44:10 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:44:10 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:44:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:44:11 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154410_0142_m_000000_142' to o3fs://bucket1.vol1/testdata
21/01/20 15:44:11 INFO SparkHadoopMapRedUtil: attempt_20210120154410_0142_m_000000_142: Committed
21/01/20 15:44:11 INFO Executor: Finished task 0.0 in stage 142.0 (TID 142). 2155 bytes result sent to driver
21/01/20 15:44:11 INFO TaskSetManager: Finished task 0.0 in stage 142.0 (TID 142) in 942 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:44:11 INFO TaskSchedulerImpl: Removed TaskSet 142.0, whose tasks have all completed, from pool 
21/01/20 15:44:11 INFO DAGScheduler: ResultStage 142 (parquet at Generate.java:61) finished in 0.983 s
21/01/20 15:44:11 INFO DAGScheduler: Job 142 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:44:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 142: Stage finished
21/01/20 15:44:11 INFO DAGScheduler: Job 142 finished: parquet at Generate.java:61, took 0.984561 s
21/01/20 15:44:11 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-875fd2ea-eb30-434d-98d4-1cf3a4a77c0c
21/01/20 15:44:11 INFO FileFormatWriter: Write Job c8ffe6dd-48ca-475f-a786-c2e3baae27e4 committed.
21/01/20 15:44:11 INFO FileFormatWriter: Finished processing stats for write job c8ffe6dd-48ca-475f-a786-c2e3baae27e4.
21/01/20 15:44:11 INFO BlockManagerInfo: Removed broadcast_142_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:44:13 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:13 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:13 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:44:13 INFO DAGScheduler: Got job 143 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:44:13 INFO DAGScheduler: Final stage: ResultStage 143 (parquet at Generate.java:61)
21/01/20 15:44:13 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:44:13 INFO DAGScheduler: Missing parents: List()
21/01/20 15:44:13 INFO DAGScheduler: Submitting ResultStage 143 (MapPartitionsRDD[573] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:44:13 INFO MemoryStore: Block broadcast_143 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:44:13 INFO MemoryStore: Block broadcast_143_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:44:13 INFO BlockManagerInfo: Added broadcast_143_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:44:13 INFO SparkContext: Created broadcast 143 from broadcast at DAGScheduler.scala:1200
21/01/20 15:44:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 143 (MapPartitionsRDD[573] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:44:13 INFO TaskSchedulerImpl: Adding task set 143.0 with 1 tasks
21/01/20 15:44:13 WARN TaskSetManager: Stage 143 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:44:13 INFO TaskSetManager: Starting task 0.0 in stage 143.0 (TID 143, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:44:13 INFO Executor: Running task 0.0 in stage 143.0 (TID 143)
21/01/20 15:44:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:13 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:13 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:44:13 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:44:13 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:44:13 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:44:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:44:13 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:44:13 INFO ParquetOutputFormat: Validation is off
21/01/20 15:44:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:44:13 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:44:13 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:44:13 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:44:13 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:44:13 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:44:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:44:14 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154413_0143_m_000000_143' to o3fs://bucket1.vol1/testdata
21/01/20 15:44:14 INFO SparkHadoopMapRedUtil: attempt_20210120154413_0143_m_000000_143: Committed
21/01/20 15:44:14 INFO Executor: Finished task 0.0 in stage 143.0 (TID 143). 2155 bytes result sent to driver
21/01/20 15:44:14 INFO TaskSetManager: Finished task 0.0 in stage 143.0 (TID 143) in 1071 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:44:14 INFO TaskSchedulerImpl: Removed TaskSet 143.0, whose tasks have all completed, from pool 
21/01/20 15:44:14 INFO DAGScheduler: ResultStage 143 (parquet at Generate.java:61) finished in 1.088 s
21/01/20 15:44:14 INFO DAGScheduler: Job 143 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:44:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 143: Stage finished
21/01/20 15:44:14 INFO DAGScheduler: Job 143 finished: parquet at Generate.java:61, took 1.089647 s
21/01/20 15:44:14 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-db5f3eae-6c72-4d4f-ab66-343ac4051dc1
21/01/20 15:44:14 INFO FileFormatWriter: Write Job 3710473c-1e9b-4e67-b8f9-87985eb17c5c committed.
21/01/20 15:44:14 INFO FileFormatWriter: Finished processing stats for write job 3710473c-1e9b-4e67-b8f9-87985eb17c5c.
21/01/20 15:44:16 INFO BlockManagerInfo: Removed broadcast_143_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:44:17 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:17 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:17 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:44:17 INFO DAGScheduler: Got job 144 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:44:17 INFO DAGScheduler: Final stage: ResultStage 144 (parquet at Generate.java:61)
21/01/20 15:44:17 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:44:17 INFO DAGScheduler: Missing parents: List()
21/01/20 15:44:17 INFO DAGScheduler: Submitting ResultStage 144 (MapPartitionsRDD[577] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:44:17 INFO MemoryStore: Block broadcast_144 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:44:17 INFO MemoryStore: Block broadcast_144_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:44:17 INFO BlockManagerInfo: Added broadcast_144_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:44:17 INFO SparkContext: Created broadcast 144 from broadcast at DAGScheduler.scala:1200
21/01/20 15:44:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 144 (MapPartitionsRDD[577] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:44:17 INFO TaskSchedulerImpl: Adding task set 144.0 with 1 tasks
21/01/20 15:44:17 WARN TaskSetManager: Stage 144 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:44:17 INFO TaskSetManager: Starting task 0.0 in stage 144.0 (TID 144, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:44:17 INFO Executor: Running task 0.0 in stage 144.0 (TID 144)
21/01/20 15:44:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:17 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:17 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:44:17 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:44:17 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:44:17 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:44:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:44:17 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:44:17 INFO ParquetOutputFormat: Validation is off
21/01/20 15:44:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:44:17 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:44:17 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:44:17 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:44:17 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:44:17 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:44:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:44:17 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-F3FD7F0EF78F->af5c9c6a-a887-4538-ae7b-ba6d49328297
21/01/20 15:44:17 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:44:18 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154417_0144_m_000000_144' to o3fs://bucket1.vol1/testdata
21/01/20 15:44:18 INFO SparkHadoopMapRedUtil: attempt_20210120154417_0144_m_000000_144: Committed
21/01/20 15:44:18 INFO Executor: Finished task 0.0 in stage 144.0 (TID 144). 2155 bytes result sent to driver
21/01/20 15:44:18 INFO TaskSetManager: Finished task 0.0 in stage 144.0 (TID 144) in 878 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:44:18 INFO TaskSchedulerImpl: Removed TaskSet 144.0, whose tasks have all completed, from pool 
21/01/20 15:44:18 INFO DAGScheduler: ResultStage 144 (parquet at Generate.java:61) finished in 0.922 s
21/01/20 15:44:18 INFO DAGScheduler: Job 144 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:44:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 144: Stage finished
21/01/20 15:44:18 INFO DAGScheduler: Job 144 finished: parquet at Generate.java:61, took 0.923587 s
21/01/20 15:44:18 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-7377af6d-75f9-4af7-b067-4fd69e6263a1
21/01/20 15:44:18 INFO FileFormatWriter: Write Job 13aab074-5211-40bf-919a-f03232d2272a committed.
21/01/20 15:44:18 INFO FileFormatWriter: Finished processing stats for write job 13aab074-5211-40bf-919a-f03232d2272a.
21/01/20 15:44:18 INFO BlockManagerInfo: Removed broadcast_144_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:44:20 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:20 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:20 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:44:20 INFO DAGScheduler: Got job 145 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:44:20 INFO DAGScheduler: Final stage: ResultStage 145 (parquet at Generate.java:61)
21/01/20 15:44:20 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:44:20 INFO DAGScheduler: Missing parents: List()
21/01/20 15:44:20 INFO DAGScheduler: Submitting ResultStage 145 (MapPartitionsRDD[581] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:44:20 INFO MemoryStore: Block broadcast_145 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:44:20 INFO MemoryStore: Block broadcast_145_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:44:20 INFO BlockManagerInfo: Added broadcast_145_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:44:20 INFO SparkContext: Created broadcast 145 from broadcast at DAGScheduler.scala:1200
21/01/20 15:44:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 145 (MapPartitionsRDD[581] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:44:20 INFO TaskSchedulerImpl: Adding task set 145.0 with 1 tasks
21/01/20 15:44:21 WARN TaskSetManager: Stage 145 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:44:21 INFO TaskSetManager: Starting task 0.0 in stage 145.0 (TID 145, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:44:21 INFO Executor: Running task 0.0 in stage 145.0 (TID 145)
21/01/20 15:44:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:21 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:21 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:44:21 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:44:21 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:44:21 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:44:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:44:21 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:44:21 INFO ParquetOutputFormat: Validation is off
21/01/20 15:44:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:44:21 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:44:21 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:44:21 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:44:21 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:44:21 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:44:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:44:21 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154420_0145_m_000000_145' to o3fs://bucket1.vol1/testdata
21/01/20 15:44:21 INFO SparkHadoopMapRedUtil: attempt_20210120154420_0145_m_000000_145: Committed
21/01/20 15:44:21 INFO Executor: Finished task 0.0 in stage 145.0 (TID 145). 2155 bytes result sent to driver
21/01/20 15:44:21 INFO TaskSetManager: Finished task 0.0 in stage 145.0 (TID 145) in 981 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:44:21 INFO TaskSchedulerImpl: Removed TaskSet 145.0, whose tasks have all completed, from pool 
21/01/20 15:44:21 INFO DAGScheduler: ResultStage 145 (parquet at Generate.java:61) finished in 1.000 s
21/01/20 15:44:21 INFO DAGScheduler: Job 145 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:44:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 145: Stage finished
21/01/20 15:44:21 INFO DAGScheduler: Job 145 finished: parquet at Generate.java:61, took 1.001319 s
21/01/20 15:44:21 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-97791f81-0c63-465c-8dcd-275180137b44
21/01/20 15:44:21 INFO FileFormatWriter: Write Job 65c8c2bc-7d35-4087-8ad7-96da5242999a committed.
21/01/20 15:44:21 INFO FileFormatWriter: Finished processing stats for write job 65c8c2bc-7d35-4087-8ad7-96da5242999a.
21/01/20 15:44:23 INFO BlockManagerInfo: Removed broadcast_145_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:44:24 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:24 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:24 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:44:24 INFO DAGScheduler: Got job 146 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:44:24 INFO DAGScheduler: Final stage: ResultStage 146 (parquet at Generate.java:61)
21/01/20 15:44:24 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:44:24 INFO DAGScheduler: Missing parents: List()
21/01/20 15:44:24 INFO DAGScheduler: Submitting ResultStage 146 (MapPartitionsRDD[585] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:44:24 INFO MemoryStore: Block broadcast_146 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:44:24 INFO MemoryStore: Block broadcast_146_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:44:24 INFO BlockManagerInfo: Added broadcast_146_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:44:24 INFO SparkContext: Created broadcast 146 from broadcast at DAGScheduler.scala:1200
21/01/20 15:44:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 146 (MapPartitionsRDD[585] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:44:24 INFO TaskSchedulerImpl: Adding task set 146.0 with 1 tasks
21/01/20 15:44:24 WARN TaskSetManager: Stage 146 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:44:24 INFO TaskSetManager: Starting task 0.0 in stage 146.0 (TID 146, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:44:24 INFO Executor: Running task 0.0 in stage 146.0 (TID 146)
21/01/20 15:44:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:24 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:24 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:44:24 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:44:24 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:44:24 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:44:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:44:24 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:44:24 INFO ParquetOutputFormat: Validation is off
21/01/20 15:44:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:44:24 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:44:24 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:44:24 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:44:24 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:44:24 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:44:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:44:25 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-71A3F5171D09->d3c4d3e3-944b-4528-9d82-e005ad427697
21/01/20 15:44:25 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:44:25 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154424_0146_m_000000_146' to o3fs://bucket1.vol1/testdata
21/01/20 15:44:25 INFO SparkHadoopMapRedUtil: attempt_20210120154424_0146_m_000000_146: Committed
21/01/20 15:44:25 INFO Executor: Finished task 0.0 in stage 146.0 (TID 146). 2155 bytes result sent to driver
21/01/20 15:44:25 INFO TaskSetManager: Finished task 0.0 in stage 146.0 (TID 146) in 931 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:44:25 INFO TaskSchedulerImpl: Removed TaskSet 146.0, whose tasks have all completed, from pool 
21/01/20 15:44:25 INFO DAGScheduler: ResultStage 146 (parquet at Generate.java:61) finished in 0.972 s
21/01/20 15:44:25 INFO DAGScheduler: Job 146 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:44:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 146: Stage finished
21/01/20 15:44:25 INFO DAGScheduler: Job 146 finished: parquet at Generate.java:61, took 0.973478 s
21/01/20 15:44:25 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-422b76fc-c7ae-4c9f-8cc0-52c96a64bd68
21/01/20 15:44:25 INFO FileFormatWriter: Write Job 627ee8f9-d315-48ec-a8d0-b86b5fe3a673 committed.
21/01/20 15:44:25 INFO FileFormatWriter: Finished processing stats for write job 627ee8f9-d315-48ec-a8d0-b86b5fe3a673.
21/01/20 15:44:25 INFO BlockManagerInfo: Removed broadcast_146_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:44:28 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:28 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:28 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:44:28 INFO DAGScheduler: Got job 147 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:44:28 INFO DAGScheduler: Final stage: ResultStage 147 (parquet at Generate.java:61)
21/01/20 15:44:28 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:44:28 INFO DAGScheduler: Missing parents: List()
21/01/20 15:44:28 INFO DAGScheduler: Submitting ResultStage 147 (MapPartitionsRDD[589] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:44:28 INFO MemoryStore: Block broadcast_147 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:44:28 INFO MemoryStore: Block broadcast_147_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:44:28 INFO BlockManagerInfo: Added broadcast_147_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:44:28 INFO SparkContext: Created broadcast 147 from broadcast at DAGScheduler.scala:1200
21/01/20 15:44:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 147 (MapPartitionsRDD[589] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:44:28 INFO TaskSchedulerImpl: Adding task set 147.0 with 1 tasks
21/01/20 15:44:28 WARN TaskSetManager: Stage 147 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:44:28 INFO TaskSetManager: Starting task 0.0 in stage 147.0 (TID 147, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:44:28 INFO Executor: Running task 0.0 in stage 147.0 (TID 147)
21/01/20 15:44:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:28 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:28 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:44:28 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:44:28 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:44:28 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:44:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:44:28 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:44:28 INFO ParquetOutputFormat: Validation is off
21/01/20 15:44:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:44:28 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:44:28 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:44:28 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:44:28 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:44:28 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:44:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:44:29 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154428_0147_m_000000_147' to o3fs://bucket1.vol1/testdata
21/01/20 15:44:29 INFO SparkHadoopMapRedUtil: attempt_20210120154428_0147_m_000000_147: Committed
21/01/20 15:44:29 INFO Executor: Finished task 0.0 in stage 147.0 (TID 147). 2155 bytes result sent to driver
21/01/20 15:44:29 INFO TaskSetManager: Finished task 0.0 in stage 147.0 (TID 147) in 1014 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:44:29 INFO TaskSchedulerImpl: Removed TaskSet 147.0, whose tasks have all completed, from pool 
21/01/20 15:44:29 INFO DAGScheduler: ResultStage 147 (parquet at Generate.java:61) finished in 1.031 s
21/01/20 15:44:29 INFO DAGScheduler: Job 147 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:44:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 147: Stage finished
21/01/20 15:44:29 INFO DAGScheduler: Job 147 finished: parquet at Generate.java:61, took 1.033246 s
21/01/20 15:44:29 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-03f0a908-0548-478f-9af3-1870cfeb1c80
21/01/20 15:44:29 INFO FileFormatWriter: Write Job 2dd8715b-10d2-40fd-8ce7-9353aa7cb41c committed.
21/01/20 15:44:29 INFO FileFormatWriter: Finished processing stats for write job 2dd8715b-10d2-40fd-8ce7-9353aa7cb41c.
21/01/20 15:44:30 INFO BlockManagerInfo: Removed broadcast_147_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:44:31 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:31 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:44:31 INFO DAGScheduler: Got job 148 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:44:31 INFO DAGScheduler: Final stage: ResultStage 148 (parquet at Generate.java:61)
21/01/20 15:44:31 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:44:31 INFO DAGScheduler: Missing parents: List()
21/01/20 15:44:31 INFO DAGScheduler: Submitting ResultStage 148 (MapPartitionsRDD[593] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:44:31 INFO MemoryStore: Block broadcast_148 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:44:31 INFO MemoryStore: Block broadcast_148_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:44:31 INFO BlockManagerInfo: Added broadcast_148_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:44:31 INFO SparkContext: Created broadcast 148 from broadcast at DAGScheduler.scala:1200
21/01/20 15:44:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 148 (MapPartitionsRDD[593] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:44:31 INFO TaskSchedulerImpl: Adding task set 148.0 with 1 tasks
21/01/20 15:44:31 WARN TaskSetManager: Stage 148 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:44:31 INFO TaskSetManager: Starting task 0.0 in stage 148.0 (TID 148, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:44:31 INFO Executor: Running task 0.0 in stage 148.0 (TID 148)
21/01/20 15:44:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:31 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:44:31 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:44:31 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:44:31 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:44:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:44:31 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:44:31 INFO ParquetOutputFormat: Validation is off
21/01/20 15:44:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:44:31 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:44:31 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:44:31 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:44:31 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:44:31 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:44:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:44:32 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154431_0148_m_000000_148' to o3fs://bucket1.vol1/testdata
21/01/20 15:44:32 INFO SparkHadoopMapRedUtil: attempt_20210120154431_0148_m_000000_148: Committed
21/01/20 15:44:32 INFO Executor: Finished task 0.0 in stage 148.0 (TID 148). 2155 bytes result sent to driver
21/01/20 15:44:32 INFO TaskSetManager: Finished task 0.0 in stage 148.0 (TID 148) in 1093 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:44:32 INFO TaskSchedulerImpl: Removed TaskSet 148.0, whose tasks have all completed, from pool 
21/01/20 15:44:32 INFO DAGScheduler: ResultStage 148 (parquet at Generate.java:61) finished in 1.111 s
21/01/20 15:44:32 INFO DAGScheduler: Job 148 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:44:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 148: Stage finished
21/01/20 15:44:32 INFO DAGScheduler: Job 148 finished: parquet at Generate.java:61, took 1.112409 s
21/01/20 15:44:32 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-976b9d3e-1a06-4ee9-bd4c-0b163e2153f4
21/01/20 15:44:32 INFO FileFormatWriter: Write Job 64f3be3c-edf3-49a1-8aff-9dff832cbe95 committed.
21/01/20 15:44:32 INFO FileFormatWriter: Finished processing stats for write job 64f3be3c-edf3-49a1-8aff-9dff832cbe95.
21/01/20 15:44:33 INFO BlockManagerInfo: Removed broadcast_148_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:44:35 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:35 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:35 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:44:35 INFO DAGScheduler: Got job 149 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:44:35 INFO DAGScheduler: Final stage: ResultStage 149 (parquet at Generate.java:61)
21/01/20 15:44:35 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:44:35 INFO DAGScheduler: Missing parents: List()
21/01/20 15:44:35 INFO DAGScheduler: Submitting ResultStage 149 (MapPartitionsRDD[597] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:44:35 INFO MemoryStore: Block broadcast_149 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:44:35 INFO MemoryStore: Block broadcast_149_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:44:35 INFO BlockManagerInfo: Added broadcast_149_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:44:35 INFO SparkContext: Created broadcast 149 from broadcast at DAGScheduler.scala:1200
21/01/20 15:44:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 149 (MapPartitionsRDD[597] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:44:35 INFO TaskSchedulerImpl: Adding task set 149.0 with 1 tasks
21/01/20 15:44:35 WARN TaskSetManager: Stage 149 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:44:35 INFO TaskSetManager: Starting task 0.0 in stage 149.0 (TID 149, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:44:35 INFO Executor: Running task 0.0 in stage 149.0 (TID 149)
21/01/20 15:44:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:35 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:35 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:44:35 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:44:35 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:44:35 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:44:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:44:35 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:44:35 INFO ParquetOutputFormat: Validation is off
21/01/20 15:44:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:44:35 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:44:35 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:44:35 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:44:35 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:44:35 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:44:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:44:36 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154435_0149_m_000000_149' to o3fs://bucket1.vol1/testdata
21/01/20 15:44:36 INFO SparkHadoopMapRedUtil: attempt_20210120154435_0149_m_000000_149: Committed
21/01/20 15:44:36 INFO Executor: Finished task 0.0 in stage 149.0 (TID 149). 2155 bytes result sent to driver
21/01/20 15:44:36 INFO TaskSetManager: Finished task 0.0 in stage 149.0 (TID 149) in 1028 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:44:36 INFO TaskSchedulerImpl: Removed TaskSet 149.0, whose tasks have all completed, from pool 
21/01/20 15:44:36 INFO DAGScheduler: ResultStage 149 (parquet at Generate.java:61) finished in 1.048 s
21/01/20 15:44:36 INFO DAGScheduler: Job 149 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:44:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 149: Stage finished
21/01/20 15:44:36 INFO DAGScheduler: Job 149 finished: parquet at Generate.java:61, took 1.051892 s
21/01/20 15:44:36 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-d5765519-739b-4b75-b996-ba7ea7fb6a8f
21/01/20 15:44:36 INFO FileFormatWriter: Write Job 72a0254e-a72c-4734-8736-33dd10c46761 committed.
21/01/20 15:44:36 INFO FileFormatWriter: Finished processing stats for write job 72a0254e-a72c-4734-8736-33dd10c46761.
21/01/20 15:44:37 INFO BlockManagerInfo: Removed broadcast_149_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:44:39 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:39 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:39 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:44:39 INFO DAGScheduler: Got job 150 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:44:39 INFO DAGScheduler: Final stage: ResultStage 150 (parquet at Generate.java:61)
21/01/20 15:44:39 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:44:39 INFO DAGScheduler: Missing parents: List()
21/01/20 15:44:39 INFO DAGScheduler: Submitting ResultStage 150 (MapPartitionsRDD[601] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:44:39 INFO MemoryStore: Block broadcast_150 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:44:39 INFO MemoryStore: Block broadcast_150_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:44:39 INFO BlockManagerInfo: Added broadcast_150_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:44:39 INFO SparkContext: Created broadcast 150 from broadcast at DAGScheduler.scala:1200
21/01/20 15:44:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 150 (MapPartitionsRDD[601] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:44:39 INFO TaskSchedulerImpl: Adding task set 150.0 with 1 tasks
21/01/20 15:44:39 WARN TaskSetManager: Stage 150 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:44:39 INFO TaskSetManager: Starting task 0.0 in stage 150.0 (TID 150, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:44:39 INFO Executor: Running task 0.0 in stage 150.0 (TID 150)
21/01/20 15:44:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:39 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:39 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:44:39 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:44:39 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:44:39 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:44:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:44:39 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:44:39 INFO ParquetOutputFormat: Validation is off
21/01/20 15:44:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:44:39 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:44:39 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:44:39 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:44:39 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:44:39 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:44:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:44:40 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154439_0150_m_000000_150' to o3fs://bucket1.vol1/testdata
21/01/20 15:44:40 INFO SparkHadoopMapRedUtil: attempt_20210120154439_0150_m_000000_150: Committed
21/01/20 15:44:40 INFO Executor: Finished task 0.0 in stage 150.0 (TID 150). 2155 bytes result sent to driver
21/01/20 15:44:40 INFO TaskSetManager: Finished task 0.0 in stage 150.0 (TID 150) in 959 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:44:40 INFO TaskSchedulerImpl: Removed TaskSet 150.0, whose tasks have all completed, from pool 
21/01/20 15:44:40 INFO DAGScheduler: ResultStage 150 (parquet at Generate.java:61) finished in 0.977 s
21/01/20 15:44:40 INFO DAGScheduler: Job 150 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:44:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 150: Stage finished
21/01/20 15:44:40 INFO DAGScheduler: Job 150 finished: parquet at Generate.java:61, took 0.978173 s
21/01/20 15:44:40 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-ab9881da-6e02-4732-9c84-62d40abe280c
21/01/20 15:44:40 INFO FileFormatWriter: Write Job 22ec2924-e8fe-4f86-88a5-0251f7afc534 committed.
21/01/20 15:44:40 INFO FileFormatWriter: Finished processing stats for write job 22ec2924-e8fe-4f86-88a5-0251f7afc534.
21/01/20 15:44:41 INFO BlockManagerInfo: Removed broadcast_150_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:44:42 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:42 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:42 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:44:42 INFO DAGScheduler: Got job 151 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:44:42 INFO DAGScheduler: Final stage: ResultStage 151 (parquet at Generate.java:61)
21/01/20 15:44:42 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:44:42 INFO DAGScheduler: Missing parents: List()
21/01/20 15:44:42 INFO DAGScheduler: Submitting ResultStage 151 (MapPartitionsRDD[605] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:44:42 INFO MemoryStore: Block broadcast_151 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:44:42 INFO MemoryStore: Block broadcast_151_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:44:42 INFO BlockManagerInfo: Added broadcast_151_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:44:42 INFO SparkContext: Created broadcast 151 from broadcast at DAGScheduler.scala:1200
21/01/20 15:44:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 151 (MapPartitionsRDD[605] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:44:42 INFO TaskSchedulerImpl: Adding task set 151.0 with 1 tasks
21/01/20 15:44:42 WARN TaskSetManager: Stage 151 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:44:42 INFO TaskSetManager: Starting task 0.0 in stage 151.0 (TID 151, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:44:42 INFO Executor: Running task 0.0 in stage 151.0 (TID 151)
21/01/20 15:44:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:43 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:43 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:44:43 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:44:43 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:44:43 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:44:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:44:43 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:44:43 INFO ParquetOutputFormat: Validation is off
21/01/20 15:44:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:44:43 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:44:43 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:44:43 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:44:43 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:44:43 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:44:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:44:43 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-B87819F985ED->d3c4d3e3-944b-4528-9d82-e005ad427697
21/01/20 15:44:43 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:44:43 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154442_0151_m_000000_151' to o3fs://bucket1.vol1/testdata
21/01/20 15:44:43 INFO SparkHadoopMapRedUtil: attempt_20210120154442_0151_m_000000_151: Committed
21/01/20 15:44:43 INFO Executor: Finished task 0.0 in stage 151.0 (TID 151). 2155 bytes result sent to driver
21/01/20 15:44:43 INFO TaskSetManager: Finished task 0.0 in stage 151.0 (TID 151) in 1077 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:44:43 INFO TaskSchedulerImpl: Removed TaskSet 151.0, whose tasks have all completed, from pool 
21/01/20 15:44:43 INFO DAGScheduler: ResultStage 151 (parquet at Generate.java:61) finished in 1.118 s
21/01/20 15:44:43 INFO DAGScheduler: Job 151 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:44:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 151: Stage finished
21/01/20 15:44:43 INFO DAGScheduler: Job 151 finished: parquet at Generate.java:61, took 1.119385 s
21/01/20 15:44:43 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-487935dc-84c6-4ad0-9785-d579d605967f
21/01/20 15:44:43 INFO FileFormatWriter: Write Job a7a8e867-2a15-4795-8582-5173779e9dc8 committed.
21/01/20 15:44:43 INFO FileFormatWriter: Finished processing stats for write job a7a8e867-2a15-4795-8582-5173779e9dc8.
21/01/20 15:44:44 INFO BlockManagerInfo: Removed broadcast_151_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:44:46 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:46 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:46 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:44:46 INFO DAGScheduler: Got job 152 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:44:46 INFO DAGScheduler: Final stage: ResultStage 152 (parquet at Generate.java:61)
21/01/20 15:44:46 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:44:46 INFO DAGScheduler: Missing parents: List()
21/01/20 15:44:46 INFO DAGScheduler: Submitting ResultStage 152 (MapPartitionsRDD[609] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:44:46 INFO MemoryStore: Block broadcast_152 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:44:46 INFO MemoryStore: Block broadcast_152_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:44:46 INFO BlockManagerInfo: Added broadcast_152_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:44:46 INFO SparkContext: Created broadcast 152 from broadcast at DAGScheduler.scala:1200
21/01/20 15:44:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 152 (MapPartitionsRDD[609] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:44:46 INFO TaskSchedulerImpl: Adding task set 152.0 with 1 tasks
21/01/20 15:44:46 WARN TaskSetManager: Stage 152 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:44:46 INFO TaskSetManager: Starting task 0.0 in stage 152.0 (TID 152, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:44:46 INFO Executor: Running task 0.0 in stage 152.0 (TID 152)
21/01/20 15:44:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:46 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:46 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:44:46 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:44:46 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:44:46 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:44:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:44:46 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:44:46 INFO ParquetOutputFormat: Validation is off
21/01/20 15:44:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:44:46 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:44:46 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:44:46 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:44:46 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:44:46 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:44:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:44:47 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154446_0152_m_000000_152' to o3fs://bucket1.vol1/testdata
21/01/20 15:44:47 INFO SparkHadoopMapRedUtil: attempt_20210120154446_0152_m_000000_152: Committed
21/01/20 15:44:47 INFO Executor: Finished task 0.0 in stage 152.0 (TID 152). 2155 bytes result sent to driver
21/01/20 15:44:47 INFO TaskSetManager: Finished task 0.0 in stage 152.0 (TID 152) in 1101 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:44:47 INFO TaskSchedulerImpl: Removed TaskSet 152.0, whose tasks have all completed, from pool 
21/01/20 15:44:47 INFO DAGScheduler: ResultStage 152 (parquet at Generate.java:61) finished in 1.119 s
21/01/20 15:44:47 INFO DAGScheduler: Job 152 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:44:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 152: Stage finished
21/01/20 15:44:47 INFO DAGScheduler: Job 152 finished: parquet at Generate.java:61, took 1.120680 s
21/01/20 15:44:47 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-966cc1de-0a89-4a92-b8dc-113f8c50b483
21/01/20 15:44:47 INFO FileFormatWriter: Write Job 86d6d1cb-e87f-4e1e-94a9-4492edf1c772 committed.
21/01/20 15:44:47 INFO FileFormatWriter: Finished processing stats for write job 86d6d1cb-e87f-4e1e-94a9-4492edf1c772.
21/01/20 15:44:49 INFO BlockManagerInfo: Removed broadcast_152_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:44:50 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:50 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:50 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:44:50 INFO DAGScheduler: Got job 153 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:44:50 INFO DAGScheduler: Final stage: ResultStage 153 (parquet at Generate.java:61)
21/01/20 15:44:50 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:44:50 INFO DAGScheduler: Missing parents: List()
21/01/20 15:44:50 INFO DAGScheduler: Submitting ResultStage 153 (MapPartitionsRDD[613] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:44:50 INFO MemoryStore: Block broadcast_153 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:44:50 INFO MemoryStore: Block broadcast_153_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:44:50 INFO BlockManagerInfo: Added broadcast_153_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:44:50 INFO SparkContext: Created broadcast 153 from broadcast at DAGScheduler.scala:1200
21/01/20 15:44:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 153 (MapPartitionsRDD[613] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:44:50 INFO TaskSchedulerImpl: Adding task set 153.0 with 1 tasks
21/01/20 15:44:50 WARN TaskSetManager: Stage 153 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:44:50 INFO TaskSetManager: Starting task 0.0 in stage 153.0 (TID 153, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:44:50 INFO Executor: Running task 0.0 in stage 153.0 (TID 153)
21/01/20 15:44:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:50 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:50 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:44:50 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:44:50 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:44:50 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:44:50 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:44:50 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:44:50 INFO ParquetOutputFormat: Validation is off
21/01/20 15:44:50 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:44:50 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:44:50 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:44:50 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:44:50 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:44:50 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:44:50 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:44:50 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-4389BDA77AF9->af5c9c6a-a887-4538-ae7b-ba6d49328297
21/01/20 15:44:50 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:44:51 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154450_0153_m_000000_153' to o3fs://bucket1.vol1/testdata
21/01/20 15:44:51 INFO SparkHadoopMapRedUtil: attempt_20210120154450_0153_m_000000_153: Committed
21/01/20 15:44:51 INFO Executor: Finished task 0.0 in stage 153.0 (TID 153). 2155 bytes result sent to driver
21/01/20 15:44:51 INFO TaskSetManager: Finished task 0.0 in stage 153.0 (TID 153) in 882 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:44:51 INFO TaskSchedulerImpl: Removed TaskSet 153.0, whose tasks have all completed, from pool 
21/01/20 15:44:51 INFO DAGScheduler: ResultStage 153 (parquet at Generate.java:61) finished in 0.924 s
21/01/20 15:44:51 INFO DAGScheduler: Job 153 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:44:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 153: Stage finished
21/01/20 15:44:51 INFO DAGScheduler: Job 153 finished: parquet at Generate.java:61, took 0.925549 s
21/01/20 15:44:51 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-61cde6e9-28dc-48b0-a95d-34229ceeb5e3
21/01/20 15:44:51 INFO FileFormatWriter: Write Job 9f657c0d-0710-46d6-a475-5073667d35a9 committed.
21/01/20 15:44:51 INFO FileFormatWriter: Finished processing stats for write job 9f657c0d-0710-46d6-a475-5073667d35a9.
21/01/20 15:44:51 INFO BlockManagerInfo: Removed broadcast_153_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:44:53 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:53 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:44:53 INFO DAGScheduler: Got job 154 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:44:53 INFO DAGScheduler: Final stage: ResultStage 154 (parquet at Generate.java:61)
21/01/20 15:44:53 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:44:53 INFO DAGScheduler: Missing parents: List()
21/01/20 15:44:53 INFO DAGScheduler: Submitting ResultStage 154 (MapPartitionsRDD[617] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:44:53 INFO MemoryStore: Block broadcast_154 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:44:53 INFO MemoryStore: Block broadcast_154_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:44:53 INFO BlockManagerInfo: Added broadcast_154_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:44:53 INFO SparkContext: Created broadcast 154 from broadcast at DAGScheduler.scala:1200
21/01/20 15:44:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 154 (MapPartitionsRDD[617] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:44:53 INFO TaskSchedulerImpl: Adding task set 154.0 with 1 tasks
21/01/20 15:44:53 WARN TaskSetManager: Stage 154 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:44:53 INFO TaskSetManager: Starting task 0.0 in stage 154.0 (TID 154, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:44:53 INFO Executor: Running task 0.0 in stage 154.0 (TID 154)
21/01/20 15:44:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:53 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:44:53 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:44:53 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:44:53 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:44:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:44:53 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:44:53 INFO ParquetOutputFormat: Validation is off
21/01/20 15:44:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:44:53 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:44:53 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:44:53 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:44:53 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:44:53 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:44:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:44:54 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154453_0154_m_000000_154' to o3fs://bucket1.vol1/testdata
21/01/20 15:44:54 INFO SparkHadoopMapRedUtil: attempt_20210120154453_0154_m_000000_154: Committed
21/01/20 15:44:54 INFO Executor: Finished task 0.0 in stage 154.0 (TID 154). 2155 bytes result sent to driver
21/01/20 15:44:54 INFO TaskSetManager: Finished task 0.0 in stage 154.0 (TID 154) in 1050 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:44:54 INFO TaskSchedulerImpl: Removed TaskSet 154.0, whose tasks have all completed, from pool 
21/01/20 15:44:54 INFO DAGScheduler: ResultStage 154 (parquet at Generate.java:61) finished in 1.068 s
21/01/20 15:44:54 INFO DAGScheduler: Job 154 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:44:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 154: Stage finished
21/01/20 15:44:54 INFO DAGScheduler: Job 154 finished: parquet at Generate.java:61, took 1.069646 s
21/01/20 15:44:54 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-03ac2d99-291d-4518-9816-cab56091fc3b
21/01/20 15:44:54 INFO FileFormatWriter: Write Job 9e078b5b-70de-49b9-b502-974813f1d121 committed.
21/01/20 15:44:54 INFO FileFormatWriter: Finished processing stats for write job 9e078b5b-70de-49b9-b502-974813f1d121.
21/01/20 15:44:56 INFO BlockManagerInfo: Removed broadcast_154_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:44:57 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:57 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:57 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:44:57 INFO DAGScheduler: Got job 155 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:44:57 INFO DAGScheduler: Final stage: ResultStage 155 (parquet at Generate.java:61)
21/01/20 15:44:57 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:44:57 INFO DAGScheduler: Missing parents: List()
21/01/20 15:44:57 INFO DAGScheduler: Submitting ResultStage 155 (MapPartitionsRDD[621] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:44:57 INFO MemoryStore: Block broadcast_155 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:44:57 INFO MemoryStore: Block broadcast_155_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:44:57 INFO BlockManagerInfo: Added broadcast_155_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:44:57 INFO SparkContext: Created broadcast 155 from broadcast at DAGScheduler.scala:1200
21/01/20 15:44:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 155 (MapPartitionsRDD[621] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:44:57 INFO TaskSchedulerImpl: Adding task set 155.0 with 1 tasks
21/01/20 15:44:57 WARN TaskSetManager: Stage 155 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:44:57 INFO TaskSetManager: Starting task 0.0 in stage 155.0 (TID 155, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:44:57 INFO Executor: Running task 0.0 in stage 155.0 (TID 155)
21/01/20 15:44:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:57 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:44:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:44:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:44:57 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:44:57 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:44:57 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:44:57 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:44:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:44:57 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:44:57 INFO ParquetOutputFormat: Validation is off
21/01/20 15:44:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:44:57 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:44:57 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:44:57 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:44:57 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:44:57 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:44:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154457_0155_m_000000_155' to o3fs://bucket1.vol1/testdata
21/01/20 15:44:58 INFO SparkHadoopMapRedUtil: attempt_20210120154457_0155_m_000000_155: Committed
21/01/20 15:44:58 INFO Executor: Finished task 0.0 in stage 155.0 (TID 155). 2155 bytes result sent to driver
21/01/20 15:44:58 INFO TaskSetManager: Finished task 0.0 in stage 155.0 (TID 155) in 863 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:44:58 INFO TaskSchedulerImpl: Removed TaskSet 155.0, whose tasks have all completed, from pool 
21/01/20 15:44:58 INFO DAGScheduler: ResultStage 155 (parquet at Generate.java:61) finished in 0.904 s
21/01/20 15:44:58 INFO DAGScheduler: Job 155 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:44:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 155: Stage finished
21/01/20 15:44:58 INFO DAGScheduler: Job 155 finished: parquet at Generate.java:61, took 0.906353 s
21/01/20 15:44:58 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-96ae6e53-b991-4efc-88da-b8249d330e92
21/01/20 15:44:58 INFO FileFormatWriter: Write Job f48d972d-a119-4bd8-8916-90f155adb0a6 committed.
21/01/20 15:44:58 INFO FileFormatWriter: Finished processing stats for write job f48d972d-a119-4bd8-8916-90f155adb0a6.
21/01/20 15:44:58 INFO BlockManagerInfo: Removed broadcast_155_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:45:01 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:01 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:01 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:45:01 INFO DAGScheduler: Got job 156 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:45:01 INFO DAGScheduler: Final stage: ResultStage 156 (parquet at Generate.java:61)
21/01/20 15:45:01 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:45:01 INFO DAGScheduler: Missing parents: List()
21/01/20 15:45:01 INFO DAGScheduler: Submitting ResultStage 156 (MapPartitionsRDD[625] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:45:01 INFO MemoryStore: Block broadcast_156 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:45:01 INFO MemoryStore: Block broadcast_156_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:45:01 INFO BlockManagerInfo: Added broadcast_156_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:45:01 INFO SparkContext: Created broadcast 156 from broadcast at DAGScheduler.scala:1200
21/01/20 15:45:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 156 (MapPartitionsRDD[625] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:45:01 INFO TaskSchedulerImpl: Adding task set 156.0 with 1 tasks
21/01/20 15:45:01 WARN TaskSetManager: Stage 156 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:45:01 INFO TaskSetManager: Starting task 0.0 in stage 156.0 (TID 156, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:45:01 INFO Executor: Running task 0.0 in stage 156.0 (TID 156)
21/01/20 15:45:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:01 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:01 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:45:01 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:45:01 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:45:01 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:45:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:45:01 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:45:01 INFO ParquetOutputFormat: Validation is off
21/01/20 15:45:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:45:01 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:45:01 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:45:01 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:45:01 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:45:01 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:45:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:45:02 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154501_0156_m_000000_156' to o3fs://bucket1.vol1/testdata
21/01/20 15:45:02 INFO SparkHadoopMapRedUtil: attempt_20210120154501_0156_m_000000_156: Committed
21/01/20 15:45:02 INFO Executor: Finished task 0.0 in stage 156.0 (TID 156). 2155 bytes result sent to driver
21/01/20 15:45:02 INFO TaskSetManager: Finished task 0.0 in stage 156.0 (TID 156) in 1053 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:45:02 INFO TaskSchedulerImpl: Removed TaskSet 156.0, whose tasks have all completed, from pool 
21/01/20 15:45:02 INFO DAGScheduler: ResultStage 156 (parquet at Generate.java:61) finished in 1.070 s
21/01/20 15:45:02 INFO DAGScheduler: Job 156 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:45:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 156: Stage finished
21/01/20 15:45:02 INFO DAGScheduler: Job 156 finished: parquet at Generate.java:61, took 1.071761 s
21/01/20 15:45:02 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-89459d11-e941-48ea-9693-84ded47ddcd3
21/01/20 15:45:02 INFO FileFormatWriter: Write Job 42f76980-d3a0-4bc8-b3ff-eb93d3561d1d committed.
21/01/20 15:45:02 INFO FileFormatWriter: Finished processing stats for write job 42f76980-d3a0-4bc8-b3ff-eb93d3561d1d.
21/01/20 15:45:03 INFO BlockManagerInfo: Removed broadcast_156_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:45:04 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:04 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:45:04 INFO DAGScheduler: Got job 157 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:45:04 INFO DAGScheduler: Final stage: ResultStage 157 (parquet at Generate.java:61)
21/01/20 15:45:04 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:45:04 INFO DAGScheduler: Missing parents: List()
21/01/20 15:45:04 INFO DAGScheduler: Submitting ResultStage 157 (MapPartitionsRDD[629] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:45:04 INFO MemoryStore: Block broadcast_157 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:45:04 INFO MemoryStore: Block broadcast_157_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:45:04 INFO BlockManagerInfo: Added broadcast_157_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:45:04 INFO SparkContext: Created broadcast 157 from broadcast at DAGScheduler.scala:1200
21/01/20 15:45:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 157 (MapPartitionsRDD[629] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:45:04 INFO TaskSchedulerImpl: Adding task set 157.0 with 1 tasks
21/01/20 15:45:04 WARN TaskSetManager: Stage 157 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:45:04 INFO TaskSetManager: Starting task 0.0 in stage 157.0 (TID 157, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:45:04 INFO Executor: Running task 0.0 in stage 157.0 (TID 157)
21/01/20 15:45:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:04 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:45:04 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:45:04 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:45:04 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:45:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:45:04 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:45:04 INFO ParquetOutputFormat: Validation is off
21/01/20 15:45:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:45:04 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:45:04 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:45:04 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:45:04 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:45:04 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:45:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:45:05 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154504_0157_m_000000_157' to o3fs://bucket1.vol1/testdata
21/01/20 15:45:05 INFO SparkHadoopMapRedUtil: attempt_20210120154504_0157_m_000000_157: Committed
21/01/20 15:45:05 INFO Executor: Finished task 0.0 in stage 157.0 (TID 157). 2155 bytes result sent to driver
21/01/20 15:45:05 INFO TaskSetManager: Finished task 0.0 in stage 157.0 (TID 157) in 970 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:45:05 INFO TaskSchedulerImpl: Removed TaskSet 157.0, whose tasks have all completed, from pool 
21/01/20 15:45:05 INFO DAGScheduler: ResultStage 157 (parquet at Generate.java:61) finished in 0.987 s
21/01/20 15:45:05 INFO DAGScheduler: Job 157 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:45:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 157: Stage finished
21/01/20 15:45:05 INFO DAGScheduler: Job 157 finished: parquet at Generate.java:61, took 0.989078 s
21/01/20 15:45:05 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-900b7d84-a8c7-463c-8e04-e63fc682d06a
21/01/20 15:45:05 INFO FileFormatWriter: Write Job f070a736-2d05-49da-98e0-d00bd343d52d committed.
21/01/20 15:45:05 INFO FileFormatWriter: Finished processing stats for write job f070a736-2d05-49da-98e0-d00bd343d52d.
21/01/20 15:45:07 INFO BlockManagerInfo: Removed broadcast_157_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:45:08 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:08 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:08 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:45:08 INFO DAGScheduler: Got job 158 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:45:08 INFO DAGScheduler: Final stage: ResultStage 158 (parquet at Generate.java:61)
21/01/20 15:45:08 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:45:08 INFO DAGScheduler: Missing parents: List()
21/01/20 15:45:08 INFO DAGScheduler: Submitting ResultStage 158 (MapPartitionsRDD[633] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:45:08 INFO MemoryStore: Block broadcast_158 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:45:08 INFO MemoryStore: Block broadcast_158_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:45:08 INFO BlockManagerInfo: Added broadcast_158_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:45:08 INFO SparkContext: Created broadcast 158 from broadcast at DAGScheduler.scala:1200
21/01/20 15:45:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 158 (MapPartitionsRDD[633] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:45:08 INFO TaskSchedulerImpl: Adding task set 158.0 with 1 tasks
21/01/20 15:45:08 WARN TaskSetManager: Stage 158 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:45:08 INFO TaskSetManager: Starting task 0.0 in stage 158.0 (TID 158, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:45:08 INFO Executor: Running task 0.0 in stage 158.0 (TID 158)
21/01/20 15:45:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:08 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:08 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:45:08 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:45:08 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:45:08 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:45:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:45:08 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:45:08 INFO ParquetOutputFormat: Validation is off
21/01/20 15:45:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:45:08 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:45:08 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:45:08 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:45:08 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:45:08 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:45:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:45:09 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154508_0158_m_000000_158' to o3fs://bucket1.vol1/testdata
21/01/20 15:45:09 INFO SparkHadoopMapRedUtil: attempt_20210120154508_0158_m_000000_158: Committed
21/01/20 15:45:09 INFO Executor: Finished task 0.0 in stage 158.0 (TID 158). 2155 bytes result sent to driver
21/01/20 15:45:09 INFO TaskSetManager: Finished task 0.0 in stage 158.0 (TID 158) in 923 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:45:09 INFO TaskSchedulerImpl: Removed TaskSet 158.0, whose tasks have all completed, from pool 
21/01/20 15:45:09 INFO DAGScheduler: ResultStage 158 (parquet at Generate.java:61) finished in 0.964 s
21/01/20 15:45:09 INFO DAGScheduler: Job 158 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:45:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 158: Stage finished
21/01/20 15:45:09 INFO DAGScheduler: Job 158 finished: parquet at Generate.java:61, took 0.964631 s
21/01/20 15:45:09 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-f1110613-cb65-4104-b771-70985f1f8f1b
21/01/20 15:45:09 INFO FileFormatWriter: Write Job 09d161ef-716c-4284-9522-9ca40e49c605 committed.
21/01/20 15:45:09 INFO FileFormatWriter: Finished processing stats for write job 09d161ef-716c-4284-9522-9ca40e49c605.
21/01/20 15:45:09 INFO BlockManagerInfo: Removed broadcast_158_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:45:11 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:11 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:45:11 INFO DAGScheduler: Got job 159 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:45:11 INFO DAGScheduler: Final stage: ResultStage 159 (parquet at Generate.java:61)
21/01/20 15:45:11 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:45:11 INFO DAGScheduler: Missing parents: List()
21/01/20 15:45:11 INFO DAGScheduler: Submitting ResultStage 159 (MapPartitionsRDD[637] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:45:11 INFO MemoryStore: Block broadcast_159 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:45:11 INFO MemoryStore: Block broadcast_159_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:45:11 INFO BlockManagerInfo: Added broadcast_159_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:45:11 INFO SparkContext: Created broadcast 159 from broadcast at DAGScheduler.scala:1200
21/01/20 15:45:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 159 (MapPartitionsRDD[637] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:45:11 INFO TaskSchedulerImpl: Adding task set 159.0 with 1 tasks
21/01/20 15:45:12 WARN TaskSetManager: Stage 159 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:45:12 INFO TaskSetManager: Starting task 0.0 in stage 159.0 (TID 159, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:45:12 INFO Executor: Running task 0.0 in stage 159.0 (TID 159)
21/01/20 15:45:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:12 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:12 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:45:12 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:45:12 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:45:12 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:45:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:45:12 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:45:12 INFO ParquetOutputFormat: Validation is off
21/01/20 15:45:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:45:12 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:45:12 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:45:12 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:45:12 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:45:12 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:45:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:45:12 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154511_0159_m_000000_159' to o3fs://bucket1.vol1/testdata
21/01/20 15:45:12 INFO SparkHadoopMapRedUtil: attempt_20210120154511_0159_m_000000_159: Committed
21/01/20 15:45:12 INFO Executor: Finished task 0.0 in stage 159.0 (TID 159). 2155 bytes result sent to driver
21/01/20 15:45:12 INFO TaskSetManager: Finished task 0.0 in stage 159.0 (TID 159) in 1039 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:45:12 INFO TaskSchedulerImpl: Removed TaskSet 159.0, whose tasks have all completed, from pool 
21/01/20 15:45:12 INFO DAGScheduler: ResultStage 159 (parquet at Generate.java:61) finished in 1.056 s
21/01/20 15:45:12 INFO DAGScheduler: Job 159 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:45:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 159: Stage finished
21/01/20 15:45:12 INFO DAGScheduler: Job 159 finished: parquet at Generate.java:61, took 1.057739 s
21/01/20 15:45:13 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-08047ae2-ebc4-4d06-b9dd-a5b7ec60aed3
21/01/20 15:45:13 INFO FileFormatWriter: Write Job db20077d-4610-4032-8ffd-ca9d1f9e1222 committed.
21/01/20 15:45:13 INFO FileFormatWriter: Finished processing stats for write job db20077d-4610-4032-8ffd-ca9d1f9e1222.
21/01/20 15:45:14 INFO BlockManagerInfo: Removed broadcast_159_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:45:15 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:15 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:15 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:45:15 INFO DAGScheduler: Got job 160 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:45:15 INFO DAGScheduler: Final stage: ResultStage 160 (parquet at Generate.java:61)
21/01/20 15:45:15 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:45:15 INFO DAGScheduler: Missing parents: List()
21/01/20 15:45:15 INFO DAGScheduler: Submitting ResultStage 160 (MapPartitionsRDD[641] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:45:15 INFO MemoryStore: Block broadcast_160 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:45:15 INFO MemoryStore: Block broadcast_160_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:45:15 INFO BlockManagerInfo: Added broadcast_160_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:45:15 INFO SparkContext: Created broadcast 160 from broadcast at DAGScheduler.scala:1200
21/01/20 15:45:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 160 (MapPartitionsRDD[641] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:45:15 INFO TaskSchedulerImpl: Adding task set 160.0 with 1 tasks
21/01/20 15:45:15 WARN TaskSetManager: Stage 160 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:45:15 INFO TaskSetManager: Starting task 0.0 in stage 160.0 (TID 160, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:45:15 INFO Executor: Running task 0.0 in stage 160.0 (TID 160)
21/01/20 15:45:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:15 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:15 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:45:15 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:45:15 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:45:15 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:45:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:45:15 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:45:15 INFO ParquetOutputFormat: Validation is off
21/01/20 15:45:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:45:15 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:45:15 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:45:15 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:45:15 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:45:15 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:45:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154515_0160_m_000000_160' to o3fs://bucket1.vol1/testdata
21/01/20 15:45:16 INFO SparkHadoopMapRedUtil: attempt_20210120154515_0160_m_000000_160: Committed
21/01/20 15:45:16 INFO Executor: Finished task 0.0 in stage 160.0 (TID 160). 2155 bytes result sent to driver
21/01/20 15:45:16 INFO TaskSetManager: Finished task 0.0 in stage 160.0 (TID 160) in 1081 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:45:16 INFO TaskSchedulerImpl: Removed TaskSet 160.0, whose tasks have all completed, from pool 
21/01/20 15:45:16 INFO DAGScheduler: ResultStage 160 (parquet at Generate.java:61) finished in 1.099 s
21/01/20 15:45:16 INFO DAGScheduler: Job 160 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:45:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 160: Stage finished
21/01/20 15:45:16 INFO DAGScheduler: Job 160 finished: parquet at Generate.java:61, took 1.100299 s
21/01/20 15:45:16 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-edacbdb1-ad32-42bb-a695-6206eb318b8f
21/01/20 15:45:16 INFO FileFormatWriter: Write Job bbac57f0-c47c-4493-a7fa-decb9f8e7dae committed.
21/01/20 15:45:16 INFO FileFormatWriter: Finished processing stats for write job bbac57f0-c47c-4493-a7fa-decb9f8e7dae.
21/01/20 15:45:17 INFO BlockManagerInfo: Removed broadcast_160_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:45:19 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:19 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:19 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:45:19 INFO DAGScheduler: Got job 161 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:45:19 INFO DAGScheduler: Final stage: ResultStage 161 (parquet at Generate.java:61)
21/01/20 15:45:19 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:45:19 INFO DAGScheduler: Missing parents: List()
21/01/20 15:45:19 INFO DAGScheduler: Submitting ResultStage 161 (MapPartitionsRDD[645] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:45:19 INFO MemoryStore: Block broadcast_161 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:45:19 INFO MemoryStore: Block broadcast_161_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:45:19 INFO BlockManagerInfo: Added broadcast_161_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:45:19 INFO SparkContext: Created broadcast 161 from broadcast at DAGScheduler.scala:1200
21/01/20 15:45:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 161 (MapPartitionsRDD[645] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:45:19 INFO TaskSchedulerImpl: Adding task set 161.0 with 1 tasks
21/01/20 15:45:19 WARN TaskSetManager: Stage 161 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:45:19 INFO TaskSetManager: Starting task 0.0 in stage 161.0 (TID 161, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:45:19 INFO Executor: Running task 0.0 in stage 161.0 (TID 161)
21/01/20 15:45:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:19 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:19 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:45:19 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:45:19 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:45:19 INFO ParquetOutputFormat: Validation is off
21/01/20 15:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:45:19 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:45:19 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:45:19 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:45:19 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:45:19 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:45:20 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154519_0161_m_000000_161' to o3fs://bucket1.vol1/testdata
21/01/20 15:45:20 INFO SparkHadoopMapRedUtil: attempt_20210120154519_0161_m_000000_161: Committed
21/01/20 15:45:20 INFO Executor: Finished task 0.0 in stage 161.0 (TID 161). 2155 bytes result sent to driver
21/01/20 15:45:20 INFO TaskSetManager: Finished task 0.0 in stage 161.0 (TID 161) in 1045 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:45:20 INFO TaskSchedulerImpl: Removed TaskSet 161.0, whose tasks have all completed, from pool 
21/01/20 15:45:20 INFO DAGScheduler: ResultStage 161 (parquet at Generate.java:61) finished in 1.062 s
21/01/20 15:45:20 INFO DAGScheduler: Job 161 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:45:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 161: Stage finished
21/01/20 15:45:20 INFO DAGScheduler: Job 161 finished: parquet at Generate.java:61, took 1.063662 s
21/01/20 15:45:20 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-98c50c17-fa3c-4221-b092-7d075739e155
21/01/20 15:45:20 INFO FileFormatWriter: Write Job 5a486ad6-ca9e-473c-bd19-0ffbf0bd02e4 committed.
21/01/20 15:45:20 INFO FileFormatWriter: Finished processing stats for write job 5a486ad6-ca9e-473c-bd19-0ffbf0bd02e4.
21/01/20 15:45:21 INFO BlockManagerInfo: Removed broadcast_161_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:45:22 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:22 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:23 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:45:23 INFO DAGScheduler: Got job 162 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:45:23 INFO DAGScheduler: Final stage: ResultStage 162 (parquet at Generate.java:61)
21/01/20 15:45:23 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:45:23 INFO DAGScheduler: Missing parents: List()
21/01/20 15:45:23 INFO DAGScheduler: Submitting ResultStage 162 (MapPartitionsRDD[649] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:45:23 INFO MemoryStore: Block broadcast_162 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:45:23 INFO MemoryStore: Block broadcast_162_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:45:23 INFO BlockManagerInfo: Added broadcast_162_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:45:23 INFO SparkContext: Created broadcast 162 from broadcast at DAGScheduler.scala:1200
21/01/20 15:45:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 162 (MapPartitionsRDD[649] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:45:23 INFO TaskSchedulerImpl: Adding task set 162.0 with 1 tasks
21/01/20 15:45:23 WARN TaskSetManager: Stage 162 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:45:23 INFO TaskSetManager: Starting task 0.0 in stage 162.0 (TID 162, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:45:23 INFO Executor: Running task 0.0 in stage 162.0 (TID 162)
21/01/20 15:45:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:23 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:23 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:45:23 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:45:23 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:45:23 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:45:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:45:23 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:45:23 INFO ParquetOutputFormat: Validation is off
21/01/20 15:45:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:45:23 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:45:23 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:45:23 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:45:23 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:45:23 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:45:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:45:24 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154522_0162_m_000000_162' to o3fs://bucket1.vol1/testdata
21/01/20 15:45:24 INFO SparkHadoopMapRedUtil: attempt_20210120154522_0162_m_000000_162: Committed
21/01/20 15:45:24 INFO Executor: Finished task 0.0 in stage 162.0 (TID 162). 2155 bytes result sent to driver
21/01/20 15:45:24 INFO TaskSetManager: Finished task 0.0 in stage 162.0 (TID 162) in 1069 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:45:24 INFO TaskSchedulerImpl: Removed TaskSet 162.0, whose tasks have all completed, from pool 
21/01/20 15:45:24 INFO DAGScheduler: ResultStage 162 (parquet at Generate.java:61) finished in 1.109 s
21/01/20 15:45:24 INFO DAGScheduler: Job 162 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:45:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 162: Stage finished
21/01/20 15:45:24 INFO DAGScheduler: Job 162 finished: parquet at Generate.java:61, took 1.109969 s
21/01/20 15:45:24 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-f4a29673-a590-40d8-b6b8-6bb721fcbc85
21/01/20 15:45:24 INFO FileFormatWriter: Write Job f060b5c8-6d05-4d82-bdad-2353935d00b4 committed.
21/01/20 15:45:24 INFO FileFormatWriter: Finished processing stats for write job f060b5c8-6d05-4d82-bdad-2353935d00b4.
21/01/20 15:45:24 INFO BlockManagerInfo: Removed broadcast_162_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:45:26 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:26 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:26 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:45:26 INFO DAGScheduler: Got job 163 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:45:26 INFO DAGScheduler: Final stage: ResultStage 163 (parquet at Generate.java:61)
21/01/20 15:45:26 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:45:26 INFO DAGScheduler: Missing parents: List()
21/01/20 15:45:26 INFO DAGScheduler: Submitting ResultStage 163 (MapPartitionsRDD[653] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:45:26 INFO MemoryStore: Block broadcast_163 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:45:26 INFO MemoryStore: Block broadcast_163_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:45:26 INFO BlockManagerInfo: Added broadcast_163_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:45:26 INFO SparkContext: Created broadcast 163 from broadcast at DAGScheduler.scala:1200
21/01/20 15:45:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 163 (MapPartitionsRDD[653] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:45:26 INFO TaskSchedulerImpl: Adding task set 163.0 with 1 tasks
21/01/20 15:45:26 WARN TaskSetManager: Stage 163 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:45:26 INFO TaskSetManager: Starting task 0.0 in stage 163.0 (TID 163, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:45:26 INFO Executor: Running task 0.0 in stage 163.0 (TID 163)
21/01/20 15:45:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:26 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:26 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:45:26 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:45:26 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:45:26 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:45:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:45:26 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:45:26 INFO ParquetOutputFormat: Validation is off
21/01/20 15:45:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:45:26 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:45:26 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:45:26 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:45:26 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:45:26 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:45:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:45:27 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-521C56C4695D->af5c9c6a-a887-4538-ae7b-ba6d49328297
21/01/20 15:45:27 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:45:27 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154526_0163_m_000000_163' to o3fs://bucket1.vol1/testdata
21/01/20 15:45:27 INFO SparkHadoopMapRedUtil: attempt_20210120154526_0163_m_000000_163: Committed
21/01/20 15:45:27 INFO Executor: Finished task 0.0 in stage 163.0 (TID 163). 2155 bytes result sent to driver
21/01/20 15:45:27 INFO TaskSetManager: Finished task 0.0 in stage 163.0 (TID 163) in 1042 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:45:27 INFO TaskSchedulerImpl: Removed TaskSet 163.0, whose tasks have all completed, from pool 
21/01/20 15:45:27 INFO DAGScheduler: ResultStage 163 (parquet at Generate.java:61) finished in 1.059 s
21/01/20 15:45:27 INFO DAGScheduler: Job 163 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:45:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 163: Stage finished
21/01/20 15:45:27 INFO DAGScheduler: Job 163 finished: parquet at Generate.java:61, took 1.060364 s
21/01/20 15:45:27 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-72e7ffc7-23e4-4f58-be62-2b65de7cb10c
21/01/20 15:45:27 INFO FileFormatWriter: Write Job e8fb4d97-45b1-4c93-b0f7-5fe6cbdc33bb committed.
21/01/20 15:45:27 INFO FileFormatWriter: Finished processing stats for write job e8fb4d97-45b1-4c93-b0f7-5fe6cbdc33bb.
21/01/20 15:45:29 INFO BlockManagerInfo: Removed broadcast_163_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:45:30 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:30 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:45:30 INFO DAGScheduler: Got job 164 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:45:30 INFO DAGScheduler: Final stage: ResultStage 164 (parquet at Generate.java:61)
21/01/20 15:45:30 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:45:30 INFO DAGScheduler: Missing parents: List()
21/01/20 15:45:30 INFO DAGScheduler: Submitting ResultStage 164 (MapPartitionsRDD[657] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:45:30 INFO MemoryStore: Block broadcast_164 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:45:30 INFO MemoryStore: Block broadcast_164_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:45:30 INFO BlockManagerInfo: Added broadcast_164_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:45:30 INFO SparkContext: Created broadcast 164 from broadcast at DAGScheduler.scala:1200
21/01/20 15:45:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 164 (MapPartitionsRDD[657] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:45:30 INFO TaskSchedulerImpl: Adding task set 164.0 with 1 tasks
21/01/20 15:45:30 WARN TaskSetManager: Stage 164 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:45:30 INFO TaskSetManager: Starting task 0.0 in stage 164.0 (TID 164, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:45:30 INFO Executor: Running task 0.0 in stage 164.0 (TID 164)
21/01/20 15:45:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:30 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:45:30 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:45:30 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:45:30 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:45:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:45:30 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:45:30 INFO ParquetOutputFormat: Validation is off
21/01/20 15:45:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:45:30 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:45:30 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:45:30 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:45:30 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:45:30 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:45:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:45:31 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154530_0164_m_000000_164' to o3fs://bucket1.vol1/testdata
21/01/20 15:45:31 INFO SparkHadoopMapRedUtil: attempt_20210120154530_0164_m_000000_164: Committed
21/01/20 15:45:31 INFO Executor: Finished task 0.0 in stage 164.0 (TID 164). 2155 bytes result sent to driver
21/01/20 15:45:31 INFO TaskSetManager: Finished task 0.0 in stage 164.0 (TID 164) in 853 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:45:31 INFO TaskSchedulerImpl: Removed TaskSet 164.0, whose tasks have all completed, from pool 
21/01/20 15:45:31 INFO DAGScheduler: ResultStage 164 (parquet at Generate.java:61) finished in 0.893 s
21/01/20 15:45:31 INFO DAGScheduler: Job 164 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:45:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 164: Stage finished
21/01/20 15:45:31 INFO DAGScheduler: Job 164 finished: parquet at Generate.java:61, took 0.894424 s
21/01/20 15:45:31 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-8935c8ab-186d-4099-9c1e-a88e22714474
21/01/20 15:45:31 INFO FileFormatWriter: Write Job ac6a944e-7904-4414-808d-175d87041a9a committed.
21/01/20 15:45:31 INFO FileFormatWriter: Finished processing stats for write job ac6a944e-7904-4414-808d-175d87041a9a.
21/01/20 15:45:31 INFO BlockManagerInfo: Removed broadcast_164_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:45:33 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:33 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:33 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:45:33 INFO DAGScheduler: Got job 165 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:45:33 INFO DAGScheduler: Final stage: ResultStage 165 (parquet at Generate.java:61)
21/01/20 15:45:33 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:45:33 INFO DAGScheduler: Missing parents: List()
21/01/20 15:45:33 INFO DAGScheduler: Submitting ResultStage 165 (MapPartitionsRDD[661] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:45:33 INFO MemoryStore: Block broadcast_165 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:45:33 INFO MemoryStore: Block broadcast_165_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:45:33 INFO BlockManagerInfo: Added broadcast_165_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:45:33 INFO SparkContext: Created broadcast 165 from broadcast at DAGScheduler.scala:1200
21/01/20 15:45:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 165 (MapPartitionsRDD[661] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:45:33 INFO TaskSchedulerImpl: Adding task set 165.0 with 1 tasks
21/01/20 15:45:34 WARN TaskSetManager: Stage 165 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:45:34 INFO TaskSetManager: Starting task 0.0 in stage 165.0 (TID 165, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:45:34 INFO Executor: Running task 0.0 in stage 165.0 (TID 165)
21/01/20 15:45:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:34 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:34 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:45:34 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:45:34 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:45:34 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:45:34 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:45:34 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:45:34 INFO ParquetOutputFormat: Validation is off
21/01/20 15:45:34 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:45:34 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:45:34 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:45:34 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:45:34 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:45:34 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:45:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:45:34 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154533_0165_m_000000_165' to o3fs://bucket1.vol1/testdata
21/01/20 15:45:34 INFO SparkHadoopMapRedUtil: attempt_20210120154533_0165_m_000000_165: Committed
21/01/20 15:45:34 INFO Executor: Finished task 0.0 in stage 165.0 (TID 165). 2155 bytes result sent to driver
21/01/20 15:45:34 INFO TaskSetManager: Finished task 0.0 in stage 165.0 (TID 165) in 998 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:45:34 INFO TaskSchedulerImpl: Removed TaskSet 165.0, whose tasks have all completed, from pool 
21/01/20 15:45:34 INFO DAGScheduler: ResultStage 165 (parquet at Generate.java:61) finished in 1.016 s
21/01/20 15:45:34 INFO DAGScheduler: Job 165 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:45:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 165: Stage finished
21/01/20 15:45:34 INFO DAGScheduler: Job 165 finished: parquet at Generate.java:61, took 1.017490 s
21/01/20 15:45:34 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-0f93cfb2-cc22-40a5-ad61-6d879bff8633
21/01/20 15:45:34 INFO FileFormatWriter: Write Job c3d29120-81fe-4ac3-b946-136ae415be0b committed.
21/01/20 15:45:34 INFO FileFormatWriter: Finished processing stats for write job c3d29120-81fe-4ac3-b946-136ae415be0b.
21/01/20 15:45:36 INFO BlockManagerInfo: Removed broadcast_165_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:45:37 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:37 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:37 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:45:37 INFO DAGScheduler: Got job 166 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:45:37 INFO DAGScheduler: Final stage: ResultStage 166 (parquet at Generate.java:61)
21/01/20 15:45:37 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:45:37 INFO DAGScheduler: Missing parents: List()
21/01/20 15:45:37 INFO DAGScheduler: Submitting ResultStage 166 (MapPartitionsRDD[665] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:45:37 INFO MemoryStore: Block broadcast_166 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:45:37 INFO MemoryStore: Block broadcast_166_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:45:37 INFO BlockManagerInfo: Added broadcast_166_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:45:37 INFO SparkContext: Created broadcast 166 from broadcast at DAGScheduler.scala:1200
21/01/20 15:45:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 166 (MapPartitionsRDD[665] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:45:37 INFO TaskSchedulerImpl: Adding task set 166.0 with 1 tasks
21/01/20 15:45:37 WARN TaskSetManager: Stage 166 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:45:37 INFO TaskSetManager: Starting task 0.0 in stage 166.0 (TID 166, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:45:37 INFO Executor: Running task 0.0 in stage 166.0 (TID 166)
21/01/20 15:45:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:37 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:37 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:45:37 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:45:37 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:45:37 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:45:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:45:37 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:45:37 INFO ParquetOutputFormat: Validation is off
21/01/20 15:45:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:45:37 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:45:37 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:45:37 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:45:37 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:45:37 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:45:37 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:45:38 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154537_0166_m_000000_166' to o3fs://bucket1.vol1/testdata
21/01/20 15:45:38 INFO SparkHadoopMapRedUtil: attempt_20210120154537_0166_m_000000_166: Committed
21/01/20 15:45:38 INFO Executor: Finished task 0.0 in stage 166.0 (TID 166). 2155 bytes result sent to driver
21/01/20 15:45:38 INFO TaskSetManager: Finished task 0.0 in stage 166.0 (TID 166) in 845 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:45:38 INFO TaskSchedulerImpl: Removed TaskSet 166.0, whose tasks have all completed, from pool 
21/01/20 15:45:38 INFO DAGScheduler: ResultStage 166 (parquet at Generate.java:61) finished in 0.886 s
21/01/20 15:45:38 INFO DAGScheduler: Job 166 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:45:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 166: Stage finished
21/01/20 15:45:38 INFO DAGScheduler: Job 166 finished: parquet at Generate.java:61, took 0.887428 s
21/01/20 15:45:38 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-afd2d870-199e-4dc2-88a8-691efd9117e5
21/01/20 15:45:38 INFO FileFormatWriter: Write Job d4815559-024a-44fb-9300-9bdb387da114 committed.
21/01/20 15:45:38 INFO FileFormatWriter: Finished processing stats for write job d4815559-024a-44fb-9300-9bdb387da114.
21/01/20 15:45:38 INFO BlockManagerInfo: Removed broadcast_166_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:45:41 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:41 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:41 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:45:41 INFO DAGScheduler: Got job 167 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:45:41 INFO DAGScheduler: Final stage: ResultStage 167 (parquet at Generate.java:61)
21/01/20 15:45:41 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:45:41 INFO DAGScheduler: Missing parents: List()
21/01/20 15:45:41 INFO DAGScheduler: Submitting ResultStage 167 (MapPartitionsRDD[669] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:45:41 INFO MemoryStore: Block broadcast_167 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:45:41 INFO MemoryStore: Block broadcast_167_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:45:41 INFO BlockManagerInfo: Added broadcast_167_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:45:41 INFO SparkContext: Created broadcast 167 from broadcast at DAGScheduler.scala:1200
21/01/20 15:45:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 167 (MapPartitionsRDD[669] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:45:41 INFO TaskSchedulerImpl: Adding task set 167.0 with 1 tasks
21/01/20 15:45:41 WARN TaskSetManager: Stage 167 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:45:41 INFO TaskSetManager: Starting task 0.0 in stage 167.0 (TID 167, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:45:41 INFO Executor: Running task 0.0 in stage 167.0 (TID 167)
21/01/20 15:45:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:41 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:41 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:45:41 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:45:41 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:45:41 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:45:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:45:41 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:45:41 INFO ParquetOutputFormat: Validation is off
21/01/20 15:45:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:45:41 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:45:41 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:45:41 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:45:41 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:45:41 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:45:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:45:42 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154541_0167_m_000000_167' to o3fs://bucket1.vol1/testdata
21/01/20 15:45:42 INFO SparkHadoopMapRedUtil: attempt_20210120154541_0167_m_000000_167: Committed
21/01/20 15:45:42 INFO Executor: Finished task 0.0 in stage 167.0 (TID 167). 2155 bytes result sent to driver
21/01/20 15:45:42 INFO TaskSetManager: Finished task 0.0 in stage 167.0 (TID 167) in 977 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:45:42 INFO TaskSchedulerImpl: Removed TaskSet 167.0, whose tasks have all completed, from pool 
21/01/20 15:45:42 INFO DAGScheduler: ResultStage 167 (parquet at Generate.java:61) finished in 0.995 s
21/01/20 15:45:42 INFO DAGScheduler: Job 167 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:45:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 167: Stage finished
21/01/20 15:45:42 INFO DAGScheduler: Job 167 finished: parquet at Generate.java:61, took 0.996084 s
21/01/20 15:45:42 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-17a0ff46-a4d1-47ee-8c52-613871339183
21/01/20 15:45:42 INFO FileFormatWriter: Write Job dcb51163-a7cc-4957-a932-3f5ba43f4239 committed.
21/01/20 15:45:42 INFO FileFormatWriter: Finished processing stats for write job dcb51163-a7cc-4957-a932-3f5ba43f4239.
21/01/20 15:45:43 INFO BlockManagerInfo: Removed broadcast_167_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:45:44 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:44 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:44 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:45:44 INFO DAGScheduler: Got job 168 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:45:44 INFO DAGScheduler: Final stage: ResultStage 168 (parquet at Generate.java:61)
21/01/20 15:45:44 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:45:44 INFO DAGScheduler: Missing parents: List()
21/01/20 15:45:44 INFO DAGScheduler: Submitting ResultStage 168 (MapPartitionsRDD[673] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:45:44 INFO MemoryStore: Block broadcast_168 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:45:44 INFO MemoryStore: Block broadcast_168_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:45:44 INFO BlockManagerInfo: Added broadcast_168_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:45:44 INFO SparkContext: Created broadcast 168 from broadcast at DAGScheduler.scala:1200
21/01/20 15:45:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 168 (MapPartitionsRDD[673] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:45:44 INFO TaskSchedulerImpl: Adding task set 168.0 with 1 tasks
21/01/20 15:45:44 WARN TaskSetManager: Stage 168 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:45:44 INFO TaskSetManager: Starting task 0.0 in stage 168.0 (TID 168, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:45:44 INFO Executor: Running task 0.0 in stage 168.0 (TID 168)
21/01/20 15:45:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:44 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:44 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:45:44 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:45:44 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:45:44 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:45:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:45:44 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:45:44 INFO ParquetOutputFormat: Validation is off
21/01/20 15:45:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:45:44 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:45:44 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:45:44 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:45:44 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:45:44 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:45:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:45:45 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154544_0168_m_000000_168' to o3fs://bucket1.vol1/testdata
21/01/20 15:45:45 INFO SparkHadoopMapRedUtil: attempt_20210120154544_0168_m_000000_168: Committed
21/01/20 15:45:45 INFO Executor: Finished task 0.0 in stage 168.0 (TID 168). 2155 bytes result sent to driver
21/01/20 15:45:45 INFO TaskSetManager: Finished task 0.0 in stage 168.0 (TID 168) in 859 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:45:45 INFO TaskSchedulerImpl: Removed TaskSet 168.0, whose tasks have all completed, from pool 
21/01/20 15:45:45 INFO DAGScheduler: ResultStage 168 (parquet at Generate.java:61) finished in 0.876 s
21/01/20 15:45:45 INFO DAGScheduler: Job 168 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:45:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 168: Stage finished
21/01/20 15:45:45 INFO DAGScheduler: Job 168 finished: parquet at Generate.java:61, took 0.899653 s
21/01/20 15:45:45 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-93ff11a2-3e5e-448a-9440-8451e75835d1
21/01/20 15:45:45 INFO FileFormatWriter: Write Job 279613c9-5321-4034-9c58-f7cf56e3e024 committed.
21/01/20 15:45:45 INFO FileFormatWriter: Finished processing stats for write job 279613c9-5321-4034-9c58-f7cf56e3e024.
21/01/20 15:45:45 INFO BlockManagerInfo: Removed broadcast_168_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:45:48 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:48 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:48 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:45:48 INFO DAGScheduler: Got job 169 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:45:48 INFO DAGScheduler: Final stage: ResultStage 169 (parquet at Generate.java:61)
21/01/20 15:45:48 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:45:48 INFO DAGScheduler: Missing parents: List()
21/01/20 15:45:48 INFO DAGScheduler: Submitting ResultStage 169 (MapPartitionsRDD[677] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:45:48 INFO MemoryStore: Block broadcast_169 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:45:48 INFO MemoryStore: Block broadcast_169_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:45:48 INFO BlockManagerInfo: Added broadcast_169_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:45:48 INFO SparkContext: Created broadcast 169 from broadcast at DAGScheduler.scala:1200
21/01/20 15:45:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 169 (MapPartitionsRDD[677] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:45:48 INFO TaskSchedulerImpl: Adding task set 169.0 with 1 tasks
21/01/20 15:45:48 WARN TaskSetManager: Stage 169 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:45:48 INFO TaskSetManager: Starting task 0.0 in stage 169.0 (TID 169, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:45:48 INFO Executor: Running task 0.0 in stage 169.0 (TID 169)
21/01/20 15:45:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:48 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:48 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:45:48 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:45:48 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:45:48 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:45:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:45:48 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:45:48 INFO ParquetOutputFormat: Validation is off
21/01/20 15:45:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:45:48 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:45:48 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:45:48 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:45:48 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:45:48 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:45:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:45:48 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-E2B45F3F6DC4->d3c4d3e3-944b-4528-9d82-e005ad427697
21/01/20 15:45:48 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:45:49 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154548_0169_m_000000_169' to o3fs://bucket1.vol1/testdata
21/01/20 15:45:49 INFO SparkHadoopMapRedUtil: attempt_20210120154548_0169_m_000000_169: Committed
21/01/20 15:45:49 INFO Executor: Finished task 0.0 in stage 169.0 (TID 169). 2155 bytes result sent to driver
21/01/20 15:45:49 INFO TaskSetManager: Finished task 0.0 in stage 169.0 (TID 169) in 1122 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:45:49 INFO TaskSchedulerImpl: Removed TaskSet 169.0, whose tasks have all completed, from pool 
21/01/20 15:45:49 INFO DAGScheduler: ResultStage 169 (parquet at Generate.java:61) finished in 1.141 s
21/01/20 15:45:49 INFO DAGScheduler: Job 169 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:45:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 169: Stage finished
21/01/20 15:45:49 INFO DAGScheduler: Job 169 finished: parquet at Generate.java:61, took 1.142638 s
21/01/20 15:45:49 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-c9940ff8-5197-4857-a71f-45e044c1547d
21/01/20 15:45:49 INFO FileFormatWriter: Write Job 8aebcbe0-eada-439a-910a-44dcd3a9ee24 committed.
21/01/20 15:45:49 INFO FileFormatWriter: Finished processing stats for write job 8aebcbe0-eada-439a-910a-44dcd3a9ee24.
21/01/20 15:45:50 INFO BlockManagerInfo: Removed broadcast_169_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:45:51 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:51 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:45:51 INFO DAGScheduler: Got job 170 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:45:51 INFO DAGScheduler: Final stage: ResultStage 170 (parquet at Generate.java:61)
21/01/20 15:45:51 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:45:51 INFO DAGScheduler: Missing parents: List()
21/01/20 15:45:51 INFO DAGScheduler: Submitting ResultStage 170 (MapPartitionsRDD[681] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:45:51 INFO MemoryStore: Block broadcast_170 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:45:51 INFO MemoryStore: Block broadcast_170_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:45:51 INFO BlockManagerInfo: Added broadcast_170_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:45:51 INFO SparkContext: Created broadcast 170 from broadcast at DAGScheduler.scala:1200
21/01/20 15:45:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 170 (MapPartitionsRDD[681] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:45:51 INFO TaskSchedulerImpl: Adding task set 170.0 with 1 tasks
21/01/20 15:45:52 WARN TaskSetManager: Stage 170 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:45:52 INFO TaskSetManager: Starting task 0.0 in stage 170.0 (TID 170, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:45:52 INFO Executor: Running task 0.0 in stage 170.0 (TID 170)
21/01/20 15:45:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:52 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:52 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:45:52 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:45:52 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:45:52 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:45:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:45:52 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:45:52 INFO ParquetOutputFormat: Validation is off
21/01/20 15:45:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:45:52 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:45:52 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:45:52 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:45:52 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:45:52 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:45:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:45:53 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154551_0170_m_000000_170' to o3fs://bucket1.vol1/testdata
21/01/20 15:45:53 INFO SparkHadoopMapRedUtil: attempt_20210120154551_0170_m_000000_170: Committed
21/01/20 15:45:53 INFO Executor: Finished task 0.0 in stage 170.0 (TID 170). 2155 bytes result sent to driver
21/01/20 15:45:53 INFO TaskSetManager: Finished task 0.0 in stage 170.0 (TID 170) in 1076 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:45:53 INFO TaskSchedulerImpl: Removed TaskSet 170.0, whose tasks have all completed, from pool 
21/01/20 15:45:53 INFO DAGScheduler: ResultStage 170 (parquet at Generate.java:61) finished in 1.096 s
21/01/20 15:45:53 INFO DAGScheduler: Job 170 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:45:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 170: Stage finished
21/01/20 15:45:53 INFO DAGScheduler: Job 170 finished: parquet at Generate.java:61, took 1.097171 s
21/01/20 15:45:53 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-86a7b8fd-17f0-4284-9c2b-bffe54f8d171
21/01/20 15:45:53 INFO FileFormatWriter: Write Job 0dc633e3-83a4-4ebf-afee-0500979496dd committed.
21/01/20 15:45:53 INFO FileFormatWriter: Finished processing stats for write job 0dc633e3-83a4-4ebf-afee-0500979496dd.
21/01/20 15:45:54 INFO BlockManagerInfo: Removed broadcast_170_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:45:55 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:55 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:45:55 INFO DAGScheduler: Got job 171 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:45:55 INFO DAGScheduler: Final stage: ResultStage 171 (parquet at Generate.java:61)
21/01/20 15:45:55 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:45:55 INFO DAGScheduler: Missing parents: List()
21/01/20 15:45:55 INFO DAGScheduler: Submitting ResultStage 171 (MapPartitionsRDD[685] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:45:55 INFO MemoryStore: Block broadcast_171 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:45:55 INFO MemoryStore: Block broadcast_171_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:45:55 INFO BlockManagerInfo: Added broadcast_171_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:45:55 INFO SparkContext: Created broadcast 171 from broadcast at DAGScheduler.scala:1200
21/01/20 15:45:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 171 (MapPartitionsRDD[685] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:45:55 INFO TaskSchedulerImpl: Adding task set 171.0 with 1 tasks
21/01/20 15:45:55 WARN TaskSetManager: Stage 171 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:45:55 INFO TaskSetManager: Starting task 0.0 in stage 171.0 (TID 171, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:45:55 INFO Executor: Running task 0.0 in stage 171.0 (TID 171)
21/01/20 15:45:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:55 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:45:55 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:45:55 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:45:55 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:45:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:45:55 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:45:55 INFO ParquetOutputFormat: Validation is off
21/01/20 15:45:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:45:55 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:45:55 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:45:55 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:45:55 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:45:55 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:45:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:45:56 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-007047E71E5E->af5c9c6a-a887-4538-ae7b-ba6d49328297
21/01/20 15:45:56 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:45:56 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154555_0171_m_000000_171' to o3fs://bucket1.vol1/testdata
21/01/20 15:45:56 INFO SparkHadoopMapRedUtil: attempt_20210120154555_0171_m_000000_171: Committed
21/01/20 15:45:56 INFO Executor: Finished task 0.0 in stage 171.0 (TID 171). 2155 bytes result sent to driver
21/01/20 15:45:56 INFO TaskSetManager: Finished task 0.0 in stage 171.0 (TID 171) in 1041 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:45:56 INFO TaskSchedulerImpl: Removed TaskSet 171.0, whose tasks have all completed, from pool 
21/01/20 15:45:56 INFO DAGScheduler: ResultStage 171 (parquet at Generate.java:61) finished in 1.058 s
21/01/20 15:45:56 INFO DAGScheduler: Job 171 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:45:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 171: Stage finished
21/01/20 15:45:56 INFO DAGScheduler: Job 171 finished: parquet at Generate.java:61, took 1.059683 s
21/01/20 15:45:56 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-72092c5b-4506-47e3-8861-2f166d88cdd5
21/01/20 15:45:56 INFO FileFormatWriter: Write Job 1bca89c6-2d95-486a-83ed-13c6adeee614 committed.
21/01/20 15:45:56 INFO FileFormatWriter: Finished processing stats for write job 1bca89c6-2d95-486a-83ed-13c6adeee614.
21/01/20 15:45:57 INFO BlockManagerInfo: Removed broadcast_171_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:45:59 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:59 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:59 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:45:59 INFO DAGScheduler: Got job 172 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:45:59 INFO DAGScheduler: Final stage: ResultStage 172 (parquet at Generate.java:61)
21/01/20 15:45:59 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:45:59 INFO DAGScheduler: Missing parents: List()
21/01/20 15:45:59 INFO DAGScheduler: Submitting ResultStage 172 (MapPartitionsRDD[689] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:45:59 INFO MemoryStore: Block broadcast_172 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:45:59 INFO MemoryStore: Block broadcast_172_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:45:59 INFO BlockManagerInfo: Added broadcast_172_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:45:59 INFO SparkContext: Created broadcast 172 from broadcast at DAGScheduler.scala:1200
21/01/20 15:45:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 172 (MapPartitionsRDD[689] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:45:59 INFO TaskSchedulerImpl: Adding task set 172.0 with 1 tasks
21/01/20 15:45:59 WARN TaskSetManager: Stage 172 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:45:59 INFO TaskSetManager: Starting task 0.0 in stage 172.0 (TID 172, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:45:59 INFO Executor: Running task 0.0 in stage 172.0 (TID 172)
21/01/20 15:45:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:59 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:45:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:45:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:45:59 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:45:59 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:45:59 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:45:59 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:45:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:45:59 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:45:59 INFO ParquetOutputFormat: Validation is off
21/01/20 15:45:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:45:59 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:45:59 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:45:59 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:45:59 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:45:59 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:45:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:46:00 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154559_0172_m_000000_172' to o3fs://bucket1.vol1/testdata
21/01/20 15:46:00 INFO SparkHadoopMapRedUtil: attempt_20210120154559_0172_m_000000_172: Committed
21/01/20 15:46:00 INFO Executor: Finished task 0.0 in stage 172.0 (TID 172). 2155 bytes result sent to driver
21/01/20 15:46:00 INFO TaskSetManager: Finished task 0.0 in stage 172.0 (TID 172) in 966 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:46:00 INFO TaskSchedulerImpl: Removed TaskSet 172.0, whose tasks have all completed, from pool 
21/01/20 15:46:00 INFO DAGScheduler: ResultStage 172 (parquet at Generate.java:61) finished in 0.984 s
21/01/20 15:46:00 INFO DAGScheduler: Job 172 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:46:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 172: Stage finished
21/01/20 15:46:00 INFO DAGScheduler: Job 172 finished: parquet at Generate.java:61, took 0.984839 s
21/01/20 15:46:00 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-2711d144-eae6-4135-9e8b-7461156308c9
21/01/20 15:46:00 INFO FileFormatWriter: Write Job e536773d-d19b-4c6c-a3e2-89926fdb5a52 committed.
21/01/20 15:46:00 INFO FileFormatWriter: Finished processing stats for write job e536773d-d19b-4c6c-a3e2-89926fdb5a52.
21/01/20 15:46:01 INFO BlockManagerInfo: Removed broadcast_172_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:46:02 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:02 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:46:02 INFO DAGScheduler: Got job 173 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:46:02 INFO DAGScheduler: Final stage: ResultStage 173 (parquet at Generate.java:61)
21/01/20 15:46:02 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:46:02 INFO DAGScheduler: Missing parents: List()
21/01/20 15:46:02 INFO DAGScheduler: Submitting ResultStage 173 (MapPartitionsRDD[693] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:46:02 INFO MemoryStore: Block broadcast_173 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:46:02 INFO MemoryStore: Block broadcast_173_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:46:02 INFO BlockManagerInfo: Added broadcast_173_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:46:02 INFO SparkContext: Created broadcast 173 from broadcast at DAGScheduler.scala:1200
21/01/20 15:46:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 173 (MapPartitionsRDD[693] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:46:02 INFO TaskSchedulerImpl: Adding task set 173.0 with 1 tasks
21/01/20 15:46:03 WARN TaskSetManager: Stage 173 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:46:03 INFO TaskSetManager: Starting task 0.0 in stage 173.0 (TID 173, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:46:03 INFO Executor: Running task 0.0 in stage 173.0 (TID 173)
21/01/20 15:46:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:03 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:46:03 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:46:03 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:46:03 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:46:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:46:03 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:46:03 INFO ParquetOutputFormat: Validation is off
21/01/20 15:46:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:46:03 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:46:03 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:46:03 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:46:03 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:46:03 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:46:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:46:03 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-E6FCE7A79EA4->d3c4d3e3-944b-4528-9d82-e005ad427697
21/01/20 15:46:03 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:46:04 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154602_0173_m_000000_173' to o3fs://bucket1.vol1/testdata
21/01/20 15:46:04 INFO SparkHadoopMapRedUtil: attempt_20210120154602_0173_m_000000_173: Committed
21/01/20 15:46:04 INFO Executor: Finished task 0.0 in stage 173.0 (TID 173). 2155 bytes result sent to driver
21/01/20 15:46:04 INFO TaskSetManager: Finished task 0.0 in stage 173.0 (TID 173) in 1064 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:46:04 INFO TaskSchedulerImpl: Removed TaskSet 173.0, whose tasks have all completed, from pool 
21/01/20 15:46:04 INFO DAGScheduler: ResultStage 173 (parquet at Generate.java:61) finished in 1.105 s
21/01/20 15:46:04 INFO DAGScheduler: Job 173 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:46:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 173: Stage finished
21/01/20 15:46:04 INFO DAGScheduler: Job 173 finished: parquet at Generate.java:61, took 1.106718 s
21/01/20 15:46:04 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-0e569f9e-6c81-4a58-a597-e9412c4bb0ef
21/01/20 15:46:04 INFO FileFormatWriter: Write Job 94ebe99a-94c8-45de-8e06-951c06ea7c35 committed.
21/01/20 15:46:04 INFO FileFormatWriter: Finished processing stats for write job 94ebe99a-94c8-45de-8e06-951c06ea7c35.
21/01/20 15:46:04 INFO BlockManagerInfo: Removed broadcast_173_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:46:06 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:06 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:06 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:46:06 INFO DAGScheduler: Got job 174 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:46:06 INFO DAGScheduler: Final stage: ResultStage 174 (parquet at Generate.java:61)
21/01/20 15:46:06 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:46:06 INFO DAGScheduler: Missing parents: List()
21/01/20 15:46:06 INFO DAGScheduler: Submitting ResultStage 174 (MapPartitionsRDD[697] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:46:06 INFO MemoryStore: Block broadcast_174 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:46:06 INFO MemoryStore: Block broadcast_174_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:46:06 INFO BlockManagerInfo: Added broadcast_174_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:46:06 INFO SparkContext: Created broadcast 174 from broadcast at DAGScheduler.scala:1200
21/01/20 15:46:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 174 (MapPartitionsRDD[697] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:46:06 INFO TaskSchedulerImpl: Adding task set 174.0 with 1 tasks
21/01/20 15:46:06 WARN TaskSetManager: Stage 174 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:46:06 INFO TaskSetManager: Starting task 0.0 in stage 174.0 (TID 174, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:46:06 INFO Executor: Running task 0.0 in stage 174.0 (TID 174)
21/01/20 15:46:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:06 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:06 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:46:06 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:46:06 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:46:06 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:46:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:46:06 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:46:06 INFO ParquetOutputFormat: Validation is off
21/01/20 15:46:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:46:06 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:46:06 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:46:06 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:46:06 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:46:06 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:46:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:46:07 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154606_0174_m_000000_174' to o3fs://bucket1.vol1/testdata
21/01/20 15:46:07 INFO SparkHadoopMapRedUtil: attempt_20210120154606_0174_m_000000_174: Committed
21/01/20 15:46:07 INFO Executor: Finished task 0.0 in stage 174.0 (TID 174). 2155 bytes result sent to driver
21/01/20 15:46:07 INFO TaskSetManager: Finished task 0.0 in stage 174.0 (TID 174) in 1087 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:46:07 INFO TaskSchedulerImpl: Removed TaskSet 174.0, whose tasks have all completed, from pool 
21/01/20 15:46:07 INFO DAGScheduler: ResultStage 174 (parquet at Generate.java:61) finished in 1.106 s
21/01/20 15:46:07 INFO DAGScheduler: Job 174 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:46:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 174: Stage finished
21/01/20 15:46:07 INFO DAGScheduler: Job 174 finished: parquet at Generate.java:61, took 1.107066 s
21/01/20 15:46:07 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-0707b43a-0701-4fce-aa04-bad3a423ba3c
21/01/20 15:46:07 INFO FileFormatWriter: Write Job c087574a-c68a-48fb-8f84-73d0788474ab committed.
21/01/20 15:46:07 INFO FileFormatWriter: Finished processing stats for write job c087574a-c68a-48fb-8f84-73d0788474ab.
21/01/20 15:46:09 INFO BlockManagerInfo: Removed broadcast_174_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:46:10 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:10 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:10 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:46:10 INFO DAGScheduler: Got job 175 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:46:10 INFO DAGScheduler: Final stage: ResultStage 175 (parquet at Generate.java:61)
21/01/20 15:46:10 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:46:10 INFO DAGScheduler: Missing parents: List()
21/01/20 15:46:10 INFO DAGScheduler: Submitting ResultStage 175 (MapPartitionsRDD[701] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:46:10 INFO MemoryStore: Block broadcast_175 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:46:10 INFO MemoryStore: Block broadcast_175_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:46:10 INFO BlockManagerInfo: Added broadcast_175_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:46:10 INFO SparkContext: Created broadcast 175 from broadcast at DAGScheduler.scala:1200
21/01/20 15:46:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 175 (MapPartitionsRDD[701] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:46:10 INFO TaskSchedulerImpl: Adding task set 175.0 with 1 tasks
21/01/20 15:46:10 WARN TaskSetManager: Stage 175 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:46:10 INFO TaskSetManager: Starting task 0.0 in stage 175.0 (TID 175, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:46:10 INFO Executor: Running task 0.0 in stage 175.0 (TID 175)
21/01/20 15:46:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:10 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:10 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:46:10 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:46:10 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:46:10 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:46:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:46:10 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:46:10 INFO ParquetOutputFormat: Validation is off
21/01/20 15:46:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:46:10 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:46:10 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:46:10 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:46:10 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:46:10 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:46:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:46:10 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-C9716D31F793->af5c9c6a-a887-4538-ae7b-ba6d49328297
21/01/20 15:46:10 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:46:11 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154610_0175_m_000000_175' to o3fs://bucket1.vol1/testdata
21/01/20 15:46:11 INFO SparkHadoopMapRedUtil: attempt_20210120154610_0175_m_000000_175: Committed
21/01/20 15:46:11 INFO Executor: Finished task 0.0 in stage 175.0 (TID 175). 2155 bytes result sent to driver
21/01/20 15:46:11 INFO TaskSetManager: Finished task 0.0 in stage 175.0 (TID 175) in 867 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:46:11 INFO TaskSchedulerImpl: Removed TaskSet 175.0, whose tasks have all completed, from pool 
21/01/20 15:46:11 INFO DAGScheduler: ResultStage 175 (parquet at Generate.java:61) finished in 0.907 s
21/01/20 15:46:11 INFO DAGScheduler: Job 175 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:46:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 175: Stage finished
21/01/20 15:46:11 INFO DAGScheduler: Job 175 finished: parquet at Generate.java:61, took 0.911177 s
21/01/20 15:46:11 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-ee3f2601-e9ea-415e-8306-9eaefa69065d
21/01/20 15:46:11 INFO FileFormatWriter: Write Job 7188e326-b636-43da-8b55-ad3c8611a6a4 committed.
21/01/20 15:46:11 INFO FileFormatWriter: Finished processing stats for write job 7188e326-b636-43da-8b55-ad3c8611a6a4.
21/01/20 15:46:11 INFO BlockManagerInfo: Removed broadcast_175_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:46:13 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:13 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:13 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:46:13 INFO DAGScheduler: Got job 176 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:46:13 INFO DAGScheduler: Final stage: ResultStage 176 (parquet at Generate.java:61)
21/01/20 15:46:13 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:46:13 INFO DAGScheduler: Missing parents: List()
21/01/20 15:46:13 INFO DAGScheduler: Submitting ResultStage 176 (MapPartitionsRDD[705] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:46:13 INFO MemoryStore: Block broadcast_176 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:46:13 INFO MemoryStore: Block broadcast_176_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:46:13 INFO BlockManagerInfo: Added broadcast_176_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:46:13 INFO SparkContext: Created broadcast 176 from broadcast at DAGScheduler.scala:1200
21/01/20 15:46:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 176 (MapPartitionsRDD[705] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:46:13 INFO TaskSchedulerImpl: Adding task set 176.0 with 1 tasks
21/01/20 15:46:13 WARN TaskSetManager: Stage 176 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:46:13 INFO TaskSetManager: Starting task 0.0 in stage 176.0 (TID 176, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:46:13 INFO Executor: Running task 0.0 in stage 176.0 (TID 176)
21/01/20 15:46:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:14 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:14 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:46:14 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:46:14 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:46:14 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:46:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:46:14 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:46:14 INFO ParquetOutputFormat: Validation is off
21/01/20 15:46:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:46:14 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:46:14 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:46:14 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:46:14 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:46:14 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:46:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:46:14 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154613_0176_m_000000_176' to o3fs://bucket1.vol1/testdata
21/01/20 15:46:14 INFO SparkHadoopMapRedUtil: attempt_20210120154613_0176_m_000000_176: Committed
21/01/20 15:46:14 INFO Executor: Finished task 0.0 in stage 176.0 (TID 176). 2155 bytes result sent to driver
21/01/20 15:46:14 INFO TaskSetManager: Finished task 0.0 in stage 176.0 (TID 176) in 969 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:46:14 INFO TaskSchedulerImpl: Removed TaskSet 176.0, whose tasks have all completed, from pool 
21/01/20 15:46:14 INFO DAGScheduler: ResultStage 176 (parquet at Generate.java:61) finished in 0.987 s
21/01/20 15:46:14 INFO DAGScheduler: Job 176 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:46:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 176: Stage finished
21/01/20 15:46:14 INFO DAGScheduler: Job 176 finished: parquet at Generate.java:61, took 0.987547 s
21/01/20 15:46:14 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-e16ccc24-f9b7-4821-afe2-072aa47bade4
21/01/20 15:46:14 INFO FileFormatWriter: Write Job 3fe4770d-a9e2-4fb3-bae2-7b404261cfbe committed.
21/01/20 15:46:14 INFO FileFormatWriter: Finished processing stats for write job 3fe4770d-a9e2-4fb3-bae2-7b404261cfbe.
21/01/20 15:46:15 INFO BlockManagerInfo: Removed broadcast_176_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:46:17 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:17 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:17 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:46:17 INFO DAGScheduler: Got job 177 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:46:17 INFO DAGScheduler: Final stage: ResultStage 177 (parquet at Generate.java:61)
21/01/20 15:46:17 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:46:17 INFO DAGScheduler: Missing parents: List()
21/01/20 15:46:17 INFO DAGScheduler: Submitting ResultStage 177 (MapPartitionsRDD[709] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:46:17 INFO MemoryStore: Block broadcast_177 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:46:17 INFO MemoryStore: Block broadcast_177_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:46:17 INFO BlockManagerInfo: Added broadcast_177_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:46:17 INFO SparkContext: Created broadcast 177 from broadcast at DAGScheduler.scala:1200
21/01/20 15:46:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 177 (MapPartitionsRDD[709] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:46:17 INFO TaskSchedulerImpl: Adding task set 177.0 with 1 tasks
21/01/20 15:46:17 WARN TaskSetManager: Stage 177 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:46:17 INFO TaskSetManager: Starting task 0.0 in stage 177.0 (TID 177, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:46:17 INFO Executor: Running task 0.0 in stage 177.0 (TID 177)
21/01/20 15:46:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:17 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:17 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:46:17 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:46:17 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:46:17 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:46:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:46:17 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:46:17 INFO ParquetOutputFormat: Validation is off
21/01/20 15:46:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:46:17 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:46:17 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:46:17 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:46:17 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:46:17 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:46:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:46:18 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-22311E93FEC4->d3c4d3e3-944b-4528-9d82-e005ad427697
21/01/20 15:46:18 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:46:18 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154617_0177_m_000000_177' to o3fs://bucket1.vol1/testdata
21/01/20 15:46:18 INFO SparkHadoopMapRedUtil: attempt_20210120154617_0177_m_000000_177: Committed
21/01/20 15:46:18 INFO Executor: Finished task 0.0 in stage 177.0 (TID 177). 2155 bytes result sent to driver
21/01/20 15:46:18 INFO TaskSetManager: Finished task 0.0 in stage 177.0 (TID 177) in 1059 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:46:18 INFO TaskSchedulerImpl: Removed TaskSet 177.0, whose tasks have all completed, from pool 
21/01/20 15:46:18 INFO DAGScheduler: ResultStage 177 (parquet at Generate.java:61) finished in 1.076 s
21/01/20 15:46:18 INFO DAGScheduler: Job 177 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:46:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 177: Stage finished
21/01/20 15:46:18 INFO DAGScheduler: Job 177 finished: parquet at Generate.java:61, took 1.077469 s
21/01/20 15:46:18 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-b9dd8729-714f-49b9-b280-db363a07d751
21/01/20 15:46:18 INFO FileFormatWriter: Write Job 49e070d3-24b1-42bb-a66b-3da63d0bd5c2 committed.
21/01/20 15:46:18 INFO FileFormatWriter: Finished processing stats for write job 49e070d3-24b1-42bb-a66b-3da63d0bd5c2.
21/01/20 15:46:19 INFO BlockManagerInfo: Removed broadcast_177_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:46:21 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:21 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:21 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:46:21 INFO DAGScheduler: Got job 178 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:46:21 INFO DAGScheduler: Final stage: ResultStage 178 (parquet at Generate.java:61)
21/01/20 15:46:21 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:46:21 INFO DAGScheduler: Missing parents: List()
21/01/20 15:46:21 INFO DAGScheduler: Submitting ResultStage 178 (MapPartitionsRDD[713] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:46:21 INFO MemoryStore: Block broadcast_178 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:46:21 INFO MemoryStore: Block broadcast_178_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:46:21 INFO BlockManagerInfo: Added broadcast_178_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:46:21 INFO SparkContext: Created broadcast 178 from broadcast at DAGScheduler.scala:1200
21/01/20 15:46:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 178 (MapPartitionsRDD[713] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:46:21 INFO TaskSchedulerImpl: Adding task set 178.0 with 1 tasks
21/01/20 15:46:21 WARN TaskSetManager: Stage 178 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:46:21 INFO TaskSetManager: Starting task 0.0 in stage 178.0 (TID 178, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:46:21 INFO Executor: Running task 0.0 in stage 178.0 (TID 178)
21/01/20 15:46:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:21 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:21 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:46:21 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:46:21 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:46:21 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:46:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:46:21 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:46:21 INFO ParquetOutputFormat: Validation is off
21/01/20 15:46:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:46:21 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:46:21 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:46:21 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:46:21 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:46:21 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:46:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:46:22 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154621_0178_m_000000_178' to o3fs://bucket1.vol1/testdata
21/01/20 15:46:22 INFO SparkHadoopMapRedUtil: attempt_20210120154621_0178_m_000000_178: Committed
21/01/20 15:46:22 INFO Executor: Finished task 0.0 in stage 178.0 (TID 178). 2155 bytes result sent to driver
21/01/20 15:46:22 INFO TaskSetManager: Finished task 0.0 in stage 178.0 (TID 178) in 865 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:46:22 INFO TaskSchedulerImpl: Removed TaskSet 178.0, whose tasks have all completed, from pool 
21/01/20 15:46:22 INFO DAGScheduler: ResultStage 178 (parquet at Generate.java:61) finished in 0.882 s
21/01/20 15:46:22 INFO DAGScheduler: Job 178 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:46:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 178: Stage finished
21/01/20 15:46:22 INFO DAGScheduler: Job 178 finished: parquet at Generate.java:61, took 0.884568 s
21/01/20 15:46:22 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-9ec77a14-80c8-435a-9778-088b13276e2b
21/01/20 15:46:22 INFO FileFormatWriter: Write Job 83191a22-c234-412e-aa12-afd1946f0e0b committed.
21/01/20 15:46:22 INFO FileFormatWriter: Finished processing stats for write job 83191a22-c234-412e-aa12-afd1946f0e0b.
21/01/20 15:46:22 INFO BlockManagerInfo: Removed broadcast_178_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:46:24 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:24 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:24 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:46:24 INFO DAGScheduler: Got job 179 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:46:24 INFO DAGScheduler: Final stage: ResultStage 179 (parquet at Generate.java:61)
21/01/20 15:46:24 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:46:24 INFO DAGScheduler: Missing parents: List()
21/01/20 15:46:24 INFO DAGScheduler: Submitting ResultStage 179 (MapPartitionsRDD[717] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:46:24 INFO MemoryStore: Block broadcast_179 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:46:24 INFO MemoryStore: Block broadcast_179_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:46:24 INFO BlockManagerInfo: Added broadcast_179_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:46:24 INFO SparkContext: Created broadcast 179 from broadcast at DAGScheduler.scala:1200
21/01/20 15:46:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 179 (MapPartitionsRDD[717] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:46:24 INFO TaskSchedulerImpl: Adding task set 179.0 with 1 tasks
21/01/20 15:46:24 WARN TaskSetManager: Stage 179 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:46:24 INFO TaskSetManager: Starting task 0.0 in stage 179.0 (TID 179, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:46:24 INFO Executor: Running task 0.0 in stage 179.0 (TID 179)
21/01/20 15:46:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:24 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:24 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:46:24 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:46:24 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:46:24 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:46:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:46:24 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:46:24 INFO ParquetOutputFormat: Validation is off
21/01/20 15:46:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:46:24 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:46:24 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:46:24 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:46:24 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:46:24 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:46:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:46:25 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154624_0179_m_000000_179' to o3fs://bucket1.vol1/testdata
21/01/20 15:46:25 INFO SparkHadoopMapRedUtil: attempt_20210120154624_0179_m_000000_179: Committed
21/01/20 15:46:25 INFO Executor: Finished task 0.0 in stage 179.0 (TID 179). 2155 bytes result sent to driver
21/01/20 15:46:25 INFO TaskSetManager: Finished task 0.0 in stage 179.0 (TID 179) in 1057 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:46:25 INFO TaskSchedulerImpl: Removed TaskSet 179.0, whose tasks have all completed, from pool 
21/01/20 15:46:25 INFO DAGScheduler: ResultStage 179 (parquet at Generate.java:61) finished in 1.074 s
21/01/20 15:46:25 INFO DAGScheduler: Job 179 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:46:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 179: Stage finished
21/01/20 15:46:25 INFO DAGScheduler: Job 179 finished: parquet at Generate.java:61, took 1.075260 s
21/01/20 15:46:25 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-fd94b882-870f-4c16-9da4-5df22502e0f8
21/01/20 15:46:25 INFO FileFormatWriter: Write Job b920e23d-f610-4fb3-b249-52e3249994f2 committed.
21/01/20 15:46:25 INFO FileFormatWriter: Finished processing stats for write job b920e23d-f610-4fb3-b249-52e3249994f2.
21/01/20 15:46:27 INFO BlockManagerInfo: Removed broadcast_179_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:46:28 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:28 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:28 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:46:28 INFO DAGScheduler: Got job 180 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:46:28 INFO DAGScheduler: Final stage: ResultStage 180 (parquet at Generate.java:61)
21/01/20 15:46:28 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:46:28 INFO DAGScheduler: Missing parents: List()
21/01/20 15:46:28 INFO DAGScheduler: Submitting ResultStage 180 (MapPartitionsRDD[721] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:46:28 INFO MemoryStore: Block broadcast_180 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:46:28 INFO MemoryStore: Block broadcast_180_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:46:28 INFO BlockManagerInfo: Added broadcast_180_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:46:28 INFO SparkContext: Created broadcast 180 from broadcast at DAGScheduler.scala:1200
21/01/20 15:46:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 180 (MapPartitionsRDD[721] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:46:28 INFO TaskSchedulerImpl: Adding task set 180.0 with 1 tasks
21/01/20 15:46:28 WARN TaskSetManager: Stage 180 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:46:28 INFO TaskSetManager: Starting task 0.0 in stage 180.0 (TID 180, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:46:28 INFO Executor: Running task 0.0 in stage 180.0 (TID 180)
21/01/20 15:46:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:28 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:28 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:46:28 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:46:28 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:46:28 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:46:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:46:28 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:46:28 INFO ParquetOutputFormat: Validation is off
21/01/20 15:46:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:46:28 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:46:28 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:46:28 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:46:28 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:46:28 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:46:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:46:29 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154628_0180_m_000000_180' to o3fs://bucket1.vol1/testdata
21/01/20 15:46:29 INFO SparkHadoopMapRedUtil: attempt_20210120154628_0180_m_000000_180: Committed
21/01/20 15:46:29 INFO Executor: Finished task 0.0 in stage 180.0 (TID 180). 2155 bytes result sent to driver
21/01/20 15:46:29 INFO TaskSetManager: Finished task 0.0 in stage 180.0 (TID 180) in 967 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:46:29 INFO TaskSchedulerImpl: Removed TaskSet 180.0, whose tasks have all completed, from pool 
21/01/20 15:46:29 INFO DAGScheduler: ResultStage 180 (parquet at Generate.java:61) finished in 1.008 s
21/01/20 15:46:29 INFO DAGScheduler: Job 180 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:46:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 180: Stage finished
21/01/20 15:46:29 INFO DAGScheduler: Job 180 finished: parquet at Generate.java:61, took 1.010491 s
21/01/20 15:46:29 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-d0839a6f-4a02-4a2f-96a3-ad916fb2aada
21/01/20 15:46:29 INFO FileFormatWriter: Write Job 1cf0e5d5-95e4-4375-a648-8021402c74ce committed.
21/01/20 15:46:29 INFO FileFormatWriter: Finished processing stats for write job 1cf0e5d5-95e4-4375-a648-8021402c74ce.
21/01/20 15:46:29 INFO BlockManagerInfo: Removed broadcast_180_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:46:31 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:32 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:46:32 INFO DAGScheduler: Got job 181 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:46:32 INFO DAGScheduler: Final stage: ResultStage 181 (parquet at Generate.java:61)
21/01/20 15:46:32 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:46:32 INFO DAGScheduler: Missing parents: List()
21/01/20 15:46:32 INFO DAGScheduler: Submitting ResultStage 181 (MapPartitionsRDD[725] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:46:32 INFO MemoryStore: Block broadcast_181 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:46:32 INFO MemoryStore: Block broadcast_181_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:46:32 INFO BlockManagerInfo: Added broadcast_181_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:46:32 INFO SparkContext: Created broadcast 181 from broadcast at DAGScheduler.scala:1200
21/01/20 15:46:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 181 (MapPartitionsRDD[725] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:46:32 INFO TaskSchedulerImpl: Adding task set 181.0 with 1 tasks
21/01/20 15:46:32 WARN TaskSetManager: Stage 181 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:46:32 INFO TaskSetManager: Starting task 0.0 in stage 181.0 (TID 181, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:46:32 INFO Executor: Running task 0.0 in stage 181.0 (TID 181)
21/01/20 15:46:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:32 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:32 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:46:32 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:46:32 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:46:32 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:46:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:46:32 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:46:32 INFO ParquetOutputFormat: Validation is off
21/01/20 15:46:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:46:32 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:46:32 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:46:32 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:46:32 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:46:32 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:46:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:46:32 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-CE4787F3B56B->af5c9c6a-a887-4538-ae7b-ba6d49328297
21/01/20 15:46:32 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:46:33 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154632_0181_m_000000_181' to o3fs://bucket1.vol1/testdata
21/01/20 15:46:33 INFO SparkHadoopMapRedUtil: attempt_20210120154632_0181_m_000000_181: Committed
21/01/20 15:46:33 INFO Executor: Finished task 0.0 in stage 181.0 (TID 181). 2155 bytes result sent to driver
21/01/20 15:46:33 INFO TaskSetManager: Finished task 0.0 in stage 181.0 (TID 181) in 1023 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:46:33 INFO TaskSchedulerImpl: Removed TaskSet 181.0, whose tasks have all completed, from pool 
21/01/20 15:46:33 INFO DAGScheduler: ResultStage 181 (parquet at Generate.java:61) finished in 1.041 s
21/01/20 15:46:33 INFO DAGScheduler: Job 181 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:46:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 181: Stage finished
21/01/20 15:46:33 INFO DAGScheduler: Job 181 finished: parquet at Generate.java:61, took 1.042557 s
21/01/20 15:46:33 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-ef6d06ab-44b8-4c1c-9f84-093546e292c7
21/01/20 15:46:33 INFO FileFormatWriter: Write Job 942a5f05-aca5-4e9e-9c88-0e810b1e3f80 committed.
21/01/20 15:46:33 INFO FileFormatWriter: Finished processing stats for write job 942a5f05-aca5-4e9e-9c88-0e810b1e3f80.
21/01/20 15:46:34 INFO BlockManagerInfo: Removed broadcast_181_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:46:35 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:35 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:35 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:46:35 INFO DAGScheduler: Got job 182 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:46:35 INFO DAGScheduler: Final stage: ResultStage 182 (parquet at Generate.java:61)
21/01/20 15:46:35 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:46:35 INFO DAGScheduler: Missing parents: List()
21/01/20 15:46:35 INFO DAGScheduler: Submitting ResultStage 182 (MapPartitionsRDD[729] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:46:35 INFO MemoryStore: Block broadcast_182 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:46:35 INFO MemoryStore: Block broadcast_182_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:46:35 INFO BlockManagerInfo: Added broadcast_182_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:46:35 INFO SparkContext: Created broadcast 182 from broadcast at DAGScheduler.scala:1200
21/01/20 15:46:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 182 (MapPartitionsRDD[729] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:46:35 INFO TaskSchedulerImpl: Adding task set 182.0 with 1 tasks
21/01/20 15:46:35 WARN TaskSetManager: Stage 182 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:46:35 INFO TaskSetManager: Starting task 0.0 in stage 182.0 (TID 182, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:46:35 INFO Executor: Running task 0.0 in stage 182.0 (TID 182)
21/01/20 15:46:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:35 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:35 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:46:35 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:46:35 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:46:35 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:46:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:46:35 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:46:35 INFO ParquetOutputFormat: Validation is off
21/01/20 15:46:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:46:35 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:46:35 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:46:35 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:46:35 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:46:35 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:46:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:46:36 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154635_0182_m_000000_182' to o3fs://bucket1.vol1/testdata
21/01/20 15:46:36 INFO SparkHadoopMapRedUtil: attempt_20210120154635_0182_m_000000_182: Committed
21/01/20 15:46:36 INFO Executor: Finished task 0.0 in stage 182.0 (TID 182). 2155 bytes result sent to driver
21/01/20 15:46:36 INFO TaskSetManager: Finished task 0.0 in stage 182.0 (TID 182) in 948 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:46:36 INFO TaskSchedulerImpl: Removed TaskSet 182.0, whose tasks have all completed, from pool 
21/01/20 15:46:36 INFO DAGScheduler: ResultStage 182 (parquet at Generate.java:61) finished in 0.966 s
21/01/20 15:46:36 INFO DAGScheduler: Job 182 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:46:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 182: Stage finished
21/01/20 15:46:36 INFO DAGScheduler: Job 182 finished: parquet at Generate.java:61, took 0.966790 s
21/01/20 15:46:36 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-773b44b0-c77a-4392-b7af-f094189a2604
21/01/20 15:46:36 INFO FileFormatWriter: Write Job a2614d68-ea0c-4bb9-bfbc-1a4a533da4d8 committed.
21/01/20 15:46:36 INFO FileFormatWriter: Finished processing stats for write job a2614d68-ea0c-4bb9-bfbc-1a4a533da4d8.
21/01/20 15:46:37 INFO BlockManagerInfo: Removed broadcast_182_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:46:39 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:39 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:39 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:46:39 INFO DAGScheduler: Got job 183 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:46:39 INFO DAGScheduler: Final stage: ResultStage 183 (parquet at Generate.java:61)
21/01/20 15:46:39 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:46:39 INFO DAGScheduler: Missing parents: List()
21/01/20 15:46:39 INFO DAGScheduler: Submitting ResultStage 183 (MapPartitionsRDD[733] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:46:39 INFO MemoryStore: Block broadcast_183 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:46:39 INFO MemoryStore: Block broadcast_183_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:46:39 INFO BlockManagerInfo: Added broadcast_183_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:46:39 INFO SparkContext: Created broadcast 183 from broadcast at DAGScheduler.scala:1200
21/01/20 15:46:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 183 (MapPartitionsRDD[733] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:46:39 INFO TaskSchedulerImpl: Adding task set 183.0 with 1 tasks
21/01/20 15:46:39 WARN TaskSetManager: Stage 183 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:46:39 INFO TaskSetManager: Starting task 0.0 in stage 183.0 (TID 183, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:46:39 INFO Executor: Running task 0.0 in stage 183.0 (TID 183)
21/01/20 15:46:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:39 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:39 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:46:39 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:46:39 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:46:39 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:46:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:46:39 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:46:39 INFO ParquetOutputFormat: Validation is off
21/01/20 15:46:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:46:39 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:46:39 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:46:39 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:46:39 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:46:39 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:46:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:46:40 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154639_0183_m_000000_183' to o3fs://bucket1.vol1/testdata
21/01/20 15:46:40 INFO SparkHadoopMapRedUtil: attempt_20210120154639_0183_m_000000_183: Committed
21/01/20 15:46:40 INFO Executor: Finished task 0.0 in stage 183.0 (TID 183). 2155 bytes result sent to driver
21/01/20 15:46:40 INFO TaskSetManager: Finished task 0.0 in stage 183.0 (TID 183) in 811 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:46:40 INFO TaskSchedulerImpl: Removed TaskSet 183.0, whose tasks have all completed, from pool 
21/01/20 15:46:40 INFO DAGScheduler: ResultStage 183 (parquet at Generate.java:61) finished in 0.856 s
21/01/20 15:46:40 INFO DAGScheduler: Job 183 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:46:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 183: Stage finished
21/01/20 15:46:40 INFO DAGScheduler: Job 183 finished: parquet at Generate.java:61, took 0.858010 s
21/01/20 15:46:40 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-05f67a12-0988-44a9-8026-f6c691457be6
21/01/20 15:46:40 INFO FileFormatWriter: Write Job 374aa1ae-294b-4e19-be47-20abcea8e0cb committed.
21/01/20 15:46:40 INFO FileFormatWriter: Finished processing stats for write job 374aa1ae-294b-4e19-be47-20abcea8e0cb.
21/01/20 15:46:40 INFO BlockManagerInfo: Removed broadcast_183_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:46:42 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:42 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:42 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:46:42 INFO DAGScheduler: Got job 184 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:46:42 INFO DAGScheduler: Final stage: ResultStage 184 (parquet at Generate.java:61)
21/01/20 15:46:42 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:46:42 INFO DAGScheduler: Missing parents: List()
21/01/20 15:46:42 INFO DAGScheduler: Submitting ResultStage 184 (MapPartitionsRDD[737] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:46:42 INFO MemoryStore: Block broadcast_184 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:46:42 INFO MemoryStore: Block broadcast_184_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:46:42 INFO BlockManagerInfo: Added broadcast_184_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:46:42 INFO SparkContext: Created broadcast 184 from broadcast at DAGScheduler.scala:1200
21/01/20 15:46:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 184 (MapPartitionsRDD[737] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:46:42 INFO TaskSchedulerImpl: Adding task set 184.0 with 1 tasks
21/01/20 15:46:42 WARN TaskSetManager: Stage 184 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:46:42 INFO TaskSetManager: Starting task 0.0 in stage 184.0 (TID 184, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:46:42 INFO Executor: Running task 0.0 in stage 184.0 (TID 184)
21/01/20 15:46:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:42 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:42 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:46:42 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:46:42 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:46:42 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:46:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:46:42 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:46:42 INFO ParquetOutputFormat: Validation is off
21/01/20 15:46:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:46:42 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:46:42 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:46:42 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:46:42 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:46:42 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:46:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:46:43 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-B1C6041DD3E1->d3c4d3e3-944b-4528-9d82-e005ad427697
21/01/20 15:46:43 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:46:43 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154642_0184_m_000000_184' to o3fs://bucket1.vol1/testdata
21/01/20 15:46:43 INFO SparkHadoopMapRedUtil: attempt_20210120154642_0184_m_000000_184: Committed
21/01/20 15:46:43 INFO Executor: Finished task 0.0 in stage 184.0 (TID 184). 2155 bytes result sent to driver
21/01/20 15:46:43 INFO TaskSetManager: Finished task 0.0 in stage 184.0 (TID 184) in 1072 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:46:43 INFO TaskSchedulerImpl: Removed TaskSet 184.0, whose tasks have all completed, from pool 
21/01/20 15:46:43 INFO DAGScheduler: ResultStage 184 (parquet at Generate.java:61) finished in 1.091 s
21/01/20 15:46:43 INFO DAGScheduler: Job 184 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:46:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 184: Stage finished
21/01/20 15:46:43 INFO DAGScheduler: Job 184 finished: parquet at Generate.java:61, took 1.092292 s
21/01/20 15:46:43 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-fde6295e-d896-45df-b64b-f235e9be79a7
21/01/20 15:46:43 INFO FileFormatWriter: Write Job 4eaf317d-28ff-49ea-86e9-a7a7a6d0596b committed.
21/01/20 15:46:43 INFO FileFormatWriter: Finished processing stats for write job 4eaf317d-28ff-49ea-86e9-a7a7a6d0596b.
21/01/20 15:46:45 INFO BlockManagerInfo: Removed broadcast_184_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:46:46 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:46 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:46 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:46:46 INFO DAGScheduler: Got job 185 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:46:46 INFO DAGScheduler: Final stage: ResultStage 185 (parquet at Generate.java:61)
21/01/20 15:46:46 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:46:46 INFO DAGScheduler: Missing parents: List()
21/01/20 15:46:46 INFO DAGScheduler: Submitting ResultStage 185 (MapPartitionsRDD[741] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:46:46 INFO MemoryStore: Block broadcast_185 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:46:46 INFO MemoryStore: Block broadcast_185_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:46:46 INFO BlockManagerInfo: Added broadcast_185_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:46:46 INFO SparkContext: Created broadcast 185 from broadcast at DAGScheduler.scala:1200
21/01/20 15:46:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 185 (MapPartitionsRDD[741] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:46:46 INFO TaskSchedulerImpl: Adding task set 185.0 with 1 tasks
21/01/20 15:46:46 WARN TaskSetManager: Stage 185 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:46:46 INFO TaskSetManager: Starting task 0.0 in stage 185.0 (TID 185, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:46:46 INFO Executor: Running task 0.0 in stage 185.0 (TID 185)
21/01/20 15:46:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:46 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:46 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:46:46 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:46:46 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:46:46 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:46:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:46:46 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:46:46 INFO ParquetOutputFormat: Validation is off
21/01/20 15:46:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:46:46 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:46:46 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:46:46 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:46:46 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:46:46 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:46:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:46:47 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154646_0185_m_000000_185' to o3fs://bucket1.vol1/testdata
21/01/20 15:46:47 INFO SparkHadoopMapRedUtil: attempt_20210120154646_0185_m_000000_185: Committed
21/01/20 15:46:47 INFO Executor: Finished task 0.0 in stage 185.0 (TID 185). 2155 bytes result sent to driver
21/01/20 15:46:47 INFO TaskSetManager: Finished task 0.0 in stage 185.0 (TID 185) in 796 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:46:47 INFO TaskSchedulerImpl: Removed TaskSet 185.0, whose tasks have all completed, from pool 
21/01/20 15:46:47 INFO DAGScheduler: ResultStage 185 (parquet at Generate.java:61) finished in 0.838 s
21/01/20 15:46:47 INFO DAGScheduler: Job 185 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:46:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 185: Stage finished
21/01/20 15:46:47 INFO DAGScheduler: Job 185 finished: parquet at Generate.java:61, took 0.839912 s
21/01/20 15:46:47 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-402d7d33-b836-4d97-bebf-db5653c7f900
21/01/20 15:46:47 INFO FileFormatWriter: Write Job 7ee659a1-8380-4a17-8e27-cbb683c313e8 committed.
21/01/20 15:46:47 INFO FileFormatWriter: Finished processing stats for write job 7ee659a1-8380-4a17-8e27-cbb683c313e8.
21/01/20 15:46:47 INFO BlockManagerInfo: Removed broadcast_185_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:46:49 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:49 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:46:49 INFO DAGScheduler: Got job 186 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:46:49 INFO DAGScheduler: Final stage: ResultStage 186 (parquet at Generate.java:61)
21/01/20 15:46:49 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:46:49 INFO DAGScheduler: Missing parents: List()
21/01/20 15:46:49 INFO DAGScheduler: Submitting ResultStage 186 (MapPartitionsRDD[745] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:46:49 INFO MemoryStore: Block broadcast_186 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:46:49 INFO MemoryStore: Block broadcast_186_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:46:49 INFO BlockManagerInfo: Added broadcast_186_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:46:49 INFO SparkContext: Created broadcast 186 from broadcast at DAGScheduler.scala:1200
21/01/20 15:46:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 186 (MapPartitionsRDD[745] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:46:49 INFO TaskSchedulerImpl: Adding task set 186.0 with 1 tasks
21/01/20 15:46:49 WARN TaskSetManager: Stage 186 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:46:49 INFO TaskSetManager: Starting task 0.0 in stage 186.0 (TID 186, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:46:49 INFO Executor: Running task 0.0 in stage 186.0 (TID 186)
21/01/20 15:46:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:49 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:46:49 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:46:49 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:46:49 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:46:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:46:49 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:46:49 INFO ParquetOutputFormat: Validation is off
21/01/20 15:46:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:46:49 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:46:49 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:46:49 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:46:49 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:46:49 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:46:50 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:46:50 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-D0AE1D219DA1->af5c9c6a-a887-4538-ae7b-ba6d49328297
21/01/20 15:46:50 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:46:50 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154649_0186_m_000000_186' to o3fs://bucket1.vol1/testdata
21/01/20 15:46:50 INFO SparkHadoopMapRedUtil: attempt_20210120154649_0186_m_000000_186: Committed
21/01/20 15:46:50 INFO Executor: Finished task 0.0 in stage 186.0 (TID 186). 2155 bytes result sent to driver
21/01/20 15:46:50 INFO TaskSetManager: Finished task 0.0 in stage 186.0 (TID 186) in 985 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:46:50 INFO TaskSchedulerImpl: Removed TaskSet 186.0, whose tasks have all completed, from pool 
21/01/20 15:46:50 INFO DAGScheduler: ResultStage 186 (parquet at Generate.java:61) finished in 1.003 s
21/01/20 15:46:50 INFO DAGScheduler: Job 186 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:46:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 186: Stage finished
21/01/20 15:46:50 INFO DAGScheduler: Job 186 finished: parquet at Generate.java:61, took 1.004211 s
21/01/20 15:46:50 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-1ab60939-5b2c-40dc-80fd-57b377c5665d
21/01/20 15:46:50 INFO FileFormatWriter: Write Job 88e7e66f-08d5-43a4-b70a-c34e536e29fe committed.
21/01/20 15:46:50 INFO FileFormatWriter: Finished processing stats for write job 88e7e66f-08d5-43a4-b70a-c34e536e29fe.
21/01/20 15:46:52 INFO BlockManagerInfo: Removed broadcast_186_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:46:53 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:53 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:46:53 INFO DAGScheduler: Got job 187 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:46:53 INFO DAGScheduler: Final stage: ResultStage 187 (parquet at Generate.java:61)
21/01/20 15:46:53 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:46:53 INFO DAGScheduler: Missing parents: List()
21/01/20 15:46:53 INFO DAGScheduler: Submitting ResultStage 187 (MapPartitionsRDD[749] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:46:53 INFO MemoryStore: Block broadcast_187 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:46:53 INFO MemoryStore: Block broadcast_187_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:46:53 INFO BlockManagerInfo: Added broadcast_187_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:46:53 INFO SparkContext: Created broadcast 187 from broadcast at DAGScheduler.scala:1200
21/01/20 15:46:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 187 (MapPartitionsRDD[749] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:46:53 INFO TaskSchedulerImpl: Adding task set 187.0 with 1 tasks
21/01/20 15:46:53 WARN TaskSetManager: Stage 187 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:46:53 INFO TaskSetManager: Starting task 0.0 in stage 187.0 (TID 187, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:46:53 INFO Executor: Running task 0.0 in stage 187.0 (TID 187)
21/01/20 15:46:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:53 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:46:53 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:46:53 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:46:53 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:46:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:46:53 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:46:53 INFO ParquetOutputFormat: Validation is off
21/01/20 15:46:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:46:53 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:46:53 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:46:53 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:46:53 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:46:53 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:46:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:46:54 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154653_0187_m_000000_187' to o3fs://bucket1.vol1/testdata
21/01/20 15:46:54 INFO SparkHadoopMapRedUtil: attempt_20210120154653_0187_m_000000_187: Committed
21/01/20 15:46:54 INFO Executor: Finished task 0.0 in stage 187.0 (TID 187). 2155 bytes result sent to driver
21/01/20 15:46:54 INFO TaskSetManager: Finished task 0.0 in stage 187.0 (TID 187) in 919 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:46:54 INFO TaskSchedulerImpl: Removed TaskSet 187.0, whose tasks have all completed, from pool 
21/01/20 15:46:54 INFO DAGScheduler: ResultStage 187 (parquet at Generate.java:61) finished in 0.961 s
21/01/20 15:46:54 INFO DAGScheduler: Job 187 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:46:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 187: Stage finished
21/01/20 15:46:54 INFO DAGScheduler: Job 187 finished: parquet at Generate.java:61, took 0.965879 s
21/01/20 15:46:54 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-de0c69a1-fccc-4292-96cc-419f21f792dc
21/01/20 15:46:54 INFO FileFormatWriter: Write Job 6caa7655-684c-43d8-88ca-38268f0cc084 committed.
21/01/20 15:46:54 INFO FileFormatWriter: Finished processing stats for write job 6caa7655-684c-43d8-88ca-38268f0cc084.
21/01/20 15:46:54 INFO BlockManagerInfo: Removed broadcast_187_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:46:57 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:57 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:57 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:46:57 INFO DAGScheduler: Got job 188 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:46:57 INFO DAGScheduler: Final stage: ResultStage 188 (parquet at Generate.java:61)
21/01/20 15:46:57 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:46:57 INFO DAGScheduler: Missing parents: List()
21/01/20 15:46:57 INFO DAGScheduler: Submitting ResultStage 188 (MapPartitionsRDD[753] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:46:57 INFO MemoryStore: Block broadcast_188 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:46:57 INFO MemoryStore: Block broadcast_188_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:46:57 INFO BlockManagerInfo: Added broadcast_188_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:46:57 INFO SparkContext: Created broadcast 188 from broadcast at DAGScheduler.scala:1200
21/01/20 15:46:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 188 (MapPartitionsRDD[753] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:46:57 INFO TaskSchedulerImpl: Adding task set 188.0 with 1 tasks
21/01/20 15:46:57 WARN TaskSetManager: Stage 188 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:46:57 INFO TaskSetManager: Starting task 0.0 in stage 188.0 (TID 188, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:46:57 INFO Executor: Running task 0.0 in stage 188.0 (TID 188)
21/01/20 15:46:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:57 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:46:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:46:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:46:57 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:46:57 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:46:57 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:46:57 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:46:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:46:57 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:46:57 INFO ParquetOutputFormat: Validation is off
21/01/20 15:46:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:46:57 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:46:57 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:46:57 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:46:57 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:46:57 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:46:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:46:58 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154657_0188_m_000000_188' to o3fs://bucket1.vol1/testdata
21/01/20 15:46:58 INFO SparkHadoopMapRedUtil: attempt_20210120154657_0188_m_000000_188: Committed
21/01/20 15:46:58 INFO Executor: Finished task 0.0 in stage 188.0 (TID 188). 2155 bytes result sent to driver
21/01/20 15:46:58 INFO TaskSetManager: Finished task 0.0 in stage 188.0 (TID 188) in 997 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:46:58 INFO TaskSchedulerImpl: Removed TaskSet 188.0, whose tasks have all completed, from pool 
21/01/20 15:46:58 INFO DAGScheduler: ResultStage 188 (parquet at Generate.java:61) finished in 1.015 s
21/01/20 15:46:58 INFO DAGScheduler: Job 188 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:46:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 188: Stage finished
21/01/20 15:46:58 INFO DAGScheduler: Job 188 finished: parquet at Generate.java:61, took 1.017584 s
21/01/20 15:46:58 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-f7e07172-eb54-490d-a494-bb82201727f8
21/01/20 15:46:58 INFO FileFormatWriter: Write Job 4625f994-9ff4-43c2-a102-a8ddbbc4baef committed.
21/01/20 15:46:58 INFO FileFormatWriter: Finished processing stats for write job 4625f994-9ff4-43c2-a102-a8ddbbc4baef.
21/01/20 15:46:59 INFO BlockManagerInfo: Removed broadcast_188_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:47:00 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:00 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:47:00 INFO DAGScheduler: Got job 189 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:47:00 INFO DAGScheduler: Final stage: ResultStage 189 (parquet at Generate.java:61)
21/01/20 15:47:00 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:47:00 INFO DAGScheduler: Missing parents: List()
21/01/20 15:47:00 INFO DAGScheduler: Submitting ResultStage 189 (MapPartitionsRDD[757] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:47:00 INFO MemoryStore: Block broadcast_189 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:47:00 INFO MemoryStore: Block broadcast_189_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:47:00 INFO BlockManagerInfo: Added broadcast_189_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:47:00 INFO SparkContext: Created broadcast 189 from broadcast at DAGScheduler.scala:1200
21/01/20 15:47:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 189 (MapPartitionsRDD[757] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:47:00 INFO TaskSchedulerImpl: Adding task set 189.0 with 1 tasks
21/01/20 15:47:00 WARN TaskSetManager: Stage 189 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:47:00 INFO TaskSetManager: Starting task 0.0 in stage 189.0 (TID 189, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:47:00 INFO Executor: Running task 0.0 in stage 189.0 (TID 189)
21/01/20 15:47:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:00 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:47:00 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:47:00 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:47:00 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:47:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:47:00 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:47:00 INFO ParquetOutputFormat: Validation is off
21/01/20 15:47:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:47:00 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:47:00 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:47:00 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:47:00 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:47:00 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:47:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:47:01 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154700_0189_m_000000_189' to o3fs://bucket1.vol1/testdata
21/01/20 15:47:01 INFO SparkHadoopMapRedUtil: attempt_20210120154700_0189_m_000000_189: Committed
21/01/20 15:47:01 INFO Executor: Finished task 0.0 in stage 189.0 (TID 189). 2155 bytes result sent to driver
21/01/20 15:47:01 INFO TaskSetManager: Finished task 0.0 in stage 189.0 (TID 189) in 865 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:47:01 INFO TaskSchedulerImpl: Removed TaskSet 189.0, whose tasks have all completed, from pool 
21/01/20 15:47:01 INFO DAGScheduler: ResultStage 189 (parquet at Generate.java:61) finished in 0.907 s
21/01/20 15:47:01 INFO DAGScheduler: Job 189 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:47:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 189: Stage finished
21/01/20 15:47:01 INFO DAGScheduler: Job 189 finished: parquet at Generate.java:61, took 0.908450 s
21/01/20 15:47:01 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-bb3be736-9a1d-4485-b014-25f93c91a250
21/01/20 15:47:01 INFO FileFormatWriter: Write Job b7571d37-ccf1-46a3-8467-9981664fd5c1 committed.
21/01/20 15:47:01 INFO FileFormatWriter: Finished processing stats for write job b7571d37-ccf1-46a3-8467-9981664fd5c1.
21/01/20 15:47:01 INFO BlockManagerInfo: Removed broadcast_189_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:47:04 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:04 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:47:04 INFO DAGScheduler: Got job 190 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:47:04 INFO DAGScheduler: Final stage: ResultStage 190 (parquet at Generate.java:61)
21/01/20 15:47:04 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:47:04 INFO DAGScheduler: Missing parents: List()
21/01/20 15:47:04 INFO DAGScheduler: Submitting ResultStage 190 (MapPartitionsRDD[761] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:47:04 INFO MemoryStore: Block broadcast_190 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:47:04 INFO MemoryStore: Block broadcast_190_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:47:04 INFO BlockManagerInfo: Added broadcast_190_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:47:04 INFO SparkContext: Created broadcast 190 from broadcast at DAGScheduler.scala:1200
21/01/20 15:47:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 190 (MapPartitionsRDD[761] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:47:04 INFO TaskSchedulerImpl: Adding task set 190.0 with 1 tasks
21/01/20 15:47:04 WARN TaskSetManager: Stage 190 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:47:04 INFO TaskSetManager: Starting task 0.0 in stage 190.0 (TID 190, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:47:04 INFO Executor: Running task 0.0 in stage 190.0 (TID 190)
21/01/20 15:47:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:04 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:47:04 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:47:04 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:47:04 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:47:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:47:04 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:47:04 INFO ParquetOutputFormat: Validation is off
21/01/20 15:47:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:47:04 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:47:04 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:47:04 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:47:04 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:47:04 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:47:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:47:05 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154704_0190_m_000000_190' to o3fs://bucket1.vol1/testdata
21/01/20 15:47:05 INFO SparkHadoopMapRedUtil: attempt_20210120154704_0190_m_000000_190: Committed
21/01/20 15:47:05 INFO Executor: Finished task 0.0 in stage 190.0 (TID 190). 2155 bytes result sent to driver
21/01/20 15:47:05 INFO TaskSetManager: Finished task 0.0 in stage 190.0 (TID 190) in 973 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:47:05 INFO TaskSchedulerImpl: Removed TaskSet 190.0, whose tasks have all completed, from pool 
21/01/20 15:47:05 INFO DAGScheduler: ResultStage 190 (parquet at Generate.java:61) finished in 0.991 s
21/01/20 15:47:05 INFO DAGScheduler: Job 190 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:47:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 190: Stage finished
21/01/20 15:47:05 INFO DAGScheduler: Job 190 finished: parquet at Generate.java:61, took 0.992450 s
21/01/20 15:47:05 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-31ff2709-82c7-431d-90e6-c2fbc6f07488
21/01/20 15:47:05 INFO FileFormatWriter: Write Job 78390a73-1836-4da8-ad74-da4ff7ba2b55 committed.
21/01/20 15:47:05 INFO FileFormatWriter: Finished processing stats for write job 78390a73-1836-4da8-ad74-da4ff7ba2b55.
21/01/20 15:47:06 INFO BlockManagerInfo: Removed broadcast_190_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:47:07 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:07 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:07 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:47:07 INFO DAGScheduler: Got job 191 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:47:07 INFO DAGScheduler: Final stage: ResultStage 191 (parquet at Generate.java:61)
21/01/20 15:47:07 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:47:07 INFO DAGScheduler: Missing parents: List()
21/01/20 15:47:07 INFO DAGScheduler: Submitting ResultStage 191 (MapPartitionsRDD[765] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:47:07 INFO MemoryStore: Block broadcast_191 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:47:07 INFO MemoryStore: Block broadcast_191_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:47:07 INFO BlockManagerInfo: Added broadcast_191_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:47:07 INFO SparkContext: Created broadcast 191 from broadcast at DAGScheduler.scala:1200
21/01/20 15:47:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 191 (MapPartitionsRDD[765] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:47:07 INFO TaskSchedulerImpl: Adding task set 191.0 with 1 tasks
21/01/20 15:47:07 WARN TaskSetManager: Stage 191 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:47:07 INFO TaskSetManager: Starting task 0.0 in stage 191.0 (TID 191, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:47:07 INFO Executor: Running task 0.0 in stage 191.0 (TID 191)
21/01/20 15:47:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:08 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:08 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:47:08 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:47:08 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:47:08 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:47:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:47:08 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:47:08 INFO ParquetOutputFormat: Validation is off
21/01/20 15:47:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:47:08 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:47:08 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:47:08 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:47:08 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:47:08 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:47:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:47:08 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154707_0191_m_000000_191' to o3fs://bucket1.vol1/testdata
21/01/20 15:47:08 INFO SparkHadoopMapRedUtil: attempt_20210120154707_0191_m_000000_191: Committed
21/01/20 15:47:08 INFO Executor: Finished task 0.0 in stage 191.0 (TID 191). 2155 bytes result sent to driver
21/01/20 15:47:08 INFO TaskSetManager: Finished task 0.0 in stage 191.0 (TID 191) in 869 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:47:08 INFO TaskSchedulerImpl: Removed TaskSet 191.0, whose tasks have all completed, from pool 
21/01/20 15:47:08 INFO DAGScheduler: ResultStage 191 (parquet at Generate.java:61) finished in 0.911 s
21/01/20 15:47:08 INFO DAGScheduler: Job 191 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:47:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 191: Stage finished
21/01/20 15:47:08 INFO DAGScheduler: Job 191 finished: parquet at Generate.java:61, took 0.913118 s
21/01/20 15:47:08 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-07e97300-af77-43e4-a6f4-37c725dd321e
21/01/20 15:47:08 INFO FileFormatWriter: Write Job 24e44900-f24f-45c9-ac9d-44635fe51252 committed.
21/01/20 15:47:08 INFO FileFormatWriter: Finished processing stats for write job 24e44900-f24f-45c9-ac9d-44635fe51252.
21/01/20 15:47:08 INFO BlockManagerInfo: Removed broadcast_191_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:47:11 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:11 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:47:11 INFO DAGScheduler: Got job 192 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:47:11 INFO DAGScheduler: Final stage: ResultStage 192 (parquet at Generate.java:61)
21/01/20 15:47:11 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:47:11 INFO DAGScheduler: Missing parents: List()
21/01/20 15:47:11 INFO DAGScheduler: Submitting ResultStage 192 (MapPartitionsRDD[769] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:47:11 INFO MemoryStore: Block broadcast_192 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:47:11 INFO MemoryStore: Block broadcast_192_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:47:11 INFO BlockManagerInfo: Added broadcast_192_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:47:11 INFO SparkContext: Created broadcast 192 from broadcast at DAGScheduler.scala:1200
21/01/20 15:47:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 192 (MapPartitionsRDD[769] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:47:11 INFO TaskSchedulerImpl: Adding task set 192.0 with 1 tasks
21/01/20 15:47:11 WARN TaskSetManager: Stage 192 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:47:11 INFO TaskSetManager: Starting task 0.0 in stage 192.0 (TID 192, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:47:11 INFO Executor: Running task 0.0 in stage 192.0 (TID 192)
21/01/20 15:47:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:11 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:47:11 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:47:11 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:47:11 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:47:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:47:11 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:47:11 INFO ParquetOutputFormat: Validation is off
21/01/20 15:47:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:47:11 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:47:11 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:47:11 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:47:11 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:47:11 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:47:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:47:12 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154711_0192_m_000000_192' to o3fs://bucket1.vol1/testdata
21/01/20 15:47:12 INFO SparkHadoopMapRedUtil: attempt_20210120154711_0192_m_000000_192: Committed
21/01/20 15:47:12 INFO Executor: Finished task 0.0 in stage 192.0 (TID 192). 2155 bytes result sent to driver
21/01/20 15:47:12 INFO TaskSetManager: Finished task 0.0 in stage 192.0 (TID 192) in 1018 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:47:12 INFO TaskSchedulerImpl: Removed TaskSet 192.0, whose tasks have all completed, from pool 
21/01/20 15:47:12 INFO DAGScheduler: ResultStage 192 (parquet at Generate.java:61) finished in 1.036 s
21/01/20 15:47:12 INFO DAGScheduler: Job 192 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:47:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 192: Stage finished
21/01/20 15:47:12 INFO DAGScheduler: Job 192 finished: parquet at Generate.java:61, took 1.037044 s
21/01/20 15:47:12 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-570f71a7-6b50-410b-a33e-2e7fbf701040
21/01/20 15:47:12 INFO FileFormatWriter: Write Job 83f8616b-17ec-4585-8428-e44f0b7adb96 committed.
21/01/20 15:47:12 INFO FileFormatWriter: Finished processing stats for write job 83f8616b-17ec-4585-8428-e44f0b7adb96.
21/01/20 15:47:13 INFO BlockManagerInfo: Removed broadcast_192_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:47:15 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:15 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:15 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:47:15 INFO DAGScheduler: Got job 193 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:47:15 INFO DAGScheduler: Final stage: ResultStage 193 (parquet at Generate.java:61)
21/01/20 15:47:15 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:47:15 INFO DAGScheduler: Missing parents: List()
21/01/20 15:47:15 INFO DAGScheduler: Submitting ResultStage 193 (MapPartitionsRDD[773] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:47:15 INFO MemoryStore: Block broadcast_193 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:47:15 INFO MemoryStore: Block broadcast_193_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:47:15 INFO BlockManagerInfo: Added broadcast_193_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:47:15 INFO SparkContext: Created broadcast 193 from broadcast at DAGScheduler.scala:1200
21/01/20 15:47:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 193 (MapPartitionsRDD[773] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:47:15 INFO TaskSchedulerImpl: Adding task set 193.0 with 1 tasks
21/01/20 15:47:15 WARN TaskSetManager: Stage 193 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:47:15 INFO TaskSetManager: Starting task 0.0 in stage 193.0 (TID 193, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:47:15 INFO Executor: Running task 0.0 in stage 193.0 (TID 193)
21/01/20 15:47:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:15 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:15 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:47:15 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:47:15 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:47:15 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:47:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:47:15 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:47:15 INFO ParquetOutputFormat: Validation is off
21/01/20 15:47:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:47:15 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:47:15 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:47:15 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:47:15 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:47:15 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:47:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:47:15 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-1181E5D798FA->d3c4d3e3-944b-4528-9d82-e005ad427697
21/01/20 15:47:15 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:47:16 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154715_0193_m_000000_193' to o3fs://bucket1.vol1/testdata
21/01/20 15:47:16 INFO SparkHadoopMapRedUtil: attempt_20210120154715_0193_m_000000_193: Committed
21/01/20 15:47:16 INFO Executor: Finished task 0.0 in stage 193.0 (TID 193). 2155 bytes result sent to driver
21/01/20 15:47:16 INFO TaskSetManager: Finished task 0.0 in stage 193.0 (TID 193) in 1090 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:47:16 INFO TaskSchedulerImpl: Removed TaskSet 193.0, whose tasks have all completed, from pool 
21/01/20 15:47:16 INFO DAGScheduler: ResultStage 193 (parquet at Generate.java:61) finished in 1.107 s
21/01/20 15:47:16 INFO DAGScheduler: Job 193 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:47:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 193: Stage finished
21/01/20 15:47:16 INFO DAGScheduler: Job 193 finished: parquet at Generate.java:61, took 1.109021 s
21/01/20 15:47:16 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-77910659-302e-4ae1-a0eb-989be1a0ad6e
21/01/20 15:47:16 INFO FileFormatWriter: Write Job b6d43f93-ceb5-4bcb-8482-48fbcd74bff5 committed.
21/01/20 15:47:16 INFO FileFormatWriter: Finished processing stats for write job b6d43f93-ceb5-4bcb-8482-48fbcd74bff5.
21/01/20 15:47:17 INFO BlockManagerInfo: Removed broadcast_193_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:47:18 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:18 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:18 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:47:18 INFO DAGScheduler: Got job 194 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:47:18 INFO DAGScheduler: Final stage: ResultStage 194 (parquet at Generate.java:61)
21/01/20 15:47:18 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:47:18 INFO DAGScheduler: Missing parents: List()
21/01/20 15:47:18 INFO DAGScheduler: Submitting ResultStage 194 (MapPartitionsRDD[777] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:47:18 INFO MemoryStore: Block broadcast_194 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:47:18 INFO MemoryStore: Block broadcast_194_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:47:18 INFO BlockManagerInfo: Added broadcast_194_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:47:18 INFO SparkContext: Created broadcast 194 from broadcast at DAGScheduler.scala:1200
21/01/20 15:47:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 194 (MapPartitionsRDD[777] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:47:18 INFO TaskSchedulerImpl: Adding task set 194.0 with 1 tasks
21/01/20 15:47:18 WARN TaskSetManager: Stage 194 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:47:18 INFO TaskSetManager: Starting task 0.0 in stage 194.0 (TID 194, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:47:18 INFO Executor: Running task 0.0 in stage 194.0 (TID 194)
21/01/20 15:47:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:18 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:18 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:47:18 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:47:18 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:47:18 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:47:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:47:18 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:47:18 INFO ParquetOutputFormat: Validation is off
21/01/20 15:47:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:47:18 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:47:18 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:47:18 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:47:18 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:47:18 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:47:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:47:19 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154718_0194_m_000000_194' to o3fs://bucket1.vol1/testdata
21/01/20 15:47:19 INFO SparkHadoopMapRedUtil: attempt_20210120154718_0194_m_000000_194: Committed
21/01/20 15:47:19 INFO Executor: Finished task 0.0 in stage 194.0 (TID 194). 2155 bytes result sent to driver
21/01/20 15:47:19 INFO TaskSetManager: Finished task 0.0 in stage 194.0 (TID 194) in 1026 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:47:19 INFO TaskSchedulerImpl: Removed TaskSet 194.0, whose tasks have all completed, from pool 
21/01/20 15:47:19 INFO DAGScheduler: ResultStage 194 (parquet at Generate.java:61) finished in 1.044 s
21/01/20 15:47:19 INFO DAGScheduler: Job 194 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:47:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 194: Stage finished
21/01/20 15:47:19 INFO DAGScheduler: Job 194 finished: parquet at Generate.java:61, took 1.045620 s
21/01/20 15:47:19 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-590d48f6-25d6-4bad-a121-70a8246ad13f
21/01/20 15:47:19 INFO FileFormatWriter: Write Job b5f2552b-a217-49f1-99e7-ffbfe7a39d15 committed.
21/01/20 15:47:19 INFO FileFormatWriter: Finished processing stats for write job b5f2552b-a217-49f1-99e7-ffbfe7a39d15.
21/01/20 15:47:20 INFO BlockManagerInfo: Removed broadcast_194_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:47:22 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:22 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:22 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:47:22 INFO DAGScheduler: Got job 195 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:47:22 INFO DAGScheduler: Final stage: ResultStage 195 (parquet at Generate.java:61)
21/01/20 15:47:22 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:47:22 INFO DAGScheduler: Missing parents: List()
21/01/20 15:47:22 INFO DAGScheduler: Submitting ResultStage 195 (MapPartitionsRDD[781] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:47:22 INFO MemoryStore: Block broadcast_195 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:47:22 INFO MemoryStore: Block broadcast_195_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:47:22 INFO BlockManagerInfo: Added broadcast_195_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:47:22 INFO SparkContext: Created broadcast 195 from broadcast at DAGScheduler.scala:1200
21/01/20 15:47:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 195 (MapPartitionsRDD[781] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:47:22 INFO TaskSchedulerImpl: Adding task set 195.0 with 1 tasks
21/01/20 15:47:22 WARN TaskSetManager: Stage 195 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:47:22 INFO TaskSetManager: Starting task 0.0 in stage 195.0 (TID 195, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:47:22 INFO Executor: Running task 0.0 in stage 195.0 (TID 195)
21/01/20 15:47:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:22 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:22 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:47:22 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:47:22 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:47:22 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:47:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:47:22 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:47:22 INFO ParquetOutputFormat: Validation is off
21/01/20 15:47:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:47:22 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:47:22 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:47:22 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:47:22 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:47:22 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:47:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:47:23 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154722_0195_m_000000_195' to o3fs://bucket1.vol1/testdata
21/01/20 15:47:23 INFO SparkHadoopMapRedUtil: attempt_20210120154722_0195_m_000000_195: Committed
21/01/20 15:47:23 INFO Executor: Finished task 0.0 in stage 195.0 (TID 195). 2155 bytes result sent to driver
21/01/20 15:47:23 INFO TaskSetManager: Finished task 0.0 in stage 195.0 (TID 195) in 1083 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:47:23 INFO TaskSchedulerImpl: Removed TaskSet 195.0, whose tasks have all completed, from pool 
21/01/20 15:47:23 INFO DAGScheduler: ResultStage 195 (parquet at Generate.java:61) finished in 1.101 s
21/01/20 15:47:23 INFO DAGScheduler: Job 195 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:47:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 195: Stage finished
21/01/20 15:47:23 INFO DAGScheduler: Job 195 finished: parquet at Generate.java:61, took 1.103062 s
21/01/20 15:47:23 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-a6a522d1-5fb7-4f6d-9ee8-7da5958de1a6
21/01/20 15:47:23 INFO FileFormatWriter: Write Job 44dfed20-1ddc-4d18-b047-e7d808b89372 committed.
21/01/20 15:47:23 INFO FileFormatWriter: Finished processing stats for write job 44dfed20-1ddc-4d18-b047-e7d808b89372.
21/01/20 15:47:24 INFO BlockManagerInfo: Removed broadcast_195_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:47:26 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:26 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:26 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:47:26 INFO DAGScheduler: Got job 196 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:47:26 INFO DAGScheduler: Final stage: ResultStage 196 (parquet at Generate.java:61)
21/01/20 15:47:26 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:47:26 INFO DAGScheduler: Missing parents: List()
21/01/20 15:47:26 INFO DAGScheduler: Submitting ResultStage 196 (MapPartitionsRDD[785] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:47:26 INFO MemoryStore: Block broadcast_196 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:47:26 INFO MemoryStore: Block broadcast_196_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:47:26 INFO BlockManagerInfo: Added broadcast_196_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:47:26 INFO SparkContext: Created broadcast 196 from broadcast at DAGScheduler.scala:1200
21/01/20 15:47:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 196 (MapPartitionsRDD[785] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:47:26 INFO TaskSchedulerImpl: Adding task set 196.0 with 1 tasks
21/01/20 15:47:26 WARN TaskSetManager: Stage 196 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:47:26 INFO TaskSetManager: Starting task 0.0 in stage 196.0 (TID 196, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:47:26 INFO Executor: Running task 0.0 in stage 196.0 (TID 196)
21/01/20 15:47:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:26 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:26 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:47:26 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:47:26 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:47:26 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:47:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:47:26 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:47:26 INFO ParquetOutputFormat: Validation is off
21/01/20 15:47:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:47:26 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:47:26 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:47:26 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:47:26 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:47:26 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:47:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:47:27 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154726_0196_m_000000_196' to o3fs://bucket1.vol1/testdata
21/01/20 15:47:27 INFO SparkHadoopMapRedUtil: attempt_20210120154726_0196_m_000000_196: Committed
21/01/20 15:47:27 INFO Executor: Finished task 0.0 in stage 196.0 (TID 196). 2155 bytes result sent to driver
21/01/20 15:47:27 INFO TaskSetManager: Finished task 0.0 in stage 196.0 (TID 196) in 1022 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:47:27 INFO TaskSchedulerImpl: Removed TaskSet 196.0, whose tasks have all completed, from pool 
21/01/20 15:47:27 INFO DAGScheduler: ResultStage 196 (parquet at Generate.java:61) finished in 1.041 s
21/01/20 15:47:27 INFO DAGScheduler: Job 196 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:47:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 196: Stage finished
21/01/20 15:47:27 INFO DAGScheduler: Job 196 finished: parquet at Generate.java:61, took 1.041820 s
21/01/20 15:47:27 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-9d27e5f6-2d04-49eb-8282-6ff6dba52e0d
21/01/20 15:47:27 INFO FileFormatWriter: Write Job 2ddbf1ca-6231-4577-b4cb-2cdc5511dbac committed.
21/01/20 15:47:27 INFO FileFormatWriter: Finished processing stats for write job 2ddbf1ca-6231-4577-b4cb-2cdc5511dbac.
21/01/20 15:47:28 INFO BlockManagerInfo: Removed broadcast_196_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:47:29 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:29 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:47:29 INFO DAGScheduler: Got job 197 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:47:29 INFO DAGScheduler: Final stage: ResultStage 197 (parquet at Generate.java:61)
21/01/20 15:47:29 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:47:29 INFO DAGScheduler: Missing parents: List()
21/01/20 15:47:29 INFO DAGScheduler: Submitting ResultStage 197 (MapPartitionsRDD[789] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:47:29 INFO MemoryStore: Block broadcast_197 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:47:29 INFO MemoryStore: Block broadcast_197_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:47:29 INFO BlockManagerInfo: Added broadcast_197_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:47:29 INFO SparkContext: Created broadcast 197 from broadcast at DAGScheduler.scala:1200
21/01/20 15:47:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 197 (MapPartitionsRDD[789] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:47:29 INFO TaskSchedulerImpl: Adding task set 197.0 with 1 tasks
21/01/20 15:47:29 WARN TaskSetManager: Stage 197 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:47:29 INFO TaskSetManager: Starting task 0.0 in stage 197.0 (TID 197, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:47:29 INFO Executor: Running task 0.0 in stage 197.0 (TID 197)
21/01/20 15:47:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:29 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:47:29 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:47:29 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:47:29 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:47:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:47:29 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:47:29 INFO ParquetOutputFormat: Validation is off
21/01/20 15:47:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:47:29 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:47:29 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:47:29 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:47:29 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:47:29 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:47:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:47:30 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-2ABDB03A43DC->af5c9c6a-a887-4538-ae7b-ba6d49328297
21/01/20 15:47:30 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 15:47:30 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154729_0197_m_000000_197' to o3fs://bucket1.vol1/testdata
21/01/20 15:47:30 INFO SparkHadoopMapRedUtil: attempt_20210120154729_0197_m_000000_197: Committed
21/01/20 15:47:30 INFO Executor: Finished task 0.0 in stage 197.0 (TID 197). 2155 bytes result sent to driver
21/01/20 15:47:30 INFO TaskSetManager: Finished task 0.0 in stage 197.0 (TID 197) in 739 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:47:30 INFO TaskSchedulerImpl: Removed TaskSet 197.0, whose tasks have all completed, from pool 
21/01/20 15:47:30 INFO DAGScheduler: ResultStage 197 (parquet at Generate.java:61) finished in 0.782 s
21/01/20 15:47:30 INFO DAGScheduler: Job 197 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:47:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 197: Stage finished
21/01/20 15:47:30 INFO DAGScheduler: Job 197 finished: parquet at Generate.java:61, took 0.783251 s
21/01/20 15:47:30 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-fcd475ab-3997-4af7-825e-87ee37fc2541
21/01/20 15:47:30 INFO FileFormatWriter: Write Job 7cfe8652-d2e2-4503-9bdc-7c3322d13609 committed.
21/01/20 15:47:30 INFO FileFormatWriter: Finished processing stats for write job 7cfe8652-d2e2-4503-9bdc-7c3322d13609.
21/01/20 15:47:30 INFO BlockManagerInfo: Removed broadcast_197_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:47:33 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:33 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:33 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:47:33 INFO DAGScheduler: Got job 198 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:47:33 INFO DAGScheduler: Final stage: ResultStage 198 (parquet at Generate.java:61)
21/01/20 15:47:33 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:47:33 INFO DAGScheduler: Missing parents: List()
21/01/20 15:47:33 INFO DAGScheduler: Submitting ResultStage 198 (MapPartitionsRDD[793] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:47:33 INFO MemoryStore: Block broadcast_198 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:47:33 INFO MemoryStore: Block broadcast_198_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:47:33 INFO BlockManagerInfo: Added broadcast_198_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:47:33 INFO SparkContext: Created broadcast 198 from broadcast at DAGScheduler.scala:1200
21/01/20 15:47:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 198 (MapPartitionsRDD[793] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:47:33 INFO TaskSchedulerImpl: Adding task set 198.0 with 1 tasks
21/01/20 15:47:33 WARN TaskSetManager: Stage 198 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:47:33 INFO TaskSetManager: Starting task 0.0 in stage 198.0 (TID 198, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:47:33 INFO Executor: Running task 0.0 in stage 198.0 (TID 198)
21/01/20 15:47:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:33 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:33 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:47:33 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:47:33 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:47:33 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:47:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:47:33 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:47:33 INFO ParquetOutputFormat: Validation is off
21/01/20 15:47:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:47:33 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:47:33 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:47:33 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:47:33 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:47:33 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:47:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:47:34 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154733_0198_m_000000_198' to o3fs://bucket1.vol1/testdata
21/01/20 15:47:34 INFO SparkHadoopMapRedUtil: attempt_20210120154733_0198_m_000000_198: Committed
21/01/20 15:47:34 INFO Executor: Finished task 0.0 in stage 198.0 (TID 198). 2155 bytes result sent to driver
21/01/20 15:47:34 INFO TaskSetManager: Finished task 0.0 in stage 198.0 (TID 198) in 1033 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:47:34 INFO TaskSchedulerImpl: Removed TaskSet 198.0, whose tasks have all completed, from pool 
21/01/20 15:47:34 INFO DAGScheduler: ResultStage 198 (parquet at Generate.java:61) finished in 1.050 s
21/01/20 15:47:34 INFO DAGScheduler: Job 198 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:47:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 198: Stage finished
21/01/20 15:47:34 INFO DAGScheduler: Job 198 finished: parquet at Generate.java:61, took 1.051800 s
21/01/20 15:47:34 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-23b9a183-916a-4e71-9622-f0b017ab797f
21/01/20 15:47:34 INFO FileFormatWriter: Write Job 24ee1670-3257-4bd9-8a00-f3ad861bc2c5 committed.
21/01/20 15:47:34 INFO FileFormatWriter: Finished processing stats for write job 24ee1670-3257-4bd9-8a00-f3ad861bc2c5.
21/01/20 15:47:35 INFO BlockManagerInfo: Removed broadcast_198_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:47:36 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:36 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:36 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:47:36 INFO DAGScheduler: Got job 199 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:47:36 INFO DAGScheduler: Final stage: ResultStage 199 (parquet at Generate.java:61)
21/01/20 15:47:36 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:47:36 INFO DAGScheduler: Missing parents: List()
21/01/20 15:47:36 INFO DAGScheduler: Submitting ResultStage 199 (MapPartitionsRDD[797] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:47:36 INFO MemoryStore: Block broadcast_199 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:47:36 INFO MemoryStore: Block broadcast_199_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:47:36 INFO BlockManagerInfo: Added broadcast_199_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:47:36 INFO SparkContext: Created broadcast 199 from broadcast at DAGScheduler.scala:1200
21/01/20 15:47:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 199 (MapPartitionsRDD[797] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:47:36 INFO TaskSchedulerImpl: Adding task set 199.0 with 1 tasks
21/01/20 15:47:36 WARN TaskSetManager: Stage 199 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:47:36 INFO TaskSetManager: Starting task 0.0 in stage 199.0 (TID 199, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:47:36 INFO Executor: Running task 0.0 in stage 199.0 (TID 199)
21/01/20 15:47:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:37 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:37 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:47:37 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:47:37 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:47:37 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:47:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:47:37 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:47:37 INFO ParquetOutputFormat: Validation is off
21/01/20 15:47:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:47:37 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:47:37 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:47:37 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:47:37 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:47:37 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:47:37 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:47:37 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154736_0199_m_000000_199' to o3fs://bucket1.vol1/testdata
21/01/20 15:47:37 INFO SparkHadoopMapRedUtil: attempt_20210120154736_0199_m_000000_199: Committed
21/01/20 15:47:37 INFO Executor: Finished task 0.0 in stage 199.0 (TID 199). 2155 bytes result sent to driver
21/01/20 15:47:37 INFO TaskSetManager: Finished task 0.0 in stage 199.0 (TID 199) in 913 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:47:37 INFO TaskSchedulerImpl: Removed TaskSet 199.0, whose tasks have all completed, from pool 
21/01/20 15:47:37 INFO DAGScheduler: ResultStage 199 (parquet at Generate.java:61) finished in 0.955 s
21/01/20 15:47:37 INFO DAGScheduler: Job 199 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:47:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 199: Stage finished
21/01/20 15:47:37 INFO DAGScheduler: Job 199 finished: parquet at Generate.java:61, took 0.956489 s
21/01/20 15:47:37 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-7de28d2e-eda8-4d2f-a05e-bf598d9b6878
21/01/20 15:47:37 INFO FileFormatWriter: Write Job 6d905f2d-225d-4fea-a5f6-289910645552 committed.
21/01/20 15:47:37 INFO FileFormatWriter: Finished processing stats for write job 6d905f2d-225d-4fea-a5f6-289910645552.
21/01/20 15:47:38 INFO BlockManagerInfo: Removed broadcast_199_piece0 on test-runner-lmfgs:39761 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:47:40 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:40 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:40 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 15:47:40 INFO DAGScheduler: Got job 200 (parquet at Generate.java:61) with 1 output partitions
21/01/20 15:47:40 INFO DAGScheduler: Final stage: ResultStage 200 (parquet at Generate.java:61)
21/01/20 15:47:40 INFO DAGScheduler: Parents of final stage: List()
21/01/20 15:47:40 INFO DAGScheduler: Missing parents: List()
21/01/20 15:47:40 INFO DAGScheduler: Submitting ResultStage 200 (MapPartitionsRDD[801] at parquet at Generate.java:61), which has no missing parents
21/01/20 15:47:40 INFO MemoryStore: Block broadcast_200 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 15:47:40 INFO MemoryStore: Block broadcast_200_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 15:47:40 INFO BlockManagerInfo: Added broadcast_200_piece0 in memory on test-runner-lmfgs:39761 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 15:47:40 INFO SparkContext: Created broadcast 200 from broadcast at DAGScheduler.scala:1200
21/01/20 15:47:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 200 (MapPartitionsRDD[801] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 15:47:40 INFO TaskSchedulerImpl: Adding task set 200.0 with 1 tasks
21/01/20 15:47:40 WARN TaskSetManager: Stage 200 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 15:47:40 INFO TaskSetManager: Starting task 0.0 in stage 200.0 (TID 200, test-runner-lmfgs, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 15:47:40 INFO Executor: Running task 0.0 in stage 200.0 (TID 200)
21/01/20 15:47:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:40 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 15:47:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 15:47:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 15:47:40 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:47:40 INFO CodecConfig: Compression: SNAPPY
21/01/20 15:47:40 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 15:47:40 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 15:47:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 15:47:40 INFO ParquetOutputFormat: Dictionary is on
21/01/20 15:47:40 INFO ParquetOutputFormat: Validation is off
21/01/20 15:47:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 15:47:40 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 15:47:40 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 15:47:40 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 15:47:40 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 15:47:40 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 15:47:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 15:47:41 INFO FileOutputCommitter: Saved output of task 'attempt_20210120154740_0200_m_000000_200' to o3fs://bucket1.vol1/testdata
21/01/20 15:47:41 INFO SparkHadoopMapRedUtil: attempt_20210120154740_0200_m_000000_200: Committed
21/01/20 15:47:41 INFO Executor: Finished task 0.0 in stage 200.0 (TID 200). 2155 bytes result sent to driver
21/01/20 15:47:41 INFO TaskSetManager: Finished task 0.0 in stage 200.0 (TID 200) in 1098 ms on test-runner-lmfgs (executor driver) (1/1)
21/01/20 15:47:41 INFO TaskSchedulerImpl: Removed TaskSet 200.0, whose tasks have all completed, from pool 
21/01/20 15:47:41 INFO DAGScheduler: ResultStage 200 (parquet at Generate.java:61) finished in 1.118 s
21/01/20 15:47:41 INFO DAGScheduler: Job 200 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 15:47:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 200: Stage finished
21/01/20 15:47:41 INFO DAGScheduler: Job 200 finished: parquet at Generate.java:61, took 1.119392 s
21/01/20 15:47:41 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-ffedabe1-5368-4b71-b922-02121de59b63
21/01/20 15:47:41 INFO FileFormatWriter: Write Job 0b404b20-9331-4534-801d-3373e2d5eb0d committed.
21/01/20 15:47:41 INFO FileFormatWriter: Finished processing stats for write job 0b404b20-9331-4534-801d-3373e2d5eb0d.
21/01/20 15:47:41 INFO SparkUI: Stopped Spark web UI at http://test-runner-lmfgs:4040
21/01/20 15:47:41 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
21/01/20 15:47:41 INFO MemoryStore: MemoryStore cleared
21/01/20 15:47:41 INFO BlockManager: BlockManager stopped
21/01/20 15:47:41 INFO BlockManagerMaster: BlockManagerMaster stopped
21/01/20 15:47:41 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
21/01/20 15:47:41 INFO SparkContext: Successfully stopped SparkContext
21/01/20 15:47:41 INFO ShutdownHookManager: Shutdown hook called
21/01/20 15:47:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-14e82dee-8c82-43bb-9e69-c24c447fc558
21/01/20 15:47:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-3560574f-432b-42f8-81ec-8e6b6fa31b6a

real	12m24.062s
user	12m20.801s
sys	0m16.890s
Process exited with exit code 0
