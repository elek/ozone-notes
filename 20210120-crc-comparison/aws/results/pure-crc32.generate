Flokkr launcher script 1.1-18-g58a3efc

===== Plugin is activated ENVTOCONF =====
hadoop.conf
File hadoop.conf has been written out successfullly.
core-site.xml
File core-site.xml has been written out successfullly.
ozone-site.xml
File ozone-site.xml has been written out successfullly.
mapred-site.xml
File mapred-site.xml has been written out successfullly.
hdfs-site.xml
File hdfs-site.xml has been written out successfullly.
Non-spark-on-k8s command provided, proceeding in pass-through mode...
======================================
*** Launching "/opt/testscripts/parquet.sh generate --iteration 200 o3fs://bucket1.vol1/testdata"
+ : /opt/spark
+ : /opt
+ : watchforcommit.btm
+ /opt/spark/bin/spark-submit --conf spark.executor.memory=4g --jars /opt/ozonefs/hadoop-ozone-filesystem.jar /opt/spark-samples-1.0-SNAPSHOT.jar generate --iteration 200 o3fs://bucket1.vol1/testdata
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.0.0.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
21/01/20 11:24:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
21/01/20 11:24:02 INFO SparkContext: Running Spark version 3.0.0
21/01/20 11:24:02 INFO ResourceUtils: ==============================================================
21/01/20 11:24:02 INFO ResourceUtils: Resources for spark.driver:

21/01/20 11:24:02 INFO ResourceUtils: ==============================================================
21/01/20 11:24:02 INFO SparkContext: Submitted application: Generate
21/01/20 11:24:02 INFO SecurityManager: Changing view acls to: spark
21/01/20 11:24:02 INFO SecurityManager: Changing modify acls to: spark
21/01/20 11:24:02 INFO SecurityManager: Changing view acls groups to: 
21/01/20 11:24:02 INFO SecurityManager: Changing modify acls groups to: 
21/01/20 11:24:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
21/01/20 11:24:03 INFO Utils: Successfully started service 'sparkDriver' on port 34455.
21/01/20 11:24:03 INFO SparkEnv: Registering MapOutputTracker
21/01/20 11:24:03 INFO SparkEnv: Registering BlockManagerMaster
21/01/20 11:24:03 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/01/20 11:24:03 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/01/20 11:24:03 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/01/20 11:24:03 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6befcf8c-011b-41d2-a6d4-93c04b147daf
21/01/20 11:24:03 INFO MemoryStore: MemoryStore started with capacity 413.9 MiB
21/01/20 11:24:03 INFO SparkEnv: Registering OutputCommitCoordinator
21/01/20 11:24:03 INFO Utils: Successfully started service 'SparkUI' on port 4040.
21/01/20 11:24:03 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://test-runner-9msxc:4040
21/01/20 11:24:03 INFO SparkContext: Added JAR file:///opt/ozonefs/hadoop-ozone-filesystem.jar at spark://test-runner-9msxc:34455/jars/hadoop-ozone-filesystem.jar with timestamp 1611141843470
21/01/20 11:24:03 INFO SparkContext: Added JAR file:/opt/spark-samples-1.0-SNAPSHOT.jar at spark://test-runner-9msxc:34455/jars/spark-samples-1.0-SNAPSHOT.jar with timestamp 1611141843471
21/01/20 11:24:03 INFO Executor: Starting executor ID driver on host test-runner-9msxc
21/01/20 11:24:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43214.
21/01/20 11:24:03 INFO NettyBlockTransferService: Server created on test-runner-9msxc:43214
21/01/20 11:24:03 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/01/20 11:24:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, test-runner-9msxc, 43214, None)
21/01/20 11:24:03 INFO BlockManagerMasterEndpoint: Registering block manager test-runner-9msxc:43214 with 413.9 MiB RAM, BlockManagerId(driver, test-runner-9msxc, 43214, None)
21/01/20 11:24:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, test-runner-9msxc, 43214, None)
21/01/20 11:24:03 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, test-runner-9msxc, 43214, None)
21/01/20 11:24:04 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/opt/spark/spark-warehouse').
21/01/20 11:24:04 INFO SharedState: Warehouse path is 'file:/opt/spark/spark-warehouse'.
21/01/20 11:24:07 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:07 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:07 INFO CodeGenerator: Code generated in 198.98866 ms
21/01/20 11:24:07 INFO SparkContext: Starting job: parquet at Generate.java:38
21/01/20 11:24:07 INFO DAGScheduler: Got job 0 (parquet at Generate.java:38) with 1 output partitions
21/01/20 11:24:07 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at Generate.java:38)
21/01/20 11:24:07 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:24:07 INFO DAGScheduler: Missing parents: List()
21/01/20 11:24:07 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at Generate.java:38), which has no missing parents
21/01/20 11:24:07 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:24:07 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 73.9 KiB, free 413.7 MiB)
21/01/20 11:24:07 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on test-runner-9msxc:43214 (size: 73.9 KiB, free: 413.9 MiB)
21/01/20 11:24:07 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1200
21/01/20 11:24:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at Generate.java:38) (first 15 tasks are for partitions Vector(0))
21/01/20 11:24:07 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
21/01/20 11:24:07 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 7541 bytes)
21/01/20 11:24:07 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
21/01/20 11:24:07 INFO Executor: Fetching spark://test-runner-9msxc:34455/jars/spark-samples-1.0-SNAPSHOT.jar with timestamp 1611141843471
21/01/20 11:24:07 INFO TransportClientFactory: Successfully created connection to test-runner-9msxc/10.42.4.36:34455 after 28 ms (0 ms spent in bootstraps)
21/01/20 11:24:07 INFO Utils: Fetching spark://test-runner-9msxc:34455/jars/spark-samples-1.0-SNAPSHOT.jar to /tmp/spark-c991cf70-fb86-46ec-95c3-decb9b5dff7a/userFiles-a85f7d49-a6ea-4b3b-a0f0-95b11c071494/fetchFileTemp17759036666649870692.tmp
21/01/20 11:24:08 INFO Executor: Adding file:/tmp/spark-c991cf70-fb86-46ec-95c3-decb9b5dff7a/userFiles-a85f7d49-a6ea-4b3b-a0f0-95b11c071494/spark-samples-1.0-SNAPSHOT.jar to class loader
21/01/20 11:24:08 INFO Executor: Fetching spark://test-runner-9msxc:34455/jars/hadoop-ozone-filesystem.jar with timestamp 1611141843470
21/01/20 11:24:08 INFO Utils: Fetching spark://test-runner-9msxc:34455/jars/hadoop-ozone-filesystem.jar to /tmp/spark-c991cf70-fb86-46ec-95c3-decb9b5dff7a/userFiles-a85f7d49-a6ea-4b3b-a0f0-95b11c071494/fetchFileTemp4675714497359177213.tmp
21/01/20 11:24:08 INFO Executor: Adding file:/tmp/spark-c991cf70-fb86-46ec-95c3-decb9b5dff7a/userFiles-a85f7d49-a6ea-4b3b-a0f0-95b11c071494/hadoop-ozone-filesystem.jar to class loader
21/01/20 11:24:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:08 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:08 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:24:08 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:24:08 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:24:08 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:24:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:24:08 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:24:08 INFO ParquetOutputFormat: Validation is off
21/01/20 11:24:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:24:08 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:24:08 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:24:08 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:24:08 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:24:08 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:24:08 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-xceiverclientmetrics.properties,hadoop-metrics2.properties
21/01/20 11:24:08 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
21/01/20 11:24:08 INFO MetricsSystemImpl: XceiverClientMetrics metrics system started
21/01/20 11:24:08 INFO CodecPool: Got brand-new compressor [.snappy]
21/01/20 11:24:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 17
21/01/20 11:24:09 INFO MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
21/01/20 11:24:09 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-0FE9A4700CE8->8df234bd-f131-4d8e-953a-b9eec8633221
21/01/20 11:24:09 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:24:09 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112407_0000_m_000000_0' to o3fs://bucket1.vol1/testdata
21/01/20 11:24:09 INFO SparkHadoopMapRedUtil: attempt_20210120112407_0000_m_000000_0: Committed
21/01/20 11:24:09 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2198 bytes result sent to driver
21/01/20 11:24:09 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1880 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:24:09 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
21/01/20 11:24:09 INFO DAGScheduler: ResultStage 0 (parquet at Generate.java:38) finished in 2.069 s
21/01/20 11:24:09 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:24:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
21/01/20 11:24:09 INFO DAGScheduler: Job 0 finished: parquet at Generate.java:38, took 2.109654 s
21/01/20 11:24:09 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-c9834ba9-f6b5-46ab-b086-0cbaa4b85e0e
21/01/20 11:24:09 INFO FileFormatWriter: Write Job 872b5d93-ada9-4cff-8e15-9453d3e1ea81 committed.
21/01/20 11:24:09 INFO FileFormatWriter: Finished processing stats for write job 872b5d93-ada9-4cff-8e15-9453d3e1ea81.
21/01/20 11:24:12 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:12 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:12 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:24:12 INFO DAGScheduler: Got job 1 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:24:12 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at Generate.java:61)
21/01/20 11:24:12 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:24:12 INFO DAGScheduler: Missing parents: List()
21/01/20 11:24:12 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:24:12 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 205.8 KiB, free 413.5 MiB)
21/01/20 11:24:12 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.4 MiB)
21/01/20 11:24:12 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.8 MiB)
21/01/20 11:24:12 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1200
21/01/20 11:24:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:24:12 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
21/01/20 11:24:12 WARN TaskSetManager: Stage 1 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:24:12 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:24:12 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
21/01/20 11:24:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:12 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:12 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:24:12 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:24:12 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:24:12 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:24:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:24:12 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:24:12 INFO ParquetOutputFormat: Validation is off
21/01/20 11:24:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:24:12 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:24:12 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:24:12 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:24:12 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:24:12 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:24:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:24:13 INFO BlockManagerInfo: Removed broadcast_0_piece0 on test-runner-9msxc:43214 in memory (size: 73.9 KiB, free: 413.9 MiB)
21/01/20 11:24:17 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112412_0001_m_000000_1' to o3fs://bucket1.vol1/testdata
21/01/20 11:24:17 INFO SparkHadoopMapRedUtil: attempt_20210120112412_0001_m_000000_1: Committed
21/01/20 11:24:17 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2155 bytes result sent to driver
21/01/20 11:24:17 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 4891 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:24:17 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
21/01/20 11:24:17 INFO DAGScheduler: ResultStage 1 (parquet at Generate.java:61) finished in 4.922 s
21/01/20 11:24:17 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:24:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
21/01/20 11:24:17 INFO DAGScheduler: Job 1 finished: parquet at Generate.java:61, took 4.927494 s
21/01/20 11:24:17 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-68170660-9969-4b70-b454-ae6f9deb7247
21/01/20 11:24:17 INFO FileFormatWriter: Write Job 3f1b3bca-4c34-4f5a-bfb6-fdc80126ea7f committed.
21/01/20 11:24:17 INFO FileFormatWriter: Finished processing stats for write job 3f1b3bca-4c34-4f5a-bfb6-fdc80126ea7f.
21/01/20 11:24:18 INFO BlockManagerInfo: Removed broadcast_1_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:24:19 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:19 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:20 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:24:20 INFO DAGScheduler: Got job 2 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:24:20 INFO DAGScheduler: Final stage: ResultStage 2 (parquet at Generate.java:61)
21/01/20 11:24:20 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:24:20 INFO DAGScheduler: Missing parents: List()
21/01/20 11:24:20 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[9] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:24:20 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:24:20 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:24:20 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:24:20 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1200
21/01/20 11:24:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:24:20 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
21/01/20 11:24:20 WARN TaskSetManager: Stage 2 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:24:20 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:24:20 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
21/01/20 11:24:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:20 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:20 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:24:20 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:24:20 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:24:20 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:24:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:24:20 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:24:20 INFO ParquetOutputFormat: Validation is off
21/01/20 11:24:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:24:20 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:24:20 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:24:20 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:24:20 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:24:20 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:24:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:24:20 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-BA60A5C681F1->27b66227-fed8-4c1d-ab57-510f5a3ce552
21/01/20 11:24:20 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:24:21 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112419_0002_m_000000_2' to o3fs://bucket1.vol1/testdata
21/01/20 11:24:21 INFO SparkHadoopMapRedUtil: attempt_20210120112419_0002_m_000000_2: Committed
21/01/20 11:24:21 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2155 bytes result sent to driver
21/01/20 11:24:21 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1143 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:24:21 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
21/01/20 11:24:21 INFO DAGScheduler: ResultStage 2 (parquet at Generate.java:61) finished in 1.171 s
21/01/20 11:24:21 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:24:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
21/01/20 11:24:21 INFO DAGScheduler: Job 2 finished: parquet at Generate.java:61, took 1.174928 s
21/01/20 11:24:21 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-b0d84854-5991-4a5d-874c-6260554f709f
21/01/20 11:24:21 INFO FileFormatWriter: Write Job 19b80f2c-5195-4a65-8c22-b1fb9d690dbe committed.
21/01/20 11:24:21 INFO FileFormatWriter: Finished processing stats for write job 19b80f2c-5195-4a65-8c22-b1fb9d690dbe.
21/01/20 11:24:21 INFO BlockManagerInfo: Removed broadcast_2_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:24:23 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:23 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:23 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:24:23 INFO DAGScheduler: Got job 3 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:24:23 INFO DAGScheduler: Final stage: ResultStage 3 (parquet at Generate.java:61)
21/01/20 11:24:23 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:24:23 INFO DAGScheduler: Missing parents: List()
21/01/20 11:24:23 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[13] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:24:23 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:24:23 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:24:23 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:24:23 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1200
21/01/20 11:24:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:24:23 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
21/01/20 11:24:23 WARN TaskSetManager: Stage 3 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:24:23 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:24:23 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
21/01/20 11:24:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:24 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:24 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:24:24 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:24:24 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:24:24 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:24:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:24:24 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:24:24 INFO ParquetOutputFormat: Validation is off
21/01/20 11:24:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:24:24 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:24:24 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:24:24 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:24:24 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:24:24 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:24:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:24:24 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112423_0003_m_000000_3' to o3fs://bucket1.vol1/testdata
21/01/20 11:24:24 INFO SparkHadoopMapRedUtil: attempt_20210120112423_0003_m_000000_3: Committed
21/01/20 11:24:24 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2155 bytes result sent to driver
21/01/20 11:24:24 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1172 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:24:24 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
21/01/20 11:24:24 INFO DAGScheduler: ResultStage 3 (parquet at Generate.java:61) finished in 1.197 s
21/01/20 11:24:24 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:24:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
21/01/20 11:24:24 INFO DAGScheduler: Job 3 finished: parquet at Generate.java:61, took 1.201668 s
21/01/20 11:24:24 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-67e2bb77-96b0-4f2d-b71f-0655fa9cb356
21/01/20 11:24:24 INFO FileFormatWriter: Write Job a5ae71ca-5501-46d2-b14b-cfb814523718 committed.
21/01/20 11:24:24 INFO FileFormatWriter: Finished processing stats for write job a5ae71ca-5501-46d2-b14b-cfb814523718.
21/01/20 11:24:26 INFO BlockManagerInfo: Removed broadcast_3_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:24:27 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:27 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:27 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:24:27 INFO DAGScheduler: Got job 4 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:24:27 INFO DAGScheduler: Final stage: ResultStage 4 (parquet at Generate.java:61)
21/01/20 11:24:27 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:24:27 INFO DAGScheduler: Missing parents: List()
21/01/20 11:24:27 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[17] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:24:27 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:24:27 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:24:27 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:24:27 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1200
21/01/20 11:24:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[17] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:24:27 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
21/01/20 11:24:27 WARN TaskSetManager: Stage 4 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:24:27 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:24:27 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
21/01/20 11:24:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:27 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:27 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:24:27 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:24:27 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:24:27 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:24:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:24:27 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:24:27 INFO ParquetOutputFormat: Validation is off
21/01/20 11:24:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:24:27 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:24:27 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:24:27 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:24:27 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:24:27 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:24:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:24:28 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-7E53BB0F3B43->8df234bd-f131-4d8e-953a-b9eec8633221
21/01/20 11:24:28 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:24:28 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112427_0004_m_000000_4' to o3fs://bucket1.vol1/testdata
21/01/20 11:24:28 INFO SparkHadoopMapRedUtil: attempt_20210120112427_0004_m_000000_4: Committed
21/01/20 11:24:28 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 2155 bytes result sent to driver
21/01/20 11:24:28 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 1057 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:24:28 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
21/01/20 11:24:28 INFO DAGScheduler: ResultStage 4 (parquet at Generate.java:61) finished in 1.086 s
21/01/20 11:24:28 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:24:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
21/01/20 11:24:28 INFO DAGScheduler: Job 4 finished: parquet at Generate.java:61, took 1.091782 s
21/01/20 11:24:28 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-a23c0552-bcbe-4dc5-a4fd-8dacb3f9ee88
21/01/20 11:24:28 INFO FileFormatWriter: Write Job d4d52d34-ebc5-485c-8f16-24f35a012be1 committed.
21/01/20 11:24:28 INFO FileFormatWriter: Finished processing stats for write job d4d52d34-ebc5-485c-8f16-24f35a012be1.
21/01/20 11:24:28 INFO BlockManagerInfo: Removed broadcast_4_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:24:31 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:31 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:24:31 INFO DAGScheduler: Got job 5 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:24:31 INFO DAGScheduler: Final stage: ResultStage 5 (parquet at Generate.java:61)
21/01/20 11:24:31 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:24:31 INFO DAGScheduler: Missing parents: List()
21/01/20 11:24:31 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[21] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:24:31 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:24:31 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:24:31 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:24:31 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1200
21/01/20 11:24:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[21] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:24:31 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
21/01/20 11:24:31 WARN TaskSetManager: Stage 5 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:24:31 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:24:31 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
21/01/20 11:24:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:31 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:24:31 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:24:31 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:24:31 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:24:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:24:31 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:24:31 INFO ParquetOutputFormat: Validation is off
21/01/20 11:24:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:24:31 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:24:31 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:24:31 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:24:31 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:24:31 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:24:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:24:32 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112431_0005_m_000000_5' to o3fs://bucket1.vol1/testdata
21/01/20 11:24:32 INFO SparkHadoopMapRedUtil: attempt_20210120112431_0005_m_000000_5: Committed
21/01/20 11:24:32 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2155 bytes result sent to driver
21/01/20 11:24:32 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 1170 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:24:32 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
21/01/20 11:24:32 INFO DAGScheduler: ResultStage 5 (parquet at Generate.java:61) finished in 1.194 s
21/01/20 11:24:32 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:24:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
21/01/20 11:24:32 INFO DAGScheduler: Job 5 finished: parquet at Generate.java:61, took 1.197425 s
21/01/20 11:24:32 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-1ebbff20-7921-496d-a780-b074e6500154
21/01/20 11:24:32 INFO FileFormatWriter: Write Job 3d5f3177-d1f6-48d9-93a4-0b4bc7c076f9 committed.
21/01/20 11:24:32 INFO FileFormatWriter: Finished processing stats for write job 3d5f3177-d1f6-48d9-93a4-0b4bc7c076f9.
21/01/20 11:24:33 INFO BlockManagerInfo: Removed broadcast_5_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:24:35 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:35 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:35 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:24:35 INFO DAGScheduler: Got job 6 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:24:35 INFO DAGScheduler: Final stage: ResultStage 6 (parquet at Generate.java:61)
21/01/20 11:24:35 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:24:35 INFO DAGScheduler: Missing parents: List()
21/01/20 11:24:35 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[25] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:24:35 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:24:35 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:24:35 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:24:35 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1200
21/01/20 11:24:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[25] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:24:35 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
21/01/20 11:24:35 WARN TaskSetManager: Stage 6 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:24:35 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:24:35 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
21/01/20 11:24:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:35 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:35 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:24:35 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:24:35 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:24:35 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:24:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:24:35 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:24:35 INFO ParquetOutputFormat: Validation is off
21/01/20 11:24:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:24:35 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:24:35 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:24:35 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:24:35 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:24:35 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:24:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:24:36 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112435_0006_m_000000_6' to o3fs://bucket1.vol1/testdata
21/01/20 11:24:36 INFO SparkHadoopMapRedUtil: attempt_20210120112435_0006_m_000000_6: Committed
21/01/20 11:24:36 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 2155 bytes result sent to driver
21/01/20 11:24:36 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 1089 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:24:36 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
21/01/20 11:24:36 INFO DAGScheduler: ResultStage 6 (parquet at Generate.java:61) finished in 1.115 s
21/01/20 11:24:36 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:24:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
21/01/20 11:24:36 INFO DAGScheduler: Job 6 finished: parquet at Generate.java:61, took 1.118849 s
21/01/20 11:24:36 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-89ecd3f4-7633-4956-aa09-f875226dd359
21/01/20 11:24:36 INFO FileFormatWriter: Write Job da0d0574-62bf-47aa-9bcc-818feef1fa36 committed.
21/01/20 11:24:36 INFO FileFormatWriter: Finished processing stats for write job da0d0574-62bf-47aa-9bcc-818feef1fa36.
21/01/20 11:24:37 INFO BlockManagerInfo: Removed broadcast_6_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:24:38 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:38 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:38 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:24:38 INFO DAGScheduler: Got job 7 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:24:38 INFO DAGScheduler: Final stage: ResultStage 7 (parquet at Generate.java:61)
21/01/20 11:24:38 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:24:38 INFO DAGScheduler: Missing parents: List()
21/01/20 11:24:38 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[29] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:24:38 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:24:38 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:24:38 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:24:38 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1200
21/01/20 11:24:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[29] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:24:38 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks
21/01/20 11:24:38 WARN TaskSetManager: Stage 7 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:24:38 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:24:38 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
21/01/20 11:24:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:38 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:38 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:24:38 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:24:38 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:24:38 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:24:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:24:38 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:24:38 INFO ParquetOutputFormat: Validation is off
21/01/20 11:24:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:24:38 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:24:38 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:24:38 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:24:38 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:24:38 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:24:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:24:42 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112438_0007_m_000000_7' to o3fs://bucket1.vol1/testdata
21/01/20 11:24:42 INFO SparkHadoopMapRedUtil: attempt_20210120112438_0007_m_000000_7: Committed
21/01/20 11:24:42 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 2155 bytes result sent to driver
21/01/20 11:24:42 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 3574 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:24:42 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
21/01/20 11:24:42 INFO DAGScheduler: ResultStage 7 (parquet at Generate.java:61) finished in 3.597 s
21/01/20 11:24:42 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:24:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
21/01/20 11:24:42 INFO DAGScheduler: Job 7 finished: parquet at Generate.java:61, took 3.600916 s
21/01/20 11:24:42 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-d50f7625-70b6-4f87-a22a-52ec875e590f
21/01/20 11:24:42 INFO FileFormatWriter: Write Job 14681362-9d5a-444d-820f-c2bba2c6943a committed.
21/01/20 11:24:42 INFO FileFormatWriter: Finished processing stats for write job 14681362-9d5a-444d-820f-c2bba2c6943a.
21/01/20 11:24:43 INFO BlockManagerInfo: Removed broadcast_7_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:24:45 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:45 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:45 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:24:45 INFO DAGScheduler: Got job 8 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:24:45 INFO DAGScheduler: Final stage: ResultStage 8 (parquet at Generate.java:61)
21/01/20 11:24:45 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:24:45 INFO DAGScheduler: Missing parents: List()
21/01/20 11:24:45 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[33] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:24:45 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:24:45 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:24:45 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:24:45 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1200
21/01/20 11:24:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[33] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:24:45 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks
21/01/20 11:24:45 WARN TaskSetManager: Stage 8 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:24:45 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:24:45 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
21/01/20 11:24:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:45 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:45 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:24:45 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:24:45 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:24:45 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:24:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:24:45 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:24:45 INFO ParquetOutputFormat: Validation is off
21/01/20 11:24:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:24:45 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:24:45 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:24:45 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:24:45 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:24:45 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:24:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:24:46 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112445_0008_m_000000_8' to o3fs://bucket1.vol1/testdata
21/01/20 11:24:46 INFO SparkHadoopMapRedUtil: attempt_20210120112445_0008_m_000000_8: Committed
21/01/20 11:24:46 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2155 bytes result sent to driver
21/01/20 11:24:46 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 1090 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:24:46 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
21/01/20 11:24:46 INFO DAGScheduler: ResultStage 8 (parquet at Generate.java:61) finished in 1.114 s
21/01/20 11:24:46 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:24:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
21/01/20 11:24:46 INFO DAGScheduler: Job 8 finished: parquet at Generate.java:61, took 1.117156 s
21/01/20 11:24:46 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-d5956606-6790-44ca-a01d-18f5f374339c
21/01/20 11:24:46 INFO FileFormatWriter: Write Job 07eda0c3-c1b2-4c91-b0eb-8dcca04d02ee committed.
21/01/20 11:24:46 INFO FileFormatWriter: Finished processing stats for write job 07eda0c3-c1b2-4c91-b0eb-8dcca04d02ee.
21/01/20 11:24:47 INFO BlockManagerInfo: Removed broadcast_8_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:24:48 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:48 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:48 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:24:48 INFO DAGScheduler: Got job 9 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:24:48 INFO DAGScheduler: Final stage: ResultStage 9 (parquet at Generate.java:61)
21/01/20 11:24:48 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:24:48 INFO DAGScheduler: Missing parents: List()
21/01/20 11:24:48 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[37] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:24:48 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:24:48 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:24:48 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:24:48 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1200
21/01/20 11:24:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[37] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:24:48 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
21/01/20 11:24:48 WARN TaskSetManager: Stage 9 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:24:48 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:24:48 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)
21/01/20 11:24:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:48 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:48 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:24:48 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:24:48 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:24:48 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:24:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:24:48 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:24:48 INFO ParquetOutputFormat: Validation is off
21/01/20 11:24:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:24:48 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:24:48 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:24:48 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:24:48 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:24:49 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:24:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:24:49 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-2F321875FF5D->27b66227-fed8-4c1d-ab57-510f5a3ce552
21/01/20 11:24:49 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:24:50 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112448_0009_m_000000_9' to o3fs://bucket1.vol1/testdata
21/01/20 11:24:50 INFO SparkHadoopMapRedUtil: attempt_20210120112448_0009_m_000000_9: Committed
21/01/20 11:24:50 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 2155 bytes result sent to driver
21/01/20 11:24:50 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 1188 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:24:50 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
21/01/20 11:24:50 INFO DAGScheduler: ResultStage 9 (parquet at Generate.java:61) finished in 1.212 s
21/01/20 11:24:50 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:24:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
21/01/20 11:24:50 INFO DAGScheduler: Job 9 finished: parquet at Generate.java:61, took 1.215789 s
21/01/20 11:24:50 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-237f1e03-9ad8-4b61-ad84-03c87763d4d7
21/01/20 11:24:50 INFO FileFormatWriter: Write Job c931b019-5689-468a-bbcf-f91ae025631b committed.
21/01/20 11:24:50 INFO FileFormatWriter: Finished processing stats for write job c931b019-5689-468a-bbcf-f91ae025631b.
21/01/20 11:24:51 INFO BlockManagerInfo: Removed broadcast_9_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:24:52 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:52 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:52 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:24:52 INFO DAGScheduler: Got job 10 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:24:52 INFO DAGScheduler: Final stage: ResultStage 10 (parquet at Generate.java:61)
21/01/20 11:24:52 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:24:52 INFO DAGScheduler: Missing parents: List()
21/01/20 11:24:52 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[41] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:24:52 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:24:52 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:24:52 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:24:52 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1200
21/01/20 11:24:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[41] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:24:52 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks
21/01/20 11:24:52 WARN TaskSetManager: Stage 10 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:24:52 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:24:52 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)
21/01/20 11:24:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:52 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:52 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:24:52 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:24:52 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:24:52 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:24:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:24:52 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:24:52 INFO ParquetOutputFormat: Validation is off
21/01/20 11:24:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:24:52 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:24:52 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:24:52 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:24:52 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:24:52 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:24:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:24:53 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112452_0010_m_000000_10' to o3fs://bucket1.vol1/testdata
21/01/20 11:24:53 INFO SparkHadoopMapRedUtil: attempt_20210120112452_0010_m_000000_10: Committed
21/01/20 11:24:53 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 2155 bytes result sent to driver
21/01/20 11:24:53 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 1073 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:24:53 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
21/01/20 11:24:53 INFO DAGScheduler: ResultStage 10 (parquet at Generate.java:61) finished in 1.096 s
21/01/20 11:24:53 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:24:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
21/01/20 11:24:53 INFO DAGScheduler: Job 10 finished: parquet at Generate.java:61, took 1.099972 s
21/01/20 11:24:53 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-2837e395-ef1d-4199-8de3-fe1f1576523f
21/01/20 11:24:53 INFO FileFormatWriter: Write Job fdb77728-cb39-4c17-8331-2927933273f0 committed.
21/01/20 11:24:53 INFO FileFormatWriter: Finished processing stats for write job fdb77728-cb39-4c17-8331-2927933273f0.
21/01/20 11:24:54 INFO BlockManagerInfo: Removed broadcast_10_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:24:56 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:56 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:56 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:24:56 INFO DAGScheduler: Got job 11 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:24:56 INFO DAGScheduler: Final stage: ResultStage 11 (parquet at Generate.java:61)
21/01/20 11:24:56 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:24:56 INFO DAGScheduler: Missing parents: List()
21/01/20 11:24:56 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[45] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:24:56 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:24:56 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:24:56 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:24:56 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1200
21/01/20 11:24:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[45] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:24:56 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks
21/01/20 11:24:56 WARN TaskSetManager: Stage 11 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:24:56 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:24:56 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)
21/01/20 11:24:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:56 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:24:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:24:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:24:56 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:24:56 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:24:56 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:24:56 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:24:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:24:56 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:24:56 INFO ParquetOutputFormat: Validation is off
21/01/20 11:24:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:24:56 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:24:56 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:24:56 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:24:56 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:24:56 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:24:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:24:57 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112456_0011_m_000000_11' to o3fs://bucket1.vol1/testdata
21/01/20 11:24:57 INFO SparkHadoopMapRedUtil: attempt_20210120112456_0011_m_000000_11: Committed
21/01/20 11:24:57 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 2155 bytes result sent to driver
21/01/20 11:24:57 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 1095 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:24:57 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
21/01/20 11:24:57 INFO DAGScheduler: ResultStage 11 (parquet at Generate.java:61) finished in 1.117 s
21/01/20 11:24:57 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:24:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
21/01/20 11:24:57 INFO DAGScheduler: Job 11 finished: parquet at Generate.java:61, took 1.119499 s
21/01/20 11:24:57 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-da40077c-ca7b-4f55-979b-7652aaf715e9
21/01/20 11:24:57 INFO FileFormatWriter: Write Job d639b765-4454-42fe-9ed8-5af061463682 committed.
21/01/20 11:24:57 INFO FileFormatWriter: Finished processing stats for write job d639b765-4454-42fe-9ed8-5af061463682.
21/01/20 11:24:58 INFO BlockManagerInfo: Removed broadcast_11_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:25:00 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:00 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:25:00 INFO DAGScheduler: Got job 12 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:25:00 INFO DAGScheduler: Final stage: ResultStage 12 (parquet at Generate.java:61)
21/01/20 11:25:00 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:25:00 INFO DAGScheduler: Missing parents: List()
21/01/20 11:25:00 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[49] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:25:00 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:25:00 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:25:00 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:25:00 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1200
21/01/20 11:25:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[49] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:25:00 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks
21/01/20 11:25:00 WARN TaskSetManager: Stage 12 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:25:00 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:25:00 INFO Executor: Running task 0.0 in stage 12.0 (TID 12)
21/01/20 11:25:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:00 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:25:00 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:25:00 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:25:00 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:25:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:25:00 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:25:00 INFO ParquetOutputFormat: Validation is off
21/01/20 11:25:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:25:00 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:25:00 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:25:00 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:25:00 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:25:00 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:25:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:25:01 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112500_0012_m_000000_12' to o3fs://bucket1.vol1/testdata
21/01/20 11:25:01 INFO SparkHadoopMapRedUtil: attempt_20210120112500_0012_m_000000_12: Committed
21/01/20 11:25:01 INFO Executor: Finished task 0.0 in stage 12.0 (TID 12). 2155 bytes result sent to driver
21/01/20 11:25:01 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 797 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:25:01 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
21/01/20 11:25:01 INFO DAGScheduler: ResultStage 12 (parquet at Generate.java:61) finished in 0.841 s
21/01/20 11:25:01 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:25:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
21/01/20 11:25:01 INFO DAGScheduler: Job 12 finished: parquet at Generate.java:61, took 0.844300 s
21/01/20 11:25:01 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-465f5070-8c0b-4cf5-90fd-853ba4ed81a1
21/01/20 11:25:01 INFO FileFormatWriter: Write Job 6c2a7325-c82d-4f11-ba1b-9aae492eee6e committed.
21/01/20 11:25:01 INFO FileFormatWriter: Finished processing stats for write job 6c2a7325-c82d-4f11-ba1b-9aae492eee6e.
21/01/20 11:25:01 INFO BlockManagerInfo: Removed broadcast_12_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:25:03 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:03 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:25:03 INFO DAGScheduler: Got job 13 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:25:03 INFO DAGScheduler: Final stage: ResultStage 13 (parquet at Generate.java:61)
21/01/20 11:25:03 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:25:03 INFO DAGScheduler: Missing parents: List()
21/01/20 11:25:03 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[53] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:25:03 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:25:03 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:25:03 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:25:03 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1200
21/01/20 11:25:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[53] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:25:03 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks
21/01/20 11:25:03 WARN TaskSetManager: Stage 13 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:25:03 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:25:03 INFO Executor: Running task 0.0 in stage 13.0 (TID 13)
21/01/20 11:25:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:03 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:25:03 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:25:03 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:25:03 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:25:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:25:03 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:25:03 INFO ParquetOutputFormat: Validation is off
21/01/20 11:25:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:25:03 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:25:03 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:25:03 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:25:03 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:25:03 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:25:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:25:04 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112503_0013_m_000000_13' to o3fs://bucket1.vol1/testdata
21/01/20 11:25:04 INFO SparkHadoopMapRedUtil: attempt_20210120112503_0013_m_000000_13: Committed
21/01/20 11:25:04 INFO Executor: Finished task 0.0 in stage 13.0 (TID 13). 2155 bytes result sent to driver
21/01/20 11:25:04 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 1069 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:25:04 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
21/01/20 11:25:04 INFO DAGScheduler: ResultStage 13 (parquet at Generate.java:61) finished in 1.092 s
21/01/20 11:25:04 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:25:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
21/01/20 11:25:04 INFO DAGScheduler: Job 13 finished: parquet at Generate.java:61, took 1.094071 s
21/01/20 11:25:04 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-d89892c1-efc9-4548-86d7-8bd41a5cc42f
21/01/20 11:25:04 INFO FileFormatWriter: Write Job e94a18eb-355e-48ff-afe0-55e80a42d34a committed.
21/01/20 11:25:04 INFO FileFormatWriter: Finished processing stats for write job e94a18eb-355e-48ff-afe0-55e80a42d34a.
21/01/20 11:25:05 INFO BlockManagerInfo: Removed broadcast_13_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:25:07 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:07 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:07 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:25:07 INFO DAGScheduler: Got job 14 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:25:07 INFO DAGScheduler: Final stage: ResultStage 14 (parquet at Generate.java:61)
21/01/20 11:25:07 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:25:07 INFO DAGScheduler: Missing parents: List()
21/01/20 11:25:07 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[57] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:25:07 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:25:07 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:25:07 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:25:07 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1200
21/01/20 11:25:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[57] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:25:07 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks
21/01/20 11:25:07 WARN TaskSetManager: Stage 14 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:25:07 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:25:07 INFO Executor: Running task 0.0 in stage 14.0 (TID 14)
21/01/20 11:25:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:07 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:07 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:25:07 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:25:07 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:25:07 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:25:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:25:07 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:25:07 INFO ParquetOutputFormat: Validation is off
21/01/20 11:25:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:25:07 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:25:07 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:25:07 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:25:07 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:25:07 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:25:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:25:08 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112507_0014_m_000000_14' to o3fs://bucket1.vol1/testdata
21/01/20 11:25:08 INFO SparkHadoopMapRedUtil: attempt_20210120112507_0014_m_000000_14: Committed
21/01/20 11:25:08 INFO Executor: Finished task 0.0 in stage 14.0 (TID 14). 2155 bytes result sent to driver
21/01/20 11:25:08 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 902 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:25:08 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
21/01/20 11:25:08 INFO DAGScheduler: ResultStage 14 (parquet at Generate.java:61) finished in 0.924 s
21/01/20 11:25:08 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:25:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
21/01/20 11:25:08 INFO DAGScheduler: Job 14 finished: parquet at Generate.java:61, took 0.926362 s
21/01/20 11:25:08 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-8a47e03c-4a7c-49d3-8341-90685e68c838
21/01/20 11:25:08 INFO FileFormatWriter: Write Job 27ebdf01-e82d-4979-a0fa-b4b8e4fd2cae committed.
21/01/20 11:25:08 INFO FileFormatWriter: Finished processing stats for write job 27ebdf01-e82d-4979-a0fa-b4b8e4fd2cae.
21/01/20 11:25:08 INFO BlockManagerInfo: Removed broadcast_14_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:25:10 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:10 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:10 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:25:10 INFO DAGScheduler: Got job 15 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:25:10 INFO DAGScheduler: Final stage: ResultStage 15 (parquet at Generate.java:61)
21/01/20 11:25:10 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:25:10 INFO DAGScheduler: Missing parents: List()
21/01/20 11:25:10 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[61] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:25:10 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:25:10 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:25:10 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:25:10 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1200
21/01/20 11:25:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[61] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:25:10 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks
21/01/20 11:25:11 WARN TaskSetManager: Stage 15 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:25:11 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 15, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:25:11 INFO Executor: Running task 0.0 in stage 15.0 (TID 15)
21/01/20 11:25:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:11 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:25:11 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:25:11 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:25:11 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:25:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:25:11 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:25:11 INFO ParquetOutputFormat: Validation is off
21/01/20 11:25:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:25:11 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:25:11 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:25:11 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:25:11 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:25:11 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:25:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:25:12 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112510_0015_m_000000_15' to o3fs://bucket1.vol1/testdata
21/01/20 11:25:12 INFO SparkHadoopMapRedUtil: attempt_20210120112510_0015_m_000000_15: Committed
21/01/20 11:25:12 INFO Executor: Finished task 0.0 in stage 15.0 (TID 15). 2155 bytes result sent to driver
21/01/20 11:25:12 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 15) in 1149 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:25:12 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
21/01/20 11:25:12 INFO DAGScheduler: ResultStage 15 (parquet at Generate.java:61) finished in 1.171 s
21/01/20 11:25:12 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:25:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
21/01/20 11:25:12 INFO DAGScheduler: Job 15 finished: parquet at Generate.java:61, took 1.173619 s
21/01/20 11:25:12 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-105f318a-1f23-4a98-a8d9-5287c91bb292
21/01/20 11:25:12 INFO FileFormatWriter: Write Job 297d5000-43b2-4048-998b-66fd7ee1bd96 committed.
21/01/20 11:25:12 INFO FileFormatWriter: Finished processing stats for write job 297d5000-43b2-4048-998b-66fd7ee1bd96.
21/01/20 11:25:13 INFO BlockManagerInfo: Removed broadcast_15_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:25:14 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:14 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:14 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:25:14 INFO DAGScheduler: Got job 16 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:25:14 INFO DAGScheduler: Final stage: ResultStage 16 (parquet at Generate.java:61)
21/01/20 11:25:14 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:25:14 INFO DAGScheduler: Missing parents: List()
21/01/20 11:25:14 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[65] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:25:14 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:25:14 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:25:14 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:25:14 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1200
21/01/20 11:25:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[65] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:25:14 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks
21/01/20 11:25:14 WARN TaskSetManager: Stage 16 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:25:14 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 16, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:25:14 INFO Executor: Running task 0.0 in stage 16.0 (TID 16)
21/01/20 11:25:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:14 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:14 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:25:14 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:25:14 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:25:14 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:25:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:25:14 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:25:14 INFO ParquetOutputFormat: Validation is off
21/01/20 11:25:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:25:14 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:25:14 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:25:14 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:25:14 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:25:14 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:25:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:25:15 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112514_0016_m_000000_16' to o3fs://bucket1.vol1/testdata
21/01/20 11:25:15 INFO SparkHadoopMapRedUtil: attempt_20210120112514_0016_m_000000_16: Committed
21/01/20 11:25:15 INFO Executor: Finished task 0.0 in stage 16.0 (TID 16). 2155 bytes result sent to driver
21/01/20 11:25:15 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 16) in 1065 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:25:15 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
21/01/20 11:25:15 INFO DAGScheduler: ResultStage 16 (parquet at Generate.java:61) finished in 1.087 s
21/01/20 11:25:15 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:25:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished
21/01/20 11:25:15 INFO DAGScheduler: Job 16 finished: parquet at Generate.java:61, took 1.088939 s
21/01/20 11:25:15 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-5ec5e90e-e668-4b73-871c-7eeba22fb257
21/01/20 11:25:15 INFO FileFormatWriter: Write Job af7506fa-edae-4cfc-a18f-ecbcc150b544 committed.
21/01/20 11:25:15 INFO FileFormatWriter: Finished processing stats for write job af7506fa-edae-4cfc-a18f-ecbcc150b544.
21/01/20 11:25:16 INFO BlockManagerInfo: Removed broadcast_16_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:25:18 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:18 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:18 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:25:18 INFO DAGScheduler: Got job 17 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:25:18 INFO DAGScheduler: Final stage: ResultStage 17 (parquet at Generate.java:61)
21/01/20 11:25:18 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:25:18 INFO DAGScheduler: Missing parents: List()
21/01/20 11:25:18 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[69] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:25:18 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:25:18 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:25:18 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:25:18 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1200
21/01/20 11:25:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[69] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:25:18 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks
21/01/20 11:25:18 WARN TaskSetManager: Stage 17 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:25:18 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 17, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:25:18 INFO Executor: Running task 0.0 in stage 17.0 (TID 17)
21/01/20 11:25:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:18 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:18 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:25:18 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:25:18 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:25:18 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:25:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:25:18 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:25:18 INFO ParquetOutputFormat: Validation is off
21/01/20 11:25:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:25:18 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:25:18 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:25:18 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:25:18 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:25:18 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:25:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:25:19 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112518_0017_m_000000_17' to o3fs://bucket1.vol1/testdata
21/01/20 11:25:19 INFO SparkHadoopMapRedUtil: attempt_20210120112518_0017_m_000000_17: Committed
21/01/20 11:25:19 INFO Executor: Finished task 0.0 in stage 17.0 (TID 17). 2155 bytes result sent to driver
21/01/20 11:25:19 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 17) in 1041 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:25:19 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
21/01/20 11:25:19 INFO DAGScheduler: ResultStage 17 (parquet at Generate.java:61) finished in 1.063 s
21/01/20 11:25:19 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:25:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
21/01/20 11:25:19 INFO DAGScheduler: Job 17 finished: parquet at Generate.java:61, took 1.065825 s
21/01/20 11:25:19 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-43d13cf3-ee16-40ac-92fc-228dfa63e5c2
21/01/20 11:25:19 INFO FileFormatWriter: Write Job 89c141f9-8919-4392-bd88-65263d0a9028 committed.
21/01/20 11:25:19 INFO FileFormatWriter: Finished processing stats for write job 89c141f9-8919-4392-bd88-65263d0a9028.
21/01/20 11:25:20 INFO BlockManagerInfo: Removed broadcast_17_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:25:22 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:22 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:22 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:25:22 INFO DAGScheduler: Got job 18 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:25:22 INFO DAGScheduler: Final stage: ResultStage 18 (parquet at Generate.java:61)
21/01/20 11:25:22 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:25:22 INFO DAGScheduler: Missing parents: List()
21/01/20 11:25:22 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[73] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:25:22 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:25:22 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:25:22 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:25:22 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1200
21/01/20 11:25:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[73] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:25:22 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks
21/01/20 11:25:22 WARN TaskSetManager: Stage 18 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:25:22 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 18, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:25:22 INFO Executor: Running task 0.0 in stage 18.0 (TID 18)
21/01/20 11:25:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:22 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:22 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:25:22 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:25:22 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:25:22 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:25:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:25:22 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:25:22 INFO ParquetOutputFormat: Validation is off
21/01/20 11:25:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:25:22 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:25:22 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:25:22 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:25:22 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:25:22 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:25:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:25:22 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-ADEE4BF4AC03->27b66227-fed8-4c1d-ab57-510f5a3ce552
21/01/20 11:25:22 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:25:23 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112522_0018_m_000000_18' to o3fs://bucket1.vol1/testdata
21/01/20 11:25:23 INFO SparkHadoopMapRedUtil: attempt_20210120112522_0018_m_000000_18: Committed
21/01/20 11:25:23 INFO Executor: Finished task 0.0 in stage 18.0 (TID 18). 2155 bytes result sent to driver
21/01/20 11:25:23 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 18) in 1162 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:25:23 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
21/01/20 11:25:23 INFO DAGScheduler: ResultStage 18 (parquet at Generate.java:61) finished in 1.182 s
21/01/20 11:25:23 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:25:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
21/01/20 11:25:23 INFO DAGScheduler: Job 18 finished: parquet at Generate.java:61, took 1.185213 s
21/01/20 11:25:23 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-90b9197b-88c6-4f99-a3a6-4a569190cb19
21/01/20 11:25:23 INFO FileFormatWriter: Write Job 8a28a9fe-1131-4997-a7b0-01614106e1ea committed.
21/01/20 11:25:23 INFO FileFormatWriter: Finished processing stats for write job 8a28a9fe-1131-4997-a7b0-01614106e1ea.
21/01/20 11:25:24 INFO BlockManagerInfo: Removed broadcast_18_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:25:25 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:25 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:25 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:25:25 INFO DAGScheduler: Got job 19 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:25:25 INFO DAGScheduler: Final stage: ResultStage 19 (parquet at Generate.java:61)
21/01/20 11:25:25 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:25:25 INFO DAGScheduler: Missing parents: List()
21/01/20 11:25:25 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[77] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:25:26 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:25:26 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:25:26 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:25:26 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1200
21/01/20 11:25:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[77] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:25:26 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks
21/01/20 11:25:26 WARN TaskSetManager: Stage 19 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:25:26 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 19, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:25:26 INFO Executor: Running task 0.0 in stage 19.0 (TID 19)
21/01/20 11:25:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:26 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:26 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:25:26 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:25:26 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:25:26 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:25:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:25:26 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:25:26 INFO ParquetOutputFormat: Validation is off
21/01/20 11:25:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:25:26 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:25:26 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:25:26 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:25:26 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:25:26 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:25:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:25:27 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112525_0019_m_000000_19' to o3fs://bucket1.vol1/testdata
21/01/20 11:25:27 INFO SparkHadoopMapRedUtil: attempt_20210120112525_0019_m_000000_19: Committed
21/01/20 11:25:27 INFO Executor: Finished task 0.0 in stage 19.0 (TID 19). 2155 bytes result sent to driver
21/01/20 11:25:27 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 19) in 1062 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:25:27 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
21/01/20 11:25:27 INFO DAGScheduler: ResultStage 19 (parquet at Generate.java:61) finished in 1.085 s
21/01/20 11:25:27 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:25:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished
21/01/20 11:25:27 INFO DAGScheduler: Job 19 finished: parquet at Generate.java:61, took 1.090114 s
21/01/20 11:25:27 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-1231a796-fd68-4a82-b6d5-b3fc7c31f68b
21/01/20 11:25:27 INFO FileFormatWriter: Write Job cb5bb267-53e5-4da8-bb97-db5768f31c3a committed.
21/01/20 11:25:27 INFO FileFormatWriter: Finished processing stats for write job cb5bb267-53e5-4da8-bb97-db5768f31c3a.
21/01/20 11:25:28 INFO BlockManagerInfo: Removed broadcast_19_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:25:29 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:29 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:25:29 INFO DAGScheduler: Got job 20 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:25:29 INFO DAGScheduler: Final stage: ResultStage 20 (parquet at Generate.java:61)
21/01/20 11:25:29 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:25:29 INFO DAGScheduler: Missing parents: List()
21/01/20 11:25:29 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[81] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:25:29 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:25:29 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:25:29 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:25:29 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1200
21/01/20 11:25:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[81] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:25:29 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks
21/01/20 11:25:29 WARN TaskSetManager: Stage 20 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:25:29 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 20, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:25:29 INFO Executor: Running task 0.0 in stage 20.0 (TID 20)
21/01/20 11:25:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:29 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:25:29 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:25:29 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:25:29 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:25:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:25:29 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:25:29 INFO ParquetOutputFormat: Validation is off
21/01/20 11:25:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:25:29 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:25:29 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:25:29 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:25:29 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:25:29 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:25:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:25:30 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112529_0020_m_000000_20' to o3fs://bucket1.vol1/testdata
21/01/20 11:25:30 INFO SparkHadoopMapRedUtil: attempt_20210120112529_0020_m_000000_20: Committed
21/01/20 11:25:30 INFO Executor: Finished task 0.0 in stage 20.0 (TID 20). 2155 bytes result sent to driver
21/01/20 11:25:30 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 20) in 994 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:25:30 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
21/01/20 11:25:30 INFO DAGScheduler: ResultStage 20 (parquet at Generate.java:61) finished in 1.017 s
21/01/20 11:25:30 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:25:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished
21/01/20 11:25:30 INFO DAGScheduler: Job 20 finished: parquet at Generate.java:61, took 1.019127 s
21/01/20 11:25:30 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-02a32608-db16-4d35-b258-b8206d574d58
21/01/20 11:25:30 INFO FileFormatWriter: Write Job aefaa282-6f7e-4d5e-a063-198b265b1430 committed.
21/01/20 11:25:30 INFO FileFormatWriter: Finished processing stats for write job aefaa282-6f7e-4d5e-a063-198b265b1430.
21/01/20 11:25:32 INFO BlockManagerInfo: Removed broadcast_20_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:25:33 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:33 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:33 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:25:33 INFO DAGScheduler: Got job 21 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:25:33 INFO DAGScheduler: Final stage: ResultStage 21 (parquet at Generate.java:61)
21/01/20 11:25:33 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:25:33 INFO DAGScheduler: Missing parents: List()
21/01/20 11:25:33 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[85] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:25:33 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:25:33 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:25:33 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:25:33 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1200
21/01/20 11:25:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[85] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:25:33 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks
21/01/20 11:25:33 WARN TaskSetManager: Stage 21 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:25:33 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 21, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:25:33 INFO Executor: Running task 0.0 in stage 21.0 (TID 21)
21/01/20 11:25:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:33 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:33 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:25:33 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:25:33 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:25:33 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:25:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:25:33 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:25:33 INFO ParquetOutputFormat: Validation is off
21/01/20 11:25:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:25:33 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:25:33 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:25:33 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:25:33 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:25:33 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:25:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:25:33 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-824055700B1F->27b66227-fed8-4c1d-ab57-510f5a3ce552
21/01/20 11:25:33 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:25:34 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112533_0021_m_000000_21' to o3fs://bucket1.vol1/testdata
21/01/20 11:25:34 INFO SparkHadoopMapRedUtil: attempt_20210120112533_0021_m_000000_21: Committed
21/01/20 11:25:34 INFO Executor: Finished task 0.0 in stage 21.0 (TID 21). 2155 bytes result sent to driver
21/01/20 11:25:34 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 21) in 949 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:25:34 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
21/01/20 11:25:34 INFO DAGScheduler: ResultStage 21 (parquet at Generate.java:61) finished in 0.992 s
21/01/20 11:25:34 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:25:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
21/01/20 11:25:34 INFO DAGScheduler: Job 21 finished: parquet at Generate.java:61, took 0.994540 s
21/01/20 11:25:34 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-9da81008-461c-4396-9c61-22af496fc200
21/01/20 11:25:34 INFO FileFormatWriter: Write Job 51815d50-362b-4af4-ad49-7fb52aca88e8 committed.
21/01/20 11:25:34 INFO FileFormatWriter: Finished processing stats for write job 51815d50-362b-4af4-ad49-7fb52aca88e8.
21/01/20 11:25:34 INFO BlockManagerInfo: Removed broadcast_21_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:25:36 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:36 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:36 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:25:36 INFO DAGScheduler: Got job 22 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:25:36 INFO DAGScheduler: Final stage: ResultStage 22 (parquet at Generate.java:61)
21/01/20 11:25:36 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:25:36 INFO DAGScheduler: Missing parents: List()
21/01/20 11:25:36 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[89] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:25:36 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:25:36 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:25:36 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:25:36 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1200
21/01/20 11:25:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[89] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:25:36 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks
21/01/20 11:25:37 WARN TaskSetManager: Stage 22 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:25:37 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 22, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:25:37 INFO Executor: Running task 0.0 in stage 22.0 (TID 22)
21/01/20 11:25:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:37 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:37 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:25:37 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:25:37 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:25:37 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:25:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:25:37 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:25:37 INFO ParquetOutputFormat: Validation is off
21/01/20 11:25:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:25:37 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:25:37 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:25:37 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:25:37 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:25:37 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:25:37 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:25:37 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112536_0022_m_000000_22' to o3fs://bucket1.vol1/testdata
21/01/20 11:25:37 INFO SparkHadoopMapRedUtil: attempt_20210120112536_0022_m_000000_22: Committed
21/01/20 11:25:37 INFO Executor: Finished task 0.0 in stage 22.0 (TID 22). 2155 bytes result sent to driver
21/01/20 11:25:37 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 22) in 996 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:25:37 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
21/01/20 11:25:37 INFO DAGScheduler: ResultStage 22 (parquet at Generate.java:61) finished in 1.016 s
21/01/20 11:25:37 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:25:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished
21/01/20 11:25:37 INFO DAGScheduler: Job 22 finished: parquet at Generate.java:61, took 1.018596 s
21/01/20 11:25:37 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-edfb2bf3-de9e-4ccd-b1a9-04837d03f6cf
21/01/20 11:25:37 INFO FileFormatWriter: Write Job 79df46cb-94fe-4b19-88a0-bb0feead637e committed.
21/01/20 11:25:37 INFO FileFormatWriter: Finished processing stats for write job 79df46cb-94fe-4b19-88a0-bb0feead637e.
21/01/20 11:25:39 INFO BlockManagerInfo: Removed broadcast_22_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:25:40 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:40 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:40 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:25:40 INFO DAGScheduler: Got job 23 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:25:40 INFO DAGScheduler: Final stage: ResultStage 23 (parquet at Generate.java:61)
21/01/20 11:25:40 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:25:40 INFO DAGScheduler: Missing parents: List()
21/01/20 11:25:40 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[93] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:25:40 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:25:40 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:25:40 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:25:40 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1200
21/01/20 11:25:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[93] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:25:40 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks
21/01/20 11:25:40 WARN TaskSetManager: Stage 23 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:25:40 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 23, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:25:40 INFO Executor: Running task 0.0 in stage 23.0 (TID 23)
21/01/20 11:25:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:40 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:40 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:25:40 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:25:40 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:25:40 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:25:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:25:40 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:25:40 INFO ParquetOutputFormat: Validation is off
21/01/20 11:25:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:25:40 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:25:40 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:25:40 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:25:40 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:25:40 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:25:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:25:41 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112540_0023_m_000000_23' to o3fs://bucket1.vol1/testdata
21/01/20 11:25:41 INFO SparkHadoopMapRedUtil: attempt_20210120112540_0023_m_000000_23: Committed
21/01/20 11:25:41 INFO Executor: Finished task 0.0 in stage 23.0 (TID 23). 2155 bytes result sent to driver
21/01/20 11:25:41 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 23) in 1059 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:25:41 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
21/01/20 11:25:41 INFO DAGScheduler: ResultStage 23 (parquet at Generate.java:61) finished in 1.079 s
21/01/20 11:25:41 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:25:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished
21/01/20 11:25:41 INFO DAGScheduler: Job 23 finished: parquet at Generate.java:61, took 1.081627 s
21/01/20 11:25:41 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-37d7f0da-c22e-48e1-8991-46681294910c
21/01/20 11:25:41 INFO FileFormatWriter: Write Job 26d629c4-9e1d-48a6-83d6-1a8f0a01ef67 committed.
21/01/20 11:25:41 INFO FileFormatWriter: Finished processing stats for write job 26d629c4-9e1d-48a6-83d6-1a8f0a01ef67.
21/01/20 11:25:42 INFO BlockManagerInfo: Removed broadcast_23_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:25:44 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:44 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:44 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:25:44 INFO DAGScheduler: Got job 24 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:25:44 INFO DAGScheduler: Final stage: ResultStage 24 (parquet at Generate.java:61)
21/01/20 11:25:44 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:25:44 INFO DAGScheduler: Missing parents: List()
21/01/20 11:25:44 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[97] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:25:44 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:25:44 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:25:44 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:25:44 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1200
21/01/20 11:25:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[97] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:25:44 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks
21/01/20 11:25:44 WARN TaskSetManager: Stage 24 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:25:44 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 24, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:25:44 INFO Executor: Running task 0.0 in stage 24.0 (TID 24)
21/01/20 11:25:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:44 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:44 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:25:44 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:25:44 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:25:44 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:25:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:25:44 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:25:44 INFO ParquetOutputFormat: Validation is off
21/01/20 11:25:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:25:44 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:25:44 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:25:44 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:25:44 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:25:44 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:25:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:25:45 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112544_0024_m_000000_24' to o3fs://bucket1.vol1/testdata
21/01/20 11:25:45 INFO SparkHadoopMapRedUtil: attempt_20210120112544_0024_m_000000_24: Committed
21/01/20 11:25:45 INFO Executor: Finished task 0.0 in stage 24.0 (TID 24). 2155 bytes result sent to driver
21/01/20 11:25:45 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 24) in 947 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:25:45 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
21/01/20 11:25:45 INFO DAGScheduler: ResultStage 24 (parquet at Generate.java:61) finished in 0.990 s
21/01/20 11:25:45 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:25:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished
21/01/20 11:25:45 INFO DAGScheduler: Job 24 finished: parquet at Generate.java:61, took 0.993243 s
21/01/20 11:25:45 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-db3eb512-8c72-4523-b9f1-82de94b1de7e
21/01/20 11:25:45 INFO FileFormatWriter: Write Job 3b4569e1-50ac-4602-9750-9abf01014957 committed.
21/01/20 11:25:45 INFO FileFormatWriter: Finished processing stats for write job 3b4569e1-50ac-4602-9750-9abf01014957.
21/01/20 11:25:45 INFO BlockManagerInfo: Removed broadcast_24_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:25:47 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:47 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:25:47 INFO DAGScheduler: Got job 25 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:25:47 INFO DAGScheduler: Final stage: ResultStage 25 (parquet at Generate.java:61)
21/01/20 11:25:47 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:25:47 INFO DAGScheduler: Missing parents: List()
21/01/20 11:25:47 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[101] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:25:47 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:25:47 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:25:47 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:25:47 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1200
21/01/20 11:25:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[101] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:25:47 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks
21/01/20 11:25:48 WARN TaskSetManager: Stage 25 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:25:48 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 25, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:25:48 INFO Executor: Running task 0.0 in stage 25.0 (TID 25)
21/01/20 11:25:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:48 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:48 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:25:48 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:25:48 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:25:48 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:25:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:25:48 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:25:48 INFO ParquetOutputFormat: Validation is off
21/01/20 11:25:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:25:48 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:25:48 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:25:48 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:25:48 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:25:48 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:25:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:25:49 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112547_0025_m_000000_25' to o3fs://bucket1.vol1/testdata
21/01/20 11:25:49 INFO SparkHadoopMapRedUtil: attempt_20210120112547_0025_m_000000_25: Committed
21/01/20 11:25:49 INFO Executor: Finished task 0.0 in stage 25.0 (TID 25). 2155 bytes result sent to driver
21/01/20 11:25:49 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 25) in 1096 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:25:49 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
21/01/20 11:25:49 INFO DAGScheduler: ResultStage 25 (parquet at Generate.java:61) finished in 1.119 s
21/01/20 11:25:49 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:25:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished
21/01/20 11:25:49 INFO DAGScheduler: Job 25 finished: parquet at Generate.java:61, took 1.120749 s
21/01/20 11:25:49 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-6a5bccb8-cce4-46a1-887c-3bc3ddd69339
21/01/20 11:25:49 INFO FileFormatWriter: Write Job 29024f9d-2627-4ab4-b1a0-9da0c791136d committed.
21/01/20 11:25:49 INFO FileFormatWriter: Finished processing stats for write job 29024f9d-2627-4ab4-b1a0-9da0c791136d.
21/01/20 11:25:50 INFO BlockManagerInfo: Removed broadcast_25_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:25:51 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:51 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:25:51 INFO DAGScheduler: Got job 26 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:25:51 INFO DAGScheduler: Final stage: ResultStage 26 (parquet at Generate.java:61)
21/01/20 11:25:51 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:25:51 INFO DAGScheduler: Missing parents: List()
21/01/20 11:25:51 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[105] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:25:51 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:25:51 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:25:51 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:25:51 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1200
21/01/20 11:25:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[105] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:25:51 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks
21/01/20 11:25:51 WARN TaskSetManager: Stage 26 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:25:51 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 26, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:25:51 INFO Executor: Running task 0.0 in stage 26.0 (TID 26)
21/01/20 11:25:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:51 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:25:51 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:25:51 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:25:51 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:25:51 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:25:51 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:25:51 INFO ParquetOutputFormat: Validation is off
21/01/20 11:25:51 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:25:51 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:25:51 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:25:51 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:25:51 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:25:51 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:25:51 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:25:52 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112551_0026_m_000000_26' to o3fs://bucket1.vol1/testdata
21/01/20 11:25:52 INFO SparkHadoopMapRedUtil: attempt_20210120112551_0026_m_000000_26: Committed
21/01/20 11:25:52 INFO Executor: Finished task 0.0 in stage 26.0 (TID 26). 2155 bytes result sent to driver
21/01/20 11:25:52 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 26) in 1082 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:25:52 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
21/01/20 11:25:52 INFO DAGScheduler: ResultStage 26 (parquet at Generate.java:61) finished in 1.103 s
21/01/20 11:25:52 INFO DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:25:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished
21/01/20 11:25:52 INFO DAGScheduler: Job 26 finished: parquet at Generate.java:61, took 1.105585 s
21/01/20 11:25:52 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-6dafa54f-3a34-41e8-a975-e8a56efb6a20
21/01/20 11:25:52 INFO FileFormatWriter: Write Job abc9c4a9-9bd8-4bb4-90d2-d823d1bd4a73 committed.
21/01/20 11:25:52 INFO FileFormatWriter: Finished processing stats for write job abc9c4a9-9bd8-4bb4-90d2-d823d1bd4a73.
21/01/20 11:25:53 INFO BlockManagerInfo: Removed broadcast_26_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:25:55 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:55 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:25:55 INFO DAGScheduler: Got job 27 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:25:55 INFO DAGScheduler: Final stage: ResultStage 27 (parquet at Generate.java:61)
21/01/20 11:25:55 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:25:55 INFO DAGScheduler: Missing parents: List()
21/01/20 11:25:55 INFO DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[109] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:25:55 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:25:55 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:25:55 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:25:55 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1200
21/01/20 11:25:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[109] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:25:55 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks
21/01/20 11:25:55 WARN TaskSetManager: Stage 27 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:25:55 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 27, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:25:55 INFO Executor: Running task 0.0 in stage 27.0 (TID 27)
21/01/20 11:25:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:55 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:25:55 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:25:55 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:25:55 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:25:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:25:55 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:25:55 INFO ParquetOutputFormat: Validation is off
21/01/20 11:25:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:25:55 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:25:55 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:25:55 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:25:55 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:25:55 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:25:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:25:56 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-E35237348365->8df234bd-f131-4d8e-953a-b9eec8633221
21/01/20 11:25:56 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:25:56 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112555_0027_m_000000_27' to o3fs://bucket1.vol1/testdata
21/01/20 11:25:56 INFO SparkHadoopMapRedUtil: attempt_20210120112555_0027_m_000000_27: Committed
21/01/20 11:25:56 INFO Executor: Finished task 0.0 in stage 27.0 (TID 27). 2155 bytes result sent to driver
21/01/20 11:25:56 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 27) in 1059 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:25:56 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool 
21/01/20 11:25:56 INFO DAGScheduler: ResultStage 27 (parquet at Generate.java:61) finished in 1.079 s
21/01/20 11:25:56 INFO DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:25:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage finished
21/01/20 11:25:56 INFO DAGScheduler: Job 27 finished: parquet at Generate.java:61, took 1.081523 s
21/01/20 11:25:56 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-6f757332-d36d-4f5f-9275-05198f1ec8d7
21/01/20 11:25:56 INFO FileFormatWriter: Write Job 49bd6007-4504-43ef-a5e9-c3ce8b96d777 committed.
21/01/20 11:25:56 INFO FileFormatWriter: Finished processing stats for write job 49bd6007-4504-43ef-a5e9-c3ce8b96d777.
21/01/20 11:25:57 INFO BlockManagerInfo: Removed broadcast_27_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:25:59 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:59 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:59 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:25:59 INFO DAGScheduler: Got job 28 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:25:59 INFO DAGScheduler: Final stage: ResultStage 28 (parquet at Generate.java:61)
21/01/20 11:25:59 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:25:59 INFO DAGScheduler: Missing parents: List()
21/01/20 11:25:59 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[113] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:25:59 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:25:59 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:25:59 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:25:59 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1200
21/01/20 11:25:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[113] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:25:59 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks
21/01/20 11:25:59 WARN TaskSetManager: Stage 28 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:25:59 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 28, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:25:59 INFO Executor: Running task 0.0 in stage 28.0 (TID 28)
21/01/20 11:25:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:59 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:25:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:25:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:25:59 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:25:59 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:25:59 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:25:59 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:25:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:25:59 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:25:59 INFO ParquetOutputFormat: Validation is off
21/01/20 11:25:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:25:59 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:25:59 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:25:59 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:25:59 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:25:59 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:25:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:26:00 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112559_0028_m_000000_28' to o3fs://bucket1.vol1/testdata
21/01/20 11:26:00 INFO SparkHadoopMapRedUtil: attempt_20210120112559_0028_m_000000_28: Committed
21/01/20 11:26:00 INFO Executor: Finished task 0.0 in stage 28.0 (TID 28). 2155 bytes result sent to driver
21/01/20 11:26:00 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 28) in 1080 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:26:00 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool 
21/01/20 11:26:00 INFO DAGScheduler: ResultStage 28 (parquet at Generate.java:61) finished in 1.101 s
21/01/20 11:26:00 INFO DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:26:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage finished
21/01/20 11:26:00 INFO DAGScheduler: Job 28 finished: parquet at Generate.java:61, took 1.103529 s
21/01/20 11:26:00 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-e6fecbeb-676c-4b0a-874d-85066c9ecde8
21/01/20 11:26:00 INFO FileFormatWriter: Write Job 6fd1eb73-5dc1-4e06-9217-a7ccdcb00b21 committed.
21/01/20 11:26:00 INFO FileFormatWriter: Finished processing stats for write job 6fd1eb73-5dc1-4e06-9217-a7ccdcb00b21.
21/01/20 11:26:01 INFO BlockManagerInfo: Removed broadcast_28_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:26:02 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:02 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:26:02 INFO DAGScheduler: Got job 29 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:26:02 INFO DAGScheduler: Final stage: ResultStage 29 (parquet at Generate.java:61)
21/01/20 11:26:02 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:26:02 INFO DAGScheduler: Missing parents: List()
21/01/20 11:26:02 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[117] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:26:02 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:26:02 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:26:02 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:26:02 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1200
21/01/20 11:26:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[117] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:26:02 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks
21/01/20 11:26:02 WARN TaskSetManager: Stage 29 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:26:02 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 29, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:26:02 INFO Executor: Running task 0.0 in stage 29.0 (TID 29)
21/01/20 11:26:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:02 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:26:02 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:26:02 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:26:02 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:26:02 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:26:02 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:26:02 INFO ParquetOutputFormat: Validation is off
21/01/20 11:26:02 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:26:02 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:26:02 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:26:02 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:26:02 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:26:02 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:26:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:26:03 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112602_0029_m_000000_29' to o3fs://bucket1.vol1/testdata
21/01/20 11:26:03 INFO SparkHadoopMapRedUtil: attempt_20210120112602_0029_m_000000_29: Committed
21/01/20 11:26:03 INFO Executor: Finished task 0.0 in stage 29.0 (TID 29). 2155 bytes result sent to driver
21/01/20 11:26:03 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 29) in 1030 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:26:03 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool 
21/01/20 11:26:03 INFO DAGScheduler: ResultStage 29 (parquet at Generate.java:61) finished in 1.049 s
21/01/20 11:26:03 INFO DAGScheduler: Job 29 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:26:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 29: Stage finished
21/01/20 11:26:03 INFO DAGScheduler: Job 29 finished: parquet at Generate.java:61, took 1.051918 s
21/01/20 11:26:03 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-97ff5348-cb0e-4604-8c2c-de70205752ca
21/01/20 11:26:03 INFO FileFormatWriter: Write Job 21bac726-d09f-4577-9c2c-f69cc6822f60 committed.
21/01/20 11:26:03 INFO FileFormatWriter: Finished processing stats for write job 21bac726-d09f-4577-9c2c-f69cc6822f60.
21/01/20 11:26:04 INFO BlockManagerInfo: Removed broadcast_29_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:26:06 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:06 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:06 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:26:06 INFO DAGScheduler: Got job 30 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:26:06 INFO DAGScheduler: Final stage: ResultStage 30 (parquet at Generate.java:61)
21/01/20 11:26:06 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:26:06 INFO DAGScheduler: Missing parents: List()
21/01/20 11:26:06 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[121] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:26:06 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:26:06 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:26:06 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:26:06 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1200
21/01/20 11:26:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[121] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:26:06 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks
21/01/20 11:26:06 WARN TaskSetManager: Stage 30 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:26:06 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 30, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:26:06 INFO Executor: Running task 0.0 in stage 30.0 (TID 30)
21/01/20 11:26:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:06 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:06 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:26:06 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:26:06 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:26:06 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:26:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:26:06 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:26:06 INFO ParquetOutputFormat: Validation is off
21/01/20 11:26:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:26:06 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:26:06 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:26:06 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:26:06 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:26:06 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:26:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:26:07 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112606_0030_m_000000_30' to o3fs://bucket1.vol1/testdata
21/01/20 11:26:07 INFO SparkHadoopMapRedUtil: attempt_20210120112606_0030_m_000000_30: Committed
21/01/20 11:26:07 INFO Executor: Finished task 0.0 in stage 30.0 (TID 30). 2155 bytes result sent to driver
21/01/20 11:26:07 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 30) in 984 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:26:07 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool 
21/01/20 11:26:07 INFO DAGScheduler: ResultStage 30 (parquet at Generate.java:61) finished in 1.004 s
21/01/20 11:26:07 INFO DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:26:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished
21/01/20 11:26:07 INFO DAGScheduler: Job 30 finished: parquet at Generate.java:61, took 1.006145 s
21/01/20 11:26:07 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-4aa2e59c-4398-4427-aaf9-90e73cf9c6f3
21/01/20 11:26:07 INFO FileFormatWriter: Write Job e1f2126a-5186-4d9e-8165-ff1ab1d0e859 committed.
21/01/20 11:26:07 INFO FileFormatWriter: Finished processing stats for write job e1f2126a-5186-4d9e-8165-ff1ab1d0e859.
21/01/20 11:26:08 INFO BlockManagerInfo: Removed broadcast_30_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:26:10 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:10 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:10 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:26:10 INFO DAGScheduler: Got job 31 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:26:10 INFO DAGScheduler: Final stage: ResultStage 31 (parquet at Generate.java:61)
21/01/20 11:26:10 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:26:10 INFO DAGScheduler: Missing parents: List()
21/01/20 11:26:10 INFO DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[125] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:26:10 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:26:10 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:26:10 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:26:10 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1200
21/01/20 11:26:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[125] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:26:10 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks
21/01/20 11:26:10 WARN TaskSetManager: Stage 31 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:26:10 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 31, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:26:10 INFO Executor: Running task 0.0 in stage 31.0 (TID 31)
21/01/20 11:26:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:10 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:10 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:26:10 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:26:10 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:26:10 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:26:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:26:10 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:26:10 INFO ParquetOutputFormat: Validation is off
21/01/20 11:26:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:26:10 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:26:10 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:26:10 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:26:10 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:26:10 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:26:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:26:10 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112610_0031_m_000000_31' to o3fs://bucket1.vol1/testdata
21/01/20 11:26:10 INFO SparkHadoopMapRedUtil: attempt_20210120112610_0031_m_000000_31: Committed
21/01/20 11:26:10 INFO Executor: Finished task 0.0 in stage 31.0 (TID 31). 2155 bytes result sent to driver
21/01/20 11:26:10 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 31) in 768 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:26:10 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool 
21/01/20 11:26:10 INFO DAGScheduler: ResultStage 31 (parquet at Generate.java:61) finished in 0.813 s
21/01/20 11:26:10 INFO DAGScheduler: Job 31 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:26:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 31: Stage finished
21/01/20 11:26:10 INFO DAGScheduler: Job 31 finished: parquet at Generate.java:61, took 0.815725 s
21/01/20 11:26:10 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-46c57454-012e-4c0f-8c88-991d489e7192
21/01/20 11:26:10 INFO FileFormatWriter: Write Job a471f251-3873-4127-ba4c-568bad632d85 committed.
21/01/20 11:26:10 INFO FileFormatWriter: Finished processing stats for write job a471f251-3873-4127-ba4c-568bad632d85.
21/01/20 11:26:11 INFO BlockManagerInfo: Removed broadcast_31_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:26:13 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:13 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:13 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:26:13 INFO DAGScheduler: Got job 32 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:26:13 INFO DAGScheduler: Final stage: ResultStage 32 (parquet at Generate.java:61)
21/01/20 11:26:13 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:26:13 INFO DAGScheduler: Missing parents: List()
21/01/20 11:26:13 INFO DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[129] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:26:13 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:26:13 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:26:13 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:26:13 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1200
21/01/20 11:26:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[129] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:26:13 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks
21/01/20 11:26:13 WARN TaskSetManager: Stage 32 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:26:13 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 32, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:26:13 INFO Executor: Running task 0.0 in stage 32.0 (TID 32)
21/01/20 11:26:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:13 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:13 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:26:13 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:26:13 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:26:13 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:26:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:26:13 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:26:13 INFO ParquetOutputFormat: Validation is off
21/01/20 11:26:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:26:13 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:26:13 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:26:13 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:26:13 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:26:13 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:26:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:26:14 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-18135D455D79->27b66227-fed8-4c1d-ab57-510f5a3ce552
21/01/20 11:26:14 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:26:14 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112613_0032_m_000000_32' to o3fs://bucket1.vol1/testdata
21/01/20 11:26:14 INFO SparkHadoopMapRedUtil: attempt_20210120112613_0032_m_000000_32: Committed
21/01/20 11:26:14 INFO Executor: Finished task 0.0 in stage 32.0 (TID 32). 2155 bytes result sent to driver
21/01/20 11:26:14 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 32) in 1058 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:26:14 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool 
21/01/20 11:26:14 INFO DAGScheduler: ResultStage 32 (parquet at Generate.java:61) finished in 1.078 s
21/01/20 11:26:14 INFO DAGScheduler: Job 32 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:26:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 32: Stage finished
21/01/20 11:26:14 INFO DAGScheduler: Job 32 finished: parquet at Generate.java:61, took 1.080349 s
21/01/20 11:26:14 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-70340923-d695-40ce-ab85-713c21678a02
21/01/20 11:26:14 INFO FileFormatWriter: Write Job c606a150-de8d-489a-b865-f7d901b470b6 committed.
21/01/20 11:26:14 INFO FileFormatWriter: Finished processing stats for write job c606a150-de8d-489a-b865-f7d901b470b6.
21/01/20 11:26:15 INFO BlockManagerInfo: Removed broadcast_32_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:26:17 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:17 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:17 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:26:17 INFO DAGScheduler: Got job 33 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:26:17 INFO DAGScheduler: Final stage: ResultStage 33 (parquet at Generate.java:61)
21/01/20 11:26:17 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:26:17 INFO DAGScheduler: Missing parents: List()
21/01/20 11:26:17 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[133] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:26:17 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:26:17 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:26:17 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:26:17 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1200
21/01/20 11:26:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[133] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:26:17 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks
21/01/20 11:26:17 WARN TaskSetManager: Stage 33 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:26:17 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 33, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:26:17 INFO Executor: Running task 0.0 in stage 33.0 (TID 33)
21/01/20 11:26:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:17 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:17 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:26:17 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:26:17 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:26:17 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:26:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:26:17 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:26:17 INFO ParquetOutputFormat: Validation is off
21/01/20 11:26:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:26:17 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:26:17 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:26:17 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:26:17 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:26:17 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:26:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:26:18 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112617_0033_m_000000_33' to o3fs://bucket1.vol1/testdata
21/01/20 11:26:18 INFO SparkHadoopMapRedUtil: attempt_20210120112617_0033_m_000000_33: Committed
21/01/20 11:26:18 INFO Executor: Finished task 0.0 in stage 33.0 (TID 33). 2155 bytes result sent to driver
21/01/20 11:26:18 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 33) in 987 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:26:18 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool 
21/01/20 11:26:18 INFO DAGScheduler: ResultStage 33 (parquet at Generate.java:61) finished in 1.006 s
21/01/20 11:26:18 INFO DAGScheduler: Job 33 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:26:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 33: Stage finished
21/01/20 11:26:18 INFO DAGScheduler: Job 33 finished: parquet at Generate.java:61, took 1.009238 s
21/01/20 11:26:18 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-62a3986b-aa93-40a8-ab5f-923f4a9ae0a6
21/01/20 11:26:18 INFO FileFormatWriter: Write Job 1904ed74-346a-4f76-b3cf-a160d7a4f9d4 committed.
21/01/20 11:26:18 INFO FileFormatWriter: Finished processing stats for write job 1904ed74-346a-4f76-b3cf-a160d7a4f9d4.
21/01/20 11:26:19 INFO BlockManagerInfo: Removed broadcast_33_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:26:20 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:20 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:20 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:26:20 INFO DAGScheduler: Got job 34 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:26:20 INFO DAGScheduler: Final stage: ResultStage 34 (parquet at Generate.java:61)
21/01/20 11:26:20 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:26:20 INFO DAGScheduler: Missing parents: List()
21/01/20 11:26:20 INFO DAGScheduler: Submitting ResultStage 34 (MapPartitionsRDD[137] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:26:20 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:26:20 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:26:20 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:26:20 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1200
21/01/20 11:26:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 34 (MapPartitionsRDD[137] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:26:20 INFO TaskSchedulerImpl: Adding task set 34.0 with 1 tasks
21/01/20 11:26:20 WARN TaskSetManager: Stage 34 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:26:20 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 34, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:26:20 INFO Executor: Running task 0.0 in stage 34.0 (TID 34)
21/01/20 11:26:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:20 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:20 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:26:20 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:26:20 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:26:20 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:26:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:26:20 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:26:20 INFO ParquetOutputFormat: Validation is off
21/01/20 11:26:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:26:20 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:26:20 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:26:20 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:26:20 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:26:20 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:26:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:26:21 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112620_0034_m_000000_34' to o3fs://bucket1.vol1/testdata
21/01/20 11:26:21 INFO SparkHadoopMapRedUtil: attempt_20210120112620_0034_m_000000_34: Committed
21/01/20 11:26:21 INFO Executor: Finished task 0.0 in stage 34.0 (TID 34). 2155 bytes result sent to driver
21/01/20 11:26:21 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 34) in 906 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:26:21 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool 
21/01/20 11:26:21 INFO DAGScheduler: ResultStage 34 (parquet at Generate.java:61) finished in 0.948 s
21/01/20 11:26:21 INFO DAGScheduler: Job 34 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:26:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 34: Stage finished
21/01/20 11:26:21 INFO DAGScheduler: Job 34 finished: parquet at Generate.java:61, took 0.950146 s
21/01/20 11:26:21 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-d936f9bc-ca0a-43a6-b6f6-2e94e2fed7cb
21/01/20 11:26:21 INFO FileFormatWriter: Write Job 73532bc3-09db-4a33-a071-689c4ba8a107 committed.
21/01/20 11:26:21 INFO FileFormatWriter: Finished processing stats for write job 73532bc3-09db-4a33-a071-689c4ba8a107.
21/01/20 11:26:21 INFO BlockManagerInfo: Removed broadcast_34_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:26:24 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:24 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:24 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:26:24 INFO DAGScheduler: Got job 35 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:26:24 INFO DAGScheduler: Final stage: ResultStage 35 (parquet at Generate.java:61)
21/01/20 11:26:24 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:26:24 INFO DAGScheduler: Missing parents: List()
21/01/20 11:26:24 INFO DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[141] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:26:24 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:26:24 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:26:24 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:26:24 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1200
21/01/20 11:26:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[141] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:26:24 INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks
21/01/20 11:26:24 WARN TaskSetManager: Stage 35 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:26:24 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 35, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:26:24 INFO Executor: Running task 0.0 in stage 35.0 (TID 35)
21/01/20 11:26:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:24 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:24 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:26:24 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:26:24 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:26:24 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:26:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:26:24 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:26:24 INFO ParquetOutputFormat: Validation is off
21/01/20 11:26:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:26:24 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:26:24 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:26:24 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:26:24 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:26:24 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:26:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:26:25 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-FE5F7ED80ABD->27b66227-fed8-4c1d-ab57-510f5a3ce552
21/01/20 11:26:25 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:26:25 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112624_0035_m_000000_35' to o3fs://bucket1.vol1/testdata
21/01/20 11:26:25 INFO SparkHadoopMapRedUtil: attempt_20210120112624_0035_m_000000_35: Committed
21/01/20 11:26:25 INFO Executor: Finished task 0.0 in stage 35.0 (TID 35). 2155 bytes result sent to driver
21/01/20 11:26:25 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 35) in 1085 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:26:25 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool 
21/01/20 11:26:25 INFO DAGScheduler: ResultStage 35 (parquet at Generate.java:61) finished in 1.103 s
21/01/20 11:26:25 INFO DAGScheduler: Job 35 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:26:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished
21/01/20 11:26:25 INFO DAGScheduler: Job 35 finished: parquet at Generate.java:61, took 1.106037 s
21/01/20 11:26:25 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-bff4084b-90a5-49d7-8564-661e57751085
21/01/20 11:26:25 INFO FileFormatWriter: Write Job 9cf1fe61-db6f-43d2-964c-4727825fbd53 committed.
21/01/20 11:26:25 INFO FileFormatWriter: Finished processing stats for write job 9cf1fe61-db6f-43d2-964c-4727825fbd53.
21/01/20 11:26:26 INFO BlockManagerInfo: Removed broadcast_35_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:26:28 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:28 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:28 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:26:28 INFO DAGScheduler: Got job 36 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:26:28 INFO DAGScheduler: Final stage: ResultStage 36 (parquet at Generate.java:61)
21/01/20 11:26:28 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:26:28 INFO DAGScheduler: Missing parents: List()
21/01/20 11:26:28 INFO DAGScheduler: Submitting ResultStage 36 (MapPartitionsRDD[145] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:26:28 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:26:28 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:26:28 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:26:28 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1200
21/01/20 11:26:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 36 (MapPartitionsRDD[145] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:26:28 INFO TaskSchedulerImpl: Adding task set 36.0 with 1 tasks
21/01/20 11:26:28 WARN TaskSetManager: Stage 36 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:26:28 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 36, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:26:28 INFO Executor: Running task 0.0 in stage 36.0 (TID 36)
21/01/20 11:26:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:28 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:28 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:26:28 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:26:28 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:26:28 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:26:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:26:28 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:26:28 INFO ParquetOutputFormat: Validation is off
21/01/20 11:26:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:26:28 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:26:28 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:26:28 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:26:28 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:26:28 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:26:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:26:29 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112628_0036_m_000000_36' to o3fs://bucket1.vol1/testdata
21/01/20 11:26:29 INFO SparkHadoopMapRedUtil: attempt_20210120112628_0036_m_000000_36: Committed
21/01/20 11:26:29 INFO Executor: Finished task 0.0 in stage 36.0 (TID 36). 2155 bytes result sent to driver
21/01/20 11:26:29 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 36) in 1045 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:26:29 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool 
21/01/20 11:26:29 INFO DAGScheduler: ResultStage 36 (parquet at Generate.java:61) finished in 1.065 s
21/01/20 11:26:29 INFO DAGScheduler: Job 36 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:26:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 36: Stage finished
21/01/20 11:26:29 INFO DAGScheduler: Job 36 finished: parquet at Generate.java:61, took 1.066876 s
21/01/20 11:26:29 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-43194f59-23ee-480a-aa58-2c422d08acb5
21/01/20 11:26:29 INFO FileFormatWriter: Write Job 45a7cf92-a6d6-4eaf-8c9f-5a83fe42cb4c committed.
21/01/20 11:26:29 INFO FileFormatWriter: Finished processing stats for write job 45a7cf92-a6d6-4eaf-8c9f-5a83fe42cb4c.
21/01/20 11:26:30 INFO BlockManagerInfo: Removed broadcast_36_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:26:31 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:31 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:26:31 INFO DAGScheduler: Got job 37 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:26:31 INFO DAGScheduler: Final stage: ResultStage 37 (parquet at Generate.java:61)
21/01/20 11:26:31 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:26:31 INFO DAGScheduler: Missing parents: List()
21/01/20 11:26:31 INFO DAGScheduler: Submitting ResultStage 37 (MapPartitionsRDD[149] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:26:31 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:26:31 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:26:31 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:26:31 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1200
21/01/20 11:26:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (MapPartitionsRDD[149] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:26:31 INFO TaskSchedulerImpl: Adding task set 37.0 with 1 tasks
21/01/20 11:26:31 WARN TaskSetManager: Stage 37 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:26:31 INFO TaskSetManager: Starting task 0.0 in stage 37.0 (TID 37, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:26:31 INFO Executor: Running task 0.0 in stage 37.0 (TID 37)
21/01/20 11:26:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:31 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:26:31 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:26:31 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:26:31 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:26:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:26:31 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:26:31 INFO ParquetOutputFormat: Validation is off
21/01/20 11:26:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:26:31 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:26:31 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:26:31 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:26:31 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:26:31 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:26:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:26:32 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112631_0037_m_000000_37' to o3fs://bucket1.vol1/testdata
21/01/20 11:26:32 INFO SparkHadoopMapRedUtil: attempt_20210120112631_0037_m_000000_37: Committed
21/01/20 11:26:32 INFO Executor: Finished task 0.0 in stage 37.0 (TID 37). 2155 bytes result sent to driver
21/01/20 11:26:32 INFO TaskSetManager: Finished task 0.0 in stage 37.0 (TID 37) in 1089 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:26:32 INFO TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool 
21/01/20 11:26:32 INFO DAGScheduler: ResultStage 37 (parquet at Generate.java:61) finished in 1.109 s
21/01/20 11:26:32 INFO DAGScheduler: Job 37 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:26:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 37: Stage finished
21/01/20 11:26:32 INFO DAGScheduler: Job 37 finished: parquet at Generate.java:61, took 1.111666 s
21/01/20 11:26:32 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-3b1988bd-02cc-4e98-bae4-dd949553431c
21/01/20 11:26:32 INFO FileFormatWriter: Write Job 72e61ca0-f29c-4752-ba1e-ec1faf352fee committed.
21/01/20 11:26:32 INFO FileFormatWriter: Finished processing stats for write job 72e61ca0-f29c-4752-ba1e-ec1faf352fee.
21/01/20 11:26:33 INFO BlockManagerInfo: Removed broadcast_37_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:26:35 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:35 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:35 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:26:35 INFO DAGScheduler: Got job 38 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:26:35 INFO DAGScheduler: Final stage: ResultStage 38 (parquet at Generate.java:61)
21/01/20 11:26:35 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:26:35 INFO DAGScheduler: Missing parents: List()
21/01/20 11:26:35 INFO DAGScheduler: Submitting ResultStage 38 (MapPartitionsRDD[153] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:26:35 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:26:35 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:26:35 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:26:35 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1200
21/01/20 11:26:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 38 (MapPartitionsRDD[153] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:26:35 INFO TaskSchedulerImpl: Adding task set 38.0 with 1 tasks
21/01/20 11:26:35 WARN TaskSetManager: Stage 38 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:26:35 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 38, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:26:35 INFO Executor: Running task 0.0 in stage 38.0 (TID 38)
21/01/20 11:26:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:35 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:35 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:26:35 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:26:35 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:26:35 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:26:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:26:35 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:26:35 INFO ParquetOutputFormat: Validation is off
21/01/20 11:26:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:26:35 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:26:35 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:26:35 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:26:35 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:26:35 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:26:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:26:36 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112635_0038_m_000000_38' to o3fs://bucket1.vol1/testdata
21/01/20 11:26:36 INFO SparkHadoopMapRedUtil: attempt_20210120112635_0038_m_000000_38: Committed
21/01/20 11:26:36 INFO Executor: Finished task 0.0 in stage 38.0 (TID 38). 2155 bytes result sent to driver
21/01/20 11:26:36 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 38) in 1022 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:26:36 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool 
21/01/20 11:26:36 INFO DAGScheduler: ResultStage 38 (parquet at Generate.java:61) finished in 1.042 s
21/01/20 11:26:36 INFO DAGScheduler: Job 38 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:26:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 38: Stage finished
21/01/20 11:26:36 INFO DAGScheduler: Job 38 finished: parquet at Generate.java:61, took 1.044254 s
21/01/20 11:26:36 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-fefc816a-8ef1-46ba-8a7b-c9177d9b2a3a
21/01/20 11:26:36 INFO FileFormatWriter: Write Job 175341d7-514a-4932-a4d2-88b91a26ebc2 committed.
21/01/20 11:26:36 INFO FileFormatWriter: Finished processing stats for write job 175341d7-514a-4932-a4d2-88b91a26ebc2.
21/01/20 11:26:37 INFO BlockManagerInfo: Removed broadcast_38_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:26:39 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:39 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:39 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:26:39 INFO DAGScheduler: Got job 39 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:26:39 INFO DAGScheduler: Final stage: ResultStage 39 (parquet at Generate.java:61)
21/01/20 11:26:39 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:26:39 INFO DAGScheduler: Missing parents: List()
21/01/20 11:26:39 INFO DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[157] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:26:39 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:26:39 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:26:39 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:26:39 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1200
21/01/20 11:26:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[157] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:26:39 INFO TaskSchedulerImpl: Adding task set 39.0 with 1 tasks
21/01/20 11:26:39 WARN TaskSetManager: Stage 39 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:26:39 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 39, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:26:39 INFO Executor: Running task 0.0 in stage 39.0 (TID 39)
21/01/20 11:26:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:39 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:39 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:26:39 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:26:39 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:26:39 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:26:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:26:39 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:26:39 INFO ParquetOutputFormat: Validation is off
21/01/20 11:26:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:26:39 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:26:39 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:26:39 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:26:39 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:26:39 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:26:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:26:40 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112639_0039_m_000000_39' to o3fs://bucket1.vol1/testdata
21/01/20 11:26:40 INFO SparkHadoopMapRedUtil: attempt_20210120112639_0039_m_000000_39: Committed
21/01/20 11:26:40 INFO Executor: Finished task 0.0 in stage 39.0 (TID 39). 2155 bytes result sent to driver
21/01/20 11:26:40 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 39) in 1053 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:26:40 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool 
21/01/20 11:26:40 INFO DAGScheduler: ResultStage 39 (parquet at Generate.java:61) finished in 1.074 s
21/01/20 11:26:40 INFO DAGScheduler: Job 39 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:26:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 39: Stage finished
21/01/20 11:26:40 INFO DAGScheduler: Job 39 finished: parquet at Generate.java:61, took 1.075859 s
21/01/20 11:26:40 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-8a9d6b37-59ca-40e5-9263-17392029a7aa
21/01/20 11:26:40 INFO FileFormatWriter: Write Job 48a24657-64ff-4cdb-8e52-30ed0e11996e committed.
21/01/20 11:26:40 INFO FileFormatWriter: Finished processing stats for write job 48a24657-64ff-4cdb-8e52-30ed0e11996e.
21/01/20 11:26:41 INFO BlockManagerInfo: Removed broadcast_39_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:26:42 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:42 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:42 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:26:42 INFO DAGScheduler: Got job 40 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:26:42 INFO DAGScheduler: Final stage: ResultStage 40 (parquet at Generate.java:61)
21/01/20 11:26:42 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:26:42 INFO DAGScheduler: Missing parents: List()
21/01/20 11:26:42 INFO DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[161] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:26:42 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:26:42 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:26:42 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:26:42 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1200
21/01/20 11:26:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[161] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:26:42 INFO TaskSchedulerImpl: Adding task set 40.0 with 1 tasks
21/01/20 11:26:43 WARN TaskSetManager: Stage 40 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:26:43 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 40, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:26:43 INFO Executor: Running task 0.0 in stage 40.0 (TID 40)
21/01/20 11:26:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:43 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:43 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:26:43 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:26:43 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:26:43 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:26:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:26:43 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:26:43 INFO ParquetOutputFormat: Validation is off
21/01/20 11:26:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:26:43 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:26:43 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:26:43 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:26:43 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:26:43 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:26:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:26:44 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112642_0040_m_000000_40' to o3fs://bucket1.vol1/testdata
21/01/20 11:26:44 INFO SparkHadoopMapRedUtil: attempt_20210120112642_0040_m_000000_40: Committed
21/01/20 11:26:44 INFO Executor: Finished task 0.0 in stage 40.0 (TID 40). 2155 bytes result sent to driver
21/01/20 11:26:44 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 40) in 1051 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:26:44 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool 
21/01/20 11:26:44 INFO DAGScheduler: ResultStage 40 (parquet at Generate.java:61) finished in 1.071 s
21/01/20 11:26:44 INFO DAGScheduler: Job 40 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:26:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 40: Stage finished
21/01/20 11:26:44 INFO DAGScheduler: Job 40 finished: parquet at Generate.java:61, took 1.073537 s
21/01/20 11:26:44 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-155ae99c-ccf1-4f87-9de4-68705ce4f490
21/01/20 11:26:44 INFO FileFormatWriter: Write Job e1bf99ff-a5e1-49f4-8345-06c6624ca0c4 committed.
21/01/20 11:26:44 INFO FileFormatWriter: Finished processing stats for write job e1bf99ff-a5e1-49f4-8345-06c6624ca0c4.
21/01/20 11:26:45 INFO BlockManagerInfo: Removed broadcast_40_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:26:46 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:46 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:46 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:26:46 INFO DAGScheduler: Got job 41 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:26:46 INFO DAGScheduler: Final stage: ResultStage 41 (parquet at Generate.java:61)
21/01/20 11:26:46 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:26:46 INFO DAGScheduler: Missing parents: List()
21/01/20 11:26:46 INFO DAGScheduler: Submitting ResultStage 41 (MapPartitionsRDD[165] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:26:46 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:26:46 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:26:46 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:26:46 INFO SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1200
21/01/20 11:26:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 41 (MapPartitionsRDD[165] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:26:46 INFO TaskSchedulerImpl: Adding task set 41.0 with 1 tasks
21/01/20 11:26:46 WARN TaskSetManager: Stage 41 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:26:46 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 41, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:26:46 INFO Executor: Running task 0.0 in stage 41.0 (TID 41)
21/01/20 11:26:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:46 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:46 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:26:46 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:26:46 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:26:46 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:26:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:26:46 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:26:46 INFO ParquetOutputFormat: Validation is off
21/01/20 11:26:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:26:46 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:26:46 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:26:46 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:26:46 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:26:46 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:26:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:26:47 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-E347ED55EA0D->27b66227-fed8-4c1d-ab57-510f5a3ce552
21/01/20 11:26:47 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:26:47 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112646_0041_m_000000_41' to o3fs://bucket1.vol1/testdata
21/01/20 11:26:47 INFO SparkHadoopMapRedUtil: attempt_20210120112646_0041_m_000000_41: Committed
21/01/20 11:26:47 INFO Executor: Finished task 0.0 in stage 41.0 (TID 41). 2155 bytes result sent to driver
21/01/20 11:26:47 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 41) in 1043 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:26:47 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool 
21/01/20 11:26:47 INFO DAGScheduler: ResultStage 41 (parquet at Generate.java:61) finished in 1.063 s
21/01/20 11:26:47 INFO DAGScheduler: Job 41 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:26:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 41: Stage finished
21/01/20 11:26:47 INFO DAGScheduler: Job 41 finished: parquet at Generate.java:61, took 1.065743 s
21/01/20 11:26:47 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-52ccd44a-cad0-41a5-bea3-6a843325b916
21/01/20 11:26:47 INFO FileFormatWriter: Write Job 67f23f88-03d1-4011-b454-dfbe64c491f1 committed.
21/01/20 11:26:47 INFO FileFormatWriter: Finished processing stats for write job 67f23f88-03d1-4011-b454-dfbe64c491f1.
21/01/20 11:26:49 INFO BlockManagerInfo: Removed broadcast_41_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:26:50 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:50 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:50 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:26:50 INFO DAGScheduler: Got job 42 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:26:50 INFO DAGScheduler: Final stage: ResultStage 42 (parquet at Generate.java:61)
21/01/20 11:26:50 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:26:50 INFO DAGScheduler: Missing parents: List()
21/01/20 11:26:50 INFO DAGScheduler: Submitting ResultStage 42 (MapPartitionsRDD[169] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:26:50 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:26:50 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:26:50 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:26:50 INFO SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1200
21/01/20 11:26:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 42 (MapPartitionsRDD[169] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:26:50 INFO TaskSchedulerImpl: Adding task set 42.0 with 1 tasks
21/01/20 11:26:50 WARN TaskSetManager: Stage 42 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:26:50 INFO TaskSetManager: Starting task 0.0 in stage 42.0 (TID 42, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:26:50 INFO Executor: Running task 0.0 in stage 42.0 (TID 42)
21/01/20 11:26:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:50 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:50 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:26:50 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:26:50 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:26:50 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:26:50 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:26:50 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:26:50 INFO ParquetOutputFormat: Validation is off
21/01/20 11:26:50 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:26:50 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:26:50 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:26:50 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:26:50 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:26:50 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:26:50 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:26:51 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112650_0042_m_000000_42' to o3fs://bucket1.vol1/testdata
21/01/20 11:26:51 INFO SparkHadoopMapRedUtil: attempt_20210120112650_0042_m_000000_42: Committed
21/01/20 11:26:51 INFO Executor: Finished task 0.0 in stage 42.0 (TID 42). 2155 bytes result sent to driver
21/01/20 11:26:51 INFO TaskSetManager: Finished task 0.0 in stage 42.0 (TID 42) in 770 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:26:51 INFO TaskSchedulerImpl: Removed TaskSet 42.0, whose tasks have all completed, from pool 
21/01/20 11:26:51 INFO DAGScheduler: ResultStage 42 (parquet at Generate.java:61) finished in 0.814 s
21/01/20 11:26:51 INFO DAGScheduler: Job 42 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:26:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 42: Stage finished
21/01/20 11:26:51 INFO DAGScheduler: Job 42 finished: parquet at Generate.java:61, took 0.816840 s
21/01/20 11:26:51 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-621701fd-5c8e-4295-9dad-96c387f91f3c
21/01/20 11:26:51 INFO FileFormatWriter: Write Job 71fa791b-3d56-4ef2-baf7-5e8ec5d7ea08 committed.
21/01/20 11:26:51 INFO FileFormatWriter: Finished processing stats for write job 71fa791b-3d56-4ef2-baf7-5e8ec5d7ea08.
21/01/20 11:26:51 INFO BlockManagerInfo: Removed broadcast_42_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:26:53 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:53 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:26:53 INFO DAGScheduler: Got job 43 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:26:53 INFO DAGScheduler: Final stage: ResultStage 43 (parquet at Generate.java:61)
21/01/20 11:26:53 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:26:53 INFO DAGScheduler: Missing parents: List()
21/01/20 11:26:53 INFO DAGScheduler: Submitting ResultStage 43 (MapPartitionsRDD[173] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:26:53 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:26:53 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:26:53 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:26:53 INFO SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1200
21/01/20 11:26:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 43 (MapPartitionsRDD[173] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:26:53 INFO TaskSchedulerImpl: Adding task set 43.0 with 1 tasks
21/01/20 11:26:53 WARN TaskSetManager: Stage 43 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:26:53 INFO TaskSetManager: Starting task 0.0 in stage 43.0 (TID 43, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:26:53 INFO Executor: Running task 0.0 in stage 43.0 (TID 43)
21/01/20 11:26:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:53 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:26:53 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:26:53 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:26:53 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:26:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:26:53 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:26:53 INFO ParquetOutputFormat: Validation is off
21/01/20 11:26:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:26:53 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:26:53 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:26:53 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:26:53 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:26:53 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:26:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:26:54 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112653_0043_m_000000_43' to o3fs://bucket1.vol1/testdata
21/01/20 11:26:54 INFO SparkHadoopMapRedUtil: attempt_20210120112653_0043_m_000000_43: Committed
21/01/20 11:26:54 INFO Executor: Finished task 0.0 in stage 43.0 (TID 43). 2155 bytes result sent to driver
21/01/20 11:26:54 INFO TaskSetManager: Finished task 0.0 in stage 43.0 (TID 43) in 1042 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:26:54 INFO TaskSchedulerImpl: Removed TaskSet 43.0, whose tasks have all completed, from pool 
21/01/20 11:26:54 INFO DAGScheduler: ResultStage 43 (parquet at Generate.java:61) finished in 1.060 s
21/01/20 11:26:54 INFO DAGScheduler: Job 43 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:26:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 43: Stage finished
21/01/20 11:26:54 INFO DAGScheduler: Job 43 finished: parquet at Generate.java:61, took 1.062441 s
21/01/20 11:26:54 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-5b8d0eba-b74c-4d64-8c5a-b62abcc874b5
21/01/20 11:26:54 INFO FileFormatWriter: Write Job 6cc41205-930a-4111-b81a-52cb5a0f4fd9 committed.
21/01/20 11:26:54 INFO FileFormatWriter: Finished processing stats for write job 6cc41205-930a-4111-b81a-52cb5a0f4fd9.
21/01/20 11:26:55 INFO BlockManagerInfo: Removed broadcast_43_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:26:57 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:57 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:57 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:26:57 INFO DAGScheduler: Got job 44 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:26:57 INFO DAGScheduler: Final stage: ResultStage 44 (parquet at Generate.java:61)
21/01/20 11:26:57 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:26:57 INFO DAGScheduler: Missing parents: List()
21/01/20 11:26:57 INFO DAGScheduler: Submitting ResultStage 44 (MapPartitionsRDD[177] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:26:57 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:26:57 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:26:57 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:26:57 INFO SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1200
21/01/20 11:26:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 44 (MapPartitionsRDD[177] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:26:57 INFO TaskSchedulerImpl: Adding task set 44.0 with 1 tasks
21/01/20 11:26:57 WARN TaskSetManager: Stage 44 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:26:57 INFO TaskSetManager: Starting task 0.0 in stage 44.0 (TID 44, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:26:57 INFO Executor: Running task 0.0 in stage 44.0 (TID 44)
21/01/20 11:26:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:57 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:26:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:26:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:26:57 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:26:57 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:26:57 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:26:57 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:26:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:26:57 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:26:57 INFO ParquetOutputFormat: Validation is off
21/01/20 11:26:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:26:57 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:26:57 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:26:57 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:26:57 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:26:57 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:26:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:26:58 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112657_0044_m_000000_44' to o3fs://bucket1.vol1/testdata
21/01/20 11:26:58 INFO SparkHadoopMapRedUtil: attempt_20210120112657_0044_m_000000_44: Committed
21/01/20 11:26:58 INFO Executor: Finished task 0.0 in stage 44.0 (TID 44). 2155 bytes result sent to driver
21/01/20 11:26:58 INFO TaskSetManager: Finished task 0.0 in stage 44.0 (TID 44) in 985 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:26:58 INFO TaskSchedulerImpl: Removed TaskSet 44.0, whose tasks have all completed, from pool 
21/01/20 11:26:58 INFO DAGScheduler: ResultStage 44 (parquet at Generate.java:61) finished in 1.005 s
21/01/20 11:26:58 INFO DAGScheduler: Job 44 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:26:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 44: Stage finished
21/01/20 11:26:58 INFO DAGScheduler: Job 44 finished: parquet at Generate.java:61, took 1.007065 s
21/01/20 11:26:58 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-6c990d3e-f7af-4ee0-899c-ef26a782d3f0
21/01/20 11:26:58 INFO FileFormatWriter: Write Job 6d4f60ed-3b64-4547-ab3e-8c890ba42094 committed.
21/01/20 11:26:58 INFO FileFormatWriter: Finished processing stats for write job 6d4f60ed-3b64-4547-ab3e-8c890ba42094.
21/01/20 11:26:59 INFO BlockManagerInfo: Removed broadcast_44_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:27:00 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:01 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:27:01 INFO DAGScheduler: Got job 45 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:27:01 INFO DAGScheduler: Final stage: ResultStage 45 (parquet at Generate.java:61)
21/01/20 11:27:01 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:27:01 INFO DAGScheduler: Missing parents: List()
21/01/20 11:27:01 INFO DAGScheduler: Submitting ResultStage 45 (MapPartitionsRDD[181] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:27:01 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:27:01 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:27:01 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:27:01 INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1200
21/01/20 11:27:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 45 (MapPartitionsRDD[181] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:27:01 INFO TaskSchedulerImpl: Adding task set 45.0 with 1 tasks
21/01/20 11:27:01 WARN TaskSetManager: Stage 45 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:27:01 INFO TaskSetManager: Starting task 0.0 in stage 45.0 (TID 45, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:27:01 INFO Executor: Running task 0.0 in stage 45.0 (TID 45)
21/01/20 11:27:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:01 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:01 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:27:01 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:27:01 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:27:01 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:27:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:27:01 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:27:01 INFO ParquetOutputFormat: Validation is off
21/01/20 11:27:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:27:01 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:27:01 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:27:01 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:27:01 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:27:01 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:27:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:27:01 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112701_0045_m_000000_45' to o3fs://bucket1.vol1/testdata
21/01/20 11:27:01 INFO SparkHadoopMapRedUtil: attempt_20210120112701_0045_m_000000_45: Committed
21/01/20 11:27:01 INFO Executor: Finished task 0.0 in stage 45.0 (TID 45). 2155 bytes result sent to driver
21/01/20 11:27:01 INFO TaskSetManager: Finished task 0.0 in stage 45.0 (TID 45) in 919 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:27:01 INFO TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool 
21/01/20 11:27:01 INFO DAGScheduler: ResultStage 45 (parquet at Generate.java:61) finished in 0.962 s
21/01/20 11:27:01 INFO DAGScheduler: Job 45 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:27:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 45: Stage finished
21/01/20 11:27:01 INFO DAGScheduler: Job 45 finished: parquet at Generate.java:61, took 0.966180 s
21/01/20 11:27:01 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-5b8bf854-d45a-4d8e-a562-ebc2516720fc
21/01/20 11:27:01 INFO FileFormatWriter: Write Job 27db0519-529b-4869-ba3b-d1848280c1df committed.
21/01/20 11:27:01 INFO FileFormatWriter: Finished processing stats for write job 27db0519-529b-4869-ba3b-d1848280c1df.
21/01/20 11:27:02 INFO BlockManagerInfo: Removed broadcast_45_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:27:04 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:04 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:27:04 INFO DAGScheduler: Got job 46 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:27:04 INFO DAGScheduler: Final stage: ResultStage 46 (parquet at Generate.java:61)
21/01/20 11:27:04 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:27:04 INFO DAGScheduler: Missing parents: List()
21/01/20 11:27:04 INFO DAGScheduler: Submitting ResultStage 46 (MapPartitionsRDD[185] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:27:04 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:27:04 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:27:04 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:27:04 INFO SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1200
21/01/20 11:27:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 46 (MapPartitionsRDD[185] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:27:04 INFO TaskSchedulerImpl: Adding task set 46.0 with 1 tasks
21/01/20 11:27:04 WARN TaskSetManager: Stage 46 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:27:04 INFO TaskSetManager: Starting task 0.0 in stage 46.0 (TID 46, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:27:04 INFO Executor: Running task 0.0 in stage 46.0 (TID 46)
21/01/20 11:27:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:04 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:27:04 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:27:04 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:27:04 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:27:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:27:04 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:27:04 INFO ParquetOutputFormat: Validation is off
21/01/20 11:27:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:27:04 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:27:04 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:27:04 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:27:04 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:27:04 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:27:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:27:05 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112704_0046_m_000000_46' to o3fs://bucket1.vol1/testdata
21/01/20 11:27:05 INFO SparkHadoopMapRedUtil: attempt_20210120112704_0046_m_000000_46: Committed
21/01/20 11:27:05 INFO Executor: Finished task 0.0 in stage 46.0 (TID 46). 2155 bytes result sent to driver
21/01/20 11:27:05 INFO TaskSetManager: Finished task 0.0 in stage 46.0 (TID 46) in 1019 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:27:05 INFO TaskSchedulerImpl: Removed TaskSet 46.0, whose tasks have all completed, from pool 
21/01/20 11:27:05 INFO DAGScheduler: ResultStage 46 (parquet at Generate.java:61) finished in 1.039 s
21/01/20 11:27:05 INFO DAGScheduler: Job 46 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:27:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 46: Stage finished
21/01/20 11:27:05 INFO DAGScheduler: Job 46 finished: parquet at Generate.java:61, took 1.040945 s
21/01/20 11:27:05 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-4d03109d-a3e6-4295-abfc-dac0079c001f
21/01/20 11:27:05 INFO FileFormatWriter: Write Job 8934c4dd-9b1f-497c-aaeb-8595959bc49d committed.
21/01/20 11:27:05 INFO FileFormatWriter: Finished processing stats for write job 8934c4dd-9b1f-497c-aaeb-8595959bc49d.
21/01/20 11:27:06 INFO BlockManagerInfo: Removed broadcast_46_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:27:08 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:08 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:08 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:27:08 INFO DAGScheduler: Got job 47 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:27:08 INFO DAGScheduler: Final stage: ResultStage 47 (parquet at Generate.java:61)
21/01/20 11:27:08 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:27:08 INFO DAGScheduler: Missing parents: List()
21/01/20 11:27:08 INFO DAGScheduler: Submitting ResultStage 47 (MapPartitionsRDD[189] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:27:08 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:27:08 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:27:08 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:27:08 INFO SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1200
21/01/20 11:27:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 47 (MapPartitionsRDD[189] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:27:08 INFO TaskSchedulerImpl: Adding task set 47.0 with 1 tasks
21/01/20 11:27:08 WARN TaskSetManager: Stage 47 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:27:08 INFO TaskSetManager: Starting task 0.0 in stage 47.0 (TID 47, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:27:08 INFO Executor: Running task 0.0 in stage 47.0 (TID 47)
21/01/20 11:27:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:08 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:08 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:27:08 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:27:08 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:27:08 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:27:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:27:08 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:27:08 INFO ParquetOutputFormat: Validation is off
21/01/20 11:27:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:27:08 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:27:08 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:27:08 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:27:08 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:27:08 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:27:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:27:09 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112708_0047_m_000000_47' to o3fs://bucket1.vol1/testdata
21/01/20 11:27:09 INFO SparkHadoopMapRedUtil: attempt_20210120112708_0047_m_000000_47: Committed
21/01/20 11:27:09 INFO Executor: Finished task 0.0 in stage 47.0 (TID 47). 2155 bytes result sent to driver
21/01/20 11:27:09 INFO TaskSetManager: Finished task 0.0 in stage 47.0 (TID 47) in 933 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:27:09 INFO TaskSchedulerImpl: Removed TaskSet 47.0, whose tasks have all completed, from pool 
21/01/20 11:27:09 INFO DAGScheduler: ResultStage 47 (parquet at Generate.java:61) finished in 0.951 s
21/01/20 11:27:09 INFO DAGScheduler: Job 47 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:27:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 47: Stage finished
21/01/20 11:27:09 INFO DAGScheduler: Job 47 finished: parquet at Generate.java:61, took 0.953947 s
21/01/20 11:27:09 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-8e8fcbff-0228-4a37-9608-4e83d0afacac
21/01/20 11:27:09 INFO FileFormatWriter: Write Job 7a45fb3d-fdb9-4ee7-b24a-dd31f7e68e22 committed.
21/01/20 11:27:09 INFO FileFormatWriter: Finished processing stats for write job 7a45fb3d-fdb9-4ee7-b24a-dd31f7e68e22.
21/01/20 11:27:09 INFO BlockManagerInfo: Removed broadcast_47_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:27:11 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:11 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:27:11 INFO DAGScheduler: Got job 48 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:27:11 INFO DAGScheduler: Final stage: ResultStage 48 (parquet at Generate.java:61)
21/01/20 11:27:11 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:27:11 INFO DAGScheduler: Missing parents: List()
21/01/20 11:27:11 INFO DAGScheduler: Submitting ResultStage 48 (MapPartitionsRDD[193] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:27:11 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:27:11 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:27:11 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:27:11 INFO SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1200
21/01/20 11:27:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 48 (MapPartitionsRDD[193] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:27:11 INFO TaskSchedulerImpl: Adding task set 48.0 with 1 tasks
21/01/20 11:27:11 WARN TaskSetManager: Stage 48 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:27:11 INFO TaskSetManager: Starting task 0.0 in stage 48.0 (TID 48, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:27:11 INFO Executor: Running task 0.0 in stage 48.0 (TID 48)
21/01/20 11:27:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:11 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:27:11 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:27:11 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:27:11 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:27:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:27:11 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:27:11 INFO ParquetOutputFormat: Validation is off
21/01/20 11:27:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:27:11 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:27:11 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:27:11 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:27:11 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:27:11 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:27:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:27:12 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112711_0048_m_000000_48' to o3fs://bucket1.vol1/testdata
21/01/20 11:27:12 INFO SparkHadoopMapRedUtil: attempt_20210120112711_0048_m_000000_48: Committed
21/01/20 11:27:12 INFO Executor: Finished task 0.0 in stage 48.0 (TID 48). 2155 bytes result sent to driver
21/01/20 11:27:12 INFO TaskSetManager: Finished task 0.0 in stage 48.0 (TID 48) in 1075 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:27:12 INFO TaskSchedulerImpl: Removed TaskSet 48.0, whose tasks have all completed, from pool 
21/01/20 11:27:12 INFO DAGScheduler: ResultStage 48 (parquet at Generate.java:61) finished in 1.094 s
21/01/20 11:27:12 INFO DAGScheduler: Job 48 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:27:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 48: Stage finished
21/01/20 11:27:12 INFO DAGScheduler: Job 48 finished: parquet at Generate.java:61, took 1.095690 s
21/01/20 11:27:12 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-d983a868-d52f-43d7-8473-a6c1bc1260cd
21/01/20 11:27:12 INFO FileFormatWriter: Write Job d31595bd-72d5-4383-86b6-23c96c6f4497 committed.
21/01/20 11:27:12 INFO FileFormatWriter: Finished processing stats for write job d31595bd-72d5-4383-86b6-23c96c6f4497.
21/01/20 11:27:14 INFO BlockManagerInfo: Removed broadcast_48_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:27:15 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:15 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:15 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:27:15 INFO DAGScheduler: Got job 49 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:27:15 INFO DAGScheduler: Final stage: ResultStage 49 (parquet at Generate.java:61)
21/01/20 11:27:15 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:27:15 INFO DAGScheduler: Missing parents: List()
21/01/20 11:27:15 INFO DAGScheduler: Submitting ResultStage 49 (MapPartitionsRDD[197] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:27:15 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:27:15 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:27:15 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:27:15 INFO SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1200
21/01/20 11:27:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 49 (MapPartitionsRDD[197] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:27:15 INFO TaskSchedulerImpl: Adding task set 49.0 with 1 tasks
21/01/20 11:27:15 WARN TaskSetManager: Stage 49 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:27:15 INFO TaskSetManager: Starting task 0.0 in stage 49.0 (TID 49, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:27:15 INFO Executor: Running task 0.0 in stage 49.0 (TID 49)
21/01/20 11:27:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:15 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:15 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:27:15 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:27:15 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:27:15 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:27:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:27:15 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:27:15 INFO ParquetOutputFormat: Validation is off
21/01/20 11:27:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:27:15 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:27:15 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:27:15 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:27:15 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:27:15 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:27:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:27:16 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-45E9609F3228->27b66227-fed8-4c1d-ab57-510f5a3ce552
21/01/20 11:27:16 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:27:16 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112715_0049_m_000000_49' to o3fs://bucket1.vol1/testdata
21/01/20 11:27:16 INFO SparkHadoopMapRedUtil: attempt_20210120112715_0049_m_000000_49: Committed
21/01/20 11:27:16 INFO Executor: Finished task 0.0 in stage 49.0 (TID 49). 2155 bytes result sent to driver
21/01/20 11:27:16 INFO TaskSetManager: Finished task 0.0 in stage 49.0 (TID 49) in 962 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:27:16 INFO TaskSchedulerImpl: Removed TaskSet 49.0, whose tasks have all completed, from pool 
21/01/20 11:27:16 INFO DAGScheduler: ResultStage 49 (parquet at Generate.java:61) finished in 1.005 s
21/01/20 11:27:16 INFO DAGScheduler: Job 49 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:27:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 49: Stage finished
21/01/20 11:27:16 INFO DAGScheduler: Job 49 finished: parquet at Generate.java:61, took 1.007138 s
21/01/20 11:27:16 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-91cb3925-431b-47e9-8f79-45ffe5c6767e
21/01/20 11:27:16 INFO FileFormatWriter: Write Job 02336355-d6ef-41aa-bbaa-78644ffac369 committed.
21/01/20 11:27:16 INFO FileFormatWriter: Finished processing stats for write job 02336355-d6ef-41aa-bbaa-78644ffac369.
21/01/20 11:27:16 INFO BlockManagerInfo: Removed broadcast_49_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:27:19 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:19 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:19 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:27:19 INFO DAGScheduler: Got job 50 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:27:19 INFO DAGScheduler: Final stage: ResultStage 50 (parquet at Generate.java:61)
21/01/20 11:27:19 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:27:19 INFO DAGScheduler: Missing parents: List()
21/01/20 11:27:19 INFO DAGScheduler: Submitting ResultStage 50 (MapPartitionsRDD[201] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:27:19 INFO MemoryStore: Block broadcast_50 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:27:19 INFO MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:27:19 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:27:19 INFO SparkContext: Created broadcast 50 from broadcast at DAGScheduler.scala:1200
21/01/20 11:27:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 50 (MapPartitionsRDD[201] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:27:19 INFO TaskSchedulerImpl: Adding task set 50.0 with 1 tasks
21/01/20 11:27:19 WARN TaskSetManager: Stage 50 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:27:19 INFO TaskSetManager: Starting task 0.0 in stage 50.0 (TID 50, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:27:19 INFO Executor: Running task 0.0 in stage 50.0 (TID 50)
21/01/20 11:27:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:19 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:19 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:27:19 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:27:19 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:27:19 INFO ParquetOutputFormat: Validation is off
21/01/20 11:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:27:19 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:27:19 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:27:19 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:27:19 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:27:19 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112719_0050_m_000000_50' to o3fs://bucket1.vol1/testdata
21/01/20 11:27:20 INFO SparkHadoopMapRedUtil: attempt_20210120112719_0050_m_000000_50: Committed
21/01/20 11:27:20 INFO Executor: Finished task 0.0 in stage 50.0 (TID 50). 2155 bytes result sent to driver
21/01/20 11:27:20 INFO TaskSetManager: Finished task 0.0 in stage 50.0 (TID 50) in 1042 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:27:20 INFO TaskSchedulerImpl: Removed TaskSet 50.0, whose tasks have all completed, from pool 
21/01/20 11:27:20 INFO DAGScheduler: ResultStage 50 (parquet at Generate.java:61) finished in 1.064 s
21/01/20 11:27:20 INFO DAGScheduler: Job 50 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:27:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 50: Stage finished
21/01/20 11:27:20 INFO DAGScheduler: Job 50 finished: parquet at Generate.java:61, took 1.065814 s
21/01/20 11:27:20 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-d77244a0-d304-46bd-8491-32c4f74538d0
21/01/20 11:27:20 INFO FileFormatWriter: Write Job d2930ec4-73d0-4d39-ad38-e1a2ac127d5f committed.
21/01/20 11:27:20 INFO FileFormatWriter: Finished processing stats for write job d2930ec4-73d0-4d39-ad38-e1a2ac127d5f.
21/01/20 11:27:21 INFO BlockManagerInfo: Removed broadcast_50_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:27:22 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:22 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:22 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:27:22 INFO DAGScheduler: Got job 51 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:27:22 INFO DAGScheduler: Final stage: ResultStage 51 (parquet at Generate.java:61)
21/01/20 11:27:22 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:27:22 INFO DAGScheduler: Missing parents: List()
21/01/20 11:27:22 INFO DAGScheduler: Submitting ResultStage 51 (MapPartitionsRDD[205] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:27:22 INFO MemoryStore: Block broadcast_51 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:27:22 INFO MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:27:22 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:27:22 INFO SparkContext: Created broadcast 51 from broadcast at DAGScheduler.scala:1200
21/01/20 11:27:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 51 (MapPartitionsRDD[205] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:27:22 INFO TaskSchedulerImpl: Adding task set 51.0 with 1 tasks
21/01/20 11:27:22 WARN TaskSetManager: Stage 51 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:27:22 INFO TaskSetManager: Starting task 0.0 in stage 51.0 (TID 51, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:27:22 INFO Executor: Running task 0.0 in stage 51.0 (TID 51)
21/01/20 11:27:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:22 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:22 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:27:22 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:27:22 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:27:22 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:27:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:27:22 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:27:22 INFO ParquetOutputFormat: Validation is off
21/01/20 11:27:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:27:22 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:27:22 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:27:22 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:27:22 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:27:22 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:27:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:27:23 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112722_0051_m_000000_51' to o3fs://bucket1.vol1/testdata
21/01/20 11:27:23 INFO SparkHadoopMapRedUtil: attempt_20210120112722_0051_m_000000_51: Committed
21/01/20 11:27:23 INFO Executor: Finished task 0.0 in stage 51.0 (TID 51). 2155 bytes result sent to driver
21/01/20 11:27:23 INFO TaskSetManager: Finished task 0.0 in stage 51.0 (TID 51) in 918 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:27:23 INFO TaskSchedulerImpl: Removed TaskSet 51.0, whose tasks have all completed, from pool 
21/01/20 11:27:23 INFO DAGScheduler: ResultStage 51 (parquet at Generate.java:61) finished in 0.937 s
21/01/20 11:27:23 INFO DAGScheduler: Job 51 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:27:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 51: Stage finished
21/01/20 11:27:23 INFO DAGScheduler: Job 51 finished: parquet at Generate.java:61, took 0.962375 s
21/01/20 11:27:23 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-1f6b8827-4cae-489f-82de-b0bb536040e3
21/01/20 11:27:23 INFO FileFormatWriter: Write Job 98cd5907-8013-4cbd-a6f0-e5e2e7066793 committed.
21/01/20 11:27:23 INFO FileFormatWriter: Finished processing stats for write job 98cd5907-8013-4cbd-a6f0-e5e2e7066793.
21/01/20 11:27:23 INFO BlockManagerInfo: Removed broadcast_51_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:27:26 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:26 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:26 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:27:26 INFO DAGScheduler: Got job 52 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:27:26 INFO DAGScheduler: Final stage: ResultStage 52 (parquet at Generate.java:61)
21/01/20 11:27:26 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:27:26 INFO DAGScheduler: Missing parents: List()
21/01/20 11:27:26 INFO DAGScheduler: Submitting ResultStage 52 (MapPartitionsRDD[209] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:27:26 INFO MemoryStore: Block broadcast_52 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:27:26 INFO MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:27:26 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:27:26 INFO SparkContext: Created broadcast 52 from broadcast at DAGScheduler.scala:1200
21/01/20 11:27:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 52 (MapPartitionsRDD[209] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:27:26 INFO TaskSchedulerImpl: Adding task set 52.0 with 1 tasks
21/01/20 11:27:26 WARN TaskSetManager: Stage 52 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:27:26 INFO TaskSetManager: Starting task 0.0 in stage 52.0 (TID 52, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:27:26 INFO Executor: Running task 0.0 in stage 52.0 (TID 52)
21/01/20 11:27:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:26 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:26 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:27:26 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:27:26 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:27:26 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:27:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:27:26 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:27:26 INFO ParquetOutputFormat: Validation is off
21/01/20 11:27:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:27:26 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:27:26 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:27:26 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:27:26 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:27:26 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:27:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:27:27 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112726_0052_m_000000_52' to o3fs://bucket1.vol1/testdata
21/01/20 11:27:27 INFO SparkHadoopMapRedUtil: attempt_20210120112726_0052_m_000000_52: Committed
21/01/20 11:27:27 INFO Executor: Finished task 0.0 in stage 52.0 (TID 52). 2155 bytes result sent to driver
21/01/20 11:27:27 INFO TaskSetManager: Finished task 0.0 in stage 52.0 (TID 52) in 1052 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:27:27 INFO TaskSchedulerImpl: Removed TaskSet 52.0, whose tasks have all completed, from pool 
21/01/20 11:27:27 INFO DAGScheduler: ResultStage 52 (parquet at Generate.java:61) finished in 1.071 s
21/01/20 11:27:27 INFO DAGScheduler: Job 52 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:27:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 52: Stage finished
21/01/20 11:27:27 INFO DAGScheduler: Job 52 finished: parquet at Generate.java:61, took 1.073001 s
21/01/20 11:27:27 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-e282b8c3-2275-46a1-8ab9-6e440ca6cde6
21/01/20 11:27:27 INFO FileFormatWriter: Write Job 3596363b-7d93-4776-ba00-0f31f1d83d44 committed.
21/01/20 11:27:27 INFO FileFormatWriter: Finished processing stats for write job 3596363b-7d93-4776-ba00-0f31f1d83d44.
21/01/20 11:27:28 INFO BlockManagerInfo: Removed broadcast_52_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:27:30 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:30 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:27:30 INFO DAGScheduler: Got job 53 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:27:30 INFO DAGScheduler: Final stage: ResultStage 53 (parquet at Generate.java:61)
21/01/20 11:27:30 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:27:30 INFO DAGScheduler: Missing parents: List()
21/01/20 11:27:30 INFO DAGScheduler: Submitting ResultStage 53 (MapPartitionsRDD[213] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:27:30 INFO MemoryStore: Block broadcast_53 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:27:30 INFO MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:27:30 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:27:30 INFO SparkContext: Created broadcast 53 from broadcast at DAGScheduler.scala:1200
21/01/20 11:27:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 53 (MapPartitionsRDD[213] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:27:30 INFO TaskSchedulerImpl: Adding task set 53.0 with 1 tasks
21/01/20 11:27:30 WARN TaskSetManager: Stage 53 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:27:30 INFO TaskSetManager: Starting task 0.0 in stage 53.0 (TID 53, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:27:30 INFO Executor: Running task 0.0 in stage 53.0 (TID 53)
21/01/20 11:27:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:30 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:27:30 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:27:30 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:27:30 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:27:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:27:30 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:27:30 INFO ParquetOutputFormat: Validation is off
21/01/20 11:27:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:27:30 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:27:30 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:27:30 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:27:30 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:27:30 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:27:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:27:30 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-7BBCB9D3D333->8df234bd-f131-4d8e-953a-b9eec8633221
21/01/20 11:27:30 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:27:30 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112730_0053_m_000000_53' to o3fs://bucket1.vol1/testdata
21/01/20 11:27:30 INFO SparkHadoopMapRedUtil: attempt_20210120112730_0053_m_000000_53: Committed
21/01/20 11:27:30 INFO Executor: Finished task 0.0 in stage 53.0 (TID 53). 2155 bytes result sent to driver
21/01/20 11:27:30 INFO TaskSetManager: Finished task 0.0 in stage 53.0 (TID 53) in 829 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:27:30 INFO TaskSchedulerImpl: Removed TaskSet 53.0, whose tasks have all completed, from pool 
21/01/20 11:27:30 INFO DAGScheduler: ResultStage 53 (parquet at Generate.java:61) finished in 0.871 s
21/01/20 11:27:30 INFO DAGScheduler: Job 53 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:27:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 53: Stage finished
21/01/20 11:27:30 INFO DAGScheduler: Job 53 finished: parquet at Generate.java:61, took 0.872878 s
21/01/20 11:27:30 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-18f2fd48-cfe1-4fa7-a683-1e2a16b0a16c
21/01/20 11:27:30 INFO FileFormatWriter: Write Job 32457899-ccbf-4223-88c2-5f70b62c94d6 committed.
21/01/20 11:27:30 INFO FileFormatWriter: Finished processing stats for write job 32457899-ccbf-4223-88c2-5f70b62c94d6.
21/01/20 11:27:31 INFO BlockManagerInfo: Removed broadcast_53_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:27:33 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:33 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:33 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:27:33 INFO DAGScheduler: Got job 54 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:27:33 INFO DAGScheduler: Final stage: ResultStage 54 (parquet at Generate.java:61)
21/01/20 11:27:33 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:27:33 INFO DAGScheduler: Missing parents: List()
21/01/20 11:27:33 INFO DAGScheduler: Submitting ResultStage 54 (MapPartitionsRDD[217] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:27:33 INFO MemoryStore: Block broadcast_54 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:27:33 INFO MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:27:33 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:27:33 INFO SparkContext: Created broadcast 54 from broadcast at DAGScheduler.scala:1200
21/01/20 11:27:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 54 (MapPartitionsRDD[217] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:27:33 INFO TaskSchedulerImpl: Adding task set 54.0 with 1 tasks
21/01/20 11:27:33 WARN TaskSetManager: Stage 54 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:27:33 INFO TaskSetManager: Starting task 0.0 in stage 54.0 (TID 54, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:27:33 INFO Executor: Running task 0.0 in stage 54.0 (TID 54)
21/01/20 11:27:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:33 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:33 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:27:33 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:27:33 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:27:33 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:27:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:27:33 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:27:33 INFO ParquetOutputFormat: Validation is off
21/01/20 11:27:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:27:33 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:27:33 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:27:33 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:27:33 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:27:33 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:27:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:27:34 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112733_0054_m_000000_54' to o3fs://bucket1.vol1/testdata
21/01/20 11:27:34 INFO SparkHadoopMapRedUtil: attempt_20210120112733_0054_m_000000_54: Committed
21/01/20 11:27:34 INFO Executor: Finished task 0.0 in stage 54.0 (TID 54). 2155 bytes result sent to driver
21/01/20 11:27:34 INFO TaskSetManager: Finished task 0.0 in stage 54.0 (TID 54) in 1113 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:27:34 INFO TaskSchedulerImpl: Removed TaskSet 54.0, whose tasks have all completed, from pool 
21/01/20 11:27:34 INFO DAGScheduler: ResultStage 54 (parquet at Generate.java:61) finished in 1.133 s
21/01/20 11:27:34 INFO DAGScheduler: Job 54 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:27:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 54: Stage finished
21/01/20 11:27:34 INFO DAGScheduler: Job 54 finished: parquet at Generate.java:61, took 1.134990 s
21/01/20 11:27:34 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-05b719b0-f8a0-4321-a9d1-fd6520bd3b4a
21/01/20 11:27:34 INFO FileFormatWriter: Write Job c917b6a7-9dbb-449e-bb7e-0badf2e00622 committed.
21/01/20 11:27:34 INFO FileFormatWriter: Finished processing stats for write job c917b6a7-9dbb-449e-bb7e-0badf2e00622.
21/01/20 11:27:36 INFO BlockManagerInfo: Removed broadcast_54_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:27:37 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:37 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:37 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:27:37 INFO DAGScheduler: Got job 55 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:27:37 INFO DAGScheduler: Final stage: ResultStage 55 (parquet at Generate.java:61)
21/01/20 11:27:37 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:27:37 INFO DAGScheduler: Missing parents: List()
21/01/20 11:27:37 INFO DAGScheduler: Submitting ResultStage 55 (MapPartitionsRDD[221] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:27:37 INFO MemoryStore: Block broadcast_55 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:27:37 INFO MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:27:37 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:27:37 INFO SparkContext: Created broadcast 55 from broadcast at DAGScheduler.scala:1200
21/01/20 11:27:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 55 (MapPartitionsRDD[221] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:27:37 INFO TaskSchedulerImpl: Adding task set 55.0 with 1 tasks
21/01/20 11:27:37 WARN TaskSetManager: Stage 55 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:27:37 INFO TaskSetManager: Starting task 0.0 in stage 55.0 (TID 55, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:27:37 INFO Executor: Running task 0.0 in stage 55.0 (TID 55)
21/01/20 11:27:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:37 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:37 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:27:37 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:27:37 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:27:37 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:27:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:27:37 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:27:37 INFO ParquetOutputFormat: Validation is off
21/01/20 11:27:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:27:37 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:27:37 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:27:37 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:27:37 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:27:37 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:27:37 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112737_0055_m_000000_55' to o3fs://bucket1.vol1/testdata
21/01/20 11:27:38 INFO SparkHadoopMapRedUtil: attempt_20210120112737_0055_m_000000_55: Committed
21/01/20 11:27:38 INFO Executor: Finished task 0.0 in stage 55.0 (TID 55). 2155 bytes result sent to driver
21/01/20 11:27:38 INFO TaskSetManager: Finished task 0.0 in stage 55.0 (TID 55) in 890 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:27:38 INFO TaskSchedulerImpl: Removed TaskSet 55.0, whose tasks have all completed, from pool 
21/01/20 11:27:38 INFO DAGScheduler: ResultStage 55 (parquet at Generate.java:61) finished in 0.931 s
21/01/20 11:27:38 INFO DAGScheduler: Job 55 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:27:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 55: Stage finished
21/01/20 11:27:38 INFO DAGScheduler: Job 55 finished: parquet at Generate.java:61, took 0.933081 s
21/01/20 11:27:38 INFO BlockManagerInfo: Removed broadcast_55_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:27:38 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-3af420c1-30b6-4288-89c6-8e2902890ebf
21/01/20 11:27:38 INFO FileFormatWriter: Write Job 94f79a6f-7f44-489f-9988-ac24c090b122 committed.
21/01/20 11:27:38 INFO FileFormatWriter: Finished processing stats for write job 94f79a6f-7f44-489f-9988-ac24c090b122.
21/01/20 11:27:40 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:40 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:40 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:27:40 INFO DAGScheduler: Got job 56 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:27:40 INFO DAGScheduler: Final stage: ResultStage 56 (parquet at Generate.java:61)
21/01/20 11:27:40 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:27:40 INFO DAGScheduler: Missing parents: List()
21/01/20 11:27:40 INFO DAGScheduler: Submitting ResultStage 56 (MapPartitionsRDD[225] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:27:40 INFO MemoryStore: Block broadcast_56 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:27:40 INFO MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:27:40 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:27:40 INFO SparkContext: Created broadcast 56 from broadcast at DAGScheduler.scala:1200
21/01/20 11:27:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 56 (MapPartitionsRDD[225] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:27:40 INFO TaskSchedulerImpl: Adding task set 56.0 with 1 tasks
21/01/20 11:27:41 WARN TaskSetManager: Stage 56 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:27:41 INFO TaskSetManager: Starting task 0.0 in stage 56.0 (TID 56, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:27:41 INFO Executor: Running task 0.0 in stage 56.0 (TID 56)
21/01/20 11:27:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:41 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:41 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:27:41 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:27:41 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:27:41 INFO ParquetOutputFormat: Validation is off
21/01/20 11:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:27:41 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:27:41 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:27:41 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:27:41 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:27:41 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112740_0056_m_000000_56' to o3fs://bucket1.vol1/testdata
21/01/20 11:27:41 INFO SparkHadoopMapRedUtil: attempt_20210120112740_0056_m_000000_56: Committed
21/01/20 11:27:41 INFO Executor: Finished task 0.0 in stage 56.0 (TID 56). 2155 bytes result sent to driver
21/01/20 11:27:41 INFO TaskSetManager: Finished task 0.0 in stage 56.0 (TID 56) in 1047 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:27:41 INFO TaskSchedulerImpl: Removed TaskSet 56.0, whose tasks have all completed, from pool 
21/01/20 11:27:41 INFO DAGScheduler: ResultStage 56 (parquet at Generate.java:61) finished in 1.067 s
21/01/20 11:27:41 INFO DAGScheduler: Job 56 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:27:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 56: Stage finished
21/01/20 11:27:41 INFO DAGScheduler: Job 56 finished: parquet at Generate.java:61, took 1.069087 s
21/01/20 11:27:42 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-d56598a1-f543-4bc6-9cf2-be35015a5f79
21/01/20 11:27:42 INFO FileFormatWriter: Write Job 71a488d1-7761-420d-9bb2-fea93ba6d60c committed.
21/01/20 11:27:42 INFO FileFormatWriter: Finished processing stats for write job 71a488d1-7761-420d-9bb2-fea93ba6d60c.
21/01/20 11:27:43 INFO BlockManagerInfo: Removed broadcast_56_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:27:44 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:44 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:44 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:27:44 INFO DAGScheduler: Got job 57 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:27:44 INFO DAGScheduler: Final stage: ResultStage 57 (parquet at Generate.java:61)
21/01/20 11:27:44 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:27:44 INFO DAGScheduler: Missing parents: List()
21/01/20 11:27:44 INFO DAGScheduler: Submitting ResultStage 57 (MapPartitionsRDD[229] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:27:44 INFO MemoryStore: Block broadcast_57 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:27:44 INFO MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:27:44 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:27:44 INFO SparkContext: Created broadcast 57 from broadcast at DAGScheduler.scala:1200
21/01/20 11:27:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 57 (MapPartitionsRDD[229] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:27:44 INFO TaskSchedulerImpl: Adding task set 57.0 with 1 tasks
21/01/20 11:27:44 WARN TaskSetManager: Stage 57 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:27:44 INFO TaskSetManager: Starting task 0.0 in stage 57.0 (TID 57, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:27:44 INFO Executor: Running task 0.0 in stage 57.0 (TID 57)
21/01/20 11:27:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:44 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:44 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:27:44 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:27:44 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:27:44 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:27:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:27:44 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:27:44 INFO ParquetOutputFormat: Validation is off
21/01/20 11:27:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:27:44 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:27:44 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:27:44 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:27:44 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:27:44 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:27:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:27:45 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112744_0057_m_000000_57' to o3fs://bucket1.vol1/testdata
21/01/20 11:27:45 INFO SparkHadoopMapRedUtil: attempt_20210120112744_0057_m_000000_57: Committed
21/01/20 11:27:45 INFO Executor: Finished task 0.0 in stage 57.0 (TID 57). 2155 bytes result sent to driver
21/01/20 11:27:45 INFO TaskSetManager: Finished task 0.0 in stage 57.0 (TID 57) in 915 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:27:45 INFO TaskSchedulerImpl: Removed TaskSet 57.0, whose tasks have all completed, from pool 
21/01/20 11:27:45 INFO DAGScheduler: ResultStage 57 (parquet at Generate.java:61) finished in 0.935 s
21/01/20 11:27:45 INFO DAGScheduler: Job 57 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:27:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 57: Stage finished
21/01/20 11:27:45 INFO DAGScheduler: Job 57 finished: parquet at Generate.java:61, took 0.936892 s
21/01/20 11:27:45 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-59606519-79f9-4597-9c90-c14f659c37cf
21/01/20 11:27:45 INFO FileFormatWriter: Write Job 0418cbdb-53ca-45e6-b827-2e4b3816995c committed.
21/01/20 11:27:45 INFO FileFormatWriter: Finished processing stats for write job 0418cbdb-53ca-45e6-b827-2e4b3816995c.
21/01/20 11:27:45 INFO BlockManagerInfo: Removed broadcast_57_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:27:48 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:48 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:48 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:27:48 INFO DAGScheduler: Got job 58 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:27:48 INFO DAGScheduler: Final stage: ResultStage 58 (parquet at Generate.java:61)
21/01/20 11:27:48 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:27:48 INFO DAGScheduler: Missing parents: List()
21/01/20 11:27:48 INFO DAGScheduler: Submitting ResultStage 58 (MapPartitionsRDD[233] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:27:48 INFO MemoryStore: Block broadcast_58 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:27:48 INFO MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:27:48 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:27:48 INFO SparkContext: Created broadcast 58 from broadcast at DAGScheduler.scala:1200
21/01/20 11:27:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 58 (MapPartitionsRDD[233] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:27:48 INFO TaskSchedulerImpl: Adding task set 58.0 with 1 tasks
21/01/20 11:27:48 WARN TaskSetManager: Stage 58 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:27:48 INFO TaskSetManager: Starting task 0.0 in stage 58.0 (TID 58, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:27:48 INFO Executor: Running task 0.0 in stage 58.0 (TID 58)
21/01/20 11:27:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:48 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:48 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:27:48 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:27:48 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:27:48 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:27:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:27:48 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:27:48 INFO ParquetOutputFormat: Validation is off
21/01/20 11:27:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:27:48 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:27:48 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:27:48 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:27:48 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:27:48 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:27:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:27:49 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112748_0058_m_000000_58' to o3fs://bucket1.vol1/testdata
21/01/20 11:27:49 INFO SparkHadoopMapRedUtil: attempt_20210120112748_0058_m_000000_58: Committed
21/01/20 11:27:49 INFO Executor: Finished task 0.0 in stage 58.0 (TID 58). 2155 bytes result sent to driver
21/01/20 11:27:49 INFO TaskSetManager: Finished task 0.0 in stage 58.0 (TID 58) in 1036 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:27:49 INFO TaskSchedulerImpl: Removed TaskSet 58.0, whose tasks have all completed, from pool 
21/01/20 11:27:49 INFO DAGScheduler: ResultStage 58 (parquet at Generate.java:61) finished in 1.056 s
21/01/20 11:27:49 INFO DAGScheduler: Job 58 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:27:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 58: Stage finished
21/01/20 11:27:49 INFO DAGScheduler: Job 58 finished: parquet at Generate.java:61, took 1.057949 s
21/01/20 11:27:49 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-b0606fd4-0a36-412f-abce-014150760a36
21/01/20 11:27:49 INFO FileFormatWriter: Write Job fd899a26-6d77-4b45-9f8c-b4ea877e2567 committed.
21/01/20 11:27:49 INFO FileFormatWriter: Finished processing stats for write job fd899a26-6d77-4b45-9f8c-b4ea877e2567.
21/01/20 11:27:50 INFO BlockManagerInfo: Removed broadcast_58_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:27:51 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:51 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:27:51 INFO DAGScheduler: Got job 59 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:27:51 INFO DAGScheduler: Final stage: ResultStage 59 (parquet at Generate.java:61)
21/01/20 11:27:51 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:27:51 INFO DAGScheduler: Missing parents: List()
21/01/20 11:27:51 INFO DAGScheduler: Submitting ResultStage 59 (MapPartitionsRDD[237] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:27:51 INFO MemoryStore: Block broadcast_59 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:27:51 INFO MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:27:51 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:27:51 INFO SparkContext: Created broadcast 59 from broadcast at DAGScheduler.scala:1200
21/01/20 11:27:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 59 (MapPartitionsRDD[237] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:27:51 INFO TaskSchedulerImpl: Adding task set 59.0 with 1 tasks
21/01/20 11:27:51 WARN TaskSetManager: Stage 59 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:27:51 INFO TaskSetManager: Starting task 0.0 in stage 59.0 (TID 59, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:27:51 INFO Executor: Running task 0.0 in stage 59.0 (TID 59)
21/01/20 11:27:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:51 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:27:51 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:27:51 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:27:51 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:27:51 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:27:51 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:27:51 INFO ParquetOutputFormat: Validation is off
21/01/20 11:27:51 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:27:51 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:27:51 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:27:51 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:27:51 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:27:51 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:27:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:27:52 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-C808BC274DE3->8df234bd-f131-4d8e-953a-b9eec8633221
21/01/20 11:27:52 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:27:52 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112751_0059_m_000000_59' to o3fs://bucket1.vol1/testdata
21/01/20 11:27:52 INFO SparkHadoopMapRedUtil: attempt_20210120112751_0059_m_000000_59: Committed
21/01/20 11:27:52 INFO Executor: Finished task 0.0 in stage 59.0 (TID 59). 2155 bytes result sent to driver
21/01/20 11:27:52 INFO TaskSetManager: Finished task 0.0 in stage 59.0 (TID 59) in 885 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:27:52 INFO TaskSchedulerImpl: Removed TaskSet 59.0, whose tasks have all completed, from pool 
21/01/20 11:27:52 INFO DAGScheduler: ResultStage 59 (parquet at Generate.java:61) finished in 0.927 s
21/01/20 11:27:52 INFO DAGScheduler: Job 59 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:27:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 59: Stage finished
21/01/20 11:27:52 INFO DAGScheduler: Job 59 finished: parquet at Generate.java:61, took 0.928956 s
21/01/20 11:27:52 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-98a4738c-f5a6-4d70-9c7c-e2d696a91f44
21/01/20 11:27:52 INFO FileFormatWriter: Write Job e9ad46de-5cbf-4955-8ff0-3588fb063de8 committed.
21/01/20 11:27:52 INFO FileFormatWriter: Finished processing stats for write job e9ad46de-5cbf-4955-8ff0-3588fb063de8.
21/01/20 11:27:52 INFO BlockManagerInfo: Removed broadcast_59_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:27:55 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:55 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:27:55 INFO DAGScheduler: Got job 60 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:27:55 INFO DAGScheduler: Final stage: ResultStage 60 (parquet at Generate.java:61)
21/01/20 11:27:55 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:27:55 INFO DAGScheduler: Missing parents: List()
21/01/20 11:27:55 INFO DAGScheduler: Submitting ResultStage 60 (MapPartitionsRDD[241] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:27:55 INFO MemoryStore: Block broadcast_60 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:27:55 INFO MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:27:55 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:27:55 INFO SparkContext: Created broadcast 60 from broadcast at DAGScheduler.scala:1200
21/01/20 11:27:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 60 (MapPartitionsRDD[241] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:27:55 INFO TaskSchedulerImpl: Adding task set 60.0 with 1 tasks
21/01/20 11:27:55 WARN TaskSetManager: Stage 60 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:27:55 INFO TaskSetManager: Starting task 0.0 in stage 60.0 (TID 60, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:27:55 INFO Executor: Running task 0.0 in stage 60.0 (TID 60)
21/01/20 11:27:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:55 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:27:55 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:27:55 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:27:55 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:27:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:27:55 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:27:55 INFO ParquetOutputFormat: Validation is off
21/01/20 11:27:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:27:55 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:27:55 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:27:55 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:27:55 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:27:55 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:27:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:27:56 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112755_0060_m_000000_60' to o3fs://bucket1.vol1/testdata
21/01/20 11:27:56 INFO SparkHadoopMapRedUtil: attempt_20210120112755_0060_m_000000_60: Committed
21/01/20 11:27:56 INFO Executor: Finished task 0.0 in stage 60.0 (TID 60). 2155 bytes result sent to driver
21/01/20 11:27:56 INFO TaskSetManager: Finished task 0.0 in stage 60.0 (TID 60) in 1051 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:27:56 INFO TaskSchedulerImpl: Removed TaskSet 60.0, whose tasks have all completed, from pool 
21/01/20 11:27:56 INFO DAGScheduler: ResultStage 60 (parquet at Generate.java:61) finished in 1.069 s
21/01/20 11:27:56 INFO DAGScheduler: Job 60 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:27:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 60: Stage finished
21/01/20 11:27:56 INFO DAGScheduler: Job 60 finished: parquet at Generate.java:61, took 1.070544 s
21/01/20 11:27:56 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-e6551dd0-de06-4afe-9e20-81ffdf589393
21/01/20 11:27:56 INFO FileFormatWriter: Write Job 61a9a3ca-d63b-48b5-8320-4bec5e15732a committed.
21/01/20 11:27:56 INFO FileFormatWriter: Finished processing stats for write job 61a9a3ca-d63b-48b5-8320-4bec5e15732a.
21/01/20 11:27:57 INFO BlockManagerInfo: Removed broadcast_60_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:27:59 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:59 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:59 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:27:59 INFO DAGScheduler: Got job 61 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:27:59 INFO DAGScheduler: Final stage: ResultStage 61 (parquet at Generate.java:61)
21/01/20 11:27:59 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:27:59 INFO DAGScheduler: Missing parents: List()
21/01/20 11:27:59 INFO DAGScheduler: Submitting ResultStage 61 (MapPartitionsRDD[245] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:27:59 INFO MemoryStore: Block broadcast_61 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:27:59 INFO MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:27:59 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:27:59 INFO SparkContext: Created broadcast 61 from broadcast at DAGScheduler.scala:1200
21/01/20 11:27:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 61 (MapPartitionsRDD[245] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:27:59 INFO TaskSchedulerImpl: Adding task set 61.0 with 1 tasks
21/01/20 11:27:59 WARN TaskSetManager: Stage 61 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:27:59 INFO TaskSetManager: Starting task 0.0 in stage 61.0 (TID 61, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:27:59 INFO Executor: Running task 0.0 in stage 61.0 (TID 61)
21/01/20 11:27:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:59 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:27:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:27:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:27:59 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:27:59 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:27:59 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:27:59 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:27:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:27:59 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:27:59 INFO ParquetOutputFormat: Validation is off
21/01/20 11:27:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:27:59 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:27:59 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:27:59 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:27:59 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:27:59 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:27:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:27:59 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-F75CDF9F4006->27b66227-fed8-4c1d-ab57-510f5a3ce552
21/01/20 11:27:59 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:28:00 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112759_0061_m_000000_61' to o3fs://bucket1.vol1/testdata
21/01/20 11:28:00 INFO SparkHadoopMapRedUtil: attempt_20210120112759_0061_m_000000_61: Committed
21/01/20 11:28:00 INFO Executor: Finished task 0.0 in stage 61.0 (TID 61). 2155 bytes result sent to driver
21/01/20 11:28:00 INFO TaskSetManager: Finished task 0.0 in stage 61.0 (TID 61) in 1107 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:28:00 INFO TaskSchedulerImpl: Removed TaskSet 61.0, whose tasks have all completed, from pool 
21/01/20 11:28:00 INFO DAGScheduler: ResultStage 61 (parquet at Generate.java:61) finished in 1.126 s
21/01/20 11:28:00 INFO DAGScheduler: Job 61 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:28:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 61: Stage finished
21/01/20 11:28:00 INFO DAGScheduler: Job 61 finished: parquet at Generate.java:61, took 1.127346 s
21/01/20 11:28:00 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-bc206b19-5811-4394-ad15-6eeafb545200
21/01/20 11:28:00 INFO FileFormatWriter: Write Job b6f1125c-cdb9-4213-89a3-bccd6429fec7 committed.
21/01/20 11:28:00 INFO FileFormatWriter: Finished processing stats for write job b6f1125c-cdb9-4213-89a3-bccd6429fec7.
21/01/20 11:28:01 INFO BlockManagerInfo: Removed broadcast_61_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:28:02 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:02 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:28:02 INFO DAGScheduler: Got job 62 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:28:02 INFO DAGScheduler: Final stage: ResultStage 62 (parquet at Generate.java:61)
21/01/20 11:28:02 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:28:02 INFO DAGScheduler: Missing parents: List()
21/01/20 11:28:02 INFO DAGScheduler: Submitting ResultStage 62 (MapPartitionsRDD[249] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:28:02 INFO MemoryStore: Block broadcast_62 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:28:02 INFO MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:28:02 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:28:02 INFO SparkContext: Created broadcast 62 from broadcast at DAGScheduler.scala:1200
21/01/20 11:28:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 62 (MapPartitionsRDD[249] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:28:02 INFO TaskSchedulerImpl: Adding task set 62.0 with 1 tasks
21/01/20 11:28:02 WARN TaskSetManager: Stage 62 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:28:02 INFO TaskSetManager: Starting task 0.0 in stage 62.0 (TID 62, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:28:02 INFO Executor: Running task 0.0 in stage 62.0 (TID 62)
21/01/20 11:28:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:02 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:28:02 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:28:02 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:28:02 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:28:02 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:28:02 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:28:02 INFO ParquetOutputFormat: Validation is off
21/01/20 11:28:02 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:28:02 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:28:02 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:28:02 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:28:02 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:28:02 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:28:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:28:03 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112802_0062_m_000000_62' to o3fs://bucket1.vol1/testdata
21/01/20 11:28:03 INFO SparkHadoopMapRedUtil: attempt_20210120112802_0062_m_000000_62: Committed
21/01/20 11:28:03 INFO Executor: Finished task 0.0 in stage 62.0 (TID 62). 2155 bytes result sent to driver
21/01/20 11:28:03 INFO TaskSetManager: Finished task 0.0 in stage 62.0 (TID 62) in 1046 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:28:03 INFO TaskSchedulerImpl: Removed TaskSet 62.0, whose tasks have all completed, from pool 
21/01/20 11:28:03 INFO DAGScheduler: ResultStage 62 (parquet at Generate.java:61) finished in 1.065 s
21/01/20 11:28:03 INFO DAGScheduler: Job 62 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:28:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 62: Stage finished
21/01/20 11:28:03 INFO DAGScheduler: Job 62 finished: parquet at Generate.java:61, took 1.066579 s
21/01/20 11:28:03 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-00f7699e-58c0-4b44-84c3-ad1b7a3b671f
21/01/20 11:28:03 INFO FileFormatWriter: Write Job 04858ef3-3bef-41a7-a506-243d6cbf021b committed.
21/01/20 11:28:03 INFO FileFormatWriter: Finished processing stats for write job 04858ef3-3bef-41a7-a506-243d6cbf021b.
21/01/20 11:28:04 INFO BlockManagerInfo: Removed broadcast_62_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:28:06 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:06 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:06 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:28:06 INFO DAGScheduler: Got job 63 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:28:06 INFO DAGScheduler: Final stage: ResultStage 63 (parquet at Generate.java:61)
21/01/20 11:28:06 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:28:06 INFO DAGScheduler: Missing parents: List()
21/01/20 11:28:06 INFO DAGScheduler: Submitting ResultStage 63 (MapPartitionsRDD[253] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:28:06 INFO MemoryStore: Block broadcast_63 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:28:06 INFO MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:28:06 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:28:06 INFO SparkContext: Created broadcast 63 from broadcast at DAGScheduler.scala:1200
21/01/20 11:28:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 63 (MapPartitionsRDD[253] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:28:06 INFO TaskSchedulerImpl: Adding task set 63.0 with 1 tasks
21/01/20 11:28:06 WARN TaskSetManager: Stage 63 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:28:06 INFO TaskSetManager: Starting task 0.0 in stage 63.0 (TID 63, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:28:06 INFO Executor: Running task 0.0 in stage 63.0 (TID 63)
21/01/20 11:28:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:06 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:06 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:28:06 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:28:06 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:28:06 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:28:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:28:06 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:28:06 INFO ParquetOutputFormat: Validation is off
21/01/20 11:28:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:28:06 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:28:06 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:28:06 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:28:06 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:28:06 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:28:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:28:07 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112806_0063_m_000000_63' to o3fs://bucket1.vol1/testdata
21/01/20 11:28:07 INFO SparkHadoopMapRedUtil: attempt_20210120112806_0063_m_000000_63: Committed
21/01/20 11:28:07 INFO Executor: Finished task 0.0 in stage 63.0 (TID 63). 2155 bytes result sent to driver
21/01/20 11:28:07 INFO TaskSetManager: Finished task 0.0 in stage 63.0 (TID 63) in 1041 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:28:07 INFO TaskSchedulerImpl: Removed TaskSet 63.0, whose tasks have all completed, from pool 
21/01/20 11:28:07 INFO DAGScheduler: ResultStage 63 (parquet at Generate.java:61) finished in 1.061 s
21/01/20 11:28:07 INFO DAGScheduler: Job 63 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:28:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 63: Stage finished
21/01/20 11:28:07 INFO DAGScheduler: Job 63 finished: parquet at Generate.java:61, took 1.063443 s
21/01/20 11:28:07 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-9c0a5092-fcf6-45a0-9461-133b9f2e1056
21/01/20 11:28:07 INFO FileFormatWriter: Write Job e86506d9-d7db-474a-b8a9-72a03836dffc committed.
21/01/20 11:28:07 INFO FileFormatWriter: Finished processing stats for write job e86506d9-d7db-474a-b8a9-72a03836dffc.
21/01/20 11:28:08 INFO BlockManagerInfo: Removed broadcast_63_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:28:10 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:10 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:10 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:28:10 INFO DAGScheduler: Got job 64 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:28:10 INFO DAGScheduler: Final stage: ResultStage 64 (parquet at Generate.java:61)
21/01/20 11:28:10 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:28:10 INFO DAGScheduler: Missing parents: List()
21/01/20 11:28:10 INFO DAGScheduler: Submitting ResultStage 64 (MapPartitionsRDD[257] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:28:10 INFO MemoryStore: Block broadcast_64 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:28:10 INFO MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:28:10 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:28:10 INFO SparkContext: Created broadcast 64 from broadcast at DAGScheduler.scala:1200
21/01/20 11:28:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 64 (MapPartitionsRDD[257] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:28:10 INFO TaskSchedulerImpl: Adding task set 64.0 with 1 tasks
21/01/20 11:28:10 WARN TaskSetManager: Stage 64 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:28:10 INFO TaskSetManager: Starting task 0.0 in stage 64.0 (TID 64, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:28:10 INFO Executor: Running task 0.0 in stage 64.0 (TID 64)
21/01/20 11:28:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:10 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:10 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:28:10 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:28:10 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:28:10 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:28:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:28:10 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:28:10 INFO ParquetOutputFormat: Validation is off
21/01/20 11:28:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:28:10 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:28:10 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:28:10 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:28:10 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:28:10 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:28:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:28:11 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112810_0064_m_000000_64' to o3fs://bucket1.vol1/testdata
21/01/20 11:28:11 INFO SparkHadoopMapRedUtil: attempt_20210120112810_0064_m_000000_64: Committed
21/01/20 11:28:11 INFO Executor: Finished task 0.0 in stage 64.0 (TID 64). 2155 bytes result sent to driver
21/01/20 11:28:11 INFO TaskSetManager: Finished task 0.0 in stage 64.0 (TID 64) in 815 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:28:11 INFO TaskSchedulerImpl: Removed TaskSet 64.0, whose tasks have all completed, from pool 
21/01/20 11:28:11 INFO DAGScheduler: ResultStage 64 (parquet at Generate.java:61) finished in 0.857 s
21/01/20 11:28:11 INFO DAGScheduler: Job 64 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:28:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 64: Stage finished
21/01/20 11:28:11 INFO DAGScheduler: Job 64 finished: parquet at Generate.java:61, took 0.859111 s
21/01/20 11:28:11 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-fd48fde8-0567-4b1b-a31e-85a582253050
21/01/20 11:28:11 INFO FileFormatWriter: Write Job dab054be-66f3-443e-9d37-52265e61213d committed.
21/01/20 11:28:11 INFO FileFormatWriter: Finished processing stats for write job dab054be-66f3-443e-9d37-52265e61213d.
21/01/20 11:28:11 INFO BlockManagerInfo: Removed broadcast_64_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:28:13 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:13 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:13 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:28:13 INFO DAGScheduler: Got job 65 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:28:13 INFO DAGScheduler: Final stage: ResultStage 65 (parquet at Generate.java:61)
21/01/20 11:28:13 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:28:13 INFO DAGScheduler: Missing parents: List()
21/01/20 11:28:13 INFO DAGScheduler: Submitting ResultStage 65 (MapPartitionsRDD[261] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:28:13 INFO MemoryStore: Block broadcast_65 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:28:13 INFO MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:28:13 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:28:13 INFO SparkContext: Created broadcast 65 from broadcast at DAGScheduler.scala:1200
21/01/20 11:28:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 65 (MapPartitionsRDD[261] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:28:13 INFO TaskSchedulerImpl: Adding task set 65.0 with 1 tasks
21/01/20 11:28:13 WARN TaskSetManager: Stage 65 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:28:13 INFO TaskSetManager: Starting task 0.0 in stage 65.0 (TID 65, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:28:13 INFO Executor: Running task 0.0 in stage 65.0 (TID 65)
21/01/20 11:28:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:13 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:13 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:28:13 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:28:13 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:28:13 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:28:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:28:13 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:28:13 INFO ParquetOutputFormat: Validation is off
21/01/20 11:28:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:28:13 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:28:13 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:28:13 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:28:13 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:28:13 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:28:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:28:14 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112813_0065_m_000000_65' to o3fs://bucket1.vol1/testdata
21/01/20 11:28:14 INFO SparkHadoopMapRedUtil: attempt_20210120112813_0065_m_000000_65: Committed
21/01/20 11:28:14 INFO Executor: Finished task 0.0 in stage 65.0 (TID 65). 2155 bytes result sent to driver
21/01/20 11:28:14 INFO TaskSetManager: Finished task 0.0 in stage 65.0 (TID 65) in 1049 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:28:14 INFO TaskSchedulerImpl: Removed TaskSet 65.0, whose tasks have all completed, from pool 
21/01/20 11:28:14 INFO DAGScheduler: ResultStage 65 (parquet at Generate.java:61) finished in 1.069 s
21/01/20 11:28:14 INFO DAGScheduler: Job 65 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:28:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 65: Stage finished
21/01/20 11:28:14 INFO DAGScheduler: Job 65 finished: parquet at Generate.java:61, took 1.071276 s
21/01/20 11:28:14 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-b9e0563a-3baf-401b-bc07-e17010fe6508
21/01/20 11:28:14 INFO FileFormatWriter: Write Job 339111db-7382-4759-b82f-ba8fc937c487 committed.
21/01/20 11:28:14 INFO FileFormatWriter: Finished processing stats for write job 339111db-7382-4759-b82f-ba8fc937c487.
21/01/20 11:28:15 INFO BlockManagerInfo: Removed broadcast_65_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:28:17 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:17 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:17 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:28:17 INFO DAGScheduler: Got job 66 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:28:17 INFO DAGScheduler: Final stage: ResultStage 66 (parquet at Generate.java:61)
21/01/20 11:28:17 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:28:17 INFO DAGScheduler: Missing parents: List()
21/01/20 11:28:17 INFO DAGScheduler: Submitting ResultStage 66 (MapPartitionsRDD[265] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:28:17 INFO MemoryStore: Block broadcast_66 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:28:17 INFO MemoryStore: Block broadcast_66_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:28:17 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:28:17 INFO SparkContext: Created broadcast 66 from broadcast at DAGScheduler.scala:1200
21/01/20 11:28:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 66 (MapPartitionsRDD[265] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:28:17 INFO TaskSchedulerImpl: Adding task set 66.0 with 1 tasks
21/01/20 11:28:17 WARN TaskSetManager: Stage 66 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:28:17 INFO TaskSetManager: Starting task 0.0 in stage 66.0 (TID 66, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:28:17 INFO Executor: Running task 0.0 in stage 66.0 (TID 66)
21/01/20 11:28:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:17 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:17 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:28:17 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:28:17 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:28:17 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:28:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:28:17 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:28:17 INFO ParquetOutputFormat: Validation is off
21/01/20 11:28:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:28:17 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:28:17 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:28:17 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:28:17 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:28:17 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:28:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:28:18 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112817_0066_m_000000_66' to o3fs://bucket1.vol1/testdata
21/01/20 11:28:18 INFO SparkHadoopMapRedUtil: attempt_20210120112817_0066_m_000000_66: Committed
21/01/20 11:28:18 INFO Executor: Finished task 0.0 in stage 66.0 (TID 66). 2155 bytes result sent to driver
21/01/20 11:28:18 INFO TaskSetManager: Finished task 0.0 in stage 66.0 (TID 66) in 1046 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:28:18 INFO TaskSchedulerImpl: Removed TaskSet 66.0, whose tasks have all completed, from pool 
21/01/20 11:28:18 INFO DAGScheduler: ResultStage 66 (parquet at Generate.java:61) finished in 1.067 s
21/01/20 11:28:18 INFO DAGScheduler: Job 66 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:28:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 66: Stage finished
21/01/20 11:28:18 INFO DAGScheduler: Job 66 finished: parquet at Generate.java:61, took 1.069116 s
21/01/20 11:28:18 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-f4b4f9fe-ea58-46e3-8d40-771450d1db06
21/01/20 11:28:18 INFO FileFormatWriter: Write Job 3efe6855-687a-4cc6-90c3-4bd5a3dc3ead committed.
21/01/20 11:28:18 INFO FileFormatWriter: Finished processing stats for write job 3efe6855-687a-4cc6-90c3-4bd5a3dc3ead.
21/01/20 11:28:19 INFO BlockManagerInfo: Removed broadcast_66_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:28:20 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:20 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:21 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:28:21 INFO DAGScheduler: Got job 67 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:28:21 INFO DAGScheduler: Final stage: ResultStage 67 (parquet at Generate.java:61)
21/01/20 11:28:21 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:28:21 INFO DAGScheduler: Missing parents: List()
21/01/20 11:28:21 INFO DAGScheduler: Submitting ResultStage 67 (MapPartitionsRDD[269] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:28:21 INFO MemoryStore: Block broadcast_67 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:28:21 INFO MemoryStore: Block broadcast_67_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:28:21 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:28:21 INFO SparkContext: Created broadcast 67 from broadcast at DAGScheduler.scala:1200
21/01/20 11:28:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 67 (MapPartitionsRDD[269] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:28:21 INFO TaskSchedulerImpl: Adding task set 67.0 with 1 tasks
21/01/20 11:28:21 WARN TaskSetManager: Stage 67 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:28:21 INFO TaskSetManager: Starting task 0.0 in stage 67.0 (TID 67, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:28:21 INFO Executor: Running task 0.0 in stage 67.0 (TID 67)
21/01/20 11:28:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:21 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:21 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:28:21 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:28:21 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:28:21 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:28:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:28:21 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:28:21 INFO ParquetOutputFormat: Validation is off
21/01/20 11:28:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:28:21 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:28:21 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:28:21 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:28:21 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:28:21 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:28:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:28:21 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-8810B7DA812A->8df234bd-f131-4d8e-953a-b9eec8633221
21/01/20 11:28:21 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:28:21 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112821_0067_m_000000_67' to o3fs://bucket1.vol1/testdata
21/01/20 11:28:21 INFO SparkHadoopMapRedUtil: attempt_20210120112821_0067_m_000000_67: Committed
21/01/20 11:28:21 INFO Executor: Finished task 0.0 in stage 67.0 (TID 67). 2155 bytes result sent to driver
21/01/20 11:28:21 INFO TaskSetManager: Finished task 0.0 in stage 67.0 (TID 67) in 921 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:28:21 INFO TaskSchedulerImpl: Removed TaskSet 67.0, whose tasks have all completed, from pool 
21/01/20 11:28:21 INFO DAGScheduler: ResultStage 67 (parquet at Generate.java:61) finished in 0.962 s
21/01/20 11:28:21 INFO DAGScheduler: Job 67 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:28:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 67: Stage finished
21/01/20 11:28:21 INFO DAGScheduler: Job 67 finished: parquet at Generate.java:61, took 0.964371 s
21/01/20 11:28:21 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-06b89ab0-f020-4faa-9e41-3f818f9e985b
21/01/20 11:28:21 INFO FileFormatWriter: Write Job acac343c-f526-4512-9906-b4dc96703bf5 committed.
21/01/20 11:28:21 INFO FileFormatWriter: Finished processing stats for write job acac343c-f526-4512-9906-b4dc96703bf5.
21/01/20 11:28:22 INFO BlockManagerInfo: Removed broadcast_67_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:28:24 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:24 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:24 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:28:24 INFO DAGScheduler: Got job 68 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:28:24 INFO DAGScheduler: Final stage: ResultStage 68 (parquet at Generate.java:61)
21/01/20 11:28:24 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:28:24 INFO DAGScheduler: Missing parents: List()
21/01/20 11:28:24 INFO DAGScheduler: Submitting ResultStage 68 (MapPartitionsRDD[273] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:28:24 INFO MemoryStore: Block broadcast_68 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:28:24 INFO MemoryStore: Block broadcast_68_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:28:24 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:28:24 INFO SparkContext: Created broadcast 68 from broadcast at DAGScheduler.scala:1200
21/01/20 11:28:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 68 (MapPartitionsRDD[273] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:28:24 INFO TaskSchedulerImpl: Adding task set 68.0 with 1 tasks
21/01/20 11:28:24 WARN TaskSetManager: Stage 68 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:28:24 INFO TaskSetManager: Starting task 0.0 in stage 68.0 (TID 68, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:28:24 INFO Executor: Running task 0.0 in stage 68.0 (TID 68)
21/01/20 11:28:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:24 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:24 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:28:24 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:28:24 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:28:24 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:28:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:28:24 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:28:24 INFO ParquetOutputFormat: Validation is off
21/01/20 11:28:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:28:24 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:28:24 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:28:24 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:28:24 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:28:24 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:28:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:28:25 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112824_0068_m_000000_68' to o3fs://bucket1.vol1/testdata
21/01/20 11:28:25 INFO SparkHadoopMapRedUtil: attempt_20210120112824_0068_m_000000_68: Committed
21/01/20 11:28:25 INFO Executor: Finished task 0.0 in stage 68.0 (TID 68). 2155 bytes result sent to driver
21/01/20 11:28:25 INFO TaskSetManager: Finished task 0.0 in stage 68.0 (TID 68) in 1109 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:28:25 INFO TaskSchedulerImpl: Removed TaskSet 68.0, whose tasks have all completed, from pool 
21/01/20 11:28:25 INFO DAGScheduler: ResultStage 68 (parquet at Generate.java:61) finished in 1.128 s
21/01/20 11:28:25 INFO DAGScheduler: Job 68 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:28:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 68: Stage finished
21/01/20 11:28:25 INFO DAGScheduler: Job 68 finished: parquet at Generate.java:61, took 1.130137 s
21/01/20 11:28:25 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-0fd706e9-2f23-499a-ac37-c3baed6877df
21/01/20 11:28:25 INFO FileFormatWriter: Write Job dc97bf98-ea52-4be4-b0f3-038a10f6a00c committed.
21/01/20 11:28:25 INFO FileFormatWriter: Finished processing stats for write job dc97bf98-ea52-4be4-b0f3-038a10f6a00c.
21/01/20 11:28:26 INFO BlockManagerInfo: Removed broadcast_68_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:28:28 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:28 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:28 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:28:28 INFO DAGScheduler: Got job 69 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:28:28 INFO DAGScheduler: Final stage: ResultStage 69 (parquet at Generate.java:61)
21/01/20 11:28:28 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:28:28 INFO DAGScheduler: Missing parents: List()
21/01/20 11:28:28 INFO DAGScheduler: Submitting ResultStage 69 (MapPartitionsRDD[277] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:28:28 INFO MemoryStore: Block broadcast_69 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:28:28 INFO MemoryStore: Block broadcast_69_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:28:28 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:28:28 INFO SparkContext: Created broadcast 69 from broadcast at DAGScheduler.scala:1200
21/01/20 11:28:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 69 (MapPartitionsRDD[277] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:28:28 INFO TaskSchedulerImpl: Adding task set 69.0 with 1 tasks
21/01/20 11:28:28 WARN TaskSetManager: Stage 69 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:28:28 INFO TaskSetManager: Starting task 0.0 in stage 69.0 (TID 69, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:28:28 INFO Executor: Running task 0.0 in stage 69.0 (TID 69)
21/01/20 11:28:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:28 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:28 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:28:28 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:28:28 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:28:28 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:28:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:28:28 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:28:28 INFO ParquetOutputFormat: Validation is off
21/01/20 11:28:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:28:28 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:28:28 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:28:28 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:28:28 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:28:28 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:28:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:28:29 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112828_0069_m_000000_69' to o3fs://bucket1.vol1/testdata
21/01/20 11:28:29 INFO SparkHadoopMapRedUtil: attempt_20210120112828_0069_m_000000_69: Committed
21/01/20 11:28:29 INFO Executor: Finished task 0.0 in stage 69.0 (TID 69). 2155 bytes result sent to driver
21/01/20 11:28:29 INFO TaskSetManager: Finished task 0.0 in stage 69.0 (TID 69) in 1047 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:28:29 INFO TaskSchedulerImpl: Removed TaskSet 69.0, whose tasks have all completed, from pool 
21/01/20 11:28:29 INFO DAGScheduler: ResultStage 69 (parquet at Generate.java:61) finished in 1.066 s
21/01/20 11:28:29 INFO DAGScheduler: Job 69 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:28:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 69: Stage finished
21/01/20 11:28:29 INFO DAGScheduler: Job 69 finished: parquet at Generate.java:61, took 1.067869 s
21/01/20 11:28:29 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-2c3422bb-9870-4556-b956-a3d42a35e8e2
21/01/20 11:28:29 INFO FileFormatWriter: Write Job ae9969fe-334f-46e0-8daa-92452b00f33e committed.
21/01/20 11:28:29 INFO FileFormatWriter: Finished processing stats for write job ae9969fe-334f-46e0-8daa-92452b00f33e.
21/01/20 11:28:30 INFO BlockManagerInfo: Removed broadcast_69_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:28:32 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:32 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:32 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:28:32 INFO DAGScheduler: Got job 70 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:28:32 INFO DAGScheduler: Final stage: ResultStage 70 (parquet at Generate.java:61)
21/01/20 11:28:32 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:28:32 INFO DAGScheduler: Missing parents: List()
21/01/20 11:28:32 INFO DAGScheduler: Submitting ResultStage 70 (MapPartitionsRDD[281] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:28:32 INFO MemoryStore: Block broadcast_70 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:28:32 INFO MemoryStore: Block broadcast_70_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:28:32 INFO BlockManagerInfo: Added broadcast_70_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:28:32 INFO SparkContext: Created broadcast 70 from broadcast at DAGScheduler.scala:1200
21/01/20 11:28:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 70 (MapPartitionsRDD[281] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:28:32 INFO TaskSchedulerImpl: Adding task set 70.0 with 1 tasks
21/01/20 11:28:32 WARN TaskSetManager: Stage 70 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:28:32 INFO TaskSetManager: Starting task 0.0 in stage 70.0 (TID 70, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:28:32 INFO Executor: Running task 0.0 in stage 70.0 (TID 70)
21/01/20 11:28:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:32 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:32 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:28:32 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:28:32 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:28:32 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:28:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:28:32 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:28:32 INFO ParquetOutputFormat: Validation is off
21/01/20 11:28:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:28:32 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:28:32 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:28:32 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:28:32 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:28:32 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:28:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:28:33 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112832_0070_m_000000_70' to o3fs://bucket1.vol1/testdata
21/01/20 11:28:33 INFO SparkHadoopMapRedUtil: attempt_20210120112832_0070_m_000000_70: Committed
21/01/20 11:28:33 INFO Executor: Finished task 0.0 in stage 70.0 (TID 70). 2155 bytes result sent to driver
21/01/20 11:28:33 INFO TaskSetManager: Finished task 0.0 in stage 70.0 (TID 70) in 1113 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:28:33 INFO TaskSchedulerImpl: Removed TaskSet 70.0, whose tasks have all completed, from pool 
21/01/20 11:28:33 INFO DAGScheduler: ResultStage 70 (parquet at Generate.java:61) finished in 1.132 s
21/01/20 11:28:33 INFO DAGScheduler: Job 70 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:28:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 70: Stage finished
21/01/20 11:28:33 INFO DAGScheduler: Job 70 finished: parquet at Generate.java:61, took 1.133375 s
21/01/20 11:28:33 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-891579d6-d81a-4699-a618-d43f0270f11f
21/01/20 11:28:33 INFO FileFormatWriter: Write Job 707d5582-508a-4f53-8df1-83bf637441a9 committed.
21/01/20 11:28:33 INFO FileFormatWriter: Finished processing stats for write job 707d5582-508a-4f53-8df1-83bf637441a9.
21/01/20 11:28:34 INFO BlockManagerInfo: Removed broadcast_70_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:28:35 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:35 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:35 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:28:35 INFO DAGScheduler: Got job 71 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:28:35 INFO DAGScheduler: Final stage: ResultStage 71 (parquet at Generate.java:61)
21/01/20 11:28:35 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:28:35 INFO DAGScheduler: Missing parents: List()
21/01/20 11:28:35 INFO DAGScheduler: Submitting ResultStage 71 (MapPartitionsRDD[285] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:28:35 INFO MemoryStore: Block broadcast_71 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:28:35 INFO MemoryStore: Block broadcast_71_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:28:35 INFO BlockManagerInfo: Added broadcast_71_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:28:35 INFO SparkContext: Created broadcast 71 from broadcast at DAGScheduler.scala:1200
21/01/20 11:28:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 71 (MapPartitionsRDD[285] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:28:35 INFO TaskSchedulerImpl: Adding task set 71.0 with 1 tasks
21/01/20 11:28:35 WARN TaskSetManager: Stage 71 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:28:35 INFO TaskSetManager: Starting task 0.0 in stage 71.0 (TID 71, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:28:35 INFO Executor: Running task 0.0 in stage 71.0 (TID 71)
21/01/20 11:28:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:35 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:35 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:28:35 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:28:35 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:28:35 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:28:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:28:35 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:28:35 INFO ParquetOutputFormat: Validation is off
21/01/20 11:28:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:28:35 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:28:35 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:28:35 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:28:35 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:28:35 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:28:36 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:28:36 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112835_0071_m_000000_71' to o3fs://bucket1.vol1/testdata
21/01/20 11:28:36 INFO SparkHadoopMapRedUtil: attempt_20210120112835_0071_m_000000_71: Committed
21/01/20 11:28:36 INFO Executor: Finished task 0.0 in stage 71.0 (TID 71). 2155 bytes result sent to driver
21/01/20 11:28:36 INFO TaskSetManager: Finished task 0.0 in stage 71.0 (TID 71) in 1035 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:28:36 INFO TaskSchedulerImpl: Removed TaskSet 71.0, whose tasks have all completed, from pool 
21/01/20 11:28:36 INFO DAGScheduler: ResultStage 71 (parquet at Generate.java:61) finished in 1.056 s
21/01/20 11:28:36 INFO DAGScheduler: Job 71 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:28:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 71: Stage finished
21/01/20 11:28:36 INFO DAGScheduler: Job 71 finished: parquet at Generate.java:61, took 1.060191 s
21/01/20 11:28:36 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-3ceb9af4-7b9d-4d56-8276-7c3d4c94e724
21/01/20 11:28:36 INFO FileFormatWriter: Write Job 529b8608-139e-4ae7-af59-866658e7e6a1 committed.
21/01/20 11:28:36 INFO FileFormatWriter: Finished processing stats for write job 529b8608-139e-4ae7-af59-866658e7e6a1.
21/01/20 11:28:37 INFO BlockManagerInfo: Removed broadcast_71_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:28:39 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:39 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:39 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:28:39 INFO DAGScheduler: Got job 72 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:28:39 INFO DAGScheduler: Final stage: ResultStage 72 (parquet at Generate.java:61)
21/01/20 11:28:39 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:28:39 INFO DAGScheduler: Missing parents: List()
21/01/20 11:28:39 INFO DAGScheduler: Submitting ResultStage 72 (MapPartitionsRDD[289] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:28:39 INFO MemoryStore: Block broadcast_72 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:28:39 INFO MemoryStore: Block broadcast_72_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:28:39 INFO BlockManagerInfo: Added broadcast_72_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:28:39 INFO SparkContext: Created broadcast 72 from broadcast at DAGScheduler.scala:1200
21/01/20 11:28:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 72 (MapPartitionsRDD[289] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:28:39 INFO TaskSchedulerImpl: Adding task set 72.0 with 1 tasks
21/01/20 11:28:39 WARN TaskSetManager: Stage 72 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:28:39 INFO TaskSetManager: Starting task 0.0 in stage 72.0 (TID 72, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:28:39 INFO Executor: Running task 0.0 in stage 72.0 (TID 72)
21/01/20 11:28:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:39 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:39 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:28:39 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:28:39 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:28:39 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:28:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:28:39 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:28:39 INFO ParquetOutputFormat: Validation is off
21/01/20 11:28:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:28:39 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:28:39 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:28:39 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:28:39 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:28:39 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:28:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:28:40 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112839_0072_m_000000_72' to o3fs://bucket1.vol1/testdata
21/01/20 11:28:40 INFO SparkHadoopMapRedUtil: attempt_20210120112839_0072_m_000000_72: Committed
21/01/20 11:28:40 INFO Executor: Finished task 0.0 in stage 72.0 (TID 72). 2155 bytes result sent to driver
21/01/20 11:28:40 INFO TaskSetManager: Finished task 0.0 in stage 72.0 (TID 72) in 1059 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:28:40 INFO TaskSchedulerImpl: Removed TaskSet 72.0, whose tasks have all completed, from pool 
21/01/20 11:28:40 INFO DAGScheduler: ResultStage 72 (parquet at Generate.java:61) finished in 1.078 s
21/01/20 11:28:40 INFO DAGScheduler: Job 72 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:28:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 72: Stage finished
21/01/20 11:28:40 INFO DAGScheduler: Job 72 finished: parquet at Generate.java:61, took 1.079712 s
21/01/20 11:28:40 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-0db25d18-67d4-4246-be78-a4c170117ab1
21/01/20 11:28:40 INFO FileFormatWriter: Write Job e24c757d-439b-4cae-bd1d-d525092ea979 committed.
21/01/20 11:28:40 INFO FileFormatWriter: Finished processing stats for write job e24c757d-439b-4cae-bd1d-d525092ea979.
21/01/20 11:28:41 INFO BlockManagerInfo: Removed broadcast_72_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:28:43 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:43 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:43 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:28:43 INFO DAGScheduler: Got job 73 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:28:43 INFO DAGScheduler: Final stage: ResultStage 73 (parquet at Generate.java:61)
21/01/20 11:28:43 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:28:43 INFO DAGScheduler: Missing parents: List()
21/01/20 11:28:43 INFO DAGScheduler: Submitting ResultStage 73 (MapPartitionsRDD[293] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:28:43 INFO MemoryStore: Block broadcast_73 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:28:43 INFO MemoryStore: Block broadcast_73_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:28:43 INFO BlockManagerInfo: Added broadcast_73_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:28:43 INFO SparkContext: Created broadcast 73 from broadcast at DAGScheduler.scala:1200
21/01/20 11:28:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 73 (MapPartitionsRDD[293] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:28:43 INFO TaskSchedulerImpl: Adding task set 73.0 with 1 tasks
21/01/20 11:28:43 WARN TaskSetManager: Stage 73 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:28:43 INFO TaskSetManager: Starting task 0.0 in stage 73.0 (TID 73, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:28:43 INFO Executor: Running task 0.0 in stage 73.0 (TID 73)
21/01/20 11:28:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:43 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:43 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:28:43 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:28:43 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:28:43 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:28:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:28:43 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:28:43 INFO ParquetOutputFormat: Validation is off
21/01/20 11:28:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:28:43 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:28:43 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:28:43 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:28:43 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:28:43 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:28:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:28:43 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-5856B20CDF35->27b66227-fed8-4c1d-ab57-510f5a3ce552
21/01/20 11:28:43 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:28:44 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112843_0073_m_000000_73' to o3fs://bucket1.vol1/testdata
21/01/20 11:28:44 INFO SparkHadoopMapRedUtil: attempt_20210120112843_0073_m_000000_73: Committed
21/01/20 11:28:44 INFO Executor: Finished task 0.0 in stage 73.0 (TID 73). 2155 bytes result sent to driver
21/01/20 11:28:44 INFO TaskSetManager: Finished task 0.0 in stage 73.0 (TID 73) in 1132 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:28:44 INFO TaskSchedulerImpl: Removed TaskSet 73.0, whose tasks have all completed, from pool 
21/01/20 11:28:44 INFO DAGScheduler: ResultStage 73 (parquet at Generate.java:61) finished in 1.151 s
21/01/20 11:28:44 INFO DAGScheduler: Job 73 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:28:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 73: Stage finished
21/01/20 11:28:44 INFO DAGScheduler: Job 73 finished: parquet at Generate.java:61, took 1.153730 s
21/01/20 11:28:44 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-854c1492-435d-4470-9ddf-93fb511a85ac
21/01/20 11:28:44 INFO FileFormatWriter: Write Job 83e2084c-977b-4432-8a39-d5c40111b64e committed.
21/01/20 11:28:44 INFO FileFormatWriter: Finished processing stats for write job 83e2084c-977b-4432-8a39-d5c40111b64e.
21/01/20 11:28:45 INFO BlockManagerInfo: Removed broadcast_73_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:28:46 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:46 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:47 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:28:47 INFO DAGScheduler: Got job 74 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:28:47 INFO DAGScheduler: Final stage: ResultStage 74 (parquet at Generate.java:61)
21/01/20 11:28:47 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:28:47 INFO DAGScheduler: Missing parents: List()
21/01/20 11:28:47 INFO DAGScheduler: Submitting ResultStage 74 (MapPartitionsRDD[297] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:28:47 INFO MemoryStore: Block broadcast_74 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:28:47 INFO MemoryStore: Block broadcast_74_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:28:47 INFO BlockManagerInfo: Added broadcast_74_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:28:47 INFO SparkContext: Created broadcast 74 from broadcast at DAGScheduler.scala:1200
21/01/20 11:28:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 74 (MapPartitionsRDD[297] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:28:47 INFO TaskSchedulerImpl: Adding task set 74.0 with 1 tasks
21/01/20 11:28:47 WARN TaskSetManager: Stage 74 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:28:47 INFO TaskSetManager: Starting task 0.0 in stage 74.0 (TID 74, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:28:47 INFO Executor: Running task 0.0 in stage 74.0 (TID 74)
21/01/20 11:28:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:47 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:28:47 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:28:47 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:28:47 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:28:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:28:47 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:28:47 INFO ParquetOutputFormat: Validation is off
21/01/20 11:28:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:28:47 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:28:47 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:28:47 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:28:47 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:28:47 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:28:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:28:48 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112846_0074_m_000000_74' to o3fs://bucket1.vol1/testdata
21/01/20 11:28:48 INFO SparkHadoopMapRedUtil: attempt_20210120112846_0074_m_000000_74: Committed
21/01/20 11:28:48 INFO Executor: Finished task 0.0 in stage 74.0 (TID 74). 2155 bytes result sent to driver
21/01/20 11:28:48 INFO TaskSetManager: Finished task 0.0 in stage 74.0 (TID 74) in 1107 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:28:48 INFO TaskSchedulerImpl: Removed TaskSet 74.0, whose tasks have all completed, from pool 
21/01/20 11:28:48 INFO DAGScheduler: ResultStage 74 (parquet at Generate.java:61) finished in 1.127 s
21/01/20 11:28:48 INFO DAGScheduler: Job 74 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:28:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 74: Stage finished
21/01/20 11:28:48 INFO DAGScheduler: Job 74 finished: parquet at Generate.java:61, took 1.128326 s
21/01/20 11:28:48 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-aca76cd9-8532-4346-81e9-ae9f32fe1cda
21/01/20 11:28:48 INFO FileFormatWriter: Write Job 86c17122-d360-49db-865b-ed9d62aab719 committed.
21/01/20 11:28:48 INFO FileFormatWriter: Finished processing stats for write job 86c17122-d360-49db-865b-ed9d62aab719.
21/01/20 11:28:49 INFO BlockManagerInfo: Removed broadcast_74_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:28:50 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:50 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:50 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:28:50 INFO DAGScheduler: Got job 75 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:28:50 INFO DAGScheduler: Final stage: ResultStage 75 (parquet at Generate.java:61)
21/01/20 11:28:50 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:28:50 INFO DAGScheduler: Missing parents: List()
21/01/20 11:28:50 INFO DAGScheduler: Submitting ResultStage 75 (MapPartitionsRDD[301] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:28:50 INFO MemoryStore: Block broadcast_75 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:28:50 INFO MemoryStore: Block broadcast_75_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:28:50 INFO BlockManagerInfo: Added broadcast_75_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:28:50 INFO SparkContext: Created broadcast 75 from broadcast at DAGScheduler.scala:1200
21/01/20 11:28:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 75 (MapPartitionsRDD[301] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:28:50 INFO TaskSchedulerImpl: Adding task set 75.0 with 1 tasks
21/01/20 11:28:50 WARN TaskSetManager: Stage 75 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:28:50 INFO TaskSetManager: Starting task 0.0 in stage 75.0 (TID 75, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:28:50 INFO Executor: Running task 0.0 in stage 75.0 (TID 75)
21/01/20 11:28:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:50 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:50 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:28:50 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:28:50 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:28:50 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:28:50 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:28:50 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:28:50 INFO ParquetOutputFormat: Validation is off
21/01/20 11:28:50 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:28:50 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:28:50 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:28:50 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:28:50 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:28:50 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:28:51 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:28:51 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112850_0075_m_000000_75' to o3fs://bucket1.vol1/testdata
21/01/20 11:28:51 INFO SparkHadoopMapRedUtil: attempt_20210120112850_0075_m_000000_75: Committed
21/01/20 11:28:51 INFO Executor: Finished task 0.0 in stage 75.0 (TID 75). 2155 bytes result sent to driver
21/01/20 11:28:51 INFO TaskSetManager: Finished task 0.0 in stage 75.0 (TID 75) in 1085 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:28:51 INFO TaskSchedulerImpl: Removed TaskSet 75.0, whose tasks have all completed, from pool 
21/01/20 11:28:51 INFO DAGScheduler: ResultStage 75 (parquet at Generate.java:61) finished in 1.104 s
21/01/20 11:28:51 INFO DAGScheduler: Job 75 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:28:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 75: Stage finished
21/01/20 11:28:51 INFO DAGScheduler: Job 75 finished: parquet at Generate.java:61, took 1.105854 s
21/01/20 11:28:51 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-64cddef2-54e3-4678-b824-890b1bfbb1c1
21/01/20 11:28:51 INFO FileFormatWriter: Write Job 8cad613c-50f4-48ea-bd78-1d367182698c committed.
21/01/20 11:28:51 INFO FileFormatWriter: Finished processing stats for write job 8cad613c-50f4-48ea-bd78-1d367182698c.
21/01/20 11:28:52 INFO BlockManagerInfo: Removed broadcast_75_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:28:54 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:54 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:54 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:28:54 INFO DAGScheduler: Got job 76 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:28:54 INFO DAGScheduler: Final stage: ResultStage 76 (parquet at Generate.java:61)
21/01/20 11:28:54 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:28:54 INFO DAGScheduler: Missing parents: List()
21/01/20 11:28:54 INFO DAGScheduler: Submitting ResultStage 76 (MapPartitionsRDD[305] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:28:54 INFO MemoryStore: Block broadcast_76 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:28:54 INFO MemoryStore: Block broadcast_76_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:28:54 INFO BlockManagerInfo: Added broadcast_76_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:28:54 INFO SparkContext: Created broadcast 76 from broadcast at DAGScheduler.scala:1200
21/01/20 11:28:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 76 (MapPartitionsRDD[305] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:28:54 INFO TaskSchedulerImpl: Adding task set 76.0 with 1 tasks
21/01/20 11:28:54 WARN TaskSetManager: Stage 76 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:28:54 INFO TaskSetManager: Starting task 0.0 in stage 76.0 (TID 76, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:28:54 INFO Executor: Running task 0.0 in stage 76.0 (TID 76)
21/01/20 11:28:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:54 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:54 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:28:54 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:28:54 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:28:54 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:28:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:28:54 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:28:54 INFO ParquetOutputFormat: Validation is off
21/01/20 11:28:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:28:54 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:28:54 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:28:54 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:28:54 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:28:54 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:28:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:28:55 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112854_0076_m_000000_76' to o3fs://bucket1.vol1/testdata
21/01/20 11:28:55 INFO SparkHadoopMapRedUtil: attempt_20210120112854_0076_m_000000_76: Committed
21/01/20 11:28:55 INFO Executor: Finished task 0.0 in stage 76.0 (TID 76). 2155 bytes result sent to driver
21/01/20 11:28:55 INFO TaskSetManager: Finished task 0.0 in stage 76.0 (TID 76) in 1110 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:28:55 INFO TaskSchedulerImpl: Removed TaskSet 76.0, whose tasks have all completed, from pool 
21/01/20 11:28:55 INFO DAGScheduler: ResultStage 76 (parquet at Generate.java:61) finished in 1.129 s
21/01/20 11:28:55 INFO DAGScheduler: Job 76 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:28:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 76: Stage finished
21/01/20 11:28:55 INFO DAGScheduler: Job 76 finished: parquet at Generate.java:61, took 1.131509 s
21/01/20 11:28:55 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-d8b5c62f-24c0-4ab3-ab6f-c7e545a33b3f
21/01/20 11:28:55 INFO FileFormatWriter: Write Job be933bc8-3e63-46cc-896c-b907184cad65 committed.
21/01/20 11:28:55 INFO FileFormatWriter: Finished processing stats for write job be933bc8-3e63-46cc-896c-b907184cad65.
21/01/20 11:28:56 INFO BlockManagerInfo: Removed broadcast_76_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:28:58 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:58 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:58 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:28:58 INFO DAGScheduler: Got job 77 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:28:58 INFO DAGScheduler: Final stage: ResultStage 77 (parquet at Generate.java:61)
21/01/20 11:28:58 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:28:58 INFO DAGScheduler: Missing parents: List()
21/01/20 11:28:58 INFO DAGScheduler: Submitting ResultStage 77 (MapPartitionsRDD[309] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:28:58 INFO MemoryStore: Block broadcast_77 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:28:58 INFO MemoryStore: Block broadcast_77_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:28:58 INFO BlockManagerInfo: Added broadcast_77_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:28:58 INFO SparkContext: Created broadcast 77 from broadcast at DAGScheduler.scala:1200
21/01/20 11:28:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 77 (MapPartitionsRDD[309] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:28:58 INFO TaskSchedulerImpl: Adding task set 77.0 with 1 tasks
21/01/20 11:28:58 WARN TaskSetManager: Stage 77 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:28:58 INFO TaskSetManager: Starting task 0.0 in stage 77.0 (TID 77, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:28:58 INFO Executor: Running task 0.0 in stage 77.0 (TID 77)
21/01/20 11:28:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:58 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:28:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:28:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:28:58 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:28:58 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:28:58 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:28:58 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:28:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:28:58 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:28:58 INFO ParquetOutputFormat: Validation is off
21/01/20 11:28:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:28:58 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:28:58 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:28:58 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:28:58 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:28:58 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:28:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:28:58 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-21E8E18BBE31->8df234bd-f131-4d8e-953a-b9eec8633221
21/01/20 11:28:58 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:28:59 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112858_0077_m_000000_77' to o3fs://bucket1.vol1/testdata
21/01/20 11:28:59 INFO SparkHadoopMapRedUtil: attempt_20210120112858_0077_m_000000_77: Committed
21/01/20 11:28:59 INFO Executor: Finished task 0.0 in stage 77.0 (TID 77). 2155 bytes result sent to driver
21/01/20 11:28:59 INFO TaskSetManager: Finished task 0.0 in stage 77.0 (TID 77) in 1072 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:28:59 INFO TaskSchedulerImpl: Removed TaskSet 77.0, whose tasks have all completed, from pool 
21/01/20 11:28:59 INFO DAGScheduler: ResultStage 77 (parquet at Generate.java:61) finished in 1.091 s
21/01/20 11:28:59 INFO DAGScheduler: Job 77 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:28:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 77: Stage finished
21/01/20 11:28:59 INFO DAGScheduler: Job 77 finished: parquet at Generate.java:61, took 1.092654 s
21/01/20 11:28:59 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-d8e9ed42-4554-432c-8100-77b24fcfd419
21/01/20 11:28:59 INFO FileFormatWriter: Write Job bde64d44-35dc-4d98-9ae3-25371f161ad2 committed.
21/01/20 11:28:59 INFO FileFormatWriter: Finished processing stats for write job bde64d44-35dc-4d98-9ae3-25371f161ad2.
21/01/20 11:29:00 INFO BlockManagerInfo: Removed broadcast_77_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:29:01 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:01 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:02 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:29:02 INFO DAGScheduler: Got job 78 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:29:02 INFO DAGScheduler: Final stage: ResultStage 78 (parquet at Generate.java:61)
21/01/20 11:29:02 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:29:02 INFO DAGScheduler: Missing parents: List()
21/01/20 11:29:02 INFO DAGScheduler: Submitting ResultStage 78 (MapPartitionsRDD[313] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:29:02 INFO MemoryStore: Block broadcast_78 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:29:02 INFO MemoryStore: Block broadcast_78_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:29:02 INFO BlockManagerInfo: Added broadcast_78_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:29:02 INFO SparkContext: Created broadcast 78 from broadcast at DAGScheduler.scala:1200
21/01/20 11:29:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 78 (MapPartitionsRDD[313] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:29:02 INFO TaskSchedulerImpl: Adding task set 78.0 with 1 tasks
21/01/20 11:29:02 WARN TaskSetManager: Stage 78 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:29:02 INFO TaskSetManager: Starting task 0.0 in stage 78.0 (TID 78, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:29:02 INFO Executor: Running task 0.0 in stage 78.0 (TID 78)
21/01/20 11:29:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:02 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:29:02 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:29:02 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:29:02 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:29:02 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:29:02 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:29:02 INFO ParquetOutputFormat: Validation is off
21/01/20 11:29:02 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:29:02 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:29:02 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:29:02 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:29:02 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:29:02 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:29:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:29:03 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112901_0078_m_000000_78' to o3fs://bucket1.vol1/testdata
21/01/20 11:29:03 INFO SparkHadoopMapRedUtil: attempt_20210120112901_0078_m_000000_78: Committed
21/01/20 11:29:03 INFO Executor: Finished task 0.0 in stage 78.0 (TID 78). 2155 bytes result sent to driver
21/01/20 11:29:03 INFO TaskSetManager: Finished task 0.0 in stage 78.0 (TID 78) in 1048 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:29:03 INFO TaskSchedulerImpl: Removed TaskSet 78.0, whose tasks have all completed, from pool 
21/01/20 11:29:03 INFO DAGScheduler: ResultStage 78 (parquet at Generate.java:61) finished in 1.066 s
21/01/20 11:29:03 INFO DAGScheduler: Job 78 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:29:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 78: Stage finished
21/01/20 11:29:03 INFO DAGScheduler: Job 78 finished: parquet at Generate.java:61, took 1.068452 s
21/01/20 11:29:03 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-05b2a1c1-39fa-4867-bdf5-4da15ee75aba
21/01/20 11:29:03 INFO FileFormatWriter: Write Job d5b5e6c1-58e1-46f3-9e87-273828528343 committed.
21/01/20 11:29:03 INFO FileFormatWriter: Finished processing stats for write job d5b5e6c1-58e1-46f3-9e87-273828528343.
21/01/20 11:29:04 INFO BlockManagerInfo: Removed broadcast_78_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:29:05 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:05 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:05 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:29:05 INFO DAGScheduler: Got job 79 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:29:05 INFO DAGScheduler: Final stage: ResultStage 79 (parquet at Generate.java:61)
21/01/20 11:29:05 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:29:05 INFO DAGScheduler: Missing parents: List()
21/01/20 11:29:05 INFO DAGScheduler: Submitting ResultStage 79 (MapPartitionsRDD[317] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:29:05 INFO MemoryStore: Block broadcast_79 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:29:05 INFO MemoryStore: Block broadcast_79_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:29:05 INFO BlockManagerInfo: Added broadcast_79_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:29:05 INFO SparkContext: Created broadcast 79 from broadcast at DAGScheduler.scala:1200
21/01/20 11:29:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 79 (MapPartitionsRDD[317] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:29:05 INFO TaskSchedulerImpl: Adding task set 79.0 with 1 tasks
21/01/20 11:29:05 WARN TaskSetManager: Stage 79 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:29:05 INFO TaskSetManager: Starting task 0.0 in stage 79.0 (TID 79, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:29:05 INFO Executor: Running task 0.0 in stage 79.0 (TID 79)
21/01/20 11:29:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:05 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:05 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:29:05 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:29:05 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:29:05 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:29:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:29:05 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:29:05 INFO ParquetOutputFormat: Validation is off
21/01/20 11:29:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:29:05 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:29:05 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:29:05 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:29:05 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:29:05 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:29:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:29:06 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-E4BB786B0FC2->27b66227-fed8-4c1d-ab57-510f5a3ce552
21/01/20 11:29:06 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:29:06 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112905_0079_m_000000_79' to o3fs://bucket1.vol1/testdata
21/01/20 11:29:06 INFO SparkHadoopMapRedUtil: attempt_20210120112905_0079_m_000000_79: Committed
21/01/20 11:29:06 INFO Executor: Finished task 0.0 in stage 79.0 (TID 79). 2155 bytes result sent to driver
21/01/20 11:29:06 INFO TaskSetManager: Finished task 0.0 in stage 79.0 (TID 79) in 1099 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:29:06 INFO TaskSchedulerImpl: Removed TaskSet 79.0, whose tasks have all completed, from pool 
21/01/20 11:29:06 INFO DAGScheduler: ResultStage 79 (parquet at Generate.java:61) finished in 1.117 s
21/01/20 11:29:06 INFO DAGScheduler: Job 79 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:29:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 79: Stage finished
21/01/20 11:29:06 INFO DAGScheduler: Job 79 finished: parquet at Generate.java:61, took 1.118797 s
21/01/20 11:29:06 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-1e4b4c91-9bde-4574-bd76-aa75c8b412cf
21/01/20 11:29:06 INFO FileFormatWriter: Write Job e2d6d167-523b-438e-af45-bae247df7ee6 committed.
21/01/20 11:29:06 INFO FileFormatWriter: Finished processing stats for write job e2d6d167-523b-438e-af45-bae247df7ee6.
21/01/20 11:29:07 INFO BlockManagerInfo: Removed broadcast_79_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:29:09 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:09 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:09 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:29:09 INFO DAGScheduler: Got job 80 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:29:09 INFO DAGScheduler: Final stage: ResultStage 80 (parquet at Generate.java:61)
21/01/20 11:29:09 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:29:09 INFO DAGScheduler: Missing parents: List()
21/01/20 11:29:09 INFO DAGScheduler: Submitting ResultStage 80 (MapPartitionsRDD[321] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:29:09 INFO MemoryStore: Block broadcast_80 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:29:09 INFO MemoryStore: Block broadcast_80_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:29:09 INFO BlockManagerInfo: Added broadcast_80_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:29:09 INFO SparkContext: Created broadcast 80 from broadcast at DAGScheduler.scala:1200
21/01/20 11:29:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 80 (MapPartitionsRDD[321] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:29:09 INFO TaskSchedulerImpl: Adding task set 80.0 with 1 tasks
21/01/20 11:29:09 WARN TaskSetManager: Stage 80 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:29:09 INFO TaskSetManager: Starting task 0.0 in stage 80.0 (TID 80, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:29:09 INFO Executor: Running task 0.0 in stage 80.0 (TID 80)
21/01/20 11:29:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:09 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:09 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:29:09 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:29:09 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:29:09 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:29:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:29:09 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:29:09 INFO ParquetOutputFormat: Validation is off
21/01/20 11:29:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:29:09 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:29:09 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:29:09 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:29:09 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:29:09 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:29:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:29:10 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112909_0080_m_000000_80' to o3fs://bucket1.vol1/testdata
21/01/20 11:29:10 INFO SparkHadoopMapRedUtil: attempt_20210120112909_0080_m_000000_80: Committed
21/01/20 11:29:10 INFO Executor: Finished task 0.0 in stage 80.0 (TID 80). 2155 bytes result sent to driver
21/01/20 11:29:10 INFO TaskSetManager: Finished task 0.0 in stage 80.0 (TID 80) in 1100 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:29:10 INFO TaskSchedulerImpl: Removed TaskSet 80.0, whose tasks have all completed, from pool 
21/01/20 11:29:10 INFO DAGScheduler: ResultStage 80 (parquet at Generate.java:61) finished in 1.118 s
21/01/20 11:29:10 INFO DAGScheduler: Job 80 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:29:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 80: Stage finished
21/01/20 11:29:10 INFO DAGScheduler: Job 80 finished: parquet at Generate.java:61, took 1.120762 s
21/01/20 11:29:10 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-4313bde8-ff60-4e23-831f-83b76b2f3748
21/01/20 11:29:10 INFO FileFormatWriter: Write Job 4a225baf-3333-4a9e-bc93-01f9e3e45282 committed.
21/01/20 11:29:10 INFO FileFormatWriter: Finished processing stats for write job 4a225baf-3333-4a9e-bc93-01f9e3e45282.
21/01/20 11:29:11 INFO BlockManagerInfo: Removed broadcast_80_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:29:13 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:13 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:13 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:29:13 INFO DAGScheduler: Got job 81 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:29:13 INFO DAGScheduler: Final stage: ResultStage 81 (parquet at Generate.java:61)
21/01/20 11:29:13 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:29:13 INFO DAGScheduler: Missing parents: List()
21/01/20 11:29:13 INFO DAGScheduler: Submitting ResultStage 81 (MapPartitionsRDD[325] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:29:13 INFO MemoryStore: Block broadcast_81 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:29:13 INFO MemoryStore: Block broadcast_81_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:29:13 INFO BlockManagerInfo: Added broadcast_81_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:29:13 INFO SparkContext: Created broadcast 81 from broadcast at DAGScheduler.scala:1200
21/01/20 11:29:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 81 (MapPartitionsRDD[325] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:29:13 INFO TaskSchedulerImpl: Adding task set 81.0 with 1 tasks
21/01/20 11:29:13 WARN TaskSetManager: Stage 81 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:29:13 INFO TaskSetManager: Starting task 0.0 in stage 81.0 (TID 81, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:29:13 INFO Executor: Running task 0.0 in stage 81.0 (TID 81)
21/01/20 11:29:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:13 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:13 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:29:13 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:29:13 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:29:13 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:29:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:29:13 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:29:13 INFO ParquetOutputFormat: Validation is off
21/01/20 11:29:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:29:13 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:29:13 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:29:13 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:29:13 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:29:13 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:29:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:29:13 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-62CD0634FC6F->8df234bd-f131-4d8e-953a-b9eec8633221
21/01/20 11:29:13 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:29:14 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112913_0081_m_000000_81' to o3fs://bucket1.vol1/testdata
21/01/20 11:29:14 INFO SparkHadoopMapRedUtil: attempt_20210120112913_0081_m_000000_81: Committed
21/01/20 11:29:14 INFO Executor: Finished task 0.0 in stage 81.0 (TID 81). 2155 bytes result sent to driver
21/01/20 11:29:14 INFO TaskSetManager: Finished task 0.0 in stage 81.0 (TID 81) in 1051 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:29:14 INFO TaskSchedulerImpl: Removed TaskSet 81.0, whose tasks have all completed, from pool 
21/01/20 11:29:14 INFO DAGScheduler: ResultStage 81 (parquet at Generate.java:61) finished in 1.071 s
21/01/20 11:29:14 INFO DAGScheduler: Job 81 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:29:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 81: Stage finished
21/01/20 11:29:14 INFO DAGScheduler: Job 81 finished: parquet at Generate.java:61, took 1.072643 s
21/01/20 11:29:14 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-1aac5d63-69af-41a2-9ff1-804c4b251481
21/01/20 11:29:14 INFO FileFormatWriter: Write Job bca3c11f-16d1-4fbe-9ddc-096ae4dda7a1 committed.
21/01/20 11:29:14 INFO FileFormatWriter: Finished processing stats for write job bca3c11f-16d1-4fbe-9ddc-096ae4dda7a1.
21/01/20 11:29:15 INFO BlockManagerInfo: Removed broadcast_81_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:29:16 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:16 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:16 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:29:16 INFO DAGScheduler: Got job 82 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:29:16 INFO DAGScheduler: Final stage: ResultStage 82 (parquet at Generate.java:61)
21/01/20 11:29:16 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:29:16 INFO DAGScheduler: Missing parents: List()
21/01/20 11:29:16 INFO DAGScheduler: Submitting ResultStage 82 (MapPartitionsRDD[329] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:29:16 INFO MemoryStore: Block broadcast_82 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:29:16 INFO MemoryStore: Block broadcast_82_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:29:16 INFO BlockManagerInfo: Added broadcast_82_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:29:16 INFO SparkContext: Created broadcast 82 from broadcast at DAGScheduler.scala:1200
21/01/20 11:29:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 82 (MapPartitionsRDD[329] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:29:16 INFO TaskSchedulerImpl: Adding task set 82.0 with 1 tasks
21/01/20 11:29:16 WARN TaskSetManager: Stage 82 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:29:16 INFO TaskSetManager: Starting task 0.0 in stage 82.0 (TID 82, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:29:16 INFO Executor: Running task 0.0 in stage 82.0 (TID 82)
21/01/20 11:29:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:16 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:16 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:29:16 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:29:16 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:29:16 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:29:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:29:16 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:29:16 INFO ParquetOutputFormat: Validation is off
21/01/20 11:29:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:29:16 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:29:16 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:29:16 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:29:16 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:29:16 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:29:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:29:17 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112916_0082_m_000000_82' to o3fs://bucket1.vol1/testdata
21/01/20 11:29:17 INFO SparkHadoopMapRedUtil: attempt_20210120112916_0082_m_000000_82: Committed
21/01/20 11:29:17 INFO Executor: Finished task 0.0 in stage 82.0 (TID 82). 2155 bytes result sent to driver
21/01/20 11:29:17 INFO TaskSetManager: Finished task 0.0 in stage 82.0 (TID 82) in 1017 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:29:17 INFO TaskSchedulerImpl: Removed TaskSet 82.0, whose tasks have all completed, from pool 
21/01/20 11:29:17 INFO DAGScheduler: ResultStage 82 (parquet at Generate.java:61) finished in 1.037 s
21/01/20 11:29:17 INFO DAGScheduler: Job 82 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:29:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 82: Stage finished
21/01/20 11:29:17 INFO DAGScheduler: Job 82 finished: parquet at Generate.java:61, took 1.038777 s
21/01/20 11:29:17 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-a0f1b1ef-6528-4045-bbf5-1f8bcb1ada71
21/01/20 11:29:17 INFO FileFormatWriter: Write Job 5f5b5d4d-c94f-45aa-bbb1-850a9905f5aa committed.
21/01/20 11:29:17 INFO FileFormatWriter: Finished processing stats for write job 5f5b5d4d-c94f-45aa-bbb1-850a9905f5aa.
21/01/20 11:29:19 INFO BlockManagerInfo: Removed broadcast_82_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:29:20 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:20 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:20 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:29:20 INFO DAGScheduler: Got job 83 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:29:20 INFO DAGScheduler: Final stage: ResultStage 83 (parquet at Generate.java:61)
21/01/20 11:29:20 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:29:20 INFO DAGScheduler: Missing parents: List()
21/01/20 11:29:20 INFO DAGScheduler: Submitting ResultStage 83 (MapPartitionsRDD[333] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:29:20 INFO MemoryStore: Block broadcast_83 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:29:20 INFO MemoryStore: Block broadcast_83_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:29:20 INFO BlockManagerInfo: Added broadcast_83_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:29:20 INFO SparkContext: Created broadcast 83 from broadcast at DAGScheduler.scala:1200
21/01/20 11:29:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 83 (MapPartitionsRDD[333] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:29:20 INFO TaskSchedulerImpl: Adding task set 83.0 with 1 tasks
21/01/20 11:29:20 WARN TaskSetManager: Stage 83 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:29:20 INFO TaskSetManager: Starting task 0.0 in stage 83.0 (TID 83, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:29:20 INFO Executor: Running task 0.0 in stage 83.0 (TID 83)
21/01/20 11:29:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:20 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:20 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:29:20 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:29:20 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:29:20 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:29:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:29:20 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:29:20 INFO ParquetOutputFormat: Validation is off
21/01/20 11:29:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:29:20 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:29:20 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:29:20 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:29:20 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:29:20 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:29:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:29:21 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112920_0083_m_000000_83' to o3fs://bucket1.vol1/testdata
21/01/20 11:29:21 INFO SparkHadoopMapRedUtil: attempt_20210120112920_0083_m_000000_83: Committed
21/01/20 11:29:21 INFO Executor: Finished task 0.0 in stage 83.0 (TID 83). 2155 bytes result sent to driver
21/01/20 11:29:21 INFO TaskSetManager: Finished task 0.0 in stage 83.0 (TID 83) in 864 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:29:21 INFO TaskSchedulerImpl: Removed TaskSet 83.0, whose tasks have all completed, from pool 
21/01/20 11:29:21 INFO DAGScheduler: ResultStage 83 (parquet at Generate.java:61) finished in 0.906 s
21/01/20 11:29:21 INFO DAGScheduler: Job 83 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:29:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 83: Stage finished
21/01/20 11:29:21 INFO DAGScheduler: Job 83 finished: parquet at Generate.java:61, took 0.907943 s
21/01/20 11:29:21 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-0de5fecb-66fa-4355-869f-f05d6f9ad4da
21/01/20 11:29:21 INFO FileFormatWriter: Write Job 0400adba-bc40-46c6-865c-2263960e06dd committed.
21/01/20 11:29:21 INFO FileFormatWriter: Finished processing stats for write job 0400adba-bc40-46c6-865c-2263960e06dd.
21/01/20 11:29:21 INFO BlockManagerInfo: Removed broadcast_83_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:29:24 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:24 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:24 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:29:24 INFO DAGScheduler: Got job 84 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:29:24 INFO DAGScheduler: Final stage: ResultStage 84 (parquet at Generate.java:61)
21/01/20 11:29:24 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:29:24 INFO DAGScheduler: Missing parents: List()
21/01/20 11:29:24 INFO DAGScheduler: Submitting ResultStage 84 (MapPartitionsRDD[337] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:29:24 INFO MemoryStore: Block broadcast_84 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:29:24 INFO MemoryStore: Block broadcast_84_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:29:24 INFO BlockManagerInfo: Added broadcast_84_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:29:24 INFO SparkContext: Created broadcast 84 from broadcast at DAGScheduler.scala:1200
21/01/20 11:29:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 84 (MapPartitionsRDD[337] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:29:24 INFO TaskSchedulerImpl: Adding task set 84.0 with 1 tasks
21/01/20 11:29:24 WARN TaskSetManager: Stage 84 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:29:24 INFO TaskSetManager: Starting task 0.0 in stage 84.0 (TID 84, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:29:24 INFO Executor: Running task 0.0 in stage 84.0 (TID 84)
21/01/20 11:29:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:24 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:24 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:29:24 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:29:24 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:29:24 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:29:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:29:24 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:29:24 INFO ParquetOutputFormat: Validation is off
21/01/20 11:29:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:29:24 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:29:24 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:29:24 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:29:24 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:29:24 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:29:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:29:25 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112924_0084_m_000000_84' to o3fs://bucket1.vol1/testdata
21/01/20 11:29:25 INFO SparkHadoopMapRedUtil: attempt_20210120112924_0084_m_000000_84: Committed
21/01/20 11:29:25 INFO Executor: Finished task 0.0 in stage 84.0 (TID 84). 2155 bytes result sent to driver
21/01/20 11:29:25 INFO TaskSetManager: Finished task 0.0 in stage 84.0 (TID 84) in 1030 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:29:25 INFO TaskSchedulerImpl: Removed TaskSet 84.0, whose tasks have all completed, from pool 
21/01/20 11:29:25 INFO DAGScheduler: ResultStage 84 (parquet at Generate.java:61) finished in 1.049 s
21/01/20 11:29:25 INFO DAGScheduler: Job 84 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:29:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 84: Stage finished
21/01/20 11:29:25 INFO DAGScheduler: Job 84 finished: parquet at Generate.java:61, took 1.051255 s
21/01/20 11:29:25 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-7a3e3b58-c6f7-4c4e-9ac0-4379aafffca3
21/01/20 11:29:25 INFO FileFormatWriter: Write Job 56df5bcf-6230-4b5b-8aeb-c3dc23f76bbe committed.
21/01/20 11:29:25 INFO FileFormatWriter: Finished processing stats for write job 56df5bcf-6230-4b5b-8aeb-c3dc23f76bbe.
21/01/20 11:29:26 INFO BlockManagerInfo: Removed broadcast_84_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:29:27 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:27 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:27 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:29:27 INFO DAGScheduler: Got job 85 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:29:27 INFO DAGScheduler: Final stage: ResultStage 85 (parquet at Generate.java:61)
21/01/20 11:29:27 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:29:27 INFO DAGScheduler: Missing parents: List()
21/01/20 11:29:27 INFO DAGScheduler: Submitting ResultStage 85 (MapPartitionsRDD[341] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:29:27 INFO MemoryStore: Block broadcast_85 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:29:27 INFO MemoryStore: Block broadcast_85_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:29:27 INFO BlockManagerInfo: Added broadcast_85_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:29:27 INFO SparkContext: Created broadcast 85 from broadcast at DAGScheduler.scala:1200
21/01/20 11:29:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 85 (MapPartitionsRDD[341] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:29:27 INFO TaskSchedulerImpl: Adding task set 85.0 with 1 tasks
21/01/20 11:29:27 WARN TaskSetManager: Stage 85 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:29:27 INFO TaskSetManager: Starting task 0.0 in stage 85.0 (TID 85, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:29:27 INFO Executor: Running task 0.0 in stage 85.0 (TID 85)
21/01/20 11:29:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:27 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:27 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:29:27 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:29:27 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:29:27 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:29:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:29:27 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:29:27 INFO ParquetOutputFormat: Validation is off
21/01/20 11:29:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:29:27 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:29:27 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:29:27 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:29:27 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:29:27 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:29:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:29:28 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112927_0085_m_000000_85' to o3fs://bucket1.vol1/testdata
21/01/20 11:29:28 INFO SparkHadoopMapRedUtil: attempt_20210120112927_0085_m_000000_85: Committed
21/01/20 11:29:28 INFO Executor: Finished task 0.0 in stage 85.0 (TID 85). 2155 bytes result sent to driver
21/01/20 11:29:28 INFO TaskSetManager: Finished task 0.0 in stage 85.0 (TID 85) in 928 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:29:28 INFO TaskSchedulerImpl: Removed TaskSet 85.0, whose tasks have all completed, from pool 
21/01/20 11:29:28 INFO DAGScheduler: ResultStage 85 (parquet at Generate.java:61) finished in 0.969 s
21/01/20 11:29:28 INFO DAGScheduler: Job 85 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:29:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 85: Stage finished
21/01/20 11:29:28 INFO DAGScheduler: Job 85 finished: parquet at Generate.java:61, took 0.971410 s
21/01/20 11:29:28 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-bca21919-1ad0-4d01-aef3-9baa64bf0434
21/01/20 11:29:28 INFO FileFormatWriter: Write Job eddad08d-62b6-40b8-90f6-80055ccf1e24 committed.
21/01/20 11:29:28 INFO FileFormatWriter: Finished processing stats for write job eddad08d-62b6-40b8-90f6-80055ccf1e24.
21/01/20 11:29:28 INFO BlockManagerInfo: Removed broadcast_85_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:29:31 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:31 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:29:31 INFO DAGScheduler: Got job 86 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:29:31 INFO DAGScheduler: Final stage: ResultStage 86 (parquet at Generate.java:61)
21/01/20 11:29:31 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:29:31 INFO DAGScheduler: Missing parents: List()
21/01/20 11:29:31 INFO DAGScheduler: Submitting ResultStage 86 (MapPartitionsRDD[345] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:29:31 INFO MemoryStore: Block broadcast_86 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:29:31 INFO MemoryStore: Block broadcast_86_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:29:31 INFO BlockManagerInfo: Added broadcast_86_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:29:31 INFO SparkContext: Created broadcast 86 from broadcast at DAGScheduler.scala:1200
21/01/20 11:29:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 86 (MapPartitionsRDD[345] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:29:31 INFO TaskSchedulerImpl: Adding task set 86.0 with 1 tasks
21/01/20 11:29:31 WARN TaskSetManager: Stage 86 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:29:31 INFO TaskSetManager: Starting task 0.0 in stage 86.0 (TID 86, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:29:31 INFO Executor: Running task 0.0 in stage 86.0 (TID 86)
21/01/20 11:29:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:31 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:29:31 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:29:31 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:29:31 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:29:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:29:31 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:29:31 INFO ParquetOutputFormat: Validation is off
21/01/20 11:29:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:29:31 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:29:31 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:29:31 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:29:31 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:29:31 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:29:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:29:32 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112931_0086_m_000000_86' to o3fs://bucket1.vol1/testdata
21/01/20 11:29:32 INFO SparkHadoopMapRedUtil: attempt_20210120112931_0086_m_000000_86: Committed
21/01/20 11:29:32 INFO Executor: Finished task 0.0 in stage 86.0 (TID 86). 2155 bytes result sent to driver
21/01/20 11:29:32 INFO TaskSetManager: Finished task 0.0 in stage 86.0 (TID 86) in 1042 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:29:32 INFO TaskSchedulerImpl: Removed TaskSet 86.0, whose tasks have all completed, from pool 
21/01/20 11:29:32 INFO DAGScheduler: ResultStage 86 (parquet at Generate.java:61) finished in 1.060 s
21/01/20 11:29:32 INFO DAGScheduler: Job 86 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:29:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 86: Stage finished
21/01/20 11:29:32 INFO DAGScheduler: Job 86 finished: parquet at Generate.java:61, took 1.062258 s
21/01/20 11:29:32 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-9a9f070d-7188-40ff-b2e9-961b0deed7c0
21/01/20 11:29:32 INFO FileFormatWriter: Write Job a4fc9299-b64e-4cac-a64c-615d25b656f4 committed.
21/01/20 11:29:32 INFO FileFormatWriter: Finished processing stats for write job a4fc9299-b64e-4cac-a64c-615d25b656f4.
21/01/20 11:29:33 INFO BlockManagerInfo: Removed broadcast_86_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:29:34 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:34 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:34 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:29:35 INFO DAGScheduler: Got job 87 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:29:35 INFO DAGScheduler: Final stage: ResultStage 87 (parquet at Generate.java:61)
21/01/20 11:29:35 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:29:35 INFO DAGScheduler: Missing parents: List()
21/01/20 11:29:35 INFO DAGScheduler: Submitting ResultStage 87 (MapPartitionsRDD[349] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:29:35 INFO MemoryStore: Block broadcast_87 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:29:35 INFO MemoryStore: Block broadcast_87_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:29:35 INFO BlockManagerInfo: Added broadcast_87_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:29:35 INFO SparkContext: Created broadcast 87 from broadcast at DAGScheduler.scala:1200
21/01/20 11:29:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 87 (MapPartitionsRDD[349] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:29:35 INFO TaskSchedulerImpl: Adding task set 87.0 with 1 tasks
21/01/20 11:29:35 WARN TaskSetManager: Stage 87 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:29:35 INFO TaskSetManager: Starting task 0.0 in stage 87.0 (TID 87, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:29:35 INFO Executor: Running task 0.0 in stage 87.0 (TID 87)
21/01/20 11:29:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:35 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:35 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:29:35 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:29:35 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:29:35 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:29:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:29:35 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:29:35 INFO ParquetOutputFormat: Validation is off
21/01/20 11:29:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:29:35 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:29:35 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:29:35 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:29:35 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:29:35 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:29:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:29:35 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-86E0A91392B4->8df234bd-f131-4d8e-953a-b9eec8633221
21/01/20 11:29:35 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:29:35 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112934_0087_m_000000_87' to o3fs://bucket1.vol1/testdata
21/01/20 11:29:35 INFO SparkHadoopMapRedUtil: attempt_20210120112934_0087_m_000000_87: Committed
21/01/20 11:29:35 INFO Executor: Finished task 0.0 in stage 87.0 (TID 87). 2155 bytes result sent to driver
21/01/20 11:29:35 INFO TaskSetManager: Finished task 0.0 in stage 87.0 (TID 87) in 904 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:29:35 INFO TaskSchedulerImpl: Removed TaskSet 87.0, whose tasks have all completed, from pool 
21/01/20 11:29:35 INFO DAGScheduler: ResultStage 87 (parquet at Generate.java:61) finished in 0.924 s
21/01/20 11:29:35 INFO DAGScheduler: Job 87 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:29:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 87: Stage finished
21/01/20 11:29:35 INFO DAGScheduler: Job 87 finished: parquet at Generate.java:61, took 0.948041 s
21/01/20 11:29:35 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-2ba7ffa9-e827-49f7-9989-b61423d5b07f
21/01/20 11:29:35 INFO FileFormatWriter: Write Job 7199f5f9-b03c-4dba-9881-91025c863b11 committed.
21/01/20 11:29:35 INFO FileFormatWriter: Finished processing stats for write job 7199f5f9-b03c-4dba-9881-91025c863b11.
21/01/20 11:29:36 INFO BlockManagerInfo: Removed broadcast_87_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:29:38 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:38 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:38 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:29:38 INFO DAGScheduler: Got job 88 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:29:38 INFO DAGScheduler: Final stage: ResultStage 88 (parquet at Generate.java:61)
21/01/20 11:29:38 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:29:38 INFO DAGScheduler: Missing parents: List()
21/01/20 11:29:38 INFO DAGScheduler: Submitting ResultStage 88 (MapPartitionsRDD[353] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:29:38 INFO MemoryStore: Block broadcast_88 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:29:38 INFO MemoryStore: Block broadcast_88_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:29:38 INFO BlockManagerInfo: Added broadcast_88_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:29:38 INFO SparkContext: Created broadcast 88 from broadcast at DAGScheduler.scala:1200
21/01/20 11:29:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 88 (MapPartitionsRDD[353] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:29:38 INFO TaskSchedulerImpl: Adding task set 88.0 with 1 tasks
21/01/20 11:29:38 WARN TaskSetManager: Stage 88 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:29:38 INFO TaskSetManager: Starting task 0.0 in stage 88.0 (TID 88, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:29:38 INFO Executor: Running task 0.0 in stage 88.0 (TID 88)
21/01/20 11:29:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:38 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:38 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:29:38 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:29:38 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:29:38 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:29:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:29:38 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:29:38 INFO ParquetOutputFormat: Validation is off
21/01/20 11:29:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:29:38 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:29:38 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:29:38 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:29:38 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:29:38 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:29:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:29:39 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112938_0088_m_000000_88' to o3fs://bucket1.vol1/testdata
21/01/20 11:29:39 INFO SparkHadoopMapRedUtil: attempt_20210120112938_0088_m_000000_88: Committed
21/01/20 11:29:39 INFO Executor: Finished task 0.0 in stage 88.0 (TID 88). 2155 bytes result sent to driver
21/01/20 11:29:39 INFO TaskSetManager: Finished task 0.0 in stage 88.0 (TID 88) in 1044 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:29:39 INFO TaskSchedulerImpl: Removed TaskSet 88.0, whose tasks have all completed, from pool 
21/01/20 11:29:39 INFO DAGScheduler: ResultStage 88 (parquet at Generate.java:61) finished in 1.063 s
21/01/20 11:29:39 INFO DAGScheduler: Job 88 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:29:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 88: Stage finished
21/01/20 11:29:39 INFO DAGScheduler: Job 88 finished: parquet at Generate.java:61, took 1.064227 s
21/01/20 11:29:39 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-90e575bb-280a-4ab5-9d23-ea28b7374891
21/01/20 11:29:39 INFO FileFormatWriter: Write Job bdfa579c-ce6a-48ad-ae4c-5a889945c092 committed.
21/01/20 11:29:39 INFO FileFormatWriter: Finished processing stats for write job bdfa579c-ce6a-48ad-ae4c-5a889945c092.
21/01/20 11:29:40 INFO BlockManagerInfo: Removed broadcast_88_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:29:42 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:42 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:42 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:29:42 INFO DAGScheduler: Got job 89 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:29:42 INFO DAGScheduler: Final stage: ResultStage 89 (parquet at Generate.java:61)
21/01/20 11:29:42 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:29:42 INFO DAGScheduler: Missing parents: List()
21/01/20 11:29:42 INFO DAGScheduler: Submitting ResultStage 89 (MapPartitionsRDD[357] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:29:42 INFO MemoryStore: Block broadcast_89 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:29:42 INFO MemoryStore: Block broadcast_89_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:29:42 INFO BlockManagerInfo: Added broadcast_89_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:29:42 INFO SparkContext: Created broadcast 89 from broadcast at DAGScheduler.scala:1200
21/01/20 11:29:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 89 (MapPartitionsRDD[357] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:29:42 INFO TaskSchedulerImpl: Adding task set 89.0 with 1 tasks
21/01/20 11:29:42 WARN TaskSetManager: Stage 89 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:29:42 INFO TaskSetManager: Starting task 0.0 in stage 89.0 (TID 89, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:29:42 INFO Executor: Running task 0.0 in stage 89.0 (TID 89)
21/01/20 11:29:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:42 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:42 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:29:42 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:29:42 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:29:42 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:29:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:29:42 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:29:42 INFO ParquetOutputFormat: Validation is off
21/01/20 11:29:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:29:42 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:29:42 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:29:42 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:29:42 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:29:42 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:29:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:29:43 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112942_0089_m_000000_89' to o3fs://bucket1.vol1/testdata
21/01/20 11:29:43 INFO SparkHadoopMapRedUtil: attempt_20210120112942_0089_m_000000_89: Committed
21/01/20 11:29:43 INFO Executor: Finished task 0.0 in stage 89.0 (TID 89). 2155 bytes result sent to driver
21/01/20 11:29:43 INFO TaskSetManager: Finished task 0.0 in stage 89.0 (TID 89) in 1034 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:29:43 INFO TaskSchedulerImpl: Removed TaskSet 89.0, whose tasks have all completed, from pool 
21/01/20 11:29:43 INFO DAGScheduler: ResultStage 89 (parquet at Generate.java:61) finished in 1.054 s
21/01/20 11:29:43 INFO DAGScheduler: Job 89 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:29:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 89: Stage finished
21/01/20 11:29:43 INFO DAGScheduler: Job 89 finished: parquet at Generate.java:61, took 1.055807 s
21/01/20 11:29:43 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-0eb9112f-9f5f-448e-a2a6-88067e2ea1e4
21/01/20 11:29:43 INFO FileFormatWriter: Write Job da8a5347-8809-44cb-ae1b-be94255cb618 committed.
21/01/20 11:29:43 INFO FileFormatWriter: Finished processing stats for write job da8a5347-8809-44cb-ae1b-be94255cb618.
21/01/20 11:29:44 INFO BlockManagerInfo: Removed broadcast_89_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:29:45 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:45 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:45 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:29:45 INFO DAGScheduler: Got job 90 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:29:45 INFO DAGScheduler: Final stage: ResultStage 90 (parquet at Generate.java:61)
21/01/20 11:29:45 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:29:45 INFO DAGScheduler: Missing parents: List()
21/01/20 11:29:45 INFO DAGScheduler: Submitting ResultStage 90 (MapPartitionsRDD[361] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:29:45 INFO MemoryStore: Block broadcast_90 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:29:45 INFO MemoryStore: Block broadcast_90_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:29:45 INFO BlockManagerInfo: Added broadcast_90_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:29:45 INFO SparkContext: Created broadcast 90 from broadcast at DAGScheduler.scala:1200
21/01/20 11:29:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 90 (MapPartitionsRDD[361] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:29:45 INFO TaskSchedulerImpl: Adding task set 90.0 with 1 tasks
21/01/20 11:29:45 WARN TaskSetManager: Stage 90 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:29:45 INFO TaskSetManager: Starting task 0.0 in stage 90.0 (TID 90, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:29:45 INFO Executor: Running task 0.0 in stage 90.0 (TID 90)
21/01/20 11:29:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:46 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:46 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:29:46 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:29:46 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:29:46 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:29:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:29:46 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:29:46 INFO ParquetOutputFormat: Validation is off
21/01/20 11:29:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:29:46 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:29:46 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:29:46 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:29:46 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:29:46 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:29:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:29:46 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112945_0090_m_000000_90' to o3fs://bucket1.vol1/testdata
21/01/20 11:29:46 INFO SparkHadoopMapRedUtil: attempt_20210120112945_0090_m_000000_90: Committed
21/01/20 11:29:46 INFO Executor: Finished task 0.0 in stage 90.0 (TID 90). 2155 bytes result sent to driver
21/01/20 11:29:46 INFO TaskSetManager: Finished task 0.0 in stage 90.0 (TID 90) in 1039 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:29:46 INFO TaskSchedulerImpl: Removed TaskSet 90.0, whose tasks have all completed, from pool 
21/01/20 11:29:46 INFO DAGScheduler: ResultStage 90 (parquet at Generate.java:61) finished in 1.057 s
21/01/20 11:29:46 INFO DAGScheduler: Job 90 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:29:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 90: Stage finished
21/01/20 11:29:46 INFO DAGScheduler: Job 90 finished: parquet at Generate.java:61, took 1.058760 s
21/01/20 11:29:46 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-e4110b63-c5a5-40e0-a17f-8e98cbd40530
21/01/20 11:29:46 INFO FileFormatWriter: Write Job 004403d4-1371-48a5-aed8-0fdf381b78e2 committed.
21/01/20 11:29:46 INFO FileFormatWriter: Finished processing stats for write job 004403d4-1371-48a5-aed8-0fdf381b78e2.
21/01/20 11:29:48 INFO BlockManagerInfo: Removed broadcast_90_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:29:49 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:49 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:29:49 INFO DAGScheduler: Got job 91 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:29:49 INFO DAGScheduler: Final stage: ResultStage 91 (parquet at Generate.java:61)
21/01/20 11:29:49 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:29:49 INFO DAGScheduler: Missing parents: List()
21/01/20 11:29:49 INFO DAGScheduler: Submitting ResultStage 91 (MapPartitionsRDD[365] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:29:49 INFO MemoryStore: Block broadcast_91 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:29:49 INFO MemoryStore: Block broadcast_91_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:29:49 INFO BlockManagerInfo: Added broadcast_91_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:29:49 INFO SparkContext: Created broadcast 91 from broadcast at DAGScheduler.scala:1200
21/01/20 11:29:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 91 (MapPartitionsRDD[365] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:29:49 INFO TaskSchedulerImpl: Adding task set 91.0 with 1 tasks
21/01/20 11:29:49 WARN TaskSetManager: Stage 91 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:29:49 INFO TaskSetManager: Starting task 0.0 in stage 91.0 (TID 91, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:29:49 INFO Executor: Running task 0.0 in stage 91.0 (TID 91)
21/01/20 11:29:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:49 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:29:49 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:29:49 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:29:49 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:29:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:29:49 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:29:49 INFO ParquetOutputFormat: Validation is off
21/01/20 11:29:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:29:49 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:29:49 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:29:49 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:29:49 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:29:49 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:29:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:29:50 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-6B8B824302C1->27b66227-fed8-4c1d-ab57-510f5a3ce552
21/01/20 11:29:50 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:29:50 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112949_0091_m_000000_91' to o3fs://bucket1.vol1/testdata
21/01/20 11:29:50 INFO SparkHadoopMapRedUtil: attempt_20210120112949_0091_m_000000_91: Committed
21/01/20 11:29:50 INFO Executor: Finished task 0.0 in stage 91.0 (TID 91). 2155 bytes result sent to driver
21/01/20 11:29:50 INFO TaskSetManager: Finished task 0.0 in stage 91.0 (TID 91) in 1013 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:29:50 INFO TaskSchedulerImpl: Removed TaskSet 91.0, whose tasks have all completed, from pool 
21/01/20 11:29:50 INFO DAGScheduler: ResultStage 91 (parquet at Generate.java:61) finished in 1.031 s
21/01/20 11:29:50 INFO DAGScheduler: Job 91 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:29:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 91: Stage finished
21/01/20 11:29:50 INFO DAGScheduler: Job 91 finished: parquet at Generate.java:61, took 1.032947 s
21/01/20 11:29:50 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-ae0d8413-2d22-4a63-ab8e-8b35c0698900
21/01/20 11:29:50 INFO FileFormatWriter: Write Job db36f7cb-ded5-4c22-a7aa-aad71acf1ad4 committed.
21/01/20 11:29:50 INFO FileFormatWriter: Finished processing stats for write job db36f7cb-ded5-4c22-a7aa-aad71acf1ad4.
21/01/20 11:29:51 INFO BlockManagerInfo: Removed broadcast_91_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:29:53 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:53 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:29:53 INFO DAGScheduler: Got job 92 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:29:53 INFO DAGScheduler: Final stage: ResultStage 92 (parquet at Generate.java:61)
21/01/20 11:29:53 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:29:53 INFO DAGScheduler: Missing parents: List()
21/01/20 11:29:53 INFO DAGScheduler: Submitting ResultStage 92 (MapPartitionsRDD[369] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:29:53 INFO MemoryStore: Block broadcast_92 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:29:53 INFO MemoryStore: Block broadcast_92_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:29:53 INFO BlockManagerInfo: Added broadcast_92_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:29:53 INFO SparkContext: Created broadcast 92 from broadcast at DAGScheduler.scala:1200
21/01/20 11:29:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 92 (MapPartitionsRDD[369] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:29:53 INFO TaskSchedulerImpl: Adding task set 92.0 with 1 tasks
21/01/20 11:29:53 WARN TaskSetManager: Stage 92 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:29:53 INFO TaskSetManager: Starting task 0.0 in stage 92.0 (TID 92, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:29:53 INFO Executor: Running task 0.0 in stage 92.0 (TID 92)
21/01/20 11:29:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:53 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:29:53 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:29:53 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:29:53 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:29:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:29:53 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:29:53 INFO ParquetOutputFormat: Validation is off
21/01/20 11:29:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:29:53 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:29:53 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:29:53 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:29:53 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:29:53 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:29:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:29:54 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112953_0092_m_000000_92' to o3fs://bucket1.vol1/testdata
21/01/20 11:29:54 INFO SparkHadoopMapRedUtil: attempt_20210120112953_0092_m_000000_92: Committed
21/01/20 11:29:54 INFO Executor: Finished task 0.0 in stage 92.0 (TID 92). 2155 bytes result sent to driver
21/01/20 11:29:54 INFO TaskSetManager: Finished task 0.0 in stage 92.0 (TID 92) in 1024 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:29:54 INFO TaskSchedulerImpl: Removed TaskSet 92.0, whose tasks have all completed, from pool 
21/01/20 11:29:54 INFO DAGScheduler: ResultStage 92 (parquet at Generate.java:61) finished in 1.066 s
21/01/20 11:29:54 INFO DAGScheduler: Job 92 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:29:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 92: Stage finished
21/01/20 11:29:54 INFO DAGScheduler: Job 92 finished: parquet at Generate.java:61, took 1.067883 s
21/01/20 11:29:54 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-9e105bbe-a8a5-4bec-9d83-c54fb412e499
21/01/20 11:29:54 INFO FileFormatWriter: Write Job bcd82d96-5923-4cfb-963a-c0bbcefa998a committed.
21/01/20 11:29:54 INFO FileFormatWriter: Finished processing stats for write job bcd82d96-5923-4cfb-963a-c0bbcefa998a.
21/01/20 11:29:54 INFO BlockManagerInfo: Removed broadcast_92_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:29:56 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:56 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:56 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:29:56 INFO DAGScheduler: Got job 93 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:29:56 INFO DAGScheduler: Final stage: ResultStage 93 (parquet at Generate.java:61)
21/01/20 11:29:56 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:29:56 INFO DAGScheduler: Missing parents: List()
21/01/20 11:29:56 INFO DAGScheduler: Submitting ResultStage 93 (MapPartitionsRDD[373] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:29:56 INFO MemoryStore: Block broadcast_93 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:29:56 INFO MemoryStore: Block broadcast_93_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:29:56 INFO BlockManagerInfo: Added broadcast_93_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:29:56 INFO SparkContext: Created broadcast 93 from broadcast at DAGScheduler.scala:1200
21/01/20 11:29:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 93 (MapPartitionsRDD[373] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:29:56 INFO TaskSchedulerImpl: Adding task set 93.0 with 1 tasks
21/01/20 11:29:56 WARN TaskSetManager: Stage 93 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:29:56 INFO TaskSetManager: Starting task 0.0 in stage 93.0 (TID 93, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:29:56 INFO Executor: Running task 0.0 in stage 93.0 (TID 93)
21/01/20 11:29:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:57 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:29:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:29:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:29:57 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:29:57 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:29:57 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:29:57 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:29:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:29:57 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:29:57 INFO ParquetOutputFormat: Validation is off
21/01/20 11:29:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:29:57 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:29:57 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:29:57 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:29:57 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:29:57 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:29:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:29:57 INFO FileOutputCommitter: Saved output of task 'attempt_20210120112956_0093_m_000000_93' to o3fs://bucket1.vol1/testdata
21/01/20 11:29:57 INFO SparkHadoopMapRedUtil: attempt_20210120112956_0093_m_000000_93: Committed
21/01/20 11:29:57 INFO Executor: Finished task 0.0 in stage 93.0 (TID 93). 2155 bytes result sent to driver
21/01/20 11:29:57 INFO TaskSetManager: Finished task 0.0 in stage 93.0 (TID 93) in 1071 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:29:57 INFO TaskSchedulerImpl: Removed TaskSet 93.0, whose tasks have all completed, from pool 
21/01/20 11:29:57 INFO DAGScheduler: ResultStage 93 (parquet at Generate.java:61) finished in 1.090 s
21/01/20 11:29:57 INFO DAGScheduler: Job 93 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:29:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 93: Stage finished
21/01/20 11:29:57 INFO DAGScheduler: Job 93 finished: parquet at Generate.java:61, took 1.091373 s
21/01/20 11:29:57 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-89277888-9245-4eb2-912b-78a9ba4220ea
21/01/20 11:29:57 INFO FileFormatWriter: Write Job 0d1e1799-5052-4426-9e67-fdf5f6f7c920 committed.
21/01/20 11:29:57 INFO FileFormatWriter: Finished processing stats for write job 0d1e1799-5052-4426-9e67-fdf5f6f7c920.
21/01/20 11:29:59 INFO BlockManagerInfo: Removed broadcast_93_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:30:00 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:00 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:30:00 INFO DAGScheduler: Got job 94 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:30:00 INFO DAGScheduler: Final stage: ResultStage 94 (parquet at Generate.java:61)
21/01/20 11:30:00 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:30:00 INFO DAGScheduler: Missing parents: List()
21/01/20 11:30:00 INFO DAGScheduler: Submitting ResultStage 94 (MapPartitionsRDD[377] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:30:00 INFO MemoryStore: Block broadcast_94 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:30:00 INFO MemoryStore: Block broadcast_94_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:30:00 INFO BlockManagerInfo: Added broadcast_94_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:30:00 INFO SparkContext: Created broadcast 94 from broadcast at DAGScheduler.scala:1200
21/01/20 11:30:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 94 (MapPartitionsRDD[377] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:30:00 INFO TaskSchedulerImpl: Adding task set 94.0 with 1 tasks
21/01/20 11:30:00 WARN TaskSetManager: Stage 94 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:30:00 INFO TaskSetManager: Starting task 0.0 in stage 94.0 (TID 94, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:30:00 INFO Executor: Running task 0.0 in stage 94.0 (TID 94)
21/01/20 11:30:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:00 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:30:00 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:30:00 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:30:00 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:30:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:30:00 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:30:00 INFO ParquetOutputFormat: Validation is off
21/01/20 11:30:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:30:00 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:30:00 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:30:00 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:30:00 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:30:00 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:30:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:30:01 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-FE3329FF4CF6->27b66227-fed8-4c1d-ab57-510f5a3ce552
21/01/20 11:30:01 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:30:01 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113000_0094_m_000000_94' to o3fs://bucket1.vol1/testdata
21/01/20 11:30:01 INFO SparkHadoopMapRedUtil: attempt_20210120113000_0094_m_000000_94: Committed
21/01/20 11:30:01 INFO Executor: Finished task 0.0 in stage 94.0 (TID 94). 2155 bytes result sent to driver
21/01/20 11:30:01 INFO TaskSetManager: Finished task 0.0 in stage 94.0 (TID 94) in 936 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:30:01 INFO TaskSchedulerImpl: Removed TaskSet 94.0, whose tasks have all completed, from pool 
21/01/20 11:30:01 INFO DAGScheduler: ResultStage 94 (parquet at Generate.java:61) finished in 0.976 s
21/01/20 11:30:01 INFO DAGScheduler: Job 94 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:30:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 94: Stage finished
21/01/20 11:30:01 INFO DAGScheduler: Job 94 finished: parquet at Generate.java:61, took 0.977547 s
21/01/20 11:30:01 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-616688cf-3f2a-421f-b58a-22adf25ef6ae
21/01/20 11:30:01 INFO FileFormatWriter: Write Job d794af85-1aae-47de-a36e-86c487e4e996 committed.
21/01/20 11:30:01 INFO FileFormatWriter: Finished processing stats for write job d794af85-1aae-47de-a36e-86c487e4e996.
21/01/20 11:30:01 INFO BlockManagerInfo: Removed broadcast_94_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:30:04 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:04 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:30:04 INFO DAGScheduler: Got job 95 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:30:04 INFO DAGScheduler: Final stage: ResultStage 95 (parquet at Generate.java:61)
21/01/20 11:30:04 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:30:04 INFO DAGScheduler: Missing parents: List()
21/01/20 11:30:04 INFO DAGScheduler: Submitting ResultStage 95 (MapPartitionsRDD[381] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:30:04 INFO MemoryStore: Block broadcast_95 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:30:04 INFO MemoryStore: Block broadcast_95_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:30:04 INFO BlockManagerInfo: Added broadcast_95_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:30:04 INFO SparkContext: Created broadcast 95 from broadcast at DAGScheduler.scala:1200
21/01/20 11:30:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 95 (MapPartitionsRDD[381] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:30:04 INFO TaskSchedulerImpl: Adding task set 95.0 with 1 tasks
21/01/20 11:30:04 WARN TaskSetManager: Stage 95 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:30:04 INFO TaskSetManager: Starting task 0.0 in stage 95.0 (TID 95, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:30:04 INFO Executor: Running task 0.0 in stage 95.0 (TID 95)
21/01/20 11:30:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:04 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:30:04 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:30:04 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:30:04 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:30:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:30:04 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:30:04 INFO ParquetOutputFormat: Validation is off
21/01/20 11:30:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:30:04 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:30:04 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:30:04 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:30:04 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:30:04 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:30:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:30:05 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113004_0095_m_000000_95' to o3fs://bucket1.vol1/testdata
21/01/20 11:30:05 INFO SparkHadoopMapRedUtil: attempt_20210120113004_0095_m_000000_95: Committed
21/01/20 11:30:05 INFO Executor: Finished task 0.0 in stage 95.0 (TID 95). 2155 bytes result sent to driver
21/01/20 11:30:05 INFO TaskSetManager: Finished task 0.0 in stage 95.0 (TID 95) in 973 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:30:05 INFO TaskSchedulerImpl: Removed TaskSet 95.0, whose tasks have all completed, from pool 
21/01/20 11:30:05 INFO DAGScheduler: ResultStage 95 (parquet at Generate.java:61) finished in 0.991 s
21/01/20 11:30:05 INFO DAGScheduler: Job 95 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:30:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 95: Stage finished
21/01/20 11:30:05 INFO DAGScheduler: Job 95 finished: parquet at Generate.java:61, took 0.993618 s
21/01/20 11:30:05 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-a9bc7075-38a9-43f2-b8a0-9bb3eb32a3fa
21/01/20 11:30:05 INFO FileFormatWriter: Write Job b237e873-ce84-48af-b022-4e4bef7f59bc committed.
21/01/20 11:30:05 INFO FileFormatWriter: Finished processing stats for write job b237e873-ce84-48af-b022-4e4bef7f59bc.
21/01/20 11:30:06 INFO BlockManagerInfo: Removed broadcast_95_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:30:07 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:07 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:07 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:30:07 INFO DAGScheduler: Got job 96 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:30:07 INFO DAGScheduler: Final stage: ResultStage 96 (parquet at Generate.java:61)
21/01/20 11:30:07 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:30:07 INFO DAGScheduler: Missing parents: List()
21/01/20 11:30:07 INFO DAGScheduler: Submitting ResultStage 96 (MapPartitionsRDD[385] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:30:07 INFO MemoryStore: Block broadcast_96 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:30:07 INFO MemoryStore: Block broadcast_96_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:30:07 INFO BlockManagerInfo: Added broadcast_96_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:30:07 INFO SparkContext: Created broadcast 96 from broadcast at DAGScheduler.scala:1200
21/01/20 11:30:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 96 (MapPartitionsRDD[385] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:30:07 INFO TaskSchedulerImpl: Adding task set 96.0 with 1 tasks
21/01/20 11:30:07 WARN TaskSetManager: Stage 96 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:30:07 INFO TaskSetManager: Starting task 0.0 in stage 96.0 (TID 96, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:30:07 INFO Executor: Running task 0.0 in stage 96.0 (TID 96)
21/01/20 11:30:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:07 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:07 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:30:07 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:30:07 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:30:07 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:30:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:30:07 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:30:07 INFO ParquetOutputFormat: Validation is off
21/01/20 11:30:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:30:07 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:30:07 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:30:07 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:30:07 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:30:07 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:30:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:30:08 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113007_0096_m_000000_96' to o3fs://bucket1.vol1/testdata
21/01/20 11:30:08 INFO SparkHadoopMapRedUtil: attempt_20210120113007_0096_m_000000_96: Committed
21/01/20 11:30:08 INFO Executor: Finished task 0.0 in stage 96.0 (TID 96). 2155 bytes result sent to driver
21/01/20 11:30:08 INFO TaskSetManager: Finished task 0.0 in stage 96.0 (TID 96) in 937 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:30:08 INFO TaskSchedulerImpl: Removed TaskSet 96.0, whose tasks have all completed, from pool 
21/01/20 11:30:08 INFO DAGScheduler: ResultStage 96 (parquet at Generate.java:61) finished in 0.956 s
21/01/20 11:30:08 INFO DAGScheduler: Job 96 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:30:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 96: Stage finished
21/01/20 11:30:08 INFO DAGScheduler: Job 96 finished: parquet at Generate.java:61, took 0.958389 s
21/01/20 11:30:08 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-60caad4d-e895-419e-b73b-8ef3b477c365
21/01/20 11:30:08 INFO FileFormatWriter: Write Job 54da94c2-a3cc-463b-8268-4f685362603f committed.
21/01/20 11:30:08 INFO FileFormatWriter: Finished processing stats for write job 54da94c2-a3cc-463b-8268-4f685362603f.
21/01/20 11:30:08 INFO BlockManagerInfo: Removed broadcast_96_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:30:11 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:11 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:30:11 INFO DAGScheduler: Got job 97 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:30:11 INFO DAGScheduler: Final stage: ResultStage 97 (parquet at Generate.java:61)
21/01/20 11:30:11 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:30:11 INFO DAGScheduler: Missing parents: List()
21/01/20 11:30:11 INFO DAGScheduler: Submitting ResultStage 97 (MapPartitionsRDD[389] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:30:11 INFO MemoryStore: Block broadcast_97 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:30:11 INFO MemoryStore: Block broadcast_97_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:30:11 INFO BlockManagerInfo: Added broadcast_97_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:30:11 INFO SparkContext: Created broadcast 97 from broadcast at DAGScheduler.scala:1200
21/01/20 11:30:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 97 (MapPartitionsRDD[389] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:30:11 INFO TaskSchedulerImpl: Adding task set 97.0 with 1 tasks
21/01/20 11:30:11 WARN TaskSetManager: Stage 97 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:30:11 INFO TaskSetManager: Starting task 0.0 in stage 97.0 (TID 97, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:30:11 INFO Executor: Running task 0.0 in stage 97.0 (TID 97)
21/01/20 11:30:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:11 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:30:11 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:30:11 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:30:11 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:30:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:30:11 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:30:11 INFO ParquetOutputFormat: Validation is off
21/01/20 11:30:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:30:11 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:30:11 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:30:11 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:30:11 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:30:11 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:30:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:30:12 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113011_0097_m_000000_97' to o3fs://bucket1.vol1/testdata
21/01/20 11:30:12 INFO SparkHadoopMapRedUtil: attempt_20210120113011_0097_m_000000_97: Committed
21/01/20 11:30:12 INFO Executor: Finished task 0.0 in stage 97.0 (TID 97). 2155 bytes result sent to driver
21/01/20 11:30:12 INFO TaskSetManager: Finished task 0.0 in stage 97.0 (TID 97) in 1041 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:30:12 INFO TaskSchedulerImpl: Removed TaskSet 97.0, whose tasks have all completed, from pool 
21/01/20 11:30:12 INFO DAGScheduler: ResultStage 97 (parquet at Generate.java:61) finished in 1.060 s
21/01/20 11:30:12 INFO DAGScheduler: Job 97 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:30:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 97: Stage finished
21/01/20 11:30:12 INFO DAGScheduler: Job 97 finished: parquet at Generate.java:61, took 1.064620 s
21/01/20 11:30:12 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-b340736e-2086-4dfb-a3bb-f68d6e3850bd
21/01/20 11:30:12 INFO FileFormatWriter: Write Job c7d4a5b1-3f83-4aca-bb66-005e62a03d5c committed.
21/01/20 11:30:12 INFO FileFormatWriter: Finished processing stats for write job c7d4a5b1-3f83-4aca-bb66-005e62a03d5c.
21/01/20 11:30:13 INFO BlockManagerInfo: Removed broadcast_97_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:30:15 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:15 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:15 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:30:15 INFO DAGScheduler: Got job 98 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:30:15 INFO DAGScheduler: Final stage: ResultStage 98 (parquet at Generate.java:61)
21/01/20 11:30:15 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:30:15 INFO DAGScheduler: Missing parents: List()
21/01/20 11:30:15 INFO DAGScheduler: Submitting ResultStage 98 (MapPartitionsRDD[393] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:30:15 INFO MemoryStore: Block broadcast_98 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:30:15 INFO MemoryStore: Block broadcast_98_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:30:15 INFO BlockManagerInfo: Added broadcast_98_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:30:15 INFO SparkContext: Created broadcast 98 from broadcast at DAGScheduler.scala:1200
21/01/20 11:30:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 98 (MapPartitionsRDD[393] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:30:15 INFO TaskSchedulerImpl: Adding task set 98.0 with 1 tasks
21/01/20 11:30:15 WARN TaskSetManager: Stage 98 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:30:15 INFO TaskSetManager: Starting task 0.0 in stage 98.0 (TID 98, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:30:15 INFO Executor: Running task 0.0 in stage 98.0 (TID 98)
21/01/20 11:30:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:15 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:15 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:30:15 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:30:15 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:30:15 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:30:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:30:15 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:30:15 INFO ParquetOutputFormat: Validation is off
21/01/20 11:30:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:30:15 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:30:15 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:30:15 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:30:15 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:30:15 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:30:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:30:16 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113015_0098_m_000000_98' to o3fs://bucket1.vol1/testdata
21/01/20 11:30:16 INFO SparkHadoopMapRedUtil: attempt_20210120113015_0098_m_000000_98: Committed
21/01/20 11:30:16 INFO Executor: Finished task 0.0 in stage 98.0 (TID 98). 2155 bytes result sent to driver
21/01/20 11:30:16 INFO TaskSetManager: Finished task 0.0 in stage 98.0 (TID 98) in 905 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:30:16 INFO TaskSchedulerImpl: Removed TaskSet 98.0, whose tasks have all completed, from pool 
21/01/20 11:30:16 INFO DAGScheduler: ResultStage 98 (parquet at Generate.java:61) finished in 0.948 s
21/01/20 11:30:16 INFO DAGScheduler: Job 98 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:30:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 98: Stage finished
21/01/20 11:30:16 INFO DAGScheduler: Job 98 finished: parquet at Generate.java:61, took 0.950440 s
21/01/20 11:30:16 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-9e043ba3-3a98-470c-b662-cfa1856983ed
21/01/20 11:30:16 INFO FileFormatWriter: Write Job 1d03dc2b-f5d4-4c8f-b79e-24f387f12e41 committed.
21/01/20 11:30:16 INFO FileFormatWriter: Finished processing stats for write job 1d03dc2b-f5d4-4c8f-b79e-24f387f12e41.
21/01/20 11:30:16 INFO BlockManagerInfo: Removed broadcast_98_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:30:18 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:18 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:18 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:30:18 INFO DAGScheduler: Got job 99 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:30:18 INFO DAGScheduler: Final stage: ResultStage 99 (parquet at Generate.java:61)
21/01/20 11:30:18 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:30:18 INFO DAGScheduler: Missing parents: List()
21/01/20 11:30:18 INFO DAGScheduler: Submitting ResultStage 99 (MapPartitionsRDD[397] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:30:18 INFO MemoryStore: Block broadcast_99 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:30:18 INFO MemoryStore: Block broadcast_99_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:30:18 INFO BlockManagerInfo: Added broadcast_99_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:30:18 INFO SparkContext: Created broadcast 99 from broadcast at DAGScheduler.scala:1200
21/01/20 11:30:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 99 (MapPartitionsRDD[397] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:30:18 INFO TaskSchedulerImpl: Adding task set 99.0 with 1 tasks
21/01/20 11:30:18 WARN TaskSetManager: Stage 99 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:30:18 INFO TaskSetManager: Starting task 0.0 in stage 99.0 (TID 99, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:30:18 INFO Executor: Running task 0.0 in stage 99.0 (TID 99)
21/01/20 11:30:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:18 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:18 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:30:18 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:30:18 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:30:18 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:30:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:30:18 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:30:18 INFO ParquetOutputFormat: Validation is off
21/01/20 11:30:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:30:18 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:30:18 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:30:18 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:30:18 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:30:18 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:30:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:30:19 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-70ACED11CF88->8df234bd-f131-4d8e-953a-b9eec8633221
21/01/20 11:30:19 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:30:19 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113018_0099_m_000000_99' to o3fs://bucket1.vol1/testdata
21/01/20 11:30:19 INFO SparkHadoopMapRedUtil: attempt_20210120113018_0099_m_000000_99: Committed
21/01/20 11:30:19 INFO Executor: Finished task 0.0 in stage 99.0 (TID 99). 2155 bytes result sent to driver
21/01/20 11:30:19 INFO TaskSetManager: Finished task 0.0 in stage 99.0 (TID 99) in 1053 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:30:19 INFO TaskSchedulerImpl: Removed TaskSet 99.0, whose tasks have all completed, from pool 
21/01/20 11:30:19 INFO DAGScheduler: ResultStage 99 (parquet at Generate.java:61) finished in 1.071 s
21/01/20 11:30:19 INFO DAGScheduler: Job 99 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:30:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 99: Stage finished
21/01/20 11:30:19 INFO DAGScheduler: Job 99 finished: parquet at Generate.java:61, took 1.072704 s
21/01/20 11:30:19 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-73fd5ee6-2472-4535-95ec-76aed5253498
21/01/20 11:30:19 INFO FileFormatWriter: Write Job 25d20515-2983-49c7-ad4c-e46bda0daec4 committed.
21/01/20 11:30:19 INFO FileFormatWriter: Finished processing stats for write job 25d20515-2983-49c7-ad4c-e46bda0daec4.
21/01/20 11:30:20 INFO BlockManagerInfo: Removed broadcast_99_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:30:22 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:22 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:22 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:30:22 INFO DAGScheduler: Got job 100 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:30:22 INFO DAGScheduler: Final stage: ResultStage 100 (parquet at Generate.java:61)
21/01/20 11:30:22 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:30:22 INFO DAGScheduler: Missing parents: List()
21/01/20 11:30:22 INFO DAGScheduler: Submitting ResultStage 100 (MapPartitionsRDD[401] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:30:22 INFO MemoryStore: Block broadcast_100 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:30:22 INFO MemoryStore: Block broadcast_100_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:30:22 INFO BlockManagerInfo: Added broadcast_100_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:30:22 INFO SparkContext: Created broadcast 100 from broadcast at DAGScheduler.scala:1200
21/01/20 11:30:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 100 (MapPartitionsRDD[401] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:30:22 INFO TaskSchedulerImpl: Adding task set 100.0 with 1 tasks
21/01/20 11:30:22 WARN TaskSetManager: Stage 100 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:30:22 INFO TaskSetManager: Starting task 0.0 in stage 100.0 (TID 100, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:30:22 INFO Executor: Running task 0.0 in stage 100.0 (TID 100)
21/01/20 11:30:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:22 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:22 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:30:22 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:30:22 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:30:22 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:30:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:30:22 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:30:22 INFO ParquetOutputFormat: Validation is off
21/01/20 11:30:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:30:22 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:30:22 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:30:22 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:30:22 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:30:22 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:30:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:30:23 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113022_0100_m_000000_100' to o3fs://bucket1.vol1/testdata
21/01/20 11:30:23 INFO SparkHadoopMapRedUtil: attempt_20210120113022_0100_m_000000_100: Committed
21/01/20 11:30:23 INFO Executor: Finished task 0.0 in stage 100.0 (TID 100). 2155 bytes result sent to driver
21/01/20 11:30:23 INFO TaskSetManager: Finished task 0.0 in stage 100.0 (TID 100) in 1081 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:30:23 INFO TaskSchedulerImpl: Removed TaskSet 100.0, whose tasks have all completed, from pool 
21/01/20 11:30:23 INFO DAGScheduler: ResultStage 100 (parquet at Generate.java:61) finished in 1.100 s
21/01/20 11:30:23 INFO DAGScheduler: Job 100 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:30:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 100: Stage finished
21/01/20 11:30:23 INFO DAGScheduler: Job 100 finished: parquet at Generate.java:61, took 1.100709 s
21/01/20 11:30:23 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-c5f8ed31-0962-4de8-9e0f-3ec148bdca8e
21/01/20 11:30:23 INFO FileFormatWriter: Write Job a95565cd-cc47-43fa-8707-a92a0acb39b6 committed.
21/01/20 11:30:23 INFO FileFormatWriter: Finished processing stats for write job a95565cd-cc47-43fa-8707-a92a0acb39b6.
21/01/20 11:30:24 INFO BlockManagerInfo: Removed broadcast_100_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:30:25 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:25 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:26 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:30:26 INFO DAGScheduler: Got job 101 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:30:26 INFO DAGScheduler: Final stage: ResultStage 101 (parquet at Generate.java:61)
21/01/20 11:30:26 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:30:26 INFO DAGScheduler: Missing parents: List()
21/01/20 11:30:26 INFO DAGScheduler: Submitting ResultStage 101 (MapPartitionsRDD[405] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:30:26 INFO MemoryStore: Block broadcast_101 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:30:26 INFO MemoryStore: Block broadcast_101_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:30:26 INFO BlockManagerInfo: Added broadcast_101_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:30:26 INFO SparkContext: Created broadcast 101 from broadcast at DAGScheduler.scala:1200
21/01/20 11:30:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 101 (MapPartitionsRDD[405] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:30:26 INFO TaskSchedulerImpl: Adding task set 101.0 with 1 tasks
21/01/20 11:30:26 WARN TaskSetManager: Stage 101 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:30:26 INFO TaskSetManager: Starting task 0.0 in stage 101.0 (TID 101, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:30:26 INFO Executor: Running task 0.0 in stage 101.0 (TID 101)
21/01/20 11:30:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:26 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:26 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:30:26 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:30:26 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:30:26 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:30:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:30:26 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:30:26 INFO ParquetOutputFormat: Validation is off
21/01/20 11:30:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:30:26 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:30:26 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:30:26 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:30:26 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:30:26 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:30:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:30:27 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113026_0101_m_000000_101' to o3fs://bucket1.vol1/testdata
21/01/20 11:30:27 INFO SparkHadoopMapRedUtil: attempt_20210120113026_0101_m_000000_101: Committed
21/01/20 11:30:27 INFO Executor: Finished task 0.0 in stage 101.0 (TID 101). 2155 bytes result sent to driver
21/01/20 11:30:27 INFO TaskSetManager: Finished task 0.0 in stage 101.0 (TID 101) in 975 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:30:27 INFO TaskSchedulerImpl: Removed TaskSet 101.0, whose tasks have all completed, from pool 
21/01/20 11:30:27 INFO DAGScheduler: ResultStage 101 (parquet at Generate.java:61) finished in 0.993 s
21/01/20 11:30:27 INFO DAGScheduler: Job 101 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:30:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 101: Stage finished
21/01/20 11:30:27 INFO DAGScheduler: Job 101 finished: parquet at Generate.java:61, took 0.994323 s
21/01/20 11:30:27 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-72f8db04-1f59-4703-b475-1eb9e5b15b62
21/01/20 11:30:27 INFO FileFormatWriter: Write Job b29b2fa4-319a-455a-bb31-2b528b9a95aa committed.
21/01/20 11:30:27 INFO FileFormatWriter: Finished processing stats for write job b29b2fa4-319a-455a-bb31-2b528b9a95aa.
21/01/20 11:30:28 INFO BlockManagerInfo: Removed broadcast_101_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:30:29 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:29 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:30:29 INFO DAGScheduler: Got job 102 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:30:29 INFO DAGScheduler: Final stage: ResultStage 102 (parquet at Generate.java:61)
21/01/20 11:30:29 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:30:29 INFO DAGScheduler: Missing parents: List()
21/01/20 11:30:29 INFO DAGScheduler: Submitting ResultStage 102 (MapPartitionsRDD[409] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:30:29 INFO MemoryStore: Block broadcast_102 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:30:29 INFO MemoryStore: Block broadcast_102_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:30:29 INFO BlockManagerInfo: Added broadcast_102_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:30:29 INFO SparkContext: Created broadcast 102 from broadcast at DAGScheduler.scala:1200
21/01/20 11:30:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 102 (MapPartitionsRDD[409] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:30:29 INFO TaskSchedulerImpl: Adding task set 102.0 with 1 tasks
21/01/20 11:30:29 WARN TaskSetManager: Stage 102 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:30:29 INFO TaskSetManager: Starting task 0.0 in stage 102.0 (TID 102, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:30:29 INFO Executor: Running task 0.0 in stage 102.0 (TID 102)
21/01/20 11:30:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:29 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:30:29 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:30:29 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:30:29 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:30:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:30:29 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:30:29 INFO ParquetOutputFormat: Validation is off
21/01/20 11:30:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:30:29 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:30:29 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:30:29 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:30:29 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:30:29 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:30:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:30:30 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113029_0102_m_000000_102' to o3fs://bucket1.vol1/testdata
21/01/20 11:30:30 INFO SparkHadoopMapRedUtil: attempt_20210120113029_0102_m_000000_102: Committed
21/01/20 11:30:30 INFO Executor: Finished task 0.0 in stage 102.0 (TID 102). 2155 bytes result sent to driver
21/01/20 11:30:30 INFO TaskSetManager: Finished task 0.0 in stage 102.0 (TID 102) in 883 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:30:30 INFO TaskSchedulerImpl: Removed TaskSet 102.0, whose tasks have all completed, from pool 
21/01/20 11:30:30 INFO DAGScheduler: ResultStage 102 (parquet at Generate.java:61) finished in 0.924 s
21/01/20 11:30:30 INFO DAGScheduler: Job 102 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:30:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 102: Stage finished
21/01/20 11:30:30 INFO DAGScheduler: Job 102 finished: parquet at Generate.java:61, took 0.926444 s
21/01/20 11:30:30 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-6f79d74a-2d99-44ee-bf97-c08ed19bf03b
21/01/20 11:30:30 INFO FileFormatWriter: Write Job ff8d28d7-0958-4374-ab74-fc722ffbc489 committed.
21/01/20 11:30:30 INFO FileFormatWriter: Finished processing stats for write job ff8d28d7-0958-4374-ab74-fc722ffbc489.
21/01/20 11:30:30 INFO BlockManagerInfo: Removed broadcast_102_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:30:33 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:33 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:33 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:30:33 INFO DAGScheduler: Got job 103 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:30:33 INFO DAGScheduler: Final stage: ResultStage 103 (parquet at Generate.java:61)
21/01/20 11:30:33 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:30:33 INFO DAGScheduler: Missing parents: List()
21/01/20 11:30:33 INFO DAGScheduler: Submitting ResultStage 103 (MapPartitionsRDD[413] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:30:33 INFO MemoryStore: Block broadcast_103 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:30:33 INFO MemoryStore: Block broadcast_103_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:30:33 INFO BlockManagerInfo: Added broadcast_103_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:30:33 INFO SparkContext: Created broadcast 103 from broadcast at DAGScheduler.scala:1200
21/01/20 11:30:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 103 (MapPartitionsRDD[413] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:30:33 INFO TaskSchedulerImpl: Adding task set 103.0 with 1 tasks
21/01/20 11:30:33 WARN TaskSetManager: Stage 103 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:30:33 INFO TaskSetManager: Starting task 0.0 in stage 103.0 (TID 103, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:30:33 INFO Executor: Running task 0.0 in stage 103.0 (TID 103)
21/01/20 11:30:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:33 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:33 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:30:33 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:30:33 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:30:33 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:30:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:30:33 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:30:33 INFO ParquetOutputFormat: Validation is off
21/01/20 11:30:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:30:33 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:30:33 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:30:33 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:30:33 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:30:33 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:30:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:30:34 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113033_0103_m_000000_103' to o3fs://bucket1.vol1/testdata
21/01/20 11:30:34 INFO SparkHadoopMapRedUtil: attempt_20210120113033_0103_m_000000_103: Committed
21/01/20 11:30:34 INFO Executor: Finished task 0.0 in stage 103.0 (TID 103). 2155 bytes result sent to driver
21/01/20 11:30:34 INFO TaskSetManager: Finished task 0.0 in stage 103.0 (TID 103) in 1026 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:30:34 INFO TaskSchedulerImpl: Removed TaskSet 103.0, whose tasks have all completed, from pool 
21/01/20 11:30:34 INFO DAGScheduler: ResultStage 103 (parquet at Generate.java:61) finished in 1.043 s
21/01/20 11:30:34 INFO DAGScheduler: Job 103 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:30:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 103: Stage finished
21/01/20 11:30:34 INFO DAGScheduler: Job 103 finished: parquet at Generate.java:61, took 1.045488 s
21/01/20 11:30:34 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-859f77b6-ad61-4147-932c-f78303c844c4
21/01/20 11:30:34 INFO FileFormatWriter: Write Job 03bf2143-1bdc-4175-b1a3-e61b07c7fd6c committed.
21/01/20 11:30:34 INFO FileFormatWriter: Finished processing stats for write job 03bf2143-1bdc-4175-b1a3-e61b07c7fd6c.
21/01/20 11:30:35 INFO BlockManagerInfo: Removed broadcast_103_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:30:36 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:36 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:36 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:30:36 INFO DAGScheduler: Got job 104 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:30:36 INFO DAGScheduler: Final stage: ResultStage 104 (parquet at Generate.java:61)
21/01/20 11:30:36 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:30:36 INFO DAGScheduler: Missing parents: List()
21/01/20 11:30:36 INFO DAGScheduler: Submitting ResultStage 104 (MapPartitionsRDD[417] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:30:36 INFO MemoryStore: Block broadcast_104 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:30:36 INFO MemoryStore: Block broadcast_104_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:30:36 INFO BlockManagerInfo: Added broadcast_104_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:30:36 INFO SparkContext: Created broadcast 104 from broadcast at DAGScheduler.scala:1200
21/01/20 11:30:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 104 (MapPartitionsRDD[417] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:30:36 INFO TaskSchedulerImpl: Adding task set 104.0 with 1 tasks
21/01/20 11:30:36 WARN TaskSetManager: Stage 104 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:30:36 INFO TaskSetManager: Starting task 0.0 in stage 104.0 (TID 104, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:30:36 INFO Executor: Running task 0.0 in stage 104.0 (TID 104)
21/01/20 11:30:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:37 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:37 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:30:37 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:30:37 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:30:37 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:30:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:30:37 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:30:37 INFO ParquetOutputFormat: Validation is off
21/01/20 11:30:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:30:37 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:30:37 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:30:37 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:30:37 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:30:37 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:30:37 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:30:37 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113036_0104_m_000000_104' to o3fs://bucket1.vol1/testdata
21/01/20 11:30:37 INFO SparkHadoopMapRedUtil: attempt_20210120113036_0104_m_000000_104: Committed
21/01/20 11:30:37 INFO Executor: Finished task 0.0 in stage 104.0 (TID 104). 2155 bytes result sent to driver
21/01/20 11:30:37 INFO TaskSetManager: Finished task 0.0 in stage 104.0 (TID 104) in 794 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:30:37 INFO TaskSchedulerImpl: Removed TaskSet 104.0, whose tasks have all completed, from pool 
21/01/20 11:30:37 INFO DAGScheduler: ResultStage 104 (parquet at Generate.java:61) finished in 0.811 s
21/01/20 11:30:37 INFO DAGScheduler: Job 104 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:30:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 104: Stage finished
21/01/20 11:30:37 INFO DAGScheduler: Job 104 finished: parquet at Generate.java:61, took 0.835645 s
21/01/20 11:30:37 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-fe9d4979-30ee-4e7b-98e1-5ccf8f675863
21/01/20 11:30:37 INFO FileFormatWriter: Write Job 9ac29bc7-1f9e-4cc9-a648-4fe63c4da172 committed.
21/01/20 11:30:37 INFO FileFormatWriter: Finished processing stats for write job 9ac29bc7-1f9e-4cc9-a648-4fe63c4da172.
21/01/20 11:30:37 INFO BlockManagerInfo: Removed broadcast_104_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:30:40 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:40 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:40 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:30:40 INFO DAGScheduler: Got job 105 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:30:40 INFO DAGScheduler: Final stage: ResultStage 105 (parquet at Generate.java:61)
21/01/20 11:30:40 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:30:40 INFO DAGScheduler: Missing parents: List()
21/01/20 11:30:40 INFO DAGScheduler: Submitting ResultStage 105 (MapPartitionsRDD[421] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:30:40 INFO MemoryStore: Block broadcast_105 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:30:40 INFO MemoryStore: Block broadcast_105_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:30:40 INFO BlockManagerInfo: Added broadcast_105_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:30:40 INFO SparkContext: Created broadcast 105 from broadcast at DAGScheduler.scala:1200
21/01/20 11:30:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 105 (MapPartitionsRDD[421] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:30:40 INFO TaskSchedulerImpl: Adding task set 105.0 with 1 tasks
21/01/20 11:30:40 WARN TaskSetManager: Stage 105 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:30:40 INFO TaskSetManager: Starting task 0.0 in stage 105.0 (TID 105, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:30:40 INFO Executor: Running task 0.0 in stage 105.0 (TID 105)
21/01/20 11:30:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:40 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:40 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:30:40 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:30:40 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:30:40 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:30:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:30:40 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:30:40 INFO ParquetOutputFormat: Validation is off
21/01/20 11:30:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:30:40 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:30:40 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:30:40 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:30:40 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:30:40 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:30:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:30:41 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113040_0105_m_000000_105' to o3fs://bucket1.vol1/testdata
21/01/20 11:30:41 INFO SparkHadoopMapRedUtil: attempt_20210120113040_0105_m_000000_105: Committed
21/01/20 11:30:41 INFO Executor: Finished task 0.0 in stage 105.0 (TID 105). 2155 bytes result sent to driver
21/01/20 11:30:41 INFO TaskSetManager: Finished task 0.0 in stage 105.0 (TID 105) in 1074 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:30:41 INFO TaskSchedulerImpl: Removed TaskSet 105.0, whose tasks have all completed, from pool 
21/01/20 11:30:41 INFO DAGScheduler: ResultStage 105 (parquet at Generate.java:61) finished in 1.093 s
21/01/20 11:30:41 INFO DAGScheduler: Job 105 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:30:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 105: Stage finished
21/01/20 11:30:41 INFO DAGScheduler: Job 105 finished: parquet at Generate.java:61, took 1.094672 s
21/01/20 11:30:41 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-4a3fde44-f811-4ba2-898a-f6bee0afd2dc
21/01/20 11:30:41 INFO FileFormatWriter: Write Job 286fe210-2bfe-44c1-a75d-429456f767d1 committed.
21/01/20 11:30:41 INFO FileFormatWriter: Finished processing stats for write job 286fe210-2bfe-44c1-a75d-429456f767d1.
21/01/20 11:30:42 INFO BlockManagerInfo: Removed broadcast_105_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:30:43 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:43 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:44 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:30:44 INFO DAGScheduler: Got job 106 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:30:44 INFO DAGScheduler: Final stage: ResultStage 106 (parquet at Generate.java:61)
21/01/20 11:30:44 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:30:44 INFO DAGScheduler: Missing parents: List()
21/01/20 11:30:44 INFO DAGScheduler: Submitting ResultStage 106 (MapPartitionsRDD[425] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:30:44 INFO MemoryStore: Block broadcast_106 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:30:44 INFO MemoryStore: Block broadcast_106_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:30:44 INFO BlockManagerInfo: Added broadcast_106_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:30:44 INFO SparkContext: Created broadcast 106 from broadcast at DAGScheduler.scala:1200
21/01/20 11:30:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 106 (MapPartitionsRDD[425] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:30:44 INFO TaskSchedulerImpl: Adding task set 106.0 with 1 tasks
21/01/20 11:30:44 WARN TaskSetManager: Stage 106 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:30:44 INFO TaskSetManager: Starting task 0.0 in stage 106.0 (TID 106, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:30:44 INFO Executor: Running task 0.0 in stage 106.0 (TID 106)
21/01/20 11:30:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:44 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:44 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:30:44 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:30:44 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:30:44 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:30:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:30:44 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:30:44 INFO ParquetOutputFormat: Validation is off
21/01/20 11:30:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:30:44 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:30:44 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:30:44 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:30:44 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:30:44 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:30:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:30:44 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-D5B85CA5152B->8df234bd-f131-4d8e-953a-b9eec8633221
21/01/20 11:30:44 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:30:45 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113044_0106_m_000000_106' to o3fs://bucket1.vol1/testdata
21/01/20 11:30:45 INFO SparkHadoopMapRedUtil: attempt_20210120113044_0106_m_000000_106: Committed
21/01/20 11:30:45 INFO Executor: Finished task 0.0 in stage 106.0 (TID 106). 2155 bytes result sent to driver
21/01/20 11:30:45 INFO TaskSetManager: Finished task 0.0 in stage 106.0 (TID 106) in 1056 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:30:45 INFO TaskSchedulerImpl: Removed TaskSet 106.0, whose tasks have all completed, from pool 
21/01/20 11:30:45 INFO DAGScheduler: ResultStage 106 (parquet at Generate.java:61) finished in 1.074 s
21/01/20 11:30:45 INFO DAGScheduler: Job 106 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:30:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 106: Stage finished
21/01/20 11:30:45 INFO DAGScheduler: Job 106 finished: parquet at Generate.java:61, took 1.076418 s
21/01/20 11:30:45 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-e565c9cf-547e-4350-aceb-775ebac48463
21/01/20 11:30:45 INFO FileFormatWriter: Write Job fea6df39-0914-4432-9c00-3fed5c98ac7e committed.
21/01/20 11:30:45 INFO FileFormatWriter: Finished processing stats for write job fea6df39-0914-4432-9c00-3fed5c98ac7e.
21/01/20 11:30:46 INFO BlockManagerInfo: Removed broadcast_106_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:30:47 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:47 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:30:47 INFO DAGScheduler: Got job 107 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:30:47 INFO DAGScheduler: Final stage: ResultStage 107 (parquet at Generate.java:61)
21/01/20 11:30:47 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:30:47 INFO DAGScheduler: Missing parents: List()
21/01/20 11:30:47 INFO DAGScheduler: Submitting ResultStage 107 (MapPartitionsRDD[429] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:30:47 INFO MemoryStore: Block broadcast_107 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:30:47 INFO MemoryStore: Block broadcast_107_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:30:47 INFO BlockManagerInfo: Added broadcast_107_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:30:47 INFO SparkContext: Created broadcast 107 from broadcast at DAGScheduler.scala:1200
21/01/20 11:30:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 107 (MapPartitionsRDD[429] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:30:47 INFO TaskSchedulerImpl: Adding task set 107.0 with 1 tasks
21/01/20 11:30:47 WARN TaskSetManager: Stage 107 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:30:47 INFO TaskSetManager: Starting task 0.0 in stage 107.0 (TID 107, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:30:47 INFO Executor: Running task 0.0 in stage 107.0 (TID 107)
21/01/20 11:30:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:47 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:30:47 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:30:47 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:30:47 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:30:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:30:47 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:30:47 INFO ParquetOutputFormat: Validation is off
21/01/20 11:30:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:30:47 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:30:47 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:30:47 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:30:47 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:30:47 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:30:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:30:48 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113047_0107_m_000000_107' to o3fs://bucket1.vol1/testdata
21/01/20 11:30:48 INFO SparkHadoopMapRedUtil: attempt_20210120113047_0107_m_000000_107: Committed
21/01/20 11:30:48 INFO Executor: Finished task 0.0 in stage 107.0 (TID 107). 2155 bytes result sent to driver
21/01/20 11:30:48 INFO TaskSetManager: Finished task 0.0 in stage 107.0 (TID 107) in 1075 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:30:48 INFO TaskSchedulerImpl: Removed TaskSet 107.0, whose tasks have all completed, from pool 
21/01/20 11:30:48 INFO DAGScheduler: ResultStage 107 (parquet at Generate.java:61) finished in 1.095 s
21/01/20 11:30:48 INFO DAGScheduler: Job 107 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:30:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 107: Stage finished
21/01/20 11:30:48 INFO DAGScheduler: Job 107 finished: parquet at Generate.java:61, took 1.096533 s
21/01/20 11:30:48 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-04e898a9-efd8-471b-81c9-6cb28aa7c422
21/01/20 11:30:48 INFO FileFormatWriter: Write Job ca29c148-4ab7-4948-bf4a-5a5e30208409 committed.
21/01/20 11:30:48 INFO FileFormatWriter: Finished processing stats for write job ca29c148-4ab7-4948-bf4a-5a5e30208409.
21/01/20 11:30:49 INFO BlockManagerInfo: Removed broadcast_107_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:30:51 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:51 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:30:51 INFO DAGScheduler: Got job 108 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:30:51 INFO DAGScheduler: Final stage: ResultStage 108 (parquet at Generate.java:61)
21/01/20 11:30:51 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:30:51 INFO DAGScheduler: Missing parents: List()
21/01/20 11:30:51 INFO DAGScheduler: Submitting ResultStage 108 (MapPartitionsRDD[433] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:30:51 INFO MemoryStore: Block broadcast_108 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:30:51 INFO MemoryStore: Block broadcast_108_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:30:51 INFO BlockManagerInfo: Added broadcast_108_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:30:51 INFO SparkContext: Created broadcast 108 from broadcast at DAGScheduler.scala:1200
21/01/20 11:30:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 108 (MapPartitionsRDD[433] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:30:51 INFO TaskSchedulerImpl: Adding task set 108.0 with 1 tasks
21/01/20 11:30:51 WARN TaskSetManager: Stage 108 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:30:51 INFO TaskSetManager: Starting task 0.0 in stage 108.0 (TID 108, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:30:51 INFO Executor: Running task 0.0 in stage 108.0 (TID 108)
21/01/20 11:30:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:51 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:30:51 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:30:51 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:30:51 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:30:51 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:30:51 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:30:51 INFO ParquetOutputFormat: Validation is off
21/01/20 11:30:51 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:30:51 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:30:51 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:30:51 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:30:51 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:30:51 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:30:51 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:30:52 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113051_0108_m_000000_108' to o3fs://bucket1.vol1/testdata
21/01/20 11:30:52 INFO SparkHadoopMapRedUtil: attempt_20210120113051_0108_m_000000_108: Committed
21/01/20 11:30:52 INFO Executor: Finished task 0.0 in stage 108.0 (TID 108). 2155 bytes result sent to driver
21/01/20 11:30:52 INFO TaskSetManager: Finished task 0.0 in stage 108.0 (TID 108) in 1069 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:30:52 INFO TaskSchedulerImpl: Removed TaskSet 108.0, whose tasks have all completed, from pool 
21/01/20 11:30:52 INFO DAGScheduler: ResultStage 108 (parquet at Generate.java:61) finished in 1.088 s
21/01/20 11:30:52 INFO DAGScheduler: Job 108 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:30:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 108: Stage finished
21/01/20 11:30:52 INFO DAGScheduler: Job 108 finished: parquet at Generate.java:61, took 1.089296 s
21/01/20 11:30:52 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-f4b0506f-0534-49ca-9858-24fb1830d72b
21/01/20 11:30:52 INFO FileFormatWriter: Write Job 8d0d7c01-3268-4062-963a-a61b0e492de3 committed.
21/01/20 11:30:52 INFO FileFormatWriter: Finished processing stats for write job 8d0d7c01-3268-4062-963a-a61b0e492de3.
21/01/20 11:30:53 INFO BlockManagerInfo: Removed broadcast_108_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:30:55 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:55 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:30:55 INFO DAGScheduler: Got job 109 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:30:55 INFO DAGScheduler: Final stage: ResultStage 109 (parquet at Generate.java:61)
21/01/20 11:30:55 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:30:55 INFO DAGScheduler: Missing parents: List()
21/01/20 11:30:55 INFO DAGScheduler: Submitting ResultStage 109 (MapPartitionsRDD[437] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:30:55 INFO MemoryStore: Block broadcast_109 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:30:55 INFO MemoryStore: Block broadcast_109_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:30:55 INFO BlockManagerInfo: Added broadcast_109_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:30:55 INFO SparkContext: Created broadcast 109 from broadcast at DAGScheduler.scala:1200
21/01/20 11:30:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 109 (MapPartitionsRDD[437] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:30:55 INFO TaskSchedulerImpl: Adding task set 109.0 with 1 tasks
21/01/20 11:30:55 WARN TaskSetManager: Stage 109 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:30:55 INFO TaskSetManager: Starting task 0.0 in stage 109.0 (TID 109, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:30:55 INFO Executor: Running task 0.0 in stage 109.0 (TID 109)
21/01/20 11:30:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:55 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:30:55 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:30:55 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:30:55 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:30:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:30:55 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:30:55 INFO ParquetOutputFormat: Validation is off
21/01/20 11:30:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:30:55 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:30:55 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:30:55 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:30:55 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:30:55 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:30:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:30:55 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-059A168A8D54->8df234bd-f131-4d8e-953a-b9eec8633221
21/01/20 11:30:55 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:30:56 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113055_0109_m_000000_109' to o3fs://bucket1.vol1/testdata
21/01/20 11:30:56 INFO SparkHadoopMapRedUtil: attempt_20210120113055_0109_m_000000_109: Committed
21/01/20 11:30:56 INFO Executor: Finished task 0.0 in stage 109.0 (TID 109). 2155 bytes result sent to driver
21/01/20 11:30:56 INFO TaskSetManager: Finished task 0.0 in stage 109.0 (TID 109) in 1054 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:30:56 INFO TaskSchedulerImpl: Removed TaskSet 109.0, whose tasks have all completed, from pool 
21/01/20 11:30:56 INFO DAGScheduler: ResultStage 109 (parquet at Generate.java:61) finished in 1.073 s
21/01/20 11:30:56 INFO DAGScheduler: Job 109 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:30:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 109: Stage finished
21/01/20 11:30:56 INFO DAGScheduler: Job 109 finished: parquet at Generate.java:61, took 1.074903 s
21/01/20 11:30:56 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-2d10b84e-b2f5-4a14-8ac1-120c04dca8f0
21/01/20 11:30:56 INFO FileFormatWriter: Write Job 9f434c78-2829-497b-abe7-d68c4d7415ee committed.
21/01/20 11:30:56 INFO FileFormatWriter: Finished processing stats for write job 9f434c78-2829-497b-abe7-d68c4d7415ee.
21/01/20 11:30:57 INFO BlockManagerInfo: Removed broadcast_109_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:30:58 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:58 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:58 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:30:58 INFO DAGScheduler: Got job 110 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:30:58 INFO DAGScheduler: Final stage: ResultStage 110 (parquet at Generate.java:61)
21/01/20 11:30:58 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:30:58 INFO DAGScheduler: Missing parents: List()
21/01/20 11:30:58 INFO DAGScheduler: Submitting ResultStage 110 (MapPartitionsRDD[441] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:30:58 INFO MemoryStore: Block broadcast_110 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:30:58 INFO MemoryStore: Block broadcast_110_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:30:58 INFO BlockManagerInfo: Added broadcast_110_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:30:58 INFO SparkContext: Created broadcast 110 from broadcast at DAGScheduler.scala:1200
21/01/20 11:30:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 110 (MapPartitionsRDD[441] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:30:58 INFO TaskSchedulerImpl: Adding task set 110.0 with 1 tasks
21/01/20 11:30:58 WARN TaskSetManager: Stage 110 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:30:58 INFO TaskSetManager: Starting task 0.0 in stage 110.0 (TID 110, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:30:58 INFO Executor: Running task 0.0 in stage 110.0 (TID 110)
21/01/20 11:30:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:58 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:30:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:30:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:30:58 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:30:58 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:30:58 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:30:58 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:30:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:30:58 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:30:58 INFO ParquetOutputFormat: Validation is off
21/01/20 11:30:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:30:58 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:30:58 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:30:58 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:30:58 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:30:58 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:30:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:30:59 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113058_0110_m_000000_110' to o3fs://bucket1.vol1/testdata
21/01/20 11:30:59 INFO SparkHadoopMapRedUtil: attempt_20210120113058_0110_m_000000_110: Committed
21/01/20 11:30:59 INFO Executor: Finished task 0.0 in stage 110.0 (TID 110). 2155 bytes result sent to driver
21/01/20 11:30:59 INFO TaskSetManager: Finished task 0.0 in stage 110.0 (TID 110) in 1083 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:30:59 INFO TaskSchedulerImpl: Removed TaskSet 110.0, whose tasks have all completed, from pool 
21/01/20 11:30:59 INFO DAGScheduler: ResultStage 110 (parquet at Generate.java:61) finished in 1.101 s
21/01/20 11:30:59 INFO DAGScheduler: Job 110 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:30:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 110: Stage finished
21/01/20 11:30:59 INFO DAGScheduler: Job 110 finished: parquet at Generate.java:61, took 1.102835 s
21/01/20 11:30:59 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-7ddc4b1b-273b-4e0c-af28-973ffd4ce09c
21/01/20 11:30:59 INFO FileFormatWriter: Write Job 4a5a0977-6327-42ad-9fd4-5b0b39e08082 committed.
21/01/20 11:30:59 INFO FileFormatWriter: Finished processing stats for write job 4a5a0977-6327-42ad-9fd4-5b0b39e08082.
21/01/20 11:31:01 INFO BlockManagerInfo: Removed broadcast_110_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:31:02 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:02 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:31:02 INFO DAGScheduler: Got job 111 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:31:02 INFO DAGScheduler: Final stage: ResultStage 111 (parquet at Generate.java:61)
21/01/20 11:31:02 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:31:02 INFO DAGScheduler: Missing parents: List()
21/01/20 11:31:02 INFO DAGScheduler: Submitting ResultStage 111 (MapPartitionsRDD[445] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:31:02 INFO MemoryStore: Block broadcast_111 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:31:02 INFO MemoryStore: Block broadcast_111_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:31:02 INFO BlockManagerInfo: Added broadcast_111_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:31:02 INFO SparkContext: Created broadcast 111 from broadcast at DAGScheduler.scala:1200
21/01/20 11:31:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 111 (MapPartitionsRDD[445] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:31:02 INFO TaskSchedulerImpl: Adding task set 111.0 with 1 tasks
21/01/20 11:31:02 WARN TaskSetManager: Stage 111 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:31:02 INFO TaskSetManager: Starting task 0.0 in stage 111.0 (TID 111, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:31:02 INFO Executor: Running task 0.0 in stage 111.0 (TID 111)
21/01/20 11:31:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:02 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:31:02 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:31:02 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:31:02 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:31:02 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:31:02 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:31:02 INFO ParquetOutputFormat: Validation is off
21/01/20 11:31:02 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:31:02 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:31:02 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:31:02 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:31:02 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:31:02 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:31:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:31:03 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113102_0111_m_000000_111' to o3fs://bucket1.vol1/testdata
21/01/20 11:31:03 INFO SparkHadoopMapRedUtil: attempt_20210120113102_0111_m_000000_111: Committed
21/01/20 11:31:03 INFO Executor: Finished task 0.0 in stage 111.0 (TID 111). 2155 bytes result sent to driver
21/01/20 11:31:03 INFO TaskSetManager: Finished task 0.0 in stage 111.0 (TID 111) in 1044 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:31:03 INFO TaskSchedulerImpl: Removed TaskSet 111.0, whose tasks have all completed, from pool 
21/01/20 11:31:03 INFO DAGScheduler: ResultStage 111 (parquet at Generate.java:61) finished in 1.063 s
21/01/20 11:31:03 INFO DAGScheduler: Job 111 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:31:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 111: Stage finished
21/01/20 11:31:03 INFO DAGScheduler: Job 111 finished: parquet at Generate.java:61, took 1.064090 s
21/01/20 11:31:03 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-69f09413-ea51-42da-a61e-b7d5e914523f
21/01/20 11:31:03 INFO FileFormatWriter: Write Job 6e3f2d09-fb09-4159-a621-0551d8c9e360 committed.
21/01/20 11:31:03 INFO FileFormatWriter: Finished processing stats for write job 6e3f2d09-fb09-4159-a621-0551d8c9e360.
21/01/20 11:31:04 INFO BlockManagerInfo: Removed broadcast_111_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:31:06 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:06 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:06 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:31:06 INFO DAGScheduler: Got job 112 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:31:06 INFO DAGScheduler: Final stage: ResultStage 112 (parquet at Generate.java:61)
21/01/20 11:31:06 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:31:06 INFO DAGScheduler: Missing parents: List()
21/01/20 11:31:06 INFO DAGScheduler: Submitting ResultStage 112 (MapPartitionsRDD[449] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:31:06 INFO MemoryStore: Block broadcast_112 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:31:06 INFO MemoryStore: Block broadcast_112_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:31:06 INFO BlockManagerInfo: Added broadcast_112_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:31:06 INFO SparkContext: Created broadcast 112 from broadcast at DAGScheduler.scala:1200
21/01/20 11:31:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 112 (MapPartitionsRDD[449] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:31:06 INFO TaskSchedulerImpl: Adding task set 112.0 with 1 tasks
21/01/20 11:31:06 WARN TaskSetManager: Stage 112 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:31:06 INFO TaskSetManager: Starting task 0.0 in stage 112.0 (TID 112, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:31:06 INFO Executor: Running task 0.0 in stage 112.0 (TID 112)
21/01/20 11:31:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:06 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:06 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:31:06 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:31:06 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:31:06 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:31:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:31:06 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:31:06 INFO ParquetOutputFormat: Validation is off
21/01/20 11:31:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:31:06 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:31:06 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:31:06 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:31:06 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:31:06 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:31:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:31:07 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113106_0112_m_000000_112' to o3fs://bucket1.vol1/testdata
21/01/20 11:31:07 INFO SparkHadoopMapRedUtil: attempt_20210120113106_0112_m_000000_112: Committed
21/01/20 11:31:07 INFO Executor: Finished task 0.0 in stage 112.0 (TID 112). 2155 bytes result sent to driver
21/01/20 11:31:07 INFO TaskSetManager: Finished task 0.0 in stage 112.0 (TID 112) in 960 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:31:07 INFO TaskSchedulerImpl: Removed TaskSet 112.0, whose tasks have all completed, from pool 
21/01/20 11:31:07 INFO DAGScheduler: ResultStage 112 (parquet at Generate.java:61) finished in 0.978 s
21/01/20 11:31:07 INFO DAGScheduler: Job 112 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:31:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 112: Stage finished
21/01/20 11:31:07 INFO DAGScheduler: Job 112 finished: parquet at Generate.java:61, took 0.979826 s
21/01/20 11:31:07 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-e82b2313-7449-45c5-a35b-fa7e2df1bfec
21/01/20 11:31:07 INFO FileFormatWriter: Write Job d8496794-95b6-463d-9017-ef7e07e3b678 committed.
21/01/20 11:31:07 INFO FileFormatWriter: Finished processing stats for write job d8496794-95b6-463d-9017-ef7e07e3b678.
21/01/20 11:31:08 INFO BlockManagerInfo: Removed broadcast_112_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:31:09 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:09 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:09 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:31:09 INFO DAGScheduler: Got job 113 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:31:09 INFO DAGScheduler: Final stage: ResultStage 113 (parquet at Generate.java:61)
21/01/20 11:31:09 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:31:09 INFO DAGScheduler: Missing parents: List()
21/01/20 11:31:09 INFO DAGScheduler: Submitting ResultStage 113 (MapPartitionsRDD[453] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:31:09 INFO MemoryStore: Block broadcast_113 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:31:09 INFO MemoryStore: Block broadcast_113_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:31:09 INFO BlockManagerInfo: Added broadcast_113_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:31:09 INFO SparkContext: Created broadcast 113 from broadcast at DAGScheduler.scala:1200
21/01/20 11:31:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 113 (MapPartitionsRDD[453] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:31:09 INFO TaskSchedulerImpl: Adding task set 113.0 with 1 tasks
21/01/20 11:31:09 WARN TaskSetManager: Stage 113 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:31:09 INFO TaskSetManager: Starting task 0.0 in stage 113.0 (TID 113, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:31:09 INFO Executor: Running task 0.0 in stage 113.0 (TID 113)
21/01/20 11:31:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:09 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:09 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:31:09 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:31:09 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:31:09 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:31:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:31:09 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:31:09 INFO ParquetOutputFormat: Validation is off
21/01/20 11:31:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:31:09 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:31:09 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:31:09 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:31:09 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:31:09 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:31:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:31:10 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113109_0113_m_000000_113' to o3fs://bucket1.vol1/testdata
21/01/20 11:31:10 INFO SparkHadoopMapRedUtil: attempt_20210120113109_0113_m_000000_113: Committed
21/01/20 11:31:10 INFO Executor: Finished task 0.0 in stage 113.0 (TID 113). 2155 bytes result sent to driver
21/01/20 11:31:10 INFO TaskSetManager: Finished task 0.0 in stage 113.0 (TID 113) in 747 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:31:10 INFO TaskSchedulerImpl: Removed TaskSet 113.0, whose tasks have all completed, from pool 
21/01/20 11:31:10 INFO DAGScheduler: ResultStage 113 (parquet at Generate.java:61) finished in 0.787 s
21/01/20 11:31:10 INFO DAGScheduler: Job 113 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:31:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 113: Stage finished
21/01/20 11:31:10 INFO DAGScheduler: Job 113 finished: parquet at Generate.java:61, took 0.788455 s
21/01/20 11:31:10 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-55ec9f01-2535-4d57-bf31-75677f2cbc01
21/01/20 11:31:10 INFO FileFormatWriter: Write Job b413b9f5-80fc-4c2c-be77-6116b6e702f6 committed.
21/01/20 11:31:10 INFO FileFormatWriter: Finished processing stats for write job b413b9f5-80fc-4c2c-be77-6116b6e702f6.
21/01/20 11:31:10 INFO BlockManagerInfo: Removed broadcast_113_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:31:13 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:13 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:13 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:31:13 INFO DAGScheduler: Got job 114 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:31:13 INFO DAGScheduler: Final stage: ResultStage 114 (parquet at Generate.java:61)
21/01/20 11:31:13 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:31:13 INFO DAGScheduler: Missing parents: List()
21/01/20 11:31:13 INFO DAGScheduler: Submitting ResultStage 114 (MapPartitionsRDD[457] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:31:13 INFO MemoryStore: Block broadcast_114 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:31:13 INFO MemoryStore: Block broadcast_114_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:31:13 INFO BlockManagerInfo: Added broadcast_114_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:31:13 INFO SparkContext: Created broadcast 114 from broadcast at DAGScheduler.scala:1200
21/01/20 11:31:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 114 (MapPartitionsRDD[457] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:31:13 INFO TaskSchedulerImpl: Adding task set 114.0 with 1 tasks
21/01/20 11:31:13 WARN TaskSetManager: Stage 114 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:31:13 INFO TaskSetManager: Starting task 0.0 in stage 114.0 (TID 114, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:31:13 INFO Executor: Running task 0.0 in stage 114.0 (TID 114)
21/01/20 11:31:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:13 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:13 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:31:13 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:31:13 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:31:13 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:31:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:31:13 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:31:13 INFO ParquetOutputFormat: Validation is off
21/01/20 11:31:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:31:13 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:31:13 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:31:13 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:31:13 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:31:13 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:31:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:31:14 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113113_0114_m_000000_114' to o3fs://bucket1.vol1/testdata
21/01/20 11:31:14 INFO SparkHadoopMapRedUtil: attempt_20210120113113_0114_m_000000_114: Committed
21/01/20 11:31:14 INFO Executor: Finished task 0.0 in stage 114.0 (TID 114). 2155 bytes result sent to driver
21/01/20 11:31:14 INFO TaskSetManager: Finished task 0.0 in stage 114.0 (TID 114) in 989 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:31:14 INFO TaskSchedulerImpl: Removed TaskSet 114.0, whose tasks have all completed, from pool 
21/01/20 11:31:14 INFO DAGScheduler: ResultStage 114 (parquet at Generate.java:61) finished in 1.007 s
21/01/20 11:31:14 INFO DAGScheduler: Job 114 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:31:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 114: Stage finished
21/01/20 11:31:14 INFO DAGScheduler: Job 114 finished: parquet at Generate.java:61, took 1.009151 s
21/01/20 11:31:14 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-d9cfc152-4bbc-4b2d-a86c-547a7bf22213
21/01/20 11:31:14 INFO FileFormatWriter: Write Job 95f2a799-f875-483e-81e0-66d48046fb1b committed.
21/01/20 11:31:14 INFO FileFormatWriter: Finished processing stats for write job 95f2a799-f875-483e-81e0-66d48046fb1b.
21/01/20 11:31:15 INFO BlockManagerInfo: Removed broadcast_114_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:31:16 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:16 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:16 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:31:16 INFO DAGScheduler: Got job 115 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:31:16 INFO DAGScheduler: Final stage: ResultStage 115 (parquet at Generate.java:61)
21/01/20 11:31:16 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:31:16 INFO DAGScheduler: Missing parents: List()
21/01/20 11:31:16 INFO DAGScheduler: Submitting ResultStage 115 (MapPartitionsRDD[461] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:31:16 INFO MemoryStore: Block broadcast_115 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:31:16 INFO MemoryStore: Block broadcast_115_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:31:16 INFO BlockManagerInfo: Added broadcast_115_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:31:16 INFO SparkContext: Created broadcast 115 from broadcast at DAGScheduler.scala:1200
21/01/20 11:31:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 115 (MapPartitionsRDD[461] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:31:16 INFO TaskSchedulerImpl: Adding task set 115.0 with 1 tasks
21/01/20 11:31:16 WARN TaskSetManager: Stage 115 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:31:16 INFO TaskSetManager: Starting task 0.0 in stage 115.0 (TID 115, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:31:16 INFO Executor: Running task 0.0 in stage 115.0 (TID 115)
21/01/20 11:31:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:16 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:16 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:31:16 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:31:16 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:31:16 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:31:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:31:16 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:31:16 INFO ParquetOutputFormat: Validation is off
21/01/20 11:31:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:31:16 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:31:16 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:31:16 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:31:16 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:31:16 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:31:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:31:17 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113116_0115_m_000000_115' to o3fs://bucket1.vol1/testdata
21/01/20 11:31:17 INFO SparkHadoopMapRedUtil: attempt_20210120113116_0115_m_000000_115: Committed
21/01/20 11:31:17 INFO Executor: Finished task 0.0 in stage 115.0 (TID 115). 2155 bytes result sent to driver
21/01/20 11:31:17 INFO TaskSetManager: Finished task 0.0 in stage 115.0 (TID 115) in 795 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:31:17 INFO TaskSchedulerImpl: Removed TaskSet 115.0, whose tasks have all completed, from pool 
21/01/20 11:31:17 INFO DAGScheduler: ResultStage 115 (parquet at Generate.java:61) finished in 0.813 s
21/01/20 11:31:17 INFO DAGScheduler: Job 115 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:31:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 115: Stage finished
21/01/20 11:31:17 INFO DAGScheduler: Job 115 finished: parquet at Generate.java:61, took 0.837884 s
21/01/20 11:31:17 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-5386dbfe-3f70-45df-adc7-9ce151a64f73
21/01/20 11:31:17 INFO FileFormatWriter: Write Job d3a87076-2e2c-4614-b38a-8f1961da079d committed.
21/01/20 11:31:17 INFO FileFormatWriter: Finished processing stats for write job d3a87076-2e2c-4614-b38a-8f1961da079d.
21/01/20 11:31:17 INFO BlockManagerInfo: Removed broadcast_115_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:31:20 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:20 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:20 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:31:20 INFO DAGScheduler: Got job 116 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:31:20 INFO DAGScheduler: Final stage: ResultStage 116 (parquet at Generate.java:61)
21/01/20 11:31:20 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:31:20 INFO DAGScheduler: Missing parents: List()
21/01/20 11:31:20 INFO DAGScheduler: Submitting ResultStage 116 (MapPartitionsRDD[465] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:31:20 INFO MemoryStore: Block broadcast_116 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:31:20 INFO MemoryStore: Block broadcast_116_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:31:20 INFO BlockManagerInfo: Added broadcast_116_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:31:20 INFO SparkContext: Created broadcast 116 from broadcast at DAGScheduler.scala:1200
21/01/20 11:31:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 116 (MapPartitionsRDD[465] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:31:20 INFO TaskSchedulerImpl: Adding task set 116.0 with 1 tasks
21/01/20 11:31:20 WARN TaskSetManager: Stage 116 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:31:20 INFO TaskSetManager: Starting task 0.0 in stage 116.0 (TID 116, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:31:20 INFO Executor: Running task 0.0 in stage 116.0 (TID 116)
21/01/20 11:31:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:20 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:20 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:31:20 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:31:20 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:31:20 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:31:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:31:20 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:31:20 INFO ParquetOutputFormat: Validation is off
21/01/20 11:31:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:31:20 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:31:20 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:31:20 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:31:20 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:31:20 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:31:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:31:21 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113120_0116_m_000000_116' to o3fs://bucket1.vol1/testdata
21/01/20 11:31:21 INFO SparkHadoopMapRedUtil: attempt_20210120113120_0116_m_000000_116: Committed
21/01/20 11:31:21 INFO Executor: Finished task 0.0 in stage 116.0 (TID 116). 2155 bytes result sent to driver
21/01/20 11:31:21 INFO TaskSetManager: Finished task 0.0 in stage 116.0 (TID 116) in 1050 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:31:21 INFO TaskSchedulerImpl: Removed TaskSet 116.0, whose tasks have all completed, from pool 
21/01/20 11:31:21 INFO DAGScheduler: ResultStage 116 (parquet at Generate.java:61) finished in 1.068 s
21/01/20 11:31:21 INFO DAGScheduler: Job 116 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:31:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 116: Stage finished
21/01/20 11:31:21 INFO DAGScheduler: Job 116 finished: parquet at Generate.java:61, took 1.069521 s
21/01/20 11:31:21 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-0f28cf96-db3a-46ff-a437-2fe3acd49a15
21/01/20 11:31:21 INFO FileFormatWriter: Write Job 23141347-a696-47ea-b29a-b09aa3e8f0d6 committed.
21/01/20 11:31:21 INFO FileFormatWriter: Finished processing stats for write job 23141347-a696-47ea-b29a-b09aa3e8f0d6.
21/01/20 11:31:22 INFO BlockManagerInfo: Removed broadcast_116_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:31:23 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:23 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:23 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:31:23 INFO DAGScheduler: Got job 117 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:31:23 INFO DAGScheduler: Final stage: ResultStage 117 (parquet at Generate.java:61)
21/01/20 11:31:23 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:31:23 INFO DAGScheduler: Missing parents: List()
21/01/20 11:31:23 INFO DAGScheduler: Submitting ResultStage 117 (MapPartitionsRDD[469] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:31:24 INFO MemoryStore: Block broadcast_117 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:31:24 INFO MemoryStore: Block broadcast_117_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:31:24 INFO BlockManagerInfo: Added broadcast_117_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:31:24 INFO SparkContext: Created broadcast 117 from broadcast at DAGScheduler.scala:1200
21/01/20 11:31:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 117 (MapPartitionsRDD[469] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:31:24 INFO TaskSchedulerImpl: Adding task set 117.0 with 1 tasks
21/01/20 11:31:24 WARN TaskSetManager: Stage 117 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:31:24 INFO TaskSetManager: Starting task 0.0 in stage 117.0 (TID 117, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:31:24 INFO Executor: Running task 0.0 in stage 117.0 (TID 117)
21/01/20 11:31:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:24 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:24 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:31:24 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:31:24 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:31:24 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:31:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:31:24 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:31:24 INFO ParquetOutputFormat: Validation is off
21/01/20 11:31:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:31:24 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:31:24 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:31:24 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:31:24 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:31:24 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:31:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:31:24 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113123_0117_m_000000_117' to o3fs://bucket1.vol1/testdata
21/01/20 11:31:24 INFO SparkHadoopMapRedUtil: attempt_20210120113123_0117_m_000000_117: Committed
21/01/20 11:31:24 INFO Executor: Finished task 0.0 in stage 117.0 (TID 117). 2155 bytes result sent to driver
21/01/20 11:31:24 INFO TaskSetManager: Finished task 0.0 in stage 117.0 (TID 117) in 869 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:31:24 INFO TaskSchedulerImpl: Removed TaskSet 117.0, whose tasks have all completed, from pool 
21/01/20 11:31:24 INFO DAGScheduler: ResultStage 117 (parquet at Generate.java:61) finished in 0.910 s
21/01/20 11:31:24 INFO DAGScheduler: Job 117 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:31:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 117: Stage finished
21/01/20 11:31:24 INFO DAGScheduler: Job 117 finished: parquet at Generate.java:61, took 0.911491 s
21/01/20 11:31:24 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-4e9d3d20-b30f-4c38-889d-2827322c22da
21/01/20 11:31:24 INFO FileFormatWriter: Write Job ca00cc03-8daa-4f45-b3fd-12c326627011 committed.
21/01/20 11:31:24 INFO FileFormatWriter: Finished processing stats for write job ca00cc03-8daa-4f45-b3fd-12c326627011.
21/01/20 11:31:25 INFO BlockManagerInfo: Removed broadcast_117_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:31:27 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:27 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:27 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:31:27 INFO DAGScheduler: Got job 118 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:31:27 INFO DAGScheduler: Final stage: ResultStage 118 (parquet at Generate.java:61)
21/01/20 11:31:27 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:31:27 INFO DAGScheduler: Missing parents: List()
21/01/20 11:31:27 INFO DAGScheduler: Submitting ResultStage 118 (MapPartitionsRDD[473] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:31:27 INFO MemoryStore: Block broadcast_118 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:31:27 INFO MemoryStore: Block broadcast_118_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:31:27 INFO BlockManagerInfo: Added broadcast_118_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:31:27 INFO SparkContext: Created broadcast 118 from broadcast at DAGScheduler.scala:1200
21/01/20 11:31:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 118 (MapPartitionsRDD[473] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:31:27 INFO TaskSchedulerImpl: Adding task set 118.0 with 1 tasks
21/01/20 11:31:27 WARN TaskSetManager: Stage 118 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:31:27 INFO TaskSetManager: Starting task 0.0 in stage 118.0 (TID 118, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:31:27 INFO Executor: Running task 0.0 in stage 118.0 (TID 118)
21/01/20 11:31:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:27 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:27 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:31:27 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:31:27 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:31:27 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:31:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:31:27 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:31:27 INFO ParquetOutputFormat: Validation is off
21/01/20 11:31:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:31:27 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:31:27 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:31:27 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:31:27 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:31:27 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:31:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:31:28 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-2C0814351B54->27b66227-fed8-4c1d-ab57-510f5a3ce552
21/01/20 11:31:28 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:31:28 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113127_0118_m_000000_118' to o3fs://bucket1.vol1/testdata
21/01/20 11:31:28 INFO SparkHadoopMapRedUtil: attempt_20210120113127_0118_m_000000_118: Committed
21/01/20 11:31:28 INFO Executor: Finished task 0.0 in stage 118.0 (TID 118). 2155 bytes result sent to driver
21/01/20 11:31:28 INFO TaskSetManager: Finished task 0.0 in stage 118.0 (TID 118) in 1089 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:31:28 INFO TaskSchedulerImpl: Removed TaskSet 118.0, whose tasks have all completed, from pool 
21/01/20 11:31:28 INFO DAGScheduler: ResultStage 118 (parquet at Generate.java:61) finished in 1.108 s
21/01/20 11:31:28 INFO DAGScheduler: Job 118 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:31:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 118: Stage finished
21/01/20 11:31:28 INFO DAGScheduler: Job 118 finished: parquet at Generate.java:61, took 1.109750 s
21/01/20 11:31:28 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-d881b4b0-5d7f-4660-89f0-6bb8396f611f
21/01/20 11:31:28 INFO FileFormatWriter: Write Job e9db0382-3c8d-4659-aa27-31f12f1d1023 committed.
21/01/20 11:31:28 INFO FileFormatWriter: Finished processing stats for write job e9db0382-3c8d-4659-aa27-31f12f1d1023.
21/01/20 11:31:29 INFO BlockManagerInfo: Removed broadcast_118_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:31:31 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:31 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:31:31 INFO DAGScheduler: Got job 119 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:31:31 INFO DAGScheduler: Final stage: ResultStage 119 (parquet at Generate.java:61)
21/01/20 11:31:31 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:31:31 INFO DAGScheduler: Missing parents: List()
21/01/20 11:31:31 INFO DAGScheduler: Submitting ResultStage 119 (MapPartitionsRDD[477] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:31:31 INFO MemoryStore: Block broadcast_119 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:31:31 INFO MemoryStore: Block broadcast_119_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:31:31 INFO BlockManagerInfo: Added broadcast_119_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:31:31 INFO SparkContext: Created broadcast 119 from broadcast at DAGScheduler.scala:1200
21/01/20 11:31:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 119 (MapPartitionsRDD[477] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:31:31 INFO TaskSchedulerImpl: Adding task set 119.0 with 1 tasks
21/01/20 11:31:31 WARN TaskSetManager: Stage 119 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:31:31 INFO TaskSetManager: Starting task 0.0 in stage 119.0 (TID 119, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:31:31 INFO Executor: Running task 0.0 in stage 119.0 (TID 119)
21/01/20 11:31:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:31 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:31:31 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:31:31 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:31:31 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:31:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:31:31 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:31:31 INFO ParquetOutputFormat: Validation is off
21/01/20 11:31:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:31:31 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:31:31 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:31:31 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:31:31 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:31:31 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:31:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:31:32 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113131_0119_m_000000_119' to o3fs://bucket1.vol1/testdata
21/01/20 11:31:32 INFO SparkHadoopMapRedUtil: attempt_20210120113131_0119_m_000000_119: Committed
21/01/20 11:31:32 INFO Executor: Finished task 0.0 in stage 119.0 (TID 119). 2155 bytes result sent to driver
21/01/20 11:31:32 INFO TaskSetManager: Finished task 0.0 in stage 119.0 (TID 119) in 1038 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:31:32 INFO TaskSchedulerImpl: Removed TaskSet 119.0, whose tasks have all completed, from pool 
21/01/20 11:31:32 INFO DAGScheduler: ResultStage 119 (parquet at Generate.java:61) finished in 1.056 s
21/01/20 11:31:32 INFO DAGScheduler: Job 119 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:31:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 119: Stage finished
21/01/20 11:31:32 INFO DAGScheduler: Job 119 finished: parquet at Generate.java:61, took 1.058177 s
21/01/20 11:31:32 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-66e91b6a-37b5-48af-8645-f9c53d122ba9
21/01/20 11:31:32 INFO FileFormatWriter: Write Job 45f8d654-492a-4e4e-9a7c-d146ce022be5 committed.
21/01/20 11:31:32 INFO FileFormatWriter: Finished processing stats for write job 45f8d654-492a-4e4e-9a7c-d146ce022be5.
21/01/20 11:31:33 INFO BlockManagerInfo: Removed broadcast_119_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:31:34 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:34 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:34 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:31:34 INFO DAGScheduler: Got job 120 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:31:34 INFO DAGScheduler: Final stage: ResultStage 120 (parquet at Generate.java:61)
21/01/20 11:31:34 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:31:34 INFO DAGScheduler: Missing parents: List()
21/01/20 11:31:34 INFO DAGScheduler: Submitting ResultStage 120 (MapPartitionsRDD[481] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:31:34 INFO MemoryStore: Block broadcast_120 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:31:34 INFO MemoryStore: Block broadcast_120_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:31:34 INFO BlockManagerInfo: Added broadcast_120_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:31:34 INFO SparkContext: Created broadcast 120 from broadcast at DAGScheduler.scala:1200
21/01/20 11:31:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 120 (MapPartitionsRDD[481] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:31:34 INFO TaskSchedulerImpl: Adding task set 120.0 with 1 tasks
21/01/20 11:31:35 WARN TaskSetManager: Stage 120 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:31:35 INFO TaskSetManager: Starting task 0.0 in stage 120.0 (TID 120, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:31:35 INFO Executor: Running task 0.0 in stage 120.0 (TID 120)
21/01/20 11:31:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:35 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:35 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:31:35 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:31:35 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:31:35 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:31:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:31:35 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:31:35 INFO ParquetOutputFormat: Validation is off
21/01/20 11:31:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:31:35 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:31:35 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:31:35 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:31:35 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:31:35 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:31:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:31:35 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-5B2066AEC2D2->8df234bd-f131-4d8e-953a-b9eec8633221
21/01/20 11:31:35 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:31:35 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113134_0120_m_000000_120' to o3fs://bucket1.vol1/testdata
21/01/20 11:31:35 INFO SparkHadoopMapRedUtil: attempt_20210120113134_0120_m_000000_120: Committed
21/01/20 11:31:35 INFO Executor: Finished task 0.0 in stage 120.0 (TID 120). 2155 bytes result sent to driver
21/01/20 11:31:35 INFO TaskSetManager: Finished task 0.0 in stage 120.0 (TID 120) in 879 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:31:35 INFO TaskSchedulerImpl: Removed TaskSet 120.0, whose tasks have all completed, from pool 
21/01/20 11:31:35 INFO DAGScheduler: ResultStage 120 (parquet at Generate.java:61) finished in 0.898 s
21/01/20 11:31:35 INFO DAGScheduler: Job 120 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:31:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 120: Stage finished
21/01/20 11:31:35 INFO DAGScheduler: Job 120 finished: parquet at Generate.java:61, took 0.921433 s
21/01/20 11:31:35 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-d9a08cf0-a74f-40ed-97d0-fb5766e3aa3d
21/01/20 11:31:35 INFO FileFormatWriter: Write Job f530e7ee-f630-4c57-a0f8-1908fe24023e committed.
21/01/20 11:31:35 INFO FileFormatWriter: Finished processing stats for write job f530e7ee-f630-4c57-a0f8-1908fe24023e.
21/01/20 11:31:36 INFO BlockManagerInfo: Removed broadcast_120_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:31:38 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:38 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:38 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:31:38 INFO DAGScheduler: Got job 121 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:31:38 INFO DAGScheduler: Final stage: ResultStage 121 (parquet at Generate.java:61)
21/01/20 11:31:38 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:31:38 INFO DAGScheduler: Missing parents: List()
21/01/20 11:31:38 INFO DAGScheduler: Submitting ResultStage 121 (MapPartitionsRDD[485] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:31:38 INFO MemoryStore: Block broadcast_121 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:31:38 INFO MemoryStore: Block broadcast_121_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:31:38 INFO BlockManagerInfo: Added broadcast_121_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:31:38 INFO SparkContext: Created broadcast 121 from broadcast at DAGScheduler.scala:1200
21/01/20 11:31:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 121 (MapPartitionsRDD[485] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:31:38 INFO TaskSchedulerImpl: Adding task set 121.0 with 1 tasks
21/01/20 11:31:38 WARN TaskSetManager: Stage 121 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:31:38 INFO TaskSetManager: Starting task 0.0 in stage 121.0 (TID 121, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:31:38 INFO Executor: Running task 0.0 in stage 121.0 (TID 121)
21/01/20 11:31:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:38 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:38 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:31:38 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:31:38 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:31:38 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:31:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:31:38 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:31:38 INFO ParquetOutputFormat: Validation is off
21/01/20 11:31:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:31:38 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:31:38 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:31:38 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:31:38 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:31:38 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:31:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:31:39 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113138_0121_m_000000_121' to o3fs://bucket1.vol1/testdata
21/01/20 11:31:39 INFO SparkHadoopMapRedUtil: attempt_20210120113138_0121_m_000000_121: Committed
21/01/20 11:31:39 INFO Executor: Finished task 0.0 in stage 121.0 (TID 121). 2155 bytes result sent to driver
21/01/20 11:31:39 INFO TaskSetManager: Finished task 0.0 in stage 121.0 (TID 121) in 982 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:31:39 INFO TaskSchedulerImpl: Removed TaskSet 121.0, whose tasks have all completed, from pool 
21/01/20 11:31:39 INFO DAGScheduler: ResultStage 121 (parquet at Generate.java:61) finished in 1.002 s
21/01/20 11:31:39 INFO DAGScheduler: Job 121 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:31:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 121: Stage finished
21/01/20 11:31:39 INFO DAGScheduler: Job 121 finished: parquet at Generate.java:61, took 1.003752 s
21/01/20 11:31:39 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-fad10bf2-bf8f-468d-8207-4d74faea5089
21/01/20 11:31:39 INFO FileFormatWriter: Write Job e0ad5df2-0c21-45d1-9fda-836e85b984c9 committed.
21/01/20 11:31:39 INFO FileFormatWriter: Finished processing stats for write job e0ad5df2-0c21-45d1-9fda-836e85b984c9.
21/01/20 11:31:40 INFO BlockManagerInfo: Removed broadcast_121_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:31:42 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:42 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:42 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:31:42 INFO DAGScheduler: Got job 122 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:31:42 INFO DAGScheduler: Final stage: ResultStage 122 (parquet at Generate.java:61)
21/01/20 11:31:42 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:31:42 INFO DAGScheduler: Missing parents: List()
21/01/20 11:31:42 INFO DAGScheduler: Submitting ResultStage 122 (MapPartitionsRDD[489] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:31:42 INFO MemoryStore: Block broadcast_122 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:31:42 INFO MemoryStore: Block broadcast_122_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:31:42 INFO BlockManagerInfo: Added broadcast_122_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:31:42 INFO SparkContext: Created broadcast 122 from broadcast at DAGScheduler.scala:1200
21/01/20 11:31:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 122 (MapPartitionsRDD[489] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:31:42 INFO TaskSchedulerImpl: Adding task set 122.0 with 1 tasks
21/01/20 11:31:42 WARN TaskSetManager: Stage 122 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:31:42 INFO TaskSetManager: Starting task 0.0 in stage 122.0 (TID 122, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:31:42 INFO Executor: Running task 0.0 in stage 122.0 (TID 122)
21/01/20 11:31:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:42 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:42 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:31:42 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:31:42 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:31:42 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:31:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:31:42 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:31:42 INFO ParquetOutputFormat: Validation is off
21/01/20 11:31:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:31:42 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:31:42 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:31:42 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:31:42 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:31:42 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:31:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:31:42 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113142_0122_m_000000_122' to o3fs://bucket1.vol1/testdata
21/01/20 11:31:42 INFO SparkHadoopMapRedUtil: attempt_20210120113142_0122_m_000000_122: Committed
21/01/20 11:31:42 INFO Executor: Finished task 0.0 in stage 122.0 (TID 122). 2155 bytes result sent to driver
21/01/20 11:31:42 INFO TaskSetManager: Finished task 0.0 in stage 122.0 (TID 122) in 876 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:31:42 INFO TaskSchedulerImpl: Removed TaskSet 122.0, whose tasks have all completed, from pool 
21/01/20 11:31:42 INFO DAGScheduler: ResultStage 122 (parquet at Generate.java:61) finished in 0.917 s
21/01/20 11:31:42 INFO DAGScheduler: Job 122 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:31:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 122: Stage finished
21/01/20 11:31:42 INFO DAGScheduler: Job 122 finished: parquet at Generate.java:61, took 0.919085 s
21/01/20 11:31:42 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-90bd6cee-b894-4084-bc08-30ea3f6b0ff7
21/01/20 11:31:42 INFO FileFormatWriter: Write Job 31afda2d-4496-4e59-9fc9-9b03809622a7 committed.
21/01/20 11:31:42 INFO FileFormatWriter: Finished processing stats for write job 31afda2d-4496-4e59-9fc9-9b03809622a7.
21/01/20 11:31:43 INFO BlockManagerInfo: Removed broadcast_122_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:31:45 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:45 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:45 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:31:45 INFO DAGScheduler: Got job 123 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:31:45 INFO DAGScheduler: Final stage: ResultStage 123 (parquet at Generate.java:61)
21/01/20 11:31:45 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:31:45 INFO DAGScheduler: Missing parents: List()
21/01/20 11:31:45 INFO DAGScheduler: Submitting ResultStage 123 (MapPartitionsRDD[493] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:31:45 INFO MemoryStore: Block broadcast_123 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:31:45 INFO MemoryStore: Block broadcast_123_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:31:45 INFO BlockManagerInfo: Added broadcast_123_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:31:45 INFO SparkContext: Created broadcast 123 from broadcast at DAGScheduler.scala:1200
21/01/20 11:31:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 123 (MapPartitionsRDD[493] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:31:45 INFO TaskSchedulerImpl: Adding task set 123.0 with 1 tasks
21/01/20 11:31:45 WARN TaskSetManager: Stage 123 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:31:45 INFO TaskSetManager: Starting task 0.0 in stage 123.0 (TID 123, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:31:45 INFO Executor: Running task 0.0 in stage 123.0 (TID 123)
21/01/20 11:31:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:45 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:45 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:31:45 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:31:45 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:31:45 INFO ParquetOutputFormat: Validation is off
21/01/20 11:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:31:45 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:31:45 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:31:45 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:31:45 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:31:45 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:31:46 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-FEB745589996->27b66227-fed8-4c1d-ab57-510f5a3ce552
21/01/20 11:31:46 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113145_0123_m_000000_123' to o3fs://bucket1.vol1/testdata
21/01/20 11:31:46 INFO SparkHadoopMapRedUtil: attempt_20210120113145_0123_m_000000_123: Committed
21/01/20 11:31:46 INFO Executor: Finished task 0.0 in stage 123.0 (TID 123). 2155 bytes result sent to driver
21/01/20 11:31:46 INFO TaskSetManager: Finished task 0.0 in stage 123.0 (TID 123) in 1100 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:31:46 INFO TaskSchedulerImpl: Removed TaskSet 123.0, whose tasks have all completed, from pool 
21/01/20 11:31:46 INFO DAGScheduler: ResultStage 123 (parquet at Generate.java:61) finished in 1.120 s
21/01/20 11:31:46 INFO DAGScheduler: Job 123 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:31:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 123: Stage finished
21/01/20 11:31:46 INFO DAGScheduler: Job 123 finished: parquet at Generate.java:61, took 1.123411 s
21/01/20 11:31:46 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-b3f8b254-25a4-44cc-9948-303fecaa5aa3
21/01/20 11:31:46 INFO FileFormatWriter: Write Job fb6094d1-69ff-4aaa-85ed-0d4f814de42a committed.
21/01/20 11:31:46 INFO FileFormatWriter: Finished processing stats for write job fb6094d1-69ff-4aaa-85ed-0d4f814de42a.
21/01/20 11:31:47 INFO BlockManagerInfo: Removed broadcast_123_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:31:49 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:49 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:31:49 INFO DAGScheduler: Got job 124 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:31:49 INFO DAGScheduler: Final stage: ResultStage 124 (parquet at Generate.java:61)
21/01/20 11:31:49 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:31:49 INFO DAGScheduler: Missing parents: List()
21/01/20 11:31:49 INFO DAGScheduler: Submitting ResultStage 124 (MapPartitionsRDD[497] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:31:49 INFO MemoryStore: Block broadcast_124 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:31:49 INFO MemoryStore: Block broadcast_124_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:31:49 INFO BlockManagerInfo: Added broadcast_124_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:31:49 INFO SparkContext: Created broadcast 124 from broadcast at DAGScheduler.scala:1200
21/01/20 11:31:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 124 (MapPartitionsRDD[497] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:31:49 INFO TaskSchedulerImpl: Adding task set 124.0 with 1 tasks
21/01/20 11:31:49 WARN TaskSetManager: Stage 124 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:31:49 INFO TaskSetManager: Starting task 0.0 in stage 124.0 (TID 124, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:31:49 INFO Executor: Running task 0.0 in stage 124.0 (TID 124)
21/01/20 11:31:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:49 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:31:49 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:31:49 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:31:49 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:31:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:31:49 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:31:49 INFO ParquetOutputFormat: Validation is off
21/01/20 11:31:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:31:49 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:31:49 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:31:49 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:31:49 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:31:49 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:31:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:31:50 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113149_0124_m_000000_124' to o3fs://bucket1.vol1/testdata
21/01/20 11:31:50 INFO SparkHadoopMapRedUtil: attempt_20210120113149_0124_m_000000_124: Committed
21/01/20 11:31:50 INFO Executor: Finished task 0.0 in stage 124.0 (TID 124). 2155 bytes result sent to driver
21/01/20 11:31:50 INFO TaskSetManager: Finished task 0.0 in stage 124.0 (TID 124) in 1023 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:31:50 INFO TaskSchedulerImpl: Removed TaskSet 124.0, whose tasks have all completed, from pool 
21/01/20 11:31:50 INFO DAGScheduler: ResultStage 124 (parquet at Generate.java:61) finished in 1.042 s
21/01/20 11:31:50 INFO DAGScheduler: Job 124 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:31:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 124: Stage finished
21/01/20 11:31:50 INFO DAGScheduler: Job 124 finished: parquet at Generate.java:61, took 1.044128 s
21/01/20 11:31:50 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-c57335a3-d924-40a8-9f1b-2babb1d7bea7
21/01/20 11:31:50 INFO FileFormatWriter: Write Job 9a85d5da-d314-4aba-b87d-8a8aaa1294af committed.
21/01/20 11:31:50 INFO FileFormatWriter: Finished processing stats for write job 9a85d5da-d314-4aba-b87d-8a8aaa1294af.
21/01/20 11:31:51 INFO BlockManagerInfo: Removed broadcast_124_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:31:52 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:52 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:53 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:31:53 INFO DAGScheduler: Got job 125 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:31:53 INFO DAGScheduler: Final stage: ResultStage 125 (parquet at Generate.java:61)
21/01/20 11:31:53 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:31:53 INFO DAGScheduler: Missing parents: List()
21/01/20 11:31:53 INFO DAGScheduler: Submitting ResultStage 125 (MapPartitionsRDD[501] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:31:53 INFO MemoryStore: Block broadcast_125 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:31:53 INFO MemoryStore: Block broadcast_125_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:31:53 INFO BlockManagerInfo: Added broadcast_125_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:31:53 INFO SparkContext: Created broadcast 125 from broadcast at DAGScheduler.scala:1200
21/01/20 11:31:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 125 (MapPartitionsRDD[501] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:31:53 INFO TaskSchedulerImpl: Adding task set 125.0 with 1 tasks
21/01/20 11:31:53 WARN TaskSetManager: Stage 125 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:31:53 INFO TaskSetManager: Starting task 0.0 in stage 125.0 (TID 125, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:31:53 INFO Executor: Running task 0.0 in stage 125.0 (TID 125)
21/01/20 11:31:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:53 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:31:53 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:31:53 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:31:53 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:31:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:31:53 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:31:53 INFO ParquetOutputFormat: Validation is off
21/01/20 11:31:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:31:53 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:31:53 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:31:53 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:31:53 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:31:53 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:31:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:31:54 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113152_0125_m_000000_125' to o3fs://bucket1.vol1/testdata
21/01/20 11:31:54 INFO SparkHadoopMapRedUtil: attempt_20210120113152_0125_m_000000_125: Committed
21/01/20 11:31:54 INFO Executor: Finished task 0.0 in stage 125.0 (TID 125). 2155 bytes result sent to driver
21/01/20 11:31:54 INFO TaskSetManager: Finished task 0.0 in stage 125.0 (TID 125) in 1039 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:31:54 INFO TaskSchedulerImpl: Removed TaskSet 125.0, whose tasks have all completed, from pool 
21/01/20 11:31:54 INFO DAGScheduler: ResultStage 125 (parquet at Generate.java:61) finished in 1.057 s
21/01/20 11:31:54 INFO DAGScheduler: Job 125 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:31:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 125: Stage finished
21/01/20 11:31:54 INFO DAGScheduler: Job 125 finished: parquet at Generate.java:61, took 1.058756 s
21/01/20 11:31:54 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-5a3b3f5e-047c-4a48-8c39-b92df0fd76c1
21/01/20 11:31:54 INFO FileFormatWriter: Write Job ac3af291-58b8-4848-ae0f-5e3ebe464e87 committed.
21/01/20 11:31:54 INFO FileFormatWriter: Finished processing stats for write job ac3af291-58b8-4848-ae0f-5e3ebe464e87.
21/01/20 11:31:55 INFO BlockManagerInfo: Removed broadcast_125_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:31:56 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:56 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:56 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:31:56 INFO DAGScheduler: Got job 126 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:31:56 INFO DAGScheduler: Final stage: ResultStage 126 (parquet at Generate.java:61)
21/01/20 11:31:56 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:31:56 INFO DAGScheduler: Missing parents: List()
21/01/20 11:31:56 INFO DAGScheduler: Submitting ResultStage 126 (MapPartitionsRDD[505] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:31:56 INFO MemoryStore: Block broadcast_126 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:31:56 INFO MemoryStore: Block broadcast_126_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:31:56 INFO BlockManagerInfo: Added broadcast_126_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:31:56 INFO SparkContext: Created broadcast 126 from broadcast at DAGScheduler.scala:1200
21/01/20 11:31:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 126 (MapPartitionsRDD[505] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:31:56 INFO TaskSchedulerImpl: Adding task set 126.0 with 1 tasks
21/01/20 11:31:56 WARN TaskSetManager: Stage 126 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:31:56 INFO TaskSetManager: Starting task 0.0 in stage 126.0 (TID 126, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:31:56 INFO Executor: Running task 0.0 in stage 126.0 (TID 126)
21/01/20 11:31:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:56 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:31:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:31:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:31:56 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:31:56 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:31:56 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:31:56 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:31:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:31:56 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:31:56 INFO ParquetOutputFormat: Validation is off
21/01/20 11:31:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:31:56 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:31:56 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:31:56 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:31:56 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:31:56 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:31:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:31:57 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-742C005FCACA->27b66227-fed8-4c1d-ab57-510f5a3ce552
21/01/20 11:31:57 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:31:57 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113156_0126_m_000000_126' to o3fs://bucket1.vol1/testdata
21/01/20 11:31:57 INFO SparkHadoopMapRedUtil: attempt_20210120113156_0126_m_000000_126: Committed
21/01/20 11:31:57 INFO Executor: Finished task 0.0 in stage 126.0 (TID 126). 2155 bytes result sent to driver
21/01/20 11:31:57 INFO TaskSetManager: Finished task 0.0 in stage 126.0 (TID 126) in 1022 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:31:57 INFO TaskSchedulerImpl: Removed TaskSet 126.0, whose tasks have all completed, from pool 
21/01/20 11:31:57 INFO DAGScheduler: ResultStage 126 (parquet at Generate.java:61) finished in 1.042 s
21/01/20 11:31:57 INFO DAGScheduler: Job 126 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:31:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 126: Stage finished
21/01/20 11:31:57 INFO DAGScheduler: Job 126 finished: parquet at Generate.java:61, took 1.043117 s
21/01/20 11:31:57 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-888ed59a-f6e5-4014-8659-0974eb83fd4e
21/01/20 11:31:57 INFO FileFormatWriter: Write Job 6be11e88-8319-4a16-b4f3-027fdfeb63c7 committed.
21/01/20 11:31:57 INFO FileFormatWriter: Finished processing stats for write job 6be11e88-8319-4a16-b4f3-027fdfeb63c7.
21/01/20 11:31:59 INFO BlockManagerInfo: Removed broadcast_126_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:32:00 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:00 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:32:00 INFO DAGScheduler: Got job 127 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:32:00 INFO DAGScheduler: Final stage: ResultStage 127 (parquet at Generate.java:61)
21/01/20 11:32:00 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:32:00 INFO DAGScheduler: Missing parents: List()
21/01/20 11:32:00 INFO DAGScheduler: Submitting ResultStage 127 (MapPartitionsRDD[509] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:32:00 INFO MemoryStore: Block broadcast_127 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:32:00 INFO MemoryStore: Block broadcast_127_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:32:00 INFO BlockManagerInfo: Added broadcast_127_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:32:00 INFO SparkContext: Created broadcast 127 from broadcast at DAGScheduler.scala:1200
21/01/20 11:32:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 127 (MapPartitionsRDD[509] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:32:00 INFO TaskSchedulerImpl: Adding task set 127.0 with 1 tasks
21/01/20 11:32:00 WARN TaskSetManager: Stage 127 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:32:00 INFO TaskSetManager: Starting task 0.0 in stage 127.0 (TID 127, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:32:00 INFO Executor: Running task 0.0 in stage 127.0 (TID 127)
21/01/20 11:32:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:00 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:32:00 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:32:00 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:32:00 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:32:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:32:00 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:32:00 INFO ParquetOutputFormat: Validation is off
21/01/20 11:32:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:32:00 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:32:00 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:32:00 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:32:00 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:32:00 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:32:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:32:01 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113200_0127_m_000000_127' to o3fs://bucket1.vol1/testdata
21/01/20 11:32:01 INFO SparkHadoopMapRedUtil: attempt_20210120113200_0127_m_000000_127: Committed
21/01/20 11:32:01 INFO Executor: Finished task 0.0 in stage 127.0 (TID 127). 2155 bytes result sent to driver
21/01/20 11:32:01 INFO TaskSetManager: Finished task 0.0 in stage 127.0 (TID 127) in 799 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:32:01 INFO TaskSchedulerImpl: Removed TaskSet 127.0, whose tasks have all completed, from pool 
21/01/20 11:32:01 INFO DAGScheduler: ResultStage 127 (parquet at Generate.java:61) finished in 0.841 s
21/01/20 11:32:01 INFO DAGScheduler: Job 127 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:32:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 127: Stage finished
21/01/20 11:32:01 INFO DAGScheduler: Job 127 finished: parquet at Generate.java:61, took 0.844592 s
21/01/20 11:32:01 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-08c7c50a-8ab7-48e0-a853-452b60b286f4
21/01/20 11:32:01 INFO FileFormatWriter: Write Job 39d93024-600f-453c-977f-b449d5bfa221 committed.
21/01/20 11:32:01 INFO FileFormatWriter: Finished processing stats for write job 39d93024-600f-453c-977f-b449d5bfa221.
21/01/20 11:32:01 INFO BlockManagerInfo: Removed broadcast_127_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:32:03 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:03 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:32:03 INFO DAGScheduler: Got job 128 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:32:03 INFO DAGScheduler: Final stage: ResultStage 128 (parquet at Generate.java:61)
21/01/20 11:32:03 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:32:03 INFO DAGScheduler: Missing parents: List()
21/01/20 11:32:03 INFO DAGScheduler: Submitting ResultStage 128 (MapPartitionsRDD[513] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:32:03 INFO MemoryStore: Block broadcast_128 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:32:03 INFO MemoryStore: Block broadcast_128_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:32:03 INFO BlockManagerInfo: Added broadcast_128_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:32:03 INFO SparkContext: Created broadcast 128 from broadcast at DAGScheduler.scala:1200
21/01/20 11:32:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 128 (MapPartitionsRDD[513] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:32:03 INFO TaskSchedulerImpl: Adding task set 128.0 with 1 tasks
21/01/20 11:32:03 WARN TaskSetManager: Stage 128 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:32:03 INFO TaskSetManager: Starting task 0.0 in stage 128.0 (TID 128, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:32:03 INFO Executor: Running task 0.0 in stage 128.0 (TID 128)
21/01/20 11:32:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:03 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:32:03 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:32:03 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:32:03 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:32:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:32:03 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:32:03 INFO ParquetOutputFormat: Validation is off
21/01/20 11:32:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:32:03 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:32:03 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:32:03 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:32:03 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:32:03 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:32:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:32:04 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113203_0128_m_000000_128' to o3fs://bucket1.vol1/testdata
21/01/20 11:32:04 INFO SparkHadoopMapRedUtil: attempt_20210120113203_0128_m_000000_128: Committed
21/01/20 11:32:04 INFO Executor: Finished task 0.0 in stage 128.0 (TID 128). 2155 bytes result sent to driver
21/01/20 11:32:04 INFO TaskSetManager: Finished task 0.0 in stage 128.0 (TID 128) in 1031 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:32:04 INFO TaskSchedulerImpl: Removed TaskSet 128.0, whose tasks have all completed, from pool 
21/01/20 11:32:04 INFO DAGScheduler: ResultStage 128 (parquet at Generate.java:61) finished in 1.049 s
21/01/20 11:32:04 INFO DAGScheduler: Job 128 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:32:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 128: Stage finished
21/01/20 11:32:04 INFO DAGScheduler: Job 128 finished: parquet at Generate.java:61, took 1.050695 s
21/01/20 11:32:04 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-111f002f-d003-4313-8d0b-bb84d885b60a
21/01/20 11:32:04 INFO FileFormatWriter: Write Job f3faa114-ec90-4784-a9c2-b13a3ac88f0d committed.
21/01/20 11:32:04 INFO FileFormatWriter: Finished processing stats for write job f3faa114-ec90-4784-a9c2-b13a3ac88f0d.
21/01/20 11:32:06 INFO BlockManagerInfo: Removed broadcast_128_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:32:07 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:07 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:07 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:32:07 INFO DAGScheduler: Got job 129 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:32:07 INFO DAGScheduler: Final stage: ResultStage 129 (parquet at Generate.java:61)
21/01/20 11:32:07 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:32:07 INFO DAGScheduler: Missing parents: List()
21/01/20 11:32:07 INFO DAGScheduler: Submitting ResultStage 129 (MapPartitionsRDD[517] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:32:07 INFO MemoryStore: Block broadcast_129 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:32:07 INFO MemoryStore: Block broadcast_129_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:32:07 INFO BlockManagerInfo: Added broadcast_129_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:32:07 INFO SparkContext: Created broadcast 129 from broadcast at DAGScheduler.scala:1200
21/01/20 11:32:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 129 (MapPartitionsRDD[517] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:32:07 INFO TaskSchedulerImpl: Adding task set 129.0 with 1 tasks
21/01/20 11:32:07 WARN TaskSetManager: Stage 129 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:32:07 INFO TaskSetManager: Starting task 0.0 in stage 129.0 (TID 129, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:32:07 INFO Executor: Running task 0.0 in stage 129.0 (TID 129)
21/01/20 11:32:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:07 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:07 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:32:07 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:32:07 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:32:07 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:32:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:32:07 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:32:07 INFO ParquetOutputFormat: Validation is off
21/01/20 11:32:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:32:07 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:32:07 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:32:07 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:32:07 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:32:07 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:32:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:32:08 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113207_0129_m_000000_129' to o3fs://bucket1.vol1/testdata
21/01/20 11:32:08 INFO SparkHadoopMapRedUtil: attempt_20210120113207_0129_m_000000_129: Committed
21/01/20 11:32:08 INFO Executor: Finished task 0.0 in stage 129.0 (TID 129). 2155 bytes result sent to driver
21/01/20 11:32:08 INFO TaskSetManager: Finished task 0.0 in stage 129.0 (TID 129) in 910 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:32:08 INFO TaskSchedulerImpl: Removed TaskSet 129.0, whose tasks have all completed, from pool 
21/01/20 11:32:08 INFO DAGScheduler: ResultStage 129 (parquet at Generate.java:61) finished in 0.951 s
21/01/20 11:32:08 INFO DAGScheduler: Job 129 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:32:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 129: Stage finished
21/01/20 11:32:08 INFO DAGScheduler: Job 129 finished: parquet at Generate.java:61, took 0.953179 s
21/01/20 11:32:08 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-30807e11-3419-440d-9a40-8fa186b7fb06
21/01/20 11:32:08 INFO FileFormatWriter: Write Job 67dcdb94-871f-458d-88e3-be8a95916b1d committed.
21/01/20 11:32:08 INFO FileFormatWriter: Finished processing stats for write job 67dcdb94-871f-458d-88e3-be8a95916b1d.
21/01/20 11:32:08 INFO BlockManagerInfo: Removed broadcast_129_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:32:10 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:10 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:11 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:32:11 INFO DAGScheduler: Got job 130 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:32:11 INFO DAGScheduler: Final stage: ResultStage 130 (parquet at Generate.java:61)
21/01/20 11:32:11 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:32:11 INFO DAGScheduler: Missing parents: List()
21/01/20 11:32:11 INFO DAGScheduler: Submitting ResultStage 130 (MapPartitionsRDD[521] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:32:11 INFO MemoryStore: Block broadcast_130 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:32:11 INFO MemoryStore: Block broadcast_130_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:32:11 INFO BlockManagerInfo: Added broadcast_130_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:32:11 INFO SparkContext: Created broadcast 130 from broadcast at DAGScheduler.scala:1200
21/01/20 11:32:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 130 (MapPartitionsRDD[521] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:32:11 INFO TaskSchedulerImpl: Adding task set 130.0 with 1 tasks
21/01/20 11:32:11 WARN TaskSetManager: Stage 130 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:32:11 INFO TaskSetManager: Starting task 0.0 in stage 130.0 (TID 130, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:32:11 INFO Executor: Running task 0.0 in stage 130.0 (TID 130)
21/01/20 11:32:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:11 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:32:11 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:32:11 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:32:11 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:32:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:32:11 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:32:11 INFO ParquetOutputFormat: Validation is off
21/01/20 11:32:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:32:11 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:32:11 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:32:11 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:32:11 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:32:11 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:32:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:32:12 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113210_0130_m_000000_130' to o3fs://bucket1.vol1/testdata
21/01/20 11:32:12 INFO SparkHadoopMapRedUtil: attempt_20210120113210_0130_m_000000_130: Committed
21/01/20 11:32:12 INFO Executor: Finished task 0.0 in stage 130.0 (TID 130). 2155 bytes result sent to driver
21/01/20 11:32:12 INFO TaskSetManager: Finished task 0.0 in stage 130.0 (TID 130) in 1070 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:32:12 INFO TaskSchedulerImpl: Removed TaskSet 130.0, whose tasks have all completed, from pool 
21/01/20 11:32:12 INFO DAGScheduler: ResultStage 130 (parquet at Generate.java:61) finished in 1.087 s
21/01/20 11:32:12 INFO DAGScheduler: Job 130 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:32:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 130: Stage finished
21/01/20 11:32:12 INFO DAGScheduler: Job 130 finished: parquet at Generate.java:61, took 1.089254 s
21/01/20 11:32:12 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-6790a8fd-6def-4333-8204-e72352598ea2
21/01/20 11:32:12 INFO FileFormatWriter: Write Job 59a81c8f-9b82-4367-9959-bf8a6610871c committed.
21/01/20 11:32:12 INFO FileFormatWriter: Finished processing stats for write job 59a81c8f-9b82-4367-9959-bf8a6610871c.
21/01/20 11:32:13 INFO BlockManagerInfo: Removed broadcast_130_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:32:14 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:14 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:14 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:32:14 INFO DAGScheduler: Got job 131 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:32:14 INFO DAGScheduler: Final stage: ResultStage 131 (parquet at Generate.java:61)
21/01/20 11:32:14 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:32:14 INFO DAGScheduler: Missing parents: List()
21/01/20 11:32:14 INFO DAGScheduler: Submitting ResultStage 131 (MapPartitionsRDD[525] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:32:14 INFO MemoryStore: Block broadcast_131 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:32:14 INFO MemoryStore: Block broadcast_131_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:32:14 INFO BlockManagerInfo: Added broadcast_131_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:32:14 INFO SparkContext: Created broadcast 131 from broadcast at DAGScheduler.scala:1200
21/01/20 11:32:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 131 (MapPartitionsRDD[525] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:32:14 INFO TaskSchedulerImpl: Adding task set 131.0 with 1 tasks
21/01/20 11:32:14 WARN TaskSetManager: Stage 131 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:32:14 INFO TaskSetManager: Starting task 0.0 in stage 131.0 (TID 131, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:32:14 INFO Executor: Running task 0.0 in stage 131.0 (TID 131)
21/01/20 11:32:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:14 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:14 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:32:14 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:32:14 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:32:14 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:32:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:32:14 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:32:14 INFO ParquetOutputFormat: Validation is off
21/01/20 11:32:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:32:14 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:32:14 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:32:14 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:32:14 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:32:14 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:32:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:32:15 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113214_0131_m_000000_131' to o3fs://bucket1.vol1/testdata
21/01/20 11:32:15 INFO SparkHadoopMapRedUtil: attempt_20210120113214_0131_m_000000_131: Committed
21/01/20 11:32:15 INFO Executor: Finished task 0.0 in stage 131.0 (TID 131). 2155 bytes result sent to driver
21/01/20 11:32:15 INFO TaskSetManager: Finished task 0.0 in stage 131.0 (TID 131) in 1072 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:32:15 INFO TaskSchedulerImpl: Removed TaskSet 131.0, whose tasks have all completed, from pool 
21/01/20 11:32:15 INFO DAGScheduler: ResultStage 131 (parquet at Generate.java:61) finished in 1.091 s
21/01/20 11:32:15 INFO DAGScheduler: Job 131 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:32:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 131: Stage finished
21/01/20 11:32:15 INFO DAGScheduler: Job 131 finished: parquet at Generate.java:61, took 1.093148 s
21/01/20 11:32:15 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-abb4be52-bdc4-4ab3-8319-6061b10ade4c
21/01/20 11:32:15 INFO FileFormatWriter: Write Job 2c248ea8-4d22-439e-9b01-269ae1d03a0d committed.
21/01/20 11:32:15 INFO FileFormatWriter: Finished processing stats for write job 2c248ea8-4d22-439e-9b01-269ae1d03a0d.
21/01/20 11:32:16 INFO BlockManagerInfo: Removed broadcast_131_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:32:18 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:18 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:18 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:32:18 INFO DAGScheduler: Got job 132 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:32:18 INFO DAGScheduler: Final stage: ResultStage 132 (parquet at Generate.java:61)
21/01/20 11:32:18 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:32:18 INFO DAGScheduler: Missing parents: List()
21/01/20 11:32:18 INFO DAGScheduler: Submitting ResultStage 132 (MapPartitionsRDD[529] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:32:18 INFO MemoryStore: Block broadcast_132 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:32:18 INFO MemoryStore: Block broadcast_132_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:32:18 INFO BlockManagerInfo: Added broadcast_132_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:32:18 INFO SparkContext: Created broadcast 132 from broadcast at DAGScheduler.scala:1200
21/01/20 11:32:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 132 (MapPartitionsRDD[529] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:32:18 INFO TaskSchedulerImpl: Adding task set 132.0 with 1 tasks
21/01/20 11:32:18 WARN TaskSetManager: Stage 132 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:32:18 INFO TaskSetManager: Starting task 0.0 in stage 132.0 (TID 132, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:32:18 INFO Executor: Running task 0.0 in stage 132.0 (TID 132)
21/01/20 11:32:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:18 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:18 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:32:18 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:32:18 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:32:18 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:32:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:32:18 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:32:18 INFO ParquetOutputFormat: Validation is off
21/01/20 11:32:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:32:18 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:32:18 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:32:18 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:32:18 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:32:18 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:32:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:32:19 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113218_0132_m_000000_132' to o3fs://bucket1.vol1/testdata
21/01/20 11:32:19 INFO SparkHadoopMapRedUtil: attempt_20210120113218_0132_m_000000_132: Committed
21/01/20 11:32:19 INFO Executor: Finished task 0.0 in stage 132.0 (TID 132). 2155 bytes result sent to driver
21/01/20 11:32:19 INFO TaskSetManager: Finished task 0.0 in stage 132.0 (TID 132) in 1102 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:32:19 INFO TaskSchedulerImpl: Removed TaskSet 132.0, whose tasks have all completed, from pool 
21/01/20 11:32:19 INFO DAGScheduler: ResultStage 132 (parquet at Generate.java:61) finished in 1.120 s
21/01/20 11:32:19 INFO DAGScheduler: Job 132 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:32:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 132: Stage finished
21/01/20 11:32:19 INFO DAGScheduler: Job 132 finished: parquet at Generate.java:61, took 1.122430 s
21/01/20 11:32:19 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-1e788631-e4a9-406f-9452-55682f0073c0
21/01/20 11:32:19 INFO FileFormatWriter: Write Job 7029c02a-ee30-4d38-b830-fd4d90c35681 committed.
21/01/20 11:32:19 INFO FileFormatWriter: Finished processing stats for write job 7029c02a-ee30-4d38-b830-fd4d90c35681.
21/01/20 11:32:20 INFO BlockManagerInfo: Removed broadcast_132_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:32:22 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:22 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:22 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:32:22 INFO DAGScheduler: Got job 133 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:32:22 INFO DAGScheduler: Final stage: ResultStage 133 (parquet at Generate.java:61)
21/01/20 11:32:22 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:32:22 INFO DAGScheduler: Missing parents: List()
21/01/20 11:32:22 INFO DAGScheduler: Submitting ResultStage 133 (MapPartitionsRDD[533] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:32:22 INFO MemoryStore: Block broadcast_133 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:32:22 INFO MemoryStore: Block broadcast_133_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:32:22 INFO BlockManagerInfo: Added broadcast_133_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:32:22 INFO SparkContext: Created broadcast 133 from broadcast at DAGScheduler.scala:1200
21/01/20 11:32:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 133 (MapPartitionsRDD[533] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:32:22 INFO TaskSchedulerImpl: Adding task set 133.0 with 1 tasks
21/01/20 11:32:22 WARN TaskSetManager: Stage 133 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:32:22 INFO TaskSetManager: Starting task 0.0 in stage 133.0 (TID 133, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:32:22 INFO Executor: Running task 0.0 in stage 133.0 (TID 133)
21/01/20 11:32:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:22 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:22 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:32:22 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:32:22 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:32:22 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:32:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:32:22 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:32:22 INFO ParquetOutputFormat: Validation is off
21/01/20 11:32:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:32:22 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:32:22 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:32:22 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:32:22 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:32:22 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:32:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:32:22 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-362DEDD741C3->8df234bd-f131-4d8e-953a-b9eec8633221
21/01/20 11:32:22 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:32:23 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113222_0133_m_000000_133' to o3fs://bucket1.vol1/testdata
21/01/20 11:32:23 INFO SparkHadoopMapRedUtil: attempt_20210120113222_0133_m_000000_133: Committed
21/01/20 11:32:23 INFO Executor: Finished task 0.0 in stage 133.0 (TID 133). 2155 bytes result sent to driver
21/01/20 11:32:23 INFO TaskSetManager: Finished task 0.0 in stage 133.0 (TID 133) in 1050 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:32:23 INFO TaskSchedulerImpl: Removed TaskSet 133.0, whose tasks have all completed, from pool 
21/01/20 11:32:23 INFO DAGScheduler: ResultStage 133 (parquet at Generate.java:61) finished in 1.068 s
21/01/20 11:32:23 INFO DAGScheduler: Job 133 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:32:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 133: Stage finished
21/01/20 11:32:23 INFO DAGScheduler: Job 133 finished: parquet at Generate.java:61, took 1.069230 s
21/01/20 11:32:23 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-bc148060-4c88-4875-9fb0-4934d7cc9342
21/01/20 11:32:23 INFO FileFormatWriter: Write Job 14b97954-7305-4419-bf4a-ba0336b4524d committed.
21/01/20 11:32:23 INFO FileFormatWriter: Finished processing stats for write job 14b97954-7305-4419-bf4a-ba0336b4524d.
21/01/20 11:32:24 INFO BlockManagerInfo: Removed broadcast_133_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:32:25 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:25 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:25 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:32:25 INFO DAGScheduler: Got job 134 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:32:25 INFO DAGScheduler: Final stage: ResultStage 134 (parquet at Generate.java:61)
21/01/20 11:32:25 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:32:25 INFO DAGScheduler: Missing parents: List()
21/01/20 11:32:25 INFO DAGScheduler: Submitting ResultStage 134 (MapPartitionsRDD[537] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:32:25 INFO MemoryStore: Block broadcast_134 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:32:25 INFO MemoryStore: Block broadcast_134_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:32:25 INFO BlockManagerInfo: Added broadcast_134_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:32:25 INFO SparkContext: Created broadcast 134 from broadcast at DAGScheduler.scala:1200
21/01/20 11:32:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 134 (MapPartitionsRDD[537] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:32:25 INFO TaskSchedulerImpl: Adding task set 134.0 with 1 tasks
21/01/20 11:32:25 WARN TaskSetManager: Stage 134 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:32:25 INFO TaskSetManager: Starting task 0.0 in stage 134.0 (TID 134, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:32:25 INFO Executor: Running task 0.0 in stage 134.0 (TID 134)
21/01/20 11:32:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:25 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:25 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:32:25 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:32:25 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:32:25 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:32:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:32:25 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:32:25 INFO ParquetOutputFormat: Validation is off
21/01/20 11:32:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:32:25 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:32:25 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:32:25 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:32:25 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:32:25 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:32:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:32:26 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113225_0134_m_000000_134' to o3fs://bucket1.vol1/testdata
21/01/20 11:32:26 INFO SparkHadoopMapRedUtil: attempt_20210120113225_0134_m_000000_134: Committed
21/01/20 11:32:26 INFO Executor: Finished task 0.0 in stage 134.0 (TID 134). 2155 bytes result sent to driver
21/01/20 11:32:26 INFO TaskSetManager: Finished task 0.0 in stage 134.0 (TID 134) in 1070 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:32:26 INFO TaskSchedulerImpl: Removed TaskSet 134.0, whose tasks have all completed, from pool 
21/01/20 11:32:26 INFO DAGScheduler: ResultStage 134 (parquet at Generate.java:61) finished in 1.090 s
21/01/20 11:32:26 INFO DAGScheduler: Job 134 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:32:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 134: Stage finished
21/01/20 11:32:26 INFO DAGScheduler: Job 134 finished: parquet at Generate.java:61, took 1.091457 s
21/01/20 11:32:26 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-10f7db05-59cb-47f2-ab84-138672810a6d
21/01/20 11:32:26 INFO FileFormatWriter: Write Job 1bc3b7b0-6d67-492a-9110-77b4af6add54 committed.
21/01/20 11:32:26 INFO FileFormatWriter: Finished processing stats for write job 1bc3b7b0-6d67-492a-9110-77b4af6add54.
21/01/20 11:32:28 INFO BlockManagerInfo: Removed broadcast_134_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:32:29 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:29 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:32:29 INFO DAGScheduler: Got job 135 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:32:29 INFO DAGScheduler: Final stage: ResultStage 135 (parquet at Generate.java:61)
21/01/20 11:32:29 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:32:29 INFO DAGScheduler: Missing parents: List()
21/01/20 11:32:29 INFO DAGScheduler: Submitting ResultStage 135 (MapPartitionsRDD[541] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:32:29 INFO MemoryStore: Block broadcast_135 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:32:29 INFO MemoryStore: Block broadcast_135_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:32:29 INFO BlockManagerInfo: Added broadcast_135_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:32:29 INFO SparkContext: Created broadcast 135 from broadcast at DAGScheduler.scala:1200
21/01/20 11:32:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 135 (MapPartitionsRDD[541] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:32:29 INFO TaskSchedulerImpl: Adding task set 135.0 with 1 tasks
21/01/20 11:32:29 WARN TaskSetManager: Stage 135 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:32:29 INFO TaskSetManager: Starting task 0.0 in stage 135.0 (TID 135, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:32:29 INFO Executor: Running task 0.0 in stage 135.0 (TID 135)
21/01/20 11:32:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:29 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:32:29 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:32:29 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:32:29 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:32:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:32:29 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:32:29 INFO ParquetOutputFormat: Validation is off
21/01/20 11:32:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:32:29 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:32:29 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:32:29 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:32:29 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:32:29 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:32:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:32:30 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-3BAB2852A23F->27b66227-fed8-4c1d-ab57-510f5a3ce552
21/01/20 11:32:30 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:32:30 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113229_0135_m_000000_135' to o3fs://bucket1.vol1/testdata
21/01/20 11:32:30 INFO SparkHadoopMapRedUtil: attempt_20210120113229_0135_m_000000_135: Committed
21/01/20 11:32:30 INFO Executor: Finished task 0.0 in stage 135.0 (TID 135). 2155 bytes result sent to driver
21/01/20 11:32:30 INFO TaskSetManager: Finished task 0.0 in stage 135.0 (TID 135) in 1021 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:32:30 INFO TaskSchedulerImpl: Removed TaskSet 135.0, whose tasks have all completed, from pool 
21/01/20 11:32:30 INFO DAGScheduler: ResultStage 135 (parquet at Generate.java:61) finished in 1.041 s
21/01/20 11:32:30 INFO DAGScheduler: Job 135 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:32:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 135: Stage finished
21/01/20 11:32:30 INFO DAGScheduler: Job 135 finished: parquet at Generate.java:61, took 1.043531 s
21/01/20 11:32:30 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-b8318734-5d1b-4e02-9e49-82e2b9d705e3
21/01/20 11:32:30 INFO FileFormatWriter: Write Job 54ad78e5-836b-49da-b9b0-555deb8a4a6f committed.
21/01/20 11:32:30 INFO FileFormatWriter: Finished processing stats for write job 54ad78e5-836b-49da-b9b0-555deb8a4a6f.
21/01/20 11:32:31 INFO BlockManagerInfo: Removed broadcast_135_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:32:33 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:33 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:33 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:32:33 INFO DAGScheduler: Got job 136 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:32:33 INFO DAGScheduler: Final stage: ResultStage 136 (parquet at Generate.java:61)
21/01/20 11:32:33 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:32:33 INFO DAGScheduler: Missing parents: List()
21/01/20 11:32:33 INFO DAGScheduler: Submitting ResultStage 136 (MapPartitionsRDD[545] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:32:33 INFO MemoryStore: Block broadcast_136 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:32:33 INFO MemoryStore: Block broadcast_136_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:32:33 INFO BlockManagerInfo: Added broadcast_136_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:32:33 INFO SparkContext: Created broadcast 136 from broadcast at DAGScheduler.scala:1200
21/01/20 11:32:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 136 (MapPartitionsRDD[545] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:32:33 INFO TaskSchedulerImpl: Adding task set 136.0 with 1 tasks
21/01/20 11:32:33 WARN TaskSetManager: Stage 136 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:32:33 INFO TaskSetManager: Starting task 0.0 in stage 136.0 (TID 136, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:32:33 INFO Executor: Running task 0.0 in stage 136.0 (TID 136)
21/01/20 11:32:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:33 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:33 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:32:33 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:32:33 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:32:33 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:32:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:32:33 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:32:33 INFO ParquetOutputFormat: Validation is off
21/01/20 11:32:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:32:33 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:32:33 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:32:33 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:32:33 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:32:33 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:32:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:32:34 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113233_0136_m_000000_136' to o3fs://bucket1.vol1/testdata
21/01/20 11:32:34 INFO SparkHadoopMapRedUtil: attempt_20210120113233_0136_m_000000_136: Committed
21/01/20 11:32:34 INFO Executor: Finished task 0.0 in stage 136.0 (TID 136). 2155 bytes result sent to driver
21/01/20 11:32:34 INFO TaskSetManager: Finished task 0.0 in stage 136.0 (TID 136) in 927 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:32:34 INFO TaskSchedulerImpl: Removed TaskSet 136.0, whose tasks have all completed, from pool 
21/01/20 11:32:34 INFO DAGScheduler: ResultStage 136 (parquet at Generate.java:61) finished in 0.967 s
21/01/20 11:32:34 INFO DAGScheduler: Job 136 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:32:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 136: Stage finished
21/01/20 11:32:34 INFO DAGScheduler: Job 136 finished: parquet at Generate.java:61, took 0.969571 s
21/01/20 11:32:34 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-35a94418-f7ea-4129-9274-942a103b6cf2
21/01/20 11:32:34 INFO FileFormatWriter: Write Job e229b389-88b3-46df-a359-6d16da99aeba committed.
21/01/20 11:32:34 INFO FileFormatWriter: Finished processing stats for write job e229b389-88b3-46df-a359-6d16da99aeba.
21/01/20 11:32:34 INFO BlockManagerInfo: Removed broadcast_136_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:32:36 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:36 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:36 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:32:36 INFO DAGScheduler: Got job 137 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:32:36 INFO DAGScheduler: Final stage: ResultStage 137 (parquet at Generate.java:61)
21/01/20 11:32:36 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:32:36 INFO DAGScheduler: Missing parents: List()
21/01/20 11:32:36 INFO DAGScheduler: Submitting ResultStage 137 (MapPartitionsRDD[549] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:32:36 INFO MemoryStore: Block broadcast_137 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:32:36 INFO MemoryStore: Block broadcast_137_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:32:36 INFO BlockManagerInfo: Added broadcast_137_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:32:36 INFO SparkContext: Created broadcast 137 from broadcast at DAGScheduler.scala:1200
21/01/20 11:32:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 137 (MapPartitionsRDD[549] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:32:36 INFO TaskSchedulerImpl: Adding task set 137.0 with 1 tasks
21/01/20 11:32:36 WARN TaskSetManager: Stage 137 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:32:36 INFO TaskSetManager: Starting task 0.0 in stage 137.0 (TID 137, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:32:36 INFO Executor: Running task 0.0 in stage 137.0 (TID 137)
21/01/20 11:32:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:36 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:36 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:32:36 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:32:36 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:32:36 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:32:36 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:32:36 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:32:36 INFO ParquetOutputFormat: Validation is off
21/01/20 11:32:36 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:32:36 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:32:36 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:32:36 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:32:36 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:32:36 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:32:37 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:32:37 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-D3475003B055->8df234bd-f131-4d8e-953a-b9eec8633221
21/01/20 11:32:37 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:32:37 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113236_0137_m_000000_137' to o3fs://bucket1.vol1/testdata
21/01/20 11:32:37 INFO SparkHadoopMapRedUtil: attempt_20210120113236_0137_m_000000_137: Committed
21/01/20 11:32:37 INFO Executor: Finished task 0.0 in stage 137.0 (TID 137). 2155 bytes result sent to driver
21/01/20 11:32:37 INFO TaskSetManager: Finished task 0.0 in stage 137.0 (TID 137) in 1003 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:32:37 INFO TaskSchedulerImpl: Removed TaskSet 137.0, whose tasks have all completed, from pool 
21/01/20 11:32:37 INFO DAGScheduler: ResultStage 137 (parquet at Generate.java:61) finished in 1.022 s
21/01/20 11:32:37 INFO DAGScheduler: Job 137 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:32:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 137: Stage finished
21/01/20 11:32:37 INFO DAGScheduler: Job 137 finished: parquet at Generate.java:61, took 1.024172 s
21/01/20 11:32:37 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-825b8369-8895-42a5-bf5f-0ebfb6e3bdd5
21/01/20 11:32:37 INFO FileFormatWriter: Write Job 81da80da-a5b8-47d8-a42a-7b00d0910dd4 committed.
21/01/20 11:32:37 INFO FileFormatWriter: Finished processing stats for write job 81da80da-a5b8-47d8-a42a-7b00d0910dd4.
21/01/20 11:32:39 INFO BlockManagerInfo: Removed broadcast_137_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:32:40 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:40 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:40 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:32:40 INFO DAGScheduler: Got job 138 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:32:40 INFO DAGScheduler: Final stage: ResultStage 138 (parquet at Generate.java:61)
21/01/20 11:32:40 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:32:40 INFO DAGScheduler: Missing parents: List()
21/01/20 11:32:40 INFO DAGScheduler: Submitting ResultStage 138 (MapPartitionsRDD[553] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:32:40 INFO MemoryStore: Block broadcast_138 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:32:40 INFO MemoryStore: Block broadcast_138_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:32:40 INFO BlockManagerInfo: Added broadcast_138_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:32:40 INFO SparkContext: Created broadcast 138 from broadcast at DAGScheduler.scala:1200
21/01/20 11:32:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 138 (MapPartitionsRDD[553] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:32:40 INFO TaskSchedulerImpl: Adding task set 138.0 with 1 tasks
21/01/20 11:32:40 WARN TaskSetManager: Stage 138 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:32:40 INFO TaskSetManager: Starting task 0.0 in stage 138.0 (TID 138, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:32:40 INFO Executor: Running task 0.0 in stage 138.0 (TID 138)
21/01/20 11:32:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:40 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:40 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:32:40 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:32:40 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:32:40 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:32:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:32:40 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:32:40 INFO ParquetOutputFormat: Validation is off
21/01/20 11:32:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:32:40 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:32:40 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:32:40 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:32:40 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:32:40 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:32:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:32:41 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113240_0138_m_000000_138' to o3fs://bucket1.vol1/testdata
21/01/20 11:32:41 INFO SparkHadoopMapRedUtil: attempt_20210120113240_0138_m_000000_138: Committed
21/01/20 11:32:41 INFO Executor: Finished task 0.0 in stage 138.0 (TID 138). 2155 bytes result sent to driver
21/01/20 11:32:41 INFO TaskSetManager: Finished task 0.0 in stage 138.0 (TID 138) in 750 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:32:41 INFO TaskSchedulerImpl: Removed TaskSet 138.0, whose tasks have all completed, from pool 
21/01/20 11:32:41 INFO DAGScheduler: ResultStage 138 (parquet at Generate.java:61) finished in 0.793 s
21/01/20 11:32:41 INFO DAGScheduler: Job 138 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:32:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 138: Stage finished
21/01/20 11:32:41 INFO DAGScheduler: Job 138 finished: parquet at Generate.java:61, took 0.794212 s
21/01/20 11:32:41 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-c4621fb8-33f7-463d-88bb-d8f9f7e311ba
21/01/20 11:32:41 INFO FileFormatWriter: Write Job ebf9fbec-f97c-46b8-b78e-d64e7955ba47 committed.
21/01/20 11:32:41 INFO FileFormatWriter: Finished processing stats for write job ebf9fbec-f97c-46b8-b78e-d64e7955ba47.
21/01/20 11:32:41 INFO BlockManagerInfo: Removed broadcast_138_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:32:43 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:43 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:43 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:32:43 INFO DAGScheduler: Got job 139 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:32:43 INFO DAGScheduler: Final stage: ResultStage 139 (parquet at Generate.java:61)
21/01/20 11:32:43 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:32:43 INFO DAGScheduler: Missing parents: List()
21/01/20 11:32:43 INFO DAGScheduler: Submitting ResultStage 139 (MapPartitionsRDD[557] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:32:43 INFO MemoryStore: Block broadcast_139 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:32:43 INFO MemoryStore: Block broadcast_139_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:32:43 INFO BlockManagerInfo: Added broadcast_139_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:32:43 INFO SparkContext: Created broadcast 139 from broadcast at DAGScheduler.scala:1200
21/01/20 11:32:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 139 (MapPartitionsRDD[557] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:32:43 INFO TaskSchedulerImpl: Adding task set 139.0 with 1 tasks
21/01/20 11:32:43 WARN TaskSetManager: Stage 139 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:32:43 INFO TaskSetManager: Starting task 0.0 in stage 139.0 (TID 139, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:32:43 INFO Executor: Running task 0.0 in stage 139.0 (TID 139)
21/01/20 11:32:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:44 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:44 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:32:44 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:32:44 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:32:44 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:32:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:32:44 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:32:44 INFO ParquetOutputFormat: Validation is off
21/01/20 11:32:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:32:44 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:32:44 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:32:44 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:32:44 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:32:44 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:32:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:32:44 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-A794E574E213->27b66227-fed8-4c1d-ab57-510f5a3ce552
21/01/20 11:32:44 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:32:45 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113243_0139_m_000000_139' to o3fs://bucket1.vol1/testdata
21/01/20 11:32:45 INFO SparkHadoopMapRedUtil: attempt_20210120113243_0139_m_000000_139: Committed
21/01/20 11:32:45 INFO Executor: Finished task 0.0 in stage 139.0 (TID 139). 2155 bytes result sent to driver
21/01/20 11:32:45 INFO TaskSetManager: Finished task 0.0 in stage 139.0 (TID 139) in 1125 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:32:45 INFO TaskSchedulerImpl: Removed TaskSet 139.0, whose tasks have all completed, from pool 
21/01/20 11:32:45 INFO DAGScheduler: ResultStage 139 (parquet at Generate.java:61) finished in 1.144 s
21/01/20 11:32:45 INFO DAGScheduler: Job 139 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:32:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 139: Stage finished
21/01/20 11:32:45 INFO DAGScheduler: Job 139 finished: parquet at Generate.java:61, took 1.146211 s
21/01/20 11:32:45 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-1140fc7e-5608-4cfe-acf5-61d690b06e78
21/01/20 11:32:45 INFO FileFormatWriter: Write Job 471f44f3-312a-437e-9b23-d0dc79ee8687 committed.
21/01/20 11:32:45 INFO FileFormatWriter: Finished processing stats for write job 471f44f3-312a-437e-9b23-d0dc79ee8687.
21/01/20 11:32:46 INFO BlockManagerInfo: Removed broadcast_139_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:32:47 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:47 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:32:47 INFO DAGScheduler: Got job 140 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:32:47 INFO DAGScheduler: Final stage: ResultStage 140 (parquet at Generate.java:61)
21/01/20 11:32:47 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:32:47 INFO DAGScheduler: Missing parents: List()
21/01/20 11:32:47 INFO DAGScheduler: Submitting ResultStage 140 (MapPartitionsRDD[561] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:32:47 INFO MemoryStore: Block broadcast_140 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:32:47 INFO MemoryStore: Block broadcast_140_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:32:47 INFO BlockManagerInfo: Added broadcast_140_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:32:47 INFO SparkContext: Created broadcast 140 from broadcast at DAGScheduler.scala:1200
21/01/20 11:32:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 140 (MapPartitionsRDD[561] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:32:47 INFO TaskSchedulerImpl: Adding task set 140.0 with 1 tasks
21/01/20 11:32:47 WARN TaskSetManager: Stage 140 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:32:47 INFO TaskSetManager: Starting task 0.0 in stage 140.0 (TID 140, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:32:47 INFO Executor: Running task 0.0 in stage 140.0 (TID 140)
21/01/20 11:32:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:47 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:32:47 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:32:47 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:32:47 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:32:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:32:47 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:32:47 INFO ParquetOutputFormat: Validation is off
21/01/20 11:32:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:32:47 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:32:47 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:32:47 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:32:47 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:32:47 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:32:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:32:48 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113247_0140_m_000000_140' to o3fs://bucket1.vol1/testdata
21/01/20 11:32:48 INFO SparkHadoopMapRedUtil: attempt_20210120113247_0140_m_000000_140: Committed
21/01/20 11:32:48 INFO Executor: Finished task 0.0 in stage 140.0 (TID 140). 2155 bytes result sent to driver
21/01/20 11:32:48 INFO TaskSetManager: Finished task 0.0 in stage 140.0 (TID 140) in 1088 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:32:48 INFO TaskSchedulerImpl: Removed TaskSet 140.0, whose tasks have all completed, from pool 
21/01/20 11:32:48 INFO DAGScheduler: ResultStage 140 (parquet at Generate.java:61) finished in 1.106 s
21/01/20 11:32:48 INFO DAGScheduler: Job 140 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:32:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 140: Stage finished
21/01/20 11:32:48 INFO DAGScheduler: Job 140 finished: parquet at Generate.java:61, took 1.108017 s
21/01/20 11:32:48 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-8c20a18f-5bb0-4a10-bf1c-f874a16f933f
21/01/20 11:32:48 INFO FileFormatWriter: Write Job de7da1a2-c01a-4861-ac03-47661997590c committed.
21/01/20 11:32:48 INFO FileFormatWriter: Finished processing stats for write job de7da1a2-c01a-4861-ac03-47661997590c.
21/01/20 11:32:49 INFO BlockManagerInfo: Removed broadcast_140_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:32:51 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:51 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:32:51 INFO DAGScheduler: Got job 141 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:32:51 INFO DAGScheduler: Final stage: ResultStage 141 (parquet at Generate.java:61)
21/01/20 11:32:51 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:32:51 INFO DAGScheduler: Missing parents: List()
21/01/20 11:32:51 INFO DAGScheduler: Submitting ResultStage 141 (MapPartitionsRDD[565] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:32:51 INFO MemoryStore: Block broadcast_141 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:32:51 INFO MemoryStore: Block broadcast_141_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:32:51 INFO BlockManagerInfo: Added broadcast_141_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:32:51 INFO SparkContext: Created broadcast 141 from broadcast at DAGScheduler.scala:1200
21/01/20 11:32:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 141 (MapPartitionsRDD[565] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:32:51 INFO TaskSchedulerImpl: Adding task set 141.0 with 1 tasks
21/01/20 11:32:51 WARN TaskSetManager: Stage 141 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:32:51 INFO TaskSetManager: Starting task 0.0 in stage 141.0 (TID 141, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:32:51 INFO Executor: Running task 0.0 in stage 141.0 (TID 141)
21/01/20 11:32:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:51 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:32:51 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:32:51 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:32:51 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:32:51 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:32:51 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:32:51 INFO ParquetOutputFormat: Validation is off
21/01/20 11:32:51 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:32:51 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:32:51 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:32:51 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:32:51 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:32:51 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:32:51 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:32:52 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113251_0141_m_000000_141' to o3fs://bucket1.vol1/testdata
21/01/20 11:32:52 INFO SparkHadoopMapRedUtil: attempt_20210120113251_0141_m_000000_141: Committed
21/01/20 11:32:52 INFO Executor: Finished task 0.0 in stage 141.0 (TID 141). 2155 bytes result sent to driver
21/01/20 11:32:52 INFO TaskSetManager: Finished task 0.0 in stage 141.0 (TID 141) in 1090 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:32:52 INFO TaskSchedulerImpl: Removed TaskSet 141.0, whose tasks have all completed, from pool 
21/01/20 11:32:52 INFO DAGScheduler: ResultStage 141 (parquet at Generate.java:61) finished in 1.109 s
21/01/20 11:32:52 INFO DAGScheduler: Job 141 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:32:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 141: Stage finished
21/01/20 11:32:52 INFO DAGScheduler: Job 141 finished: parquet at Generate.java:61, took 1.111227 s
21/01/20 11:32:52 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-392eb9e9-a263-400a-8197-fd943e4ed0f7
21/01/20 11:32:52 INFO FileFormatWriter: Write Job 0fa35914-802f-4512-9dcc-8887e7773797 committed.
21/01/20 11:32:52 INFO FileFormatWriter: Finished processing stats for write job 0fa35914-802f-4512-9dcc-8887e7773797.
21/01/20 11:32:53 INFO BlockManagerInfo: Removed broadcast_141_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:32:55 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:55 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:32:55 INFO DAGScheduler: Got job 142 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:32:55 INFO DAGScheduler: Final stage: ResultStage 142 (parquet at Generate.java:61)
21/01/20 11:32:55 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:32:55 INFO DAGScheduler: Missing parents: List()
21/01/20 11:32:55 INFO DAGScheduler: Submitting ResultStage 142 (MapPartitionsRDD[569] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:32:55 INFO MemoryStore: Block broadcast_142 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:32:55 INFO MemoryStore: Block broadcast_142_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:32:55 INFO BlockManagerInfo: Added broadcast_142_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:32:55 INFO SparkContext: Created broadcast 142 from broadcast at DAGScheduler.scala:1200
21/01/20 11:32:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 142 (MapPartitionsRDD[569] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:32:55 INFO TaskSchedulerImpl: Adding task set 142.0 with 1 tasks
21/01/20 11:32:55 WARN TaskSetManager: Stage 142 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:32:55 INFO TaskSetManager: Starting task 0.0 in stage 142.0 (TID 142, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:32:55 INFO Executor: Running task 0.0 in stage 142.0 (TID 142)
21/01/20 11:32:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:55 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:32:55 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:32:55 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:32:55 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:32:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:32:55 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:32:55 INFO ParquetOutputFormat: Validation is off
21/01/20 11:32:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:32:55 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:32:55 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:32:55 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:32:55 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:32:55 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:32:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:32:55 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-128341067DF0->8df234bd-f131-4d8e-953a-b9eec8633221
21/01/20 11:32:55 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:32:56 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113255_0142_m_000000_142' to o3fs://bucket1.vol1/testdata
21/01/20 11:32:56 INFO SparkHadoopMapRedUtil: attempt_20210120113255_0142_m_000000_142: Committed
21/01/20 11:32:56 INFO Executor: Finished task 0.0 in stage 142.0 (TID 142). 2155 bytes result sent to driver
21/01/20 11:32:56 INFO TaskSetManager: Finished task 0.0 in stage 142.0 (TID 142) in 1053 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:32:56 INFO TaskSchedulerImpl: Removed TaskSet 142.0, whose tasks have all completed, from pool 
21/01/20 11:32:56 INFO DAGScheduler: ResultStage 142 (parquet at Generate.java:61) finished in 1.071 s
21/01/20 11:32:56 INFO DAGScheduler: Job 142 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:32:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 142: Stage finished
21/01/20 11:32:56 INFO DAGScheduler: Job 142 finished: parquet at Generate.java:61, took 1.072522 s
21/01/20 11:32:56 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-157a6231-fb6d-40de-950b-eb91292f1729
21/01/20 11:32:56 INFO FileFormatWriter: Write Job 994a912c-8906-4453-bf62-27b9290a84e4 committed.
21/01/20 11:32:56 INFO FileFormatWriter: Finished processing stats for write job 994a912c-8906-4453-bf62-27b9290a84e4.
21/01/20 11:32:57 INFO BlockManagerInfo: Removed broadcast_142_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:32:58 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:58 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:58 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:32:58 INFO DAGScheduler: Got job 143 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:32:58 INFO DAGScheduler: Final stage: ResultStage 143 (parquet at Generate.java:61)
21/01/20 11:32:58 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:32:58 INFO DAGScheduler: Missing parents: List()
21/01/20 11:32:58 INFO DAGScheduler: Submitting ResultStage 143 (MapPartitionsRDD[573] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:32:58 INFO MemoryStore: Block broadcast_143 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:32:58 INFO MemoryStore: Block broadcast_143_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:32:58 INFO BlockManagerInfo: Added broadcast_143_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:32:58 INFO SparkContext: Created broadcast 143 from broadcast at DAGScheduler.scala:1200
21/01/20 11:32:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 143 (MapPartitionsRDD[573] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:32:58 INFO TaskSchedulerImpl: Adding task set 143.0 with 1 tasks
21/01/20 11:32:58 WARN TaskSetManager: Stage 143 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:32:58 INFO TaskSetManager: Starting task 0.0 in stage 143.0 (TID 143, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:32:58 INFO Executor: Running task 0.0 in stage 143.0 (TID 143)
21/01/20 11:32:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:58 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:32:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:32:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:32:58 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:32:58 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:32:58 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:32:58 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:32:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:32:58 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:32:58 INFO ParquetOutputFormat: Validation is off
21/01/20 11:32:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:32:58 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:32:58 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:32:58 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:32:58 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:32:58 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:32:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:32:59 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113258_0143_m_000000_143' to o3fs://bucket1.vol1/testdata
21/01/20 11:32:59 INFO SparkHadoopMapRedUtil: attempt_20210120113258_0143_m_000000_143: Committed
21/01/20 11:32:59 INFO Executor: Finished task 0.0 in stage 143.0 (TID 143). 2155 bytes result sent to driver
21/01/20 11:32:59 INFO TaskSetManager: Finished task 0.0 in stage 143.0 (TID 143) in 1039 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:32:59 INFO TaskSchedulerImpl: Removed TaskSet 143.0, whose tasks have all completed, from pool 
21/01/20 11:32:59 INFO DAGScheduler: ResultStage 143 (parquet at Generate.java:61) finished in 1.060 s
21/01/20 11:32:59 INFO DAGScheduler: Job 143 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:32:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 143: Stage finished
21/01/20 11:32:59 INFO DAGScheduler: Job 143 finished: parquet at Generate.java:61, took 1.061451 s
21/01/20 11:32:59 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-bb2c31e3-a1af-4417-81c1-41d64fd7fdba
21/01/20 11:32:59 INFO FileFormatWriter: Write Job 5113ffb7-a209-44a5-a2ff-573bd7f3b304 committed.
21/01/20 11:32:59 INFO FileFormatWriter: Finished processing stats for write job 5113ffb7-a209-44a5-a2ff-573bd7f3b304.
21/01/20 11:33:00 INFO BlockManagerInfo: Removed broadcast_143_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:33:02 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:02 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:33:02 INFO DAGScheduler: Got job 144 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:33:02 INFO DAGScheduler: Final stage: ResultStage 144 (parquet at Generate.java:61)
21/01/20 11:33:02 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:33:02 INFO DAGScheduler: Missing parents: List()
21/01/20 11:33:02 INFO DAGScheduler: Submitting ResultStage 144 (MapPartitionsRDD[577] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:33:02 INFO MemoryStore: Block broadcast_144 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:33:02 INFO MemoryStore: Block broadcast_144_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:33:02 INFO BlockManagerInfo: Added broadcast_144_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:33:02 INFO SparkContext: Created broadcast 144 from broadcast at DAGScheduler.scala:1200
21/01/20 11:33:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 144 (MapPartitionsRDD[577] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:33:02 INFO TaskSchedulerImpl: Adding task set 144.0 with 1 tasks
21/01/20 11:33:02 WARN TaskSetManager: Stage 144 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:33:02 INFO TaskSetManager: Starting task 0.0 in stage 144.0 (TID 144, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:33:02 INFO Executor: Running task 0.0 in stage 144.0 (TID 144)
21/01/20 11:33:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:02 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:33:02 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:33:02 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:33:02 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:33:02 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:33:02 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:33:02 INFO ParquetOutputFormat: Validation is off
21/01/20 11:33:02 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:33:02 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:33:02 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:33:02 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:33:02 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:33:02 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:33:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:33:03 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113302_0144_m_000000_144' to o3fs://bucket1.vol1/testdata
21/01/20 11:33:03 INFO SparkHadoopMapRedUtil: attempt_20210120113302_0144_m_000000_144: Committed
21/01/20 11:33:03 INFO Executor: Finished task 0.0 in stage 144.0 (TID 144). 2155 bytes result sent to driver
21/01/20 11:33:03 INFO TaskSetManager: Finished task 0.0 in stage 144.0 (TID 144) in 1056 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:33:03 INFO TaskSchedulerImpl: Removed TaskSet 144.0, whose tasks have all completed, from pool 
21/01/20 11:33:03 INFO DAGScheduler: ResultStage 144 (parquet at Generate.java:61) finished in 1.074 s
21/01/20 11:33:03 INFO DAGScheduler: Job 144 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:33:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 144: Stage finished
21/01/20 11:33:03 INFO DAGScheduler: Job 144 finished: parquet at Generate.java:61, took 1.075466 s
21/01/20 11:33:03 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-9349fdb6-b502-4c4d-8b1f-a52337c10f66
21/01/20 11:33:03 INFO FileFormatWriter: Write Job 3f388655-9052-42a8-ae51-489ee3a6ee5f committed.
21/01/20 11:33:03 INFO FileFormatWriter: Finished processing stats for write job 3f388655-9052-42a8-ae51-489ee3a6ee5f.
21/01/20 11:33:04 INFO BlockManagerInfo: Removed broadcast_144_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:33:06 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:06 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:06 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:33:06 INFO DAGScheduler: Got job 145 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:33:06 INFO DAGScheduler: Final stage: ResultStage 145 (parquet at Generate.java:61)
21/01/20 11:33:06 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:33:06 INFO DAGScheduler: Missing parents: List()
21/01/20 11:33:06 INFO DAGScheduler: Submitting ResultStage 145 (MapPartitionsRDD[581] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:33:06 INFO MemoryStore: Block broadcast_145 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:33:06 INFO MemoryStore: Block broadcast_145_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:33:06 INFO BlockManagerInfo: Added broadcast_145_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:33:06 INFO SparkContext: Created broadcast 145 from broadcast at DAGScheduler.scala:1200
21/01/20 11:33:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 145 (MapPartitionsRDD[581] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:33:06 INFO TaskSchedulerImpl: Adding task set 145.0 with 1 tasks
21/01/20 11:33:06 WARN TaskSetManager: Stage 145 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:33:06 INFO TaskSetManager: Starting task 0.0 in stage 145.0 (TID 145, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:33:06 INFO Executor: Running task 0.0 in stage 145.0 (TID 145)
21/01/20 11:33:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:06 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:06 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:33:06 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:33:06 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:33:06 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:33:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:33:06 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:33:06 INFO ParquetOutputFormat: Validation is off
21/01/20 11:33:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:33:06 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:33:06 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:33:06 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:33:06 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:33:06 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:33:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:33:07 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113306_0145_m_000000_145' to o3fs://bucket1.vol1/testdata
21/01/20 11:33:07 INFO SparkHadoopMapRedUtil: attempt_20210120113306_0145_m_000000_145: Committed
21/01/20 11:33:07 INFO Executor: Finished task 0.0 in stage 145.0 (TID 145). 2155 bytes result sent to driver
21/01/20 11:33:07 INFO TaskSetManager: Finished task 0.0 in stage 145.0 (TID 145) in 971 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:33:07 INFO TaskSchedulerImpl: Removed TaskSet 145.0, whose tasks have all completed, from pool 
21/01/20 11:33:07 INFO DAGScheduler: ResultStage 145 (parquet at Generate.java:61) finished in 0.990 s
21/01/20 11:33:07 INFO DAGScheduler: Job 145 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:33:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 145: Stage finished
21/01/20 11:33:07 INFO DAGScheduler: Job 145 finished: parquet at Generate.java:61, took 0.991854 s
21/01/20 11:33:07 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-4eb08f0c-8170-4e71-ad07-300bfcc1851d
21/01/20 11:33:07 INFO FileFormatWriter: Write Job 8cb35da1-ce61-451b-a2dc-51ee228e10e1 committed.
21/01/20 11:33:07 INFO FileFormatWriter: Finished processing stats for write job 8cb35da1-ce61-451b-a2dc-51ee228e10e1.
21/01/20 11:33:08 INFO BlockManagerInfo: Removed broadcast_145_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:33:09 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:09 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:09 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:33:09 INFO DAGScheduler: Got job 146 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:33:09 INFO DAGScheduler: Final stage: ResultStage 146 (parquet at Generate.java:61)
21/01/20 11:33:09 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:33:09 INFO DAGScheduler: Missing parents: List()
21/01/20 11:33:09 INFO DAGScheduler: Submitting ResultStage 146 (MapPartitionsRDD[585] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:33:09 INFO MemoryStore: Block broadcast_146 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:33:09 INFO MemoryStore: Block broadcast_146_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:33:09 INFO BlockManagerInfo: Added broadcast_146_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:33:09 INFO SparkContext: Created broadcast 146 from broadcast at DAGScheduler.scala:1200
21/01/20 11:33:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 146 (MapPartitionsRDD[585] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:33:09 INFO TaskSchedulerImpl: Adding task set 146.0 with 1 tasks
21/01/20 11:33:09 WARN TaskSetManager: Stage 146 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:33:09 INFO TaskSetManager: Starting task 0.0 in stage 146.0 (TID 146, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:33:09 INFO Executor: Running task 0.0 in stage 146.0 (TID 146)
21/01/20 11:33:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:09 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:09 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:33:09 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:33:09 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:33:09 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:33:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:33:09 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:33:09 INFO ParquetOutputFormat: Validation is off
21/01/20 11:33:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:33:09 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:33:09 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:33:09 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:33:09 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:33:09 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:33:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:33:10 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113309_0146_m_000000_146' to o3fs://bucket1.vol1/testdata
21/01/20 11:33:10 INFO SparkHadoopMapRedUtil: attempt_20210120113309_0146_m_000000_146: Committed
21/01/20 11:33:10 INFO Executor: Finished task 0.0 in stage 146.0 (TID 146). 2155 bytes result sent to driver
21/01/20 11:33:10 INFO TaskSetManager: Finished task 0.0 in stage 146.0 (TID 146) in 755 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:33:10 INFO TaskSchedulerImpl: Removed TaskSet 146.0, whose tasks have all completed, from pool 
21/01/20 11:33:10 INFO DAGScheduler: ResultStage 146 (parquet at Generate.java:61) finished in 0.798 s
21/01/20 11:33:10 INFO DAGScheduler: Job 146 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:33:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 146: Stage finished
21/01/20 11:33:10 INFO DAGScheduler: Job 146 finished: parquet at Generate.java:61, took 0.799922 s
21/01/20 11:33:10 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-65ea2505-5a8e-48bb-9fad-22872678366b
21/01/20 11:33:10 INFO FileFormatWriter: Write Job caef27dc-80a4-44dd-9cf2-abcfcd0c01ea committed.
21/01/20 11:33:10 INFO FileFormatWriter: Finished processing stats for write job caef27dc-80a4-44dd-9cf2-abcfcd0c01ea.
21/01/20 11:33:10 INFO BlockManagerInfo: Removed broadcast_146_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:33:13 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:13 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:13 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:33:13 INFO DAGScheduler: Got job 147 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:33:13 INFO DAGScheduler: Final stage: ResultStage 147 (parquet at Generate.java:61)
21/01/20 11:33:13 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:33:13 INFO DAGScheduler: Missing parents: List()
21/01/20 11:33:13 INFO DAGScheduler: Submitting ResultStage 147 (MapPartitionsRDD[589] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:33:13 INFO MemoryStore: Block broadcast_147 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:33:13 INFO MemoryStore: Block broadcast_147_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:33:13 INFO BlockManagerInfo: Added broadcast_147_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:33:13 INFO SparkContext: Created broadcast 147 from broadcast at DAGScheduler.scala:1200
21/01/20 11:33:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 147 (MapPartitionsRDD[589] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:33:13 INFO TaskSchedulerImpl: Adding task set 147.0 with 1 tasks
21/01/20 11:33:13 WARN TaskSetManager: Stage 147 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:33:13 INFO TaskSetManager: Starting task 0.0 in stage 147.0 (TID 147, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:33:13 INFO Executor: Running task 0.0 in stage 147.0 (TID 147)
21/01/20 11:33:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:13 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:13 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:33:13 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:33:13 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:33:13 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:33:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:33:13 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:33:13 INFO ParquetOutputFormat: Validation is off
21/01/20 11:33:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:33:13 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:33:13 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:33:13 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:33:13 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:33:13 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:33:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:33:14 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113313_0147_m_000000_147' to o3fs://bucket1.vol1/testdata
21/01/20 11:33:14 INFO SparkHadoopMapRedUtil: attempt_20210120113313_0147_m_000000_147: Committed
21/01/20 11:33:14 INFO Executor: Finished task 0.0 in stage 147.0 (TID 147). 2155 bytes result sent to driver
21/01/20 11:33:14 INFO TaskSetManager: Finished task 0.0 in stage 147.0 (TID 147) in 991 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:33:14 INFO TaskSchedulerImpl: Removed TaskSet 147.0, whose tasks have all completed, from pool 
21/01/20 11:33:14 INFO DAGScheduler: ResultStage 147 (parquet at Generate.java:61) finished in 1.009 s
21/01/20 11:33:14 INFO DAGScheduler: Job 147 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:33:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 147: Stage finished
21/01/20 11:33:14 INFO DAGScheduler: Job 147 finished: parquet at Generate.java:61, took 1.011978 s
21/01/20 11:33:14 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-321e55a3-29d5-48fc-9213-e6c3201c58b6
21/01/20 11:33:14 INFO FileFormatWriter: Write Job ab38d471-20a7-413d-9fd7-eb9e14e147e3 committed.
21/01/20 11:33:14 INFO FileFormatWriter: Finished processing stats for write job ab38d471-20a7-413d-9fd7-eb9e14e147e3.
21/01/20 11:33:15 INFO BlockManagerInfo: Removed broadcast_147_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:33:16 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:16 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:16 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:33:16 INFO DAGScheduler: Got job 148 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:33:16 INFO DAGScheduler: Final stage: ResultStage 148 (parquet at Generate.java:61)
21/01/20 11:33:16 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:33:16 INFO DAGScheduler: Missing parents: List()
21/01/20 11:33:16 INFO DAGScheduler: Submitting ResultStage 148 (MapPartitionsRDD[593] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:33:16 INFO MemoryStore: Block broadcast_148 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:33:16 INFO MemoryStore: Block broadcast_148_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:33:16 INFO BlockManagerInfo: Added broadcast_148_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:33:16 INFO SparkContext: Created broadcast 148 from broadcast at DAGScheduler.scala:1200
21/01/20 11:33:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 148 (MapPartitionsRDD[593] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:33:16 INFO TaskSchedulerImpl: Adding task set 148.0 with 1 tasks
21/01/20 11:33:16 WARN TaskSetManager: Stage 148 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:33:16 INFO TaskSetManager: Starting task 0.0 in stage 148.0 (TID 148, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:33:16 INFO Executor: Running task 0.0 in stage 148.0 (TID 148)
21/01/20 11:33:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:16 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:16 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:33:16 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:33:16 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:33:16 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:33:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:33:16 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:33:16 INFO ParquetOutputFormat: Validation is off
21/01/20 11:33:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:33:16 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:33:16 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:33:16 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:33:16 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:33:16 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:33:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:33:17 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-7284D78DA781->27b66227-fed8-4c1d-ab57-510f5a3ce552
21/01/20 11:33:17 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:33:17 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113316_0148_m_000000_148' to o3fs://bucket1.vol1/testdata
21/01/20 11:33:17 INFO SparkHadoopMapRedUtil: attempt_20210120113316_0148_m_000000_148: Committed
21/01/20 11:33:17 INFO Executor: Finished task 0.0 in stage 148.0 (TID 148). 2155 bytes result sent to driver
21/01/20 11:33:17 INFO TaskSetManager: Finished task 0.0 in stage 148.0 (TID 148) in 872 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:33:17 INFO TaskSchedulerImpl: Removed TaskSet 148.0, whose tasks have all completed, from pool 
21/01/20 11:33:17 INFO DAGScheduler: ResultStage 148 (parquet at Generate.java:61) finished in 0.913 s
21/01/20 11:33:17 INFO DAGScheduler: Job 148 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:33:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 148: Stage finished
21/01/20 11:33:17 INFO DAGScheduler: Job 148 finished: parquet at Generate.java:61, took 0.914244 s
21/01/20 11:33:17 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-79902a15-995e-484a-9e5e-ff4f66fcecc0
21/01/20 11:33:17 INFO FileFormatWriter: Write Job 2886ccdb-45a1-4c5f-9b5a-c5d866ed268a committed.
21/01/20 11:33:17 INFO FileFormatWriter: Finished processing stats for write job 2886ccdb-45a1-4c5f-9b5a-c5d866ed268a.
21/01/20 11:33:17 INFO BlockManagerInfo: Removed broadcast_148_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:33:20 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:20 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:20 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:33:20 INFO DAGScheduler: Got job 149 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:33:20 INFO DAGScheduler: Final stage: ResultStage 149 (parquet at Generate.java:61)
21/01/20 11:33:20 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:33:20 INFO DAGScheduler: Missing parents: List()
21/01/20 11:33:20 INFO DAGScheduler: Submitting ResultStage 149 (MapPartitionsRDD[597] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:33:20 INFO MemoryStore: Block broadcast_149 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:33:20 INFO MemoryStore: Block broadcast_149_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:33:20 INFO BlockManagerInfo: Added broadcast_149_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:33:20 INFO SparkContext: Created broadcast 149 from broadcast at DAGScheduler.scala:1200
21/01/20 11:33:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 149 (MapPartitionsRDD[597] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:33:20 INFO TaskSchedulerImpl: Adding task set 149.0 with 1 tasks
21/01/20 11:33:20 WARN TaskSetManager: Stage 149 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:33:20 INFO TaskSetManager: Starting task 0.0 in stage 149.0 (TID 149, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:33:20 INFO Executor: Running task 0.0 in stage 149.0 (TID 149)
21/01/20 11:33:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:20 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:20 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:33:20 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:33:20 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:33:20 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:33:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:33:20 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:33:20 INFO ParquetOutputFormat: Validation is off
21/01/20 11:33:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:33:20 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:33:20 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:33:20 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:33:20 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:33:20 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:33:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:33:21 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113320_0149_m_000000_149' to o3fs://bucket1.vol1/testdata
21/01/20 11:33:21 INFO SparkHadoopMapRedUtil: attempt_20210120113320_0149_m_000000_149: Committed
21/01/20 11:33:21 INFO Executor: Finished task 0.0 in stage 149.0 (TID 149). 2155 bytes result sent to driver
21/01/20 11:33:21 INFO TaskSetManager: Finished task 0.0 in stage 149.0 (TID 149) in 1105 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:33:21 INFO TaskSchedulerImpl: Removed TaskSet 149.0, whose tasks have all completed, from pool 
21/01/20 11:33:21 INFO DAGScheduler: ResultStage 149 (parquet at Generate.java:61) finished in 1.125 s
21/01/20 11:33:21 INFO DAGScheduler: Job 149 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:33:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 149: Stage finished
21/01/20 11:33:21 INFO DAGScheduler: Job 149 finished: parquet at Generate.java:61, took 1.129115 s
21/01/20 11:33:21 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-4f4852f1-3f1d-429c-8a85-6dacd9af3a9c
21/01/20 11:33:21 INFO FileFormatWriter: Write Job 2cd04ccc-53e6-43b5-a9d4-fe3b6651965d committed.
21/01/20 11:33:21 INFO FileFormatWriter: Finished processing stats for write job 2cd04ccc-53e6-43b5-a9d4-fe3b6651965d.
21/01/20 11:33:22 INFO BlockManagerInfo: Removed broadcast_149_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:33:24 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:24 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:24 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:33:24 INFO DAGScheduler: Got job 150 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:33:24 INFO DAGScheduler: Final stage: ResultStage 150 (parquet at Generate.java:61)
21/01/20 11:33:24 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:33:24 INFO DAGScheduler: Missing parents: List()
21/01/20 11:33:24 INFO DAGScheduler: Submitting ResultStage 150 (MapPartitionsRDD[601] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:33:24 INFO MemoryStore: Block broadcast_150 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:33:24 INFO MemoryStore: Block broadcast_150_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:33:24 INFO BlockManagerInfo: Added broadcast_150_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:33:24 INFO SparkContext: Created broadcast 150 from broadcast at DAGScheduler.scala:1200
21/01/20 11:33:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 150 (MapPartitionsRDD[601] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:33:24 INFO TaskSchedulerImpl: Adding task set 150.0 with 1 tasks
21/01/20 11:33:24 WARN TaskSetManager: Stage 150 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:33:24 INFO TaskSetManager: Starting task 0.0 in stage 150.0 (TID 150, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:33:24 INFO Executor: Running task 0.0 in stage 150.0 (TID 150)
21/01/20 11:33:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:24 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:24 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:33:24 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:33:24 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:33:24 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:33:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:33:24 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:33:24 INFO ParquetOutputFormat: Validation is off
21/01/20 11:33:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:33:24 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:33:24 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:33:24 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:33:24 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:33:24 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:33:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:33:25 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113324_0150_m_000000_150' to o3fs://bucket1.vol1/testdata
21/01/20 11:33:25 INFO SparkHadoopMapRedUtil: attempt_20210120113324_0150_m_000000_150: Committed
21/01/20 11:33:25 INFO Executor: Finished task 0.0 in stage 150.0 (TID 150). 2155 bytes result sent to driver
21/01/20 11:33:25 INFO TaskSetManager: Finished task 0.0 in stage 150.0 (TID 150) in 942 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:33:25 INFO TaskSchedulerImpl: Removed TaskSet 150.0, whose tasks have all completed, from pool 
21/01/20 11:33:25 INFO DAGScheduler: ResultStage 150 (parquet at Generate.java:61) finished in 0.983 s
21/01/20 11:33:25 INFO DAGScheduler: Job 150 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:33:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 150: Stage finished
21/01/20 11:33:25 INFO DAGScheduler: Job 150 finished: parquet at Generate.java:61, took 0.984992 s
21/01/20 11:33:25 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-7cbe3b38-28a5-4736-ba1a-7443090d36d5
21/01/20 11:33:25 INFO FileFormatWriter: Write Job 94f0a442-521e-466a-9409-cdd79a3c0f3f committed.
21/01/20 11:33:25 INFO FileFormatWriter: Finished processing stats for write job 94f0a442-521e-466a-9409-cdd79a3c0f3f.
21/01/20 11:33:25 INFO BlockManagerInfo: Removed broadcast_150_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:33:27 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:27 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:27 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:33:27 INFO DAGScheduler: Got job 151 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:33:27 INFO DAGScheduler: Final stage: ResultStage 151 (parquet at Generate.java:61)
21/01/20 11:33:27 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:33:27 INFO DAGScheduler: Missing parents: List()
21/01/20 11:33:27 INFO DAGScheduler: Submitting ResultStage 151 (MapPartitionsRDD[605] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:33:27 INFO MemoryStore: Block broadcast_151 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:33:27 INFO MemoryStore: Block broadcast_151_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:33:27 INFO BlockManagerInfo: Added broadcast_151_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:33:27 INFO SparkContext: Created broadcast 151 from broadcast at DAGScheduler.scala:1200
21/01/20 11:33:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 151 (MapPartitionsRDD[605] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:33:27 INFO TaskSchedulerImpl: Adding task set 151.0 with 1 tasks
21/01/20 11:33:27 WARN TaskSetManager: Stage 151 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:33:27 INFO TaskSetManager: Starting task 0.0 in stage 151.0 (TID 151, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:33:27 INFO Executor: Running task 0.0 in stage 151.0 (TID 151)
21/01/20 11:33:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:27 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:27 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:33:27 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:33:27 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:33:27 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:33:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:33:27 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:33:27 INFO ParquetOutputFormat: Validation is off
21/01/20 11:33:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:33:27 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:33:27 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:33:27 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:33:27 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:33:27 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:33:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:33:28 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-2C848CBB7934->8df234bd-f131-4d8e-953a-b9eec8633221
21/01/20 11:33:28 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:33:28 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113327_0151_m_000000_151' to o3fs://bucket1.vol1/testdata
21/01/20 11:33:28 INFO SparkHadoopMapRedUtil: attempt_20210120113327_0151_m_000000_151: Committed
21/01/20 11:33:28 INFO Executor: Finished task 0.0 in stage 151.0 (TID 151). 2155 bytes result sent to driver
21/01/20 11:33:28 INFO TaskSetManager: Finished task 0.0 in stage 151.0 (TID 151) in 1018 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:33:28 INFO TaskSchedulerImpl: Removed TaskSet 151.0, whose tasks have all completed, from pool 
21/01/20 11:33:28 INFO DAGScheduler: ResultStage 151 (parquet at Generate.java:61) finished in 1.036 s
21/01/20 11:33:28 INFO DAGScheduler: Job 151 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:33:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 151: Stage finished
21/01/20 11:33:28 INFO DAGScheduler: Job 151 finished: parquet at Generate.java:61, took 1.037596 s
21/01/20 11:33:28 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-38076f59-5e03-4999-878f-1db08606695b
21/01/20 11:33:28 INFO FileFormatWriter: Write Job d7df738f-8d36-4d08-9366-3e2634b71d57 committed.
21/01/20 11:33:28 INFO FileFormatWriter: Finished processing stats for write job d7df738f-8d36-4d08-9366-3e2634b71d57.
21/01/20 11:33:30 INFO BlockManagerInfo: Removed broadcast_151_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:33:31 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:31 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:33:31 INFO DAGScheduler: Got job 152 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:33:31 INFO DAGScheduler: Final stage: ResultStage 152 (parquet at Generate.java:61)
21/01/20 11:33:31 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:33:31 INFO DAGScheduler: Missing parents: List()
21/01/20 11:33:31 INFO DAGScheduler: Submitting ResultStage 152 (MapPartitionsRDD[609] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:33:31 INFO MemoryStore: Block broadcast_152 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:33:31 INFO MemoryStore: Block broadcast_152_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:33:31 INFO BlockManagerInfo: Added broadcast_152_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:33:31 INFO SparkContext: Created broadcast 152 from broadcast at DAGScheduler.scala:1200
21/01/20 11:33:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 152 (MapPartitionsRDD[609] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:33:31 INFO TaskSchedulerImpl: Adding task set 152.0 with 1 tasks
21/01/20 11:33:31 WARN TaskSetManager: Stage 152 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:33:31 INFO TaskSetManager: Starting task 0.0 in stage 152.0 (TID 152, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:33:31 INFO Executor: Running task 0.0 in stage 152.0 (TID 152)
21/01/20 11:33:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:31 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:33:31 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:33:31 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:33:31 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:33:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:33:31 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:33:31 INFO ParquetOutputFormat: Validation is off
21/01/20 11:33:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:33:31 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:33:31 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:33:31 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:33:31 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:33:31 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:33:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:33:32 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113331_0152_m_000000_152' to o3fs://bucket1.vol1/testdata
21/01/20 11:33:32 INFO SparkHadoopMapRedUtil: attempt_20210120113331_0152_m_000000_152: Committed
21/01/20 11:33:32 INFO Executor: Finished task 0.0 in stage 152.0 (TID 152). 2155 bytes result sent to driver
21/01/20 11:33:32 INFO TaskSetManager: Finished task 0.0 in stage 152.0 (TID 152) in 932 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:33:32 INFO TaskSchedulerImpl: Removed TaskSet 152.0, whose tasks have all completed, from pool 
21/01/20 11:33:32 INFO DAGScheduler: ResultStage 152 (parquet at Generate.java:61) finished in 0.973 s
21/01/20 11:33:32 INFO DAGScheduler: Job 152 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:33:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 152: Stage finished
21/01/20 11:33:32 INFO DAGScheduler: Job 152 finished: parquet at Generate.java:61, took 0.974406 s
21/01/20 11:33:32 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-8efd9d4f-1e49-4f83-8954-deb22d080441
21/01/20 11:33:32 INFO FileFormatWriter: Write Job 828e9e6f-f8f0-4e49-a5f4-c6aa2a498f95 committed.
21/01/20 11:33:32 INFO FileFormatWriter: Finished processing stats for write job 828e9e6f-f8f0-4e49-a5f4-c6aa2a498f95.
21/01/20 11:33:32 INFO BlockManagerInfo: Removed broadcast_152_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:33:34 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:34 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:34 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:33:34 INFO DAGScheduler: Got job 153 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:33:34 INFO DAGScheduler: Final stage: ResultStage 153 (parquet at Generate.java:61)
21/01/20 11:33:34 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:33:34 INFO DAGScheduler: Missing parents: List()
21/01/20 11:33:34 INFO DAGScheduler: Submitting ResultStage 153 (MapPartitionsRDD[613] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:33:34 INFO MemoryStore: Block broadcast_153 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:33:34 INFO MemoryStore: Block broadcast_153_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:33:34 INFO BlockManagerInfo: Added broadcast_153_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:33:34 INFO SparkContext: Created broadcast 153 from broadcast at DAGScheduler.scala:1200
21/01/20 11:33:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 153 (MapPartitionsRDD[613] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:33:34 INFO TaskSchedulerImpl: Adding task set 153.0 with 1 tasks
21/01/20 11:33:35 WARN TaskSetManager: Stage 153 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:33:35 INFO TaskSetManager: Starting task 0.0 in stage 153.0 (TID 153, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:33:35 INFO Executor: Running task 0.0 in stage 153.0 (TID 153)
21/01/20 11:33:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:35 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:35 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:33:35 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:33:35 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:33:35 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:33:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:33:35 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:33:35 INFO ParquetOutputFormat: Validation is off
21/01/20 11:33:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:33:35 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:33:35 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:33:35 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:33:35 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:33:35 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:33:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:33:35 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113334_0153_m_000000_153' to o3fs://bucket1.vol1/testdata
21/01/20 11:33:35 INFO SparkHadoopMapRedUtil: attempt_20210120113334_0153_m_000000_153: Committed
21/01/20 11:33:35 INFO Executor: Finished task 0.0 in stage 153.0 (TID 153). 2155 bytes result sent to driver
21/01/20 11:33:35 INFO TaskSetManager: Finished task 0.0 in stage 153.0 (TID 153) in 1008 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:33:35 INFO TaskSchedulerImpl: Removed TaskSet 153.0, whose tasks have all completed, from pool 
21/01/20 11:33:35 INFO DAGScheduler: ResultStage 153 (parquet at Generate.java:61) finished in 1.027 s
21/01/20 11:33:35 INFO DAGScheduler: Job 153 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:33:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 153: Stage finished
21/01/20 11:33:35 INFO DAGScheduler: Job 153 finished: parquet at Generate.java:61, took 1.028852 s
21/01/20 11:33:35 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-911a99da-b917-4091-a9b2-baf3717826b2
21/01/20 11:33:35 INFO FileFormatWriter: Write Job 3b33d523-ce10-4cfd-b5ea-c64fec082587 committed.
21/01/20 11:33:35 INFO FileFormatWriter: Finished processing stats for write job 3b33d523-ce10-4cfd-b5ea-c64fec082587.
21/01/20 11:33:37 INFO BlockManagerInfo: Removed broadcast_153_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:33:38 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:38 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:38 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:33:38 INFO DAGScheduler: Got job 154 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:33:38 INFO DAGScheduler: Final stage: ResultStage 154 (parquet at Generate.java:61)
21/01/20 11:33:38 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:33:38 INFO DAGScheduler: Missing parents: List()
21/01/20 11:33:38 INFO DAGScheduler: Submitting ResultStage 154 (MapPartitionsRDD[617] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:33:38 INFO MemoryStore: Block broadcast_154 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:33:38 INFO MemoryStore: Block broadcast_154_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:33:38 INFO BlockManagerInfo: Added broadcast_154_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:33:38 INFO SparkContext: Created broadcast 154 from broadcast at DAGScheduler.scala:1200
21/01/20 11:33:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 154 (MapPartitionsRDD[617] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:33:38 INFO TaskSchedulerImpl: Adding task set 154.0 with 1 tasks
21/01/20 11:33:38 WARN TaskSetManager: Stage 154 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:33:38 INFO TaskSetManager: Starting task 0.0 in stage 154.0 (TID 154, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:33:38 INFO Executor: Running task 0.0 in stage 154.0 (TID 154)
21/01/20 11:33:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:38 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:38 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:33:38 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:33:38 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:33:38 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:33:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:33:38 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:33:38 INFO ParquetOutputFormat: Validation is off
21/01/20 11:33:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:33:38 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:33:38 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:33:38 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:33:38 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:33:38 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:33:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:33:39 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113338_0154_m_000000_154' to o3fs://bucket1.vol1/testdata
21/01/20 11:33:39 INFO SparkHadoopMapRedUtil: attempt_20210120113338_0154_m_000000_154: Committed
21/01/20 11:33:39 INFO Executor: Finished task 0.0 in stage 154.0 (TID 154). 2155 bytes result sent to driver
21/01/20 11:33:39 INFO TaskSetManager: Finished task 0.0 in stage 154.0 (TID 154) in 977 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:33:39 INFO TaskSchedulerImpl: Removed TaskSet 154.0, whose tasks have all completed, from pool 
21/01/20 11:33:39 INFO DAGScheduler: ResultStage 154 (parquet at Generate.java:61) finished in 0.997 s
21/01/20 11:33:39 INFO DAGScheduler: Job 154 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:33:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 154: Stage finished
21/01/20 11:33:39 INFO DAGScheduler: Job 154 finished: parquet at Generate.java:61, took 0.998198 s
21/01/20 11:33:39 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-4b7895fc-5646-4053-b4fd-a92223d21223
21/01/20 11:33:39 INFO FileFormatWriter: Write Job 778067d1-2c4f-4e17-974f-960aca460e29 committed.
21/01/20 11:33:39 INFO FileFormatWriter: Finished processing stats for write job 778067d1-2c4f-4e17-974f-960aca460e29.
21/01/20 11:33:40 INFO BlockManagerInfo: Removed broadcast_154_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:33:42 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:42 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:42 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:33:42 INFO DAGScheduler: Got job 155 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:33:42 INFO DAGScheduler: Final stage: ResultStage 155 (parquet at Generate.java:61)
21/01/20 11:33:42 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:33:42 INFO DAGScheduler: Missing parents: List()
21/01/20 11:33:42 INFO DAGScheduler: Submitting ResultStage 155 (MapPartitionsRDD[621] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:33:42 INFO MemoryStore: Block broadcast_155 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:33:42 INFO MemoryStore: Block broadcast_155_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:33:42 INFO BlockManagerInfo: Added broadcast_155_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:33:42 INFO SparkContext: Created broadcast 155 from broadcast at DAGScheduler.scala:1200
21/01/20 11:33:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 155 (MapPartitionsRDD[621] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:33:42 INFO TaskSchedulerImpl: Adding task set 155.0 with 1 tasks
21/01/20 11:33:42 WARN TaskSetManager: Stage 155 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:33:42 INFO TaskSetManager: Starting task 0.0 in stage 155.0 (TID 155, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:33:42 INFO Executor: Running task 0.0 in stage 155.0 (TID 155)
21/01/20 11:33:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:42 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:42 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:33:42 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:33:42 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:33:42 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:33:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:33:42 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:33:42 INFO ParquetOutputFormat: Validation is off
21/01/20 11:33:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:33:42 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:33:42 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:33:42 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:33:42 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:33:42 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:33:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:33:43 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113342_0155_m_000000_155' to o3fs://bucket1.vol1/testdata
21/01/20 11:33:43 INFO SparkHadoopMapRedUtil: attempt_20210120113342_0155_m_000000_155: Committed
21/01/20 11:33:43 INFO Executor: Finished task 0.0 in stage 155.0 (TID 155). 2155 bytes result sent to driver
21/01/20 11:33:43 INFO TaskSetManager: Finished task 0.0 in stage 155.0 (TID 155) in 896 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:33:43 INFO TaskSchedulerImpl: Removed TaskSet 155.0, whose tasks have all completed, from pool 
21/01/20 11:33:43 INFO DAGScheduler: ResultStage 155 (parquet at Generate.java:61) finished in 0.936 s
21/01/20 11:33:43 INFO DAGScheduler: Job 155 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:33:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 155: Stage finished
21/01/20 11:33:43 INFO DAGScheduler: Job 155 finished: parquet at Generate.java:61, took 0.936838 s
21/01/20 11:33:43 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-927c2bb8-4de5-418b-836f-8f12c94d4659
21/01/20 11:33:43 INFO FileFormatWriter: Write Job 19aeb55e-fc6e-45e7-8bd5-62695004812e committed.
21/01/20 11:33:43 INFO FileFormatWriter: Finished processing stats for write job 19aeb55e-fc6e-45e7-8bd5-62695004812e.
21/01/20 11:33:43 INFO BlockManagerInfo: Removed broadcast_155_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:33:45 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:45 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:45 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:33:45 INFO DAGScheduler: Got job 156 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:33:45 INFO DAGScheduler: Final stage: ResultStage 156 (parquet at Generate.java:61)
21/01/20 11:33:45 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:33:45 INFO DAGScheduler: Missing parents: List()
21/01/20 11:33:45 INFO DAGScheduler: Submitting ResultStage 156 (MapPartitionsRDD[625] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:33:45 INFO MemoryStore: Block broadcast_156 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:33:45 INFO MemoryStore: Block broadcast_156_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:33:45 INFO BlockManagerInfo: Added broadcast_156_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:33:45 INFO SparkContext: Created broadcast 156 from broadcast at DAGScheduler.scala:1200
21/01/20 11:33:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 156 (MapPartitionsRDD[625] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:33:45 INFO TaskSchedulerImpl: Adding task set 156.0 with 1 tasks
21/01/20 11:33:45 WARN TaskSetManager: Stage 156 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:33:45 INFO TaskSetManager: Starting task 0.0 in stage 156.0 (TID 156, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:33:45 INFO Executor: Running task 0.0 in stage 156.0 (TID 156)
21/01/20 11:33:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:45 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:45 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:33:45 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:33:45 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:33:45 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:33:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:33:45 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:33:45 INFO ParquetOutputFormat: Validation is off
21/01/20 11:33:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:33:45 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:33:45 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:33:45 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:33:45 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:33:45 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:33:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:33:46 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-897DD50DE833->27b66227-fed8-4c1d-ab57-510f5a3ce552
21/01/20 11:33:46 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:33:46 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113345_0156_m_000000_156' to o3fs://bucket1.vol1/testdata
21/01/20 11:33:46 INFO SparkHadoopMapRedUtil: attempt_20210120113345_0156_m_000000_156: Committed
21/01/20 11:33:46 INFO Executor: Finished task 0.0 in stage 156.0 (TID 156). 2155 bytes result sent to driver
21/01/20 11:33:46 INFO TaskSetManager: Finished task 0.0 in stage 156.0 (TID 156) in 1097 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:33:46 INFO TaskSchedulerImpl: Removed TaskSet 156.0, whose tasks have all completed, from pool 
21/01/20 11:33:46 INFO DAGScheduler: ResultStage 156 (parquet at Generate.java:61) finished in 1.115 s
21/01/20 11:33:46 INFO DAGScheduler: Job 156 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:33:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 156: Stage finished
21/01/20 11:33:46 INFO DAGScheduler: Job 156 finished: parquet at Generate.java:61, took 1.116502 s
21/01/20 11:33:46 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-e1b02b91-d1db-48fb-b6a9-3b5d430c6682
21/01/20 11:33:46 INFO FileFormatWriter: Write Job a8610b69-2461-4051-a6fd-14fbe89630e4 committed.
21/01/20 11:33:46 INFO FileFormatWriter: Finished processing stats for write job a8610b69-2461-4051-a6fd-14fbe89630e4.
21/01/20 11:33:47 INFO BlockManagerInfo: Removed broadcast_156_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:33:49 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:49 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:33:49 INFO DAGScheduler: Got job 157 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:33:49 INFO DAGScheduler: Final stage: ResultStage 157 (parquet at Generate.java:61)
21/01/20 11:33:49 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:33:49 INFO DAGScheduler: Missing parents: List()
21/01/20 11:33:49 INFO DAGScheduler: Submitting ResultStage 157 (MapPartitionsRDD[629] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:33:49 INFO MemoryStore: Block broadcast_157 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:33:49 INFO MemoryStore: Block broadcast_157_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:33:49 INFO BlockManagerInfo: Added broadcast_157_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:33:49 INFO SparkContext: Created broadcast 157 from broadcast at DAGScheduler.scala:1200
21/01/20 11:33:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 157 (MapPartitionsRDD[629] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:33:49 INFO TaskSchedulerImpl: Adding task set 157.0 with 1 tasks
21/01/20 11:33:49 WARN TaskSetManager: Stage 157 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:33:49 INFO TaskSetManager: Starting task 0.0 in stage 157.0 (TID 157, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:33:49 INFO Executor: Running task 0.0 in stage 157.0 (TID 157)
21/01/20 11:33:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:49 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:33:49 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:33:49 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:33:49 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:33:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:33:49 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:33:49 INFO ParquetOutputFormat: Validation is off
21/01/20 11:33:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:33:49 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:33:49 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:33:49 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:33:49 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:33:49 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:33:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:33:50 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113349_0157_m_000000_157' to o3fs://bucket1.vol1/testdata
21/01/20 11:33:50 INFO SparkHadoopMapRedUtil: attempt_20210120113349_0157_m_000000_157: Committed
21/01/20 11:33:50 INFO Executor: Finished task 0.0 in stage 157.0 (TID 157). 2155 bytes result sent to driver
21/01/20 11:33:50 INFO TaskSetManager: Finished task 0.0 in stage 157.0 (TID 157) in 1033 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:33:50 INFO TaskSchedulerImpl: Removed TaskSet 157.0, whose tasks have all completed, from pool 
21/01/20 11:33:50 INFO DAGScheduler: ResultStage 157 (parquet at Generate.java:61) finished in 1.050 s
21/01/20 11:33:50 INFO DAGScheduler: Job 157 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:33:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 157: Stage finished
21/01/20 11:33:50 INFO DAGScheduler: Job 157 finished: parquet at Generate.java:61, took 1.051718 s
21/01/20 11:33:50 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-2198c165-4db3-4651-832a-69996332ab7f
21/01/20 11:33:50 INFO FileFormatWriter: Write Job 5a823086-6b8c-4a87-b2f7-3a37e0981695 committed.
21/01/20 11:33:50 INFO FileFormatWriter: Finished processing stats for write job 5a823086-6b8c-4a87-b2f7-3a37e0981695.
21/01/20 11:33:51 INFO BlockManagerInfo: Removed broadcast_157_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:33:53 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:53 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:33:53 INFO DAGScheduler: Got job 158 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:33:53 INFO DAGScheduler: Final stage: ResultStage 158 (parquet at Generate.java:61)
21/01/20 11:33:53 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:33:53 INFO DAGScheduler: Missing parents: List()
21/01/20 11:33:53 INFO DAGScheduler: Submitting ResultStage 158 (MapPartitionsRDD[633] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:33:53 INFO MemoryStore: Block broadcast_158 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:33:53 INFO MemoryStore: Block broadcast_158_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:33:53 INFO BlockManagerInfo: Added broadcast_158_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:33:53 INFO SparkContext: Created broadcast 158 from broadcast at DAGScheduler.scala:1200
21/01/20 11:33:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 158 (MapPartitionsRDD[633] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:33:53 INFO TaskSchedulerImpl: Adding task set 158.0 with 1 tasks
21/01/20 11:33:53 WARN TaskSetManager: Stage 158 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:33:53 INFO TaskSetManager: Starting task 0.0 in stage 158.0 (TID 158, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:33:53 INFO Executor: Running task 0.0 in stage 158.0 (TID 158)
21/01/20 11:33:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:53 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:33:53 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:33:53 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:33:53 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:33:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:33:53 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:33:53 INFO ParquetOutputFormat: Validation is off
21/01/20 11:33:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:33:53 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:33:53 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:33:53 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:33:53 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:33:53 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:33:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113353_0158_m_000000_158' to o3fs://bucket1.vol1/testdata
21/01/20 11:33:54 INFO SparkHadoopMapRedUtil: attempt_20210120113353_0158_m_000000_158: Committed
21/01/20 11:33:54 INFO Executor: Finished task 0.0 in stage 158.0 (TID 158). 2155 bytes result sent to driver
21/01/20 11:33:54 INFO TaskSetManager: Finished task 0.0 in stage 158.0 (TID 158) in 1090 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:33:54 INFO TaskSchedulerImpl: Removed TaskSet 158.0, whose tasks have all completed, from pool 
21/01/20 11:33:54 INFO DAGScheduler: ResultStage 158 (parquet at Generate.java:61) finished in 1.108 s
21/01/20 11:33:54 INFO DAGScheduler: Job 158 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:33:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 158: Stage finished
21/01/20 11:33:54 INFO DAGScheduler: Job 158 finished: parquet at Generate.java:61, took 1.109375 s
21/01/20 11:33:54 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-fc70b729-7f20-4115-a3cd-58b319561e8d
21/01/20 11:33:54 INFO FileFormatWriter: Write Job c93f20c8-8e8e-4fd6-94e3-46cf30439cb6 committed.
21/01/20 11:33:54 INFO FileFormatWriter: Finished processing stats for write job c93f20c8-8e8e-4fd6-94e3-46cf30439cb6.
21/01/20 11:33:55 INFO BlockManagerInfo: Removed broadcast_158_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:33:56 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:56 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:56 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:33:56 INFO DAGScheduler: Got job 159 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:33:56 INFO DAGScheduler: Final stage: ResultStage 159 (parquet at Generate.java:61)
21/01/20 11:33:56 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:33:56 INFO DAGScheduler: Missing parents: List()
21/01/20 11:33:56 INFO DAGScheduler: Submitting ResultStage 159 (MapPartitionsRDD[637] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:33:56 INFO MemoryStore: Block broadcast_159 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:33:56 INFO MemoryStore: Block broadcast_159_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:33:56 INFO BlockManagerInfo: Added broadcast_159_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:33:56 INFO SparkContext: Created broadcast 159 from broadcast at DAGScheduler.scala:1200
21/01/20 11:33:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 159 (MapPartitionsRDD[637] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:33:56 INFO TaskSchedulerImpl: Adding task set 159.0 with 1 tasks
21/01/20 11:33:56 WARN TaskSetManager: Stage 159 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:33:56 INFO TaskSetManager: Starting task 0.0 in stage 159.0 (TID 159, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:33:56 INFO Executor: Running task 0.0 in stage 159.0 (TID 159)
21/01/20 11:33:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:56 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:33:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:33:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:33:56 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:33:56 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:33:56 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:33:56 INFO ParquetOutputFormat: Validation is off
21/01/20 11:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:33:56 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:33:56 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:33:56 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:33:56 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:33:56 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113356_0159_m_000000_159' to o3fs://bucket1.vol1/testdata
21/01/20 11:33:57 INFO SparkHadoopMapRedUtil: attempt_20210120113356_0159_m_000000_159: Committed
21/01/20 11:33:57 INFO Executor: Finished task 0.0 in stage 159.0 (TID 159). 2155 bytes result sent to driver
21/01/20 11:33:57 INFO TaskSetManager: Finished task 0.0 in stage 159.0 (TID 159) in 1021 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:33:57 INFO TaskSchedulerImpl: Removed TaskSet 159.0, whose tasks have all completed, from pool 
21/01/20 11:33:57 INFO DAGScheduler: ResultStage 159 (parquet at Generate.java:61) finished in 1.039 s
21/01/20 11:33:57 INFO DAGScheduler: Job 159 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:33:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 159: Stage finished
21/01/20 11:33:57 INFO DAGScheduler: Job 159 finished: parquet at Generate.java:61, took 1.041253 s
21/01/20 11:33:57 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-17e58ea7-7382-48d9-bbcb-cd4caf92281c
21/01/20 11:33:57 INFO FileFormatWriter: Write Job 00fa9018-9086-4e46-8cfb-789186c565f0 committed.
21/01/20 11:33:57 INFO FileFormatWriter: Finished processing stats for write job 00fa9018-9086-4e46-8cfb-789186c565f0.
21/01/20 11:33:59 INFO BlockManagerInfo: Removed broadcast_159_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:34:00 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:00 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:34:00 INFO DAGScheduler: Got job 160 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:34:00 INFO DAGScheduler: Final stage: ResultStage 160 (parquet at Generate.java:61)
21/01/20 11:34:00 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:34:00 INFO DAGScheduler: Missing parents: List()
21/01/20 11:34:00 INFO DAGScheduler: Submitting ResultStage 160 (MapPartitionsRDD[641] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:34:00 INFO MemoryStore: Block broadcast_160 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:34:00 INFO MemoryStore: Block broadcast_160_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:34:00 INFO BlockManagerInfo: Added broadcast_160_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:34:00 INFO SparkContext: Created broadcast 160 from broadcast at DAGScheduler.scala:1200
21/01/20 11:34:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 160 (MapPartitionsRDD[641] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:34:00 INFO TaskSchedulerImpl: Adding task set 160.0 with 1 tasks
21/01/20 11:34:00 WARN TaskSetManager: Stage 160 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:34:00 INFO TaskSetManager: Starting task 0.0 in stage 160.0 (TID 160, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:34:00 INFO Executor: Running task 0.0 in stage 160.0 (TID 160)
21/01/20 11:34:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:00 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:34:00 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:34:00 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:34:00 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:34:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:34:00 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:34:00 INFO ParquetOutputFormat: Validation is off
21/01/20 11:34:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:34:00 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:34:00 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:34:00 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:34:00 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:34:00 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:34:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:34:01 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-5388CE0AEB3D->8df234bd-f131-4d8e-953a-b9eec8633221
21/01/20 11:34:01 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:34:01 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113400_0160_m_000000_160' to o3fs://bucket1.vol1/testdata
21/01/20 11:34:01 INFO SparkHadoopMapRedUtil: attempt_20210120113400_0160_m_000000_160: Committed
21/01/20 11:34:01 INFO Executor: Finished task 0.0 in stage 160.0 (TID 160). 2155 bytes result sent to driver
21/01/20 11:34:01 INFO TaskSetManager: Finished task 0.0 in stage 160.0 (TID 160) in 882 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:34:01 INFO TaskSchedulerImpl: Removed TaskSet 160.0, whose tasks have all completed, from pool 
21/01/20 11:34:01 INFO DAGScheduler: ResultStage 160 (parquet at Generate.java:61) finished in 0.922 s
21/01/20 11:34:01 INFO DAGScheduler: Job 160 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:34:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 160: Stage finished
21/01/20 11:34:01 INFO DAGScheduler: Job 160 finished: parquet at Generate.java:61, took 0.924811 s
21/01/20 11:34:01 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-a49bf11a-979d-4679-a794-d277162512a4
21/01/20 11:34:01 INFO FileFormatWriter: Write Job e2319f2c-e849-4626-9e30-2a1ab333229b committed.
21/01/20 11:34:01 INFO FileFormatWriter: Finished processing stats for write job e2319f2c-e849-4626-9e30-2a1ab333229b.
21/01/20 11:34:01 INFO BlockManagerInfo: Removed broadcast_160_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:34:04 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:04 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:34:04 INFO DAGScheduler: Got job 161 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:34:04 INFO DAGScheduler: Final stage: ResultStage 161 (parquet at Generate.java:61)
21/01/20 11:34:04 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:34:04 INFO DAGScheduler: Missing parents: List()
21/01/20 11:34:04 INFO DAGScheduler: Submitting ResultStage 161 (MapPartitionsRDD[645] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:34:04 INFO MemoryStore: Block broadcast_161 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:34:04 INFO MemoryStore: Block broadcast_161_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:34:04 INFO BlockManagerInfo: Added broadcast_161_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:34:04 INFO SparkContext: Created broadcast 161 from broadcast at DAGScheduler.scala:1200
21/01/20 11:34:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 161 (MapPartitionsRDD[645] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:34:04 INFO TaskSchedulerImpl: Adding task set 161.0 with 1 tasks
21/01/20 11:34:04 WARN TaskSetManager: Stage 161 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:34:04 INFO TaskSetManager: Starting task 0.0 in stage 161.0 (TID 161, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:34:04 INFO Executor: Running task 0.0 in stage 161.0 (TID 161)
21/01/20 11:34:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:04 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:34:04 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:34:04 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:34:04 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:34:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:34:04 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:34:04 INFO ParquetOutputFormat: Validation is off
21/01/20 11:34:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:34:04 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:34:04 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:34:04 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:34:04 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:34:04 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:34:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:34:05 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113404_0161_m_000000_161' to o3fs://bucket1.vol1/testdata
21/01/20 11:34:05 INFO SparkHadoopMapRedUtil: attempt_20210120113404_0161_m_000000_161: Committed
21/01/20 11:34:05 INFO Executor: Finished task 0.0 in stage 161.0 (TID 161). 2155 bytes result sent to driver
21/01/20 11:34:05 INFO TaskSetManager: Finished task 0.0 in stage 161.0 (TID 161) in 1028 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:34:05 INFO TaskSchedulerImpl: Removed TaskSet 161.0, whose tasks have all completed, from pool 
21/01/20 11:34:05 INFO DAGScheduler: ResultStage 161 (parquet at Generate.java:61) finished in 1.048 s
21/01/20 11:34:05 INFO DAGScheduler: Job 161 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:34:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 161: Stage finished
21/01/20 11:34:05 INFO DAGScheduler: Job 161 finished: parquet at Generate.java:61, took 1.050170 s
21/01/20 11:34:05 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-3e582544-2cdd-40e2-9f4f-ae8754aa7634
21/01/20 11:34:05 INFO FileFormatWriter: Write Job 3871ed76-5d0c-4ab3-bd36-61e052895ed6 committed.
21/01/20 11:34:05 INFO FileFormatWriter: Finished processing stats for write job 3871ed76-5d0c-4ab3-bd36-61e052895ed6.
21/01/20 11:34:06 INFO BlockManagerInfo: Removed broadcast_161_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:34:07 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:07 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:07 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:34:07 INFO DAGScheduler: Got job 162 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:34:07 INFO DAGScheduler: Final stage: ResultStage 162 (parquet at Generate.java:61)
21/01/20 11:34:07 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:34:07 INFO DAGScheduler: Missing parents: List()
21/01/20 11:34:07 INFO DAGScheduler: Submitting ResultStage 162 (MapPartitionsRDD[649] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:34:07 INFO MemoryStore: Block broadcast_162 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:34:07 INFO MemoryStore: Block broadcast_162_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:34:07 INFO BlockManagerInfo: Added broadcast_162_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:34:07 INFO SparkContext: Created broadcast 162 from broadcast at DAGScheduler.scala:1200
21/01/20 11:34:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 162 (MapPartitionsRDD[649] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:34:07 INFO TaskSchedulerImpl: Adding task set 162.0 with 1 tasks
21/01/20 11:34:07 WARN TaskSetManager: Stage 162 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:34:07 INFO TaskSetManager: Starting task 0.0 in stage 162.0 (TID 162, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:34:07 INFO Executor: Running task 0.0 in stage 162.0 (TID 162)
21/01/20 11:34:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:07 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:07 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:34:07 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:34:07 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:34:07 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:34:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:34:07 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:34:07 INFO ParquetOutputFormat: Validation is off
21/01/20 11:34:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:34:07 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:34:07 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:34:07 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:34:07 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:34:07 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:34:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:34:08 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113407_0162_m_000000_162' to o3fs://bucket1.vol1/testdata
21/01/20 11:34:08 INFO SparkHadoopMapRedUtil: attempt_20210120113407_0162_m_000000_162: Committed
21/01/20 11:34:08 INFO Executor: Finished task 0.0 in stage 162.0 (TID 162). 2155 bytes result sent to driver
21/01/20 11:34:08 INFO TaskSetManager: Finished task 0.0 in stage 162.0 (TID 162) in 992 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:34:08 INFO TaskSchedulerImpl: Removed TaskSet 162.0, whose tasks have all completed, from pool 
21/01/20 11:34:08 INFO DAGScheduler: ResultStage 162 (parquet at Generate.java:61) finished in 1.011 s
21/01/20 11:34:08 INFO DAGScheduler: Job 162 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:34:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 162: Stage finished
21/01/20 11:34:08 INFO DAGScheduler: Job 162 finished: parquet at Generate.java:61, took 1.013009 s
21/01/20 11:34:08 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-e9c253b6-d46b-4609-8b25-9266f3c4b8c3
21/01/20 11:34:08 INFO FileFormatWriter: Write Job ea4933ce-b51d-44e3-996f-246a3e9f5e41 committed.
21/01/20 11:34:08 INFO FileFormatWriter: Finished processing stats for write job ea4933ce-b51d-44e3-996f-246a3e9f5e41.
21/01/20 11:34:10 INFO BlockManagerInfo: Removed broadcast_162_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:34:11 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:11 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:34:11 INFO DAGScheduler: Got job 163 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:34:11 INFO DAGScheduler: Final stage: ResultStage 163 (parquet at Generate.java:61)
21/01/20 11:34:11 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:34:11 INFO DAGScheduler: Missing parents: List()
21/01/20 11:34:11 INFO DAGScheduler: Submitting ResultStage 163 (MapPartitionsRDD[653] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:34:11 INFO MemoryStore: Block broadcast_163 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:34:11 INFO MemoryStore: Block broadcast_163_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:34:11 INFO BlockManagerInfo: Added broadcast_163_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:34:11 INFO SparkContext: Created broadcast 163 from broadcast at DAGScheduler.scala:1200
21/01/20 11:34:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 163 (MapPartitionsRDD[653] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:34:11 INFO TaskSchedulerImpl: Adding task set 163.0 with 1 tasks
21/01/20 11:34:11 WARN TaskSetManager: Stage 163 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:34:11 INFO TaskSetManager: Starting task 0.0 in stage 163.0 (TID 163, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:34:11 INFO Executor: Running task 0.0 in stage 163.0 (TID 163)
21/01/20 11:34:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:11 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:34:11 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:34:11 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:34:11 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:34:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:34:11 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:34:11 INFO ParquetOutputFormat: Validation is off
21/01/20 11:34:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:34:11 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:34:11 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:34:11 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:34:11 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:34:11 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:34:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113411_0163_m_000000_163' to o3fs://bucket1.vol1/testdata
21/01/20 11:34:12 INFO SparkHadoopMapRedUtil: attempt_20210120113411_0163_m_000000_163: Committed
21/01/20 11:34:12 INFO Executor: Finished task 0.0 in stage 163.0 (TID 163). 2155 bytes result sent to driver
21/01/20 11:34:12 INFO TaskSetManager: Finished task 0.0 in stage 163.0 (TID 163) in 924 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:34:12 INFO TaskSchedulerImpl: Removed TaskSet 163.0, whose tasks have all completed, from pool 
21/01/20 11:34:12 INFO DAGScheduler: ResultStage 163 (parquet at Generate.java:61) finished in 0.966 s
21/01/20 11:34:12 INFO DAGScheduler: Job 163 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:34:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 163: Stage finished
21/01/20 11:34:12 INFO DAGScheduler: Job 163 finished: parquet at Generate.java:61, took 0.967446 s
21/01/20 11:34:12 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-bfc30d1b-c477-40b8-9691-0c9343fc3271
21/01/20 11:34:12 INFO FileFormatWriter: Write Job 85d5599b-c64c-42ce-a482-ee14f3eee4ab committed.
21/01/20 11:34:12 INFO FileFormatWriter: Finished processing stats for write job 85d5599b-c64c-42ce-a482-ee14f3eee4ab.
21/01/20 11:34:12 INFO BlockManagerInfo: Removed broadcast_163_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:34:14 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:14 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:15 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:34:15 INFO DAGScheduler: Got job 164 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:34:15 INFO DAGScheduler: Final stage: ResultStage 164 (parquet at Generate.java:61)
21/01/20 11:34:15 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:34:15 INFO DAGScheduler: Missing parents: List()
21/01/20 11:34:15 INFO DAGScheduler: Submitting ResultStage 164 (MapPartitionsRDD[657] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:34:15 INFO MemoryStore: Block broadcast_164 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:34:15 INFO MemoryStore: Block broadcast_164_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:34:15 INFO BlockManagerInfo: Added broadcast_164_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:34:15 INFO SparkContext: Created broadcast 164 from broadcast at DAGScheduler.scala:1200
21/01/20 11:34:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 164 (MapPartitionsRDD[657] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:34:15 INFO TaskSchedulerImpl: Adding task set 164.0 with 1 tasks
21/01/20 11:34:15 WARN TaskSetManager: Stage 164 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:34:15 INFO TaskSetManager: Starting task 0.0 in stage 164.0 (TID 164, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:34:15 INFO Executor: Running task 0.0 in stage 164.0 (TID 164)
21/01/20 11:34:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:15 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:15 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:34:15 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:34:15 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:34:15 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:34:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:34:15 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:34:15 INFO ParquetOutputFormat: Validation is off
21/01/20 11:34:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:34:15 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:34:15 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:34:15 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:34:15 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:34:15 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:34:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:34:16 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113414_0164_m_000000_164' to o3fs://bucket1.vol1/testdata
21/01/20 11:34:16 INFO SparkHadoopMapRedUtil: attempt_20210120113414_0164_m_000000_164: Committed
21/01/20 11:34:16 INFO Executor: Finished task 0.0 in stage 164.0 (TID 164). 2155 bytes result sent to driver
21/01/20 11:34:16 INFO TaskSetManager: Finished task 0.0 in stage 164.0 (TID 164) in 993 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:34:16 INFO TaskSchedulerImpl: Removed TaskSet 164.0, whose tasks have all completed, from pool 
21/01/20 11:34:16 INFO DAGScheduler: ResultStage 164 (parquet at Generate.java:61) finished in 1.012 s
21/01/20 11:34:16 INFO DAGScheduler: Job 164 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:34:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 164: Stage finished
21/01/20 11:34:16 INFO DAGScheduler: Job 164 finished: parquet at Generate.java:61, took 1.013855 s
21/01/20 11:34:16 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-b06eb485-c161-48be-b06b-94bfe5b5e937
21/01/20 11:34:16 INFO FileFormatWriter: Write Job f8f360e3-10fc-4be8-b3eb-c8d1a237a8d1 committed.
21/01/20 11:34:16 INFO FileFormatWriter: Finished processing stats for write job f8f360e3-10fc-4be8-b3eb-c8d1a237a8d1.
21/01/20 11:34:17 INFO BlockManagerInfo: Removed broadcast_164_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:34:18 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:18 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:18 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:34:18 INFO DAGScheduler: Got job 165 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:34:18 INFO DAGScheduler: Final stage: ResultStage 165 (parquet at Generate.java:61)
21/01/20 11:34:18 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:34:18 INFO DAGScheduler: Missing parents: List()
21/01/20 11:34:18 INFO DAGScheduler: Submitting ResultStage 165 (MapPartitionsRDD[661] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:34:18 INFO MemoryStore: Block broadcast_165 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:34:18 INFO MemoryStore: Block broadcast_165_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:34:18 INFO BlockManagerInfo: Added broadcast_165_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:34:18 INFO SparkContext: Created broadcast 165 from broadcast at DAGScheduler.scala:1200
21/01/20 11:34:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 165 (MapPartitionsRDD[661] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:34:18 INFO TaskSchedulerImpl: Adding task set 165.0 with 1 tasks
21/01/20 11:34:18 WARN TaskSetManager: Stage 165 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:34:18 INFO TaskSetManager: Starting task 0.0 in stage 165.0 (TID 165, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:34:18 INFO Executor: Running task 0.0 in stage 165.0 (TID 165)
21/01/20 11:34:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:18 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:18 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:34:18 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:34:18 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:34:18 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:34:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:34:18 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:34:18 INFO ParquetOutputFormat: Validation is off
21/01/20 11:34:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:34:18 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:34:18 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:34:18 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:34:18 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:34:18 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:34:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:34:19 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113418_0165_m_000000_165' to o3fs://bucket1.vol1/testdata
21/01/20 11:34:19 INFO SparkHadoopMapRedUtil: attempt_20210120113418_0165_m_000000_165: Committed
21/01/20 11:34:19 INFO Executor: Finished task 0.0 in stage 165.0 (TID 165). 2155 bytes result sent to driver
21/01/20 11:34:19 INFO TaskSetManager: Finished task 0.0 in stage 165.0 (TID 165) in 882 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:34:19 INFO TaskSchedulerImpl: Removed TaskSet 165.0, whose tasks have all completed, from pool 
21/01/20 11:34:19 INFO DAGScheduler: ResultStage 165 (parquet at Generate.java:61) finished in 0.923 s
21/01/20 11:34:19 INFO DAGScheduler: Job 165 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:34:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 165: Stage finished
21/01/20 11:34:19 INFO DAGScheduler: Job 165 finished: parquet at Generate.java:61, took 0.925416 s
21/01/20 11:34:19 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-18375847-5d8b-4382-a7a0-cc985af3b77b
21/01/20 11:34:19 INFO FileFormatWriter: Write Job f0c9cbe0-f007-431a-b36c-9bfef0a99ab2 committed.
21/01/20 11:34:19 INFO FileFormatWriter: Finished processing stats for write job f0c9cbe0-f007-431a-b36c-9bfef0a99ab2.
21/01/20 11:34:19 INFO BlockManagerInfo: Removed broadcast_165_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:34:22 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:22 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:22 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:34:22 INFO DAGScheduler: Got job 166 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:34:22 INFO DAGScheduler: Final stage: ResultStage 166 (parquet at Generate.java:61)
21/01/20 11:34:22 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:34:22 INFO DAGScheduler: Missing parents: List()
21/01/20 11:34:22 INFO DAGScheduler: Submitting ResultStage 166 (MapPartitionsRDD[665] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:34:22 INFO MemoryStore: Block broadcast_166 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:34:22 INFO MemoryStore: Block broadcast_166_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:34:22 INFO BlockManagerInfo: Added broadcast_166_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:34:22 INFO SparkContext: Created broadcast 166 from broadcast at DAGScheduler.scala:1200
21/01/20 11:34:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 166 (MapPartitionsRDD[665] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:34:22 INFO TaskSchedulerImpl: Adding task set 166.0 with 1 tasks
21/01/20 11:34:22 WARN TaskSetManager: Stage 166 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:34:22 INFO TaskSetManager: Starting task 0.0 in stage 166.0 (TID 166, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:34:22 INFO Executor: Running task 0.0 in stage 166.0 (TID 166)
21/01/20 11:34:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:22 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:22 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:34:22 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:34:22 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:34:22 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:34:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:34:22 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:34:22 INFO ParquetOutputFormat: Validation is off
21/01/20 11:34:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:34:22 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:34:22 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:34:22 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:34:22 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:34:22 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:34:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:34:23 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113422_0166_m_000000_166' to o3fs://bucket1.vol1/testdata
21/01/20 11:34:23 INFO SparkHadoopMapRedUtil: attempt_20210120113422_0166_m_000000_166: Committed
21/01/20 11:34:23 INFO Executor: Finished task 0.0 in stage 166.0 (TID 166). 2155 bytes result sent to driver
21/01/20 11:34:23 INFO TaskSetManager: Finished task 0.0 in stage 166.0 (TID 166) in 1029 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:34:23 INFO TaskSchedulerImpl: Removed TaskSet 166.0, whose tasks have all completed, from pool 
21/01/20 11:34:23 INFO DAGScheduler: ResultStage 166 (parquet at Generate.java:61) finished in 1.046 s
21/01/20 11:34:23 INFO DAGScheduler: Job 166 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:34:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 166: Stage finished
21/01/20 11:34:23 INFO DAGScheduler: Job 166 finished: parquet at Generate.java:61, took 1.048095 s
21/01/20 11:34:23 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-db39b9f6-98d2-4c0c-8163-e5136806e042
21/01/20 11:34:23 INFO FileFormatWriter: Write Job 20273a26-4cd2-480b-ba11-b8e6c61e5c82 committed.
21/01/20 11:34:23 INFO FileFormatWriter: Finished processing stats for write job 20273a26-4cd2-480b-ba11-b8e6c61e5c82.
21/01/20 11:34:24 INFO BlockManagerInfo: Removed broadcast_166_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:34:25 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:25 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:25 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:34:25 INFO DAGScheduler: Got job 167 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:34:25 INFO DAGScheduler: Final stage: ResultStage 167 (parquet at Generate.java:61)
21/01/20 11:34:25 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:34:25 INFO DAGScheduler: Missing parents: List()
21/01/20 11:34:25 INFO DAGScheduler: Submitting ResultStage 167 (MapPartitionsRDD[669] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:34:25 INFO MemoryStore: Block broadcast_167 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:34:25 INFO MemoryStore: Block broadcast_167_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:34:25 INFO BlockManagerInfo: Added broadcast_167_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:34:25 INFO SparkContext: Created broadcast 167 from broadcast at DAGScheduler.scala:1200
21/01/20 11:34:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 167 (MapPartitionsRDD[669] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:34:25 INFO TaskSchedulerImpl: Adding task set 167.0 with 1 tasks
21/01/20 11:34:25 WARN TaskSetManager: Stage 167 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:34:25 INFO TaskSetManager: Starting task 0.0 in stage 167.0 (TID 167, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:34:25 INFO Executor: Running task 0.0 in stage 167.0 (TID 167)
21/01/20 11:34:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:25 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:25 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:34:25 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:34:25 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:34:25 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:34:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:34:25 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:34:25 INFO ParquetOutputFormat: Validation is off
21/01/20 11:34:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:34:25 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:34:25 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:34:25 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:34:25 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:34:25 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:34:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:34:26 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113425_0167_m_000000_167' to o3fs://bucket1.vol1/testdata
21/01/20 11:34:26 INFO SparkHadoopMapRedUtil: attempt_20210120113425_0167_m_000000_167: Committed
21/01/20 11:34:26 INFO Executor: Finished task 0.0 in stage 167.0 (TID 167). 2155 bytes result sent to driver
21/01/20 11:34:26 INFO TaskSetManager: Finished task 0.0 in stage 167.0 (TID 167) in 1055 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:34:26 INFO TaskSchedulerImpl: Removed TaskSet 167.0, whose tasks have all completed, from pool 
21/01/20 11:34:26 INFO DAGScheduler: ResultStage 167 (parquet at Generate.java:61) finished in 1.077 s
21/01/20 11:34:26 INFO DAGScheduler: Job 167 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:34:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 167: Stage finished
21/01/20 11:34:26 INFO DAGScheduler: Job 167 finished: parquet at Generate.java:61, took 1.078479 s
21/01/20 11:34:26 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-641f3c0b-4dd4-4b1e-9ed8-92298de0b0db
21/01/20 11:34:26 INFO FileFormatWriter: Write Job d08f7419-fd44-4548-be5a-c008c8228513 committed.
21/01/20 11:34:26 INFO FileFormatWriter: Finished processing stats for write job d08f7419-fd44-4548-be5a-c008c8228513.
21/01/20 11:34:27 INFO BlockManagerInfo: Removed broadcast_167_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:34:29 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:29 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:34:29 INFO DAGScheduler: Got job 168 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:34:29 INFO DAGScheduler: Final stage: ResultStage 168 (parquet at Generate.java:61)
21/01/20 11:34:29 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:34:29 INFO DAGScheduler: Missing parents: List()
21/01/20 11:34:29 INFO DAGScheduler: Submitting ResultStage 168 (MapPartitionsRDD[673] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:34:29 INFO MemoryStore: Block broadcast_168 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:34:29 INFO MemoryStore: Block broadcast_168_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:34:29 INFO BlockManagerInfo: Added broadcast_168_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:34:29 INFO SparkContext: Created broadcast 168 from broadcast at DAGScheduler.scala:1200
21/01/20 11:34:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 168 (MapPartitionsRDD[673] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:34:29 INFO TaskSchedulerImpl: Adding task set 168.0 with 1 tasks
21/01/20 11:34:29 WARN TaskSetManager: Stage 168 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:34:29 INFO TaskSetManager: Starting task 0.0 in stage 168.0 (TID 168, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:34:29 INFO Executor: Running task 0.0 in stage 168.0 (TID 168)
21/01/20 11:34:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:29 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:34:29 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:34:29 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:34:29 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:34:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:34:29 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:34:29 INFO ParquetOutputFormat: Validation is off
21/01/20 11:34:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:34:29 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:34:29 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:34:29 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:34:29 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:34:29 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:34:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:34:30 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113429_0168_m_000000_168' to o3fs://bucket1.vol1/testdata
21/01/20 11:34:30 INFO SparkHadoopMapRedUtil: attempt_20210120113429_0168_m_000000_168: Committed
21/01/20 11:34:30 INFO Executor: Finished task 0.0 in stage 168.0 (TID 168). 2155 bytes result sent to driver
21/01/20 11:34:30 INFO TaskSetManager: Finished task 0.0 in stage 168.0 (TID 168) in 1026 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:34:30 INFO TaskSchedulerImpl: Removed TaskSet 168.0, whose tasks have all completed, from pool 
21/01/20 11:34:30 INFO DAGScheduler: ResultStage 168 (parquet at Generate.java:61) finished in 1.043 s
21/01/20 11:34:30 INFO DAGScheduler: Job 168 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:34:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 168: Stage finished
21/01/20 11:34:30 INFO DAGScheduler: Job 168 finished: parquet at Generate.java:61, took 1.045170 s
21/01/20 11:34:30 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-dd1cb2f4-591d-458c-9f93-dc46ff8b4a1e
21/01/20 11:34:30 INFO FileFormatWriter: Write Job a4ef8428-e2c1-4ea0-8d0a-dbb9062980d3 committed.
21/01/20 11:34:30 INFO FileFormatWriter: Finished processing stats for write job a4ef8428-e2c1-4ea0-8d0a-dbb9062980d3.
21/01/20 11:34:31 INFO BlockManagerInfo: Removed broadcast_168_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:34:33 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:33 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:33 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:34:33 INFO DAGScheduler: Got job 169 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:34:33 INFO DAGScheduler: Final stage: ResultStage 169 (parquet at Generate.java:61)
21/01/20 11:34:33 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:34:33 INFO DAGScheduler: Missing parents: List()
21/01/20 11:34:33 INFO DAGScheduler: Submitting ResultStage 169 (MapPartitionsRDD[677] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:34:33 INFO MemoryStore: Block broadcast_169 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:34:33 INFO MemoryStore: Block broadcast_169_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:34:33 INFO BlockManagerInfo: Added broadcast_169_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:34:33 INFO SparkContext: Created broadcast 169 from broadcast at DAGScheduler.scala:1200
21/01/20 11:34:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 169 (MapPartitionsRDD[677] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:34:33 INFO TaskSchedulerImpl: Adding task set 169.0 with 1 tasks
21/01/20 11:34:33 WARN TaskSetManager: Stage 169 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:34:33 INFO TaskSetManager: Starting task 0.0 in stage 169.0 (TID 169, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:34:33 INFO Executor: Running task 0.0 in stage 169.0 (TID 169)
21/01/20 11:34:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:33 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:33 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:34:33 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:34:33 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:34:33 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:34:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:34:33 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:34:33 INFO ParquetOutputFormat: Validation is off
21/01/20 11:34:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:34:33 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:34:33 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:34:33 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:34:33 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:34:33 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:34:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:34:34 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113433_0169_m_000000_169' to o3fs://bucket1.vol1/testdata
21/01/20 11:34:34 INFO SparkHadoopMapRedUtil: attempt_20210120113433_0169_m_000000_169: Committed
21/01/20 11:34:34 INFO Executor: Finished task 0.0 in stage 169.0 (TID 169). 2155 bytes result sent to driver
21/01/20 11:34:34 INFO TaskSetManager: Finished task 0.0 in stage 169.0 (TID 169) in 1040 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:34:34 INFO TaskSchedulerImpl: Removed TaskSet 169.0, whose tasks have all completed, from pool 
21/01/20 11:34:34 INFO DAGScheduler: ResultStage 169 (parquet at Generate.java:61) finished in 1.063 s
21/01/20 11:34:34 INFO DAGScheduler: Job 169 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:34:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 169: Stage finished
21/01/20 11:34:34 INFO DAGScheduler: Job 169 finished: parquet at Generate.java:61, took 1.064175 s
21/01/20 11:34:34 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-a6ea22ba-578c-4685-b15f-7fe4a0d2c998
21/01/20 11:34:34 INFO FileFormatWriter: Write Job 424404e5-b122-4c1a-9761-f1ff9500e372 committed.
21/01/20 11:34:34 INFO FileFormatWriter: Finished processing stats for write job 424404e5-b122-4c1a-9761-f1ff9500e372.
21/01/20 11:34:35 INFO BlockManagerInfo: Removed broadcast_169_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:34:36 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:36 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:36 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:34:36 INFO DAGScheduler: Got job 170 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:34:36 INFO DAGScheduler: Final stage: ResultStage 170 (parquet at Generate.java:61)
21/01/20 11:34:36 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:34:36 INFO DAGScheduler: Missing parents: List()
21/01/20 11:34:36 INFO DAGScheduler: Submitting ResultStage 170 (MapPartitionsRDD[681] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:34:36 INFO MemoryStore: Block broadcast_170 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:34:36 INFO MemoryStore: Block broadcast_170_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:34:36 INFO BlockManagerInfo: Added broadcast_170_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:34:36 INFO SparkContext: Created broadcast 170 from broadcast at DAGScheduler.scala:1200
21/01/20 11:34:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 170 (MapPartitionsRDD[681] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:34:36 INFO TaskSchedulerImpl: Adding task set 170.0 with 1 tasks
21/01/20 11:34:36 WARN TaskSetManager: Stage 170 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:34:36 INFO TaskSetManager: Starting task 0.0 in stage 170.0 (TID 170, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:34:36 INFO Executor: Running task 0.0 in stage 170.0 (TID 170)
21/01/20 11:34:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:36 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:36 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:34:36 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:34:36 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:34:36 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:34:36 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:34:36 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:34:36 INFO ParquetOutputFormat: Validation is off
21/01/20 11:34:36 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:34:36 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:34:36 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:34:36 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:34:36 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:34:36 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:34:37 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:34:37 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113436_0170_m_000000_170' to o3fs://bucket1.vol1/testdata
21/01/20 11:34:37 INFO SparkHadoopMapRedUtil: attempt_20210120113436_0170_m_000000_170: Committed
21/01/20 11:34:37 INFO Executor: Finished task 0.0 in stage 170.0 (TID 170). 2155 bytes result sent to driver
21/01/20 11:34:37 INFO TaskSetManager: Finished task 0.0 in stage 170.0 (TID 170) in 1000 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:34:37 INFO TaskSchedulerImpl: Removed TaskSet 170.0, whose tasks have all completed, from pool 
21/01/20 11:34:37 INFO DAGScheduler: ResultStage 170 (parquet at Generate.java:61) finished in 1.017 s
21/01/20 11:34:37 INFO DAGScheduler: Job 170 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:34:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 170: Stage finished
21/01/20 11:34:37 INFO DAGScheduler: Job 170 finished: parquet at Generate.java:61, took 1.019054 s
21/01/20 11:34:37 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-45e72951-3d36-41b3-99cc-71fd2c3a5b2b
21/01/20 11:34:37 INFO FileFormatWriter: Write Job 84d80106-c142-43ec-83a7-3e817291292d committed.
21/01/20 11:34:37 INFO FileFormatWriter: Finished processing stats for write job 84d80106-c142-43ec-83a7-3e817291292d.
21/01/20 11:34:39 INFO BlockManagerInfo: Removed broadcast_170_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:34:40 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:40 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:40 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:34:40 INFO DAGScheduler: Got job 171 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:34:40 INFO DAGScheduler: Final stage: ResultStage 171 (parquet at Generate.java:61)
21/01/20 11:34:40 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:34:40 INFO DAGScheduler: Missing parents: List()
21/01/20 11:34:40 INFO DAGScheduler: Submitting ResultStage 171 (MapPartitionsRDD[685] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:34:40 INFO MemoryStore: Block broadcast_171 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:34:40 INFO MemoryStore: Block broadcast_171_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:34:40 INFO BlockManagerInfo: Added broadcast_171_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:34:40 INFO SparkContext: Created broadcast 171 from broadcast at DAGScheduler.scala:1200
21/01/20 11:34:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 171 (MapPartitionsRDD[685] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:34:40 INFO TaskSchedulerImpl: Adding task set 171.0 with 1 tasks
21/01/20 11:34:40 WARN TaskSetManager: Stage 171 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:34:40 INFO TaskSetManager: Starting task 0.0 in stage 171.0 (TID 171, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:34:40 INFO Executor: Running task 0.0 in stage 171.0 (TID 171)
21/01/20 11:34:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:40 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:40 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:34:40 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:34:40 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:34:40 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:34:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:34:40 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:34:40 INFO ParquetOutputFormat: Validation is off
21/01/20 11:34:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:34:40 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:34:40 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:34:40 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:34:40 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:34:40 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:34:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:34:40 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-BD0564EB45CD->27b66227-fed8-4c1d-ab57-510f5a3ce552
21/01/20 11:34:40 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:34:41 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113440_0171_m_000000_171' to o3fs://bucket1.vol1/testdata
21/01/20 11:34:41 INFO SparkHadoopMapRedUtil: attempt_20210120113440_0171_m_000000_171: Committed
21/01/20 11:34:41 INFO Executor: Finished task 0.0 in stage 171.0 (TID 171). 2155 bytes result sent to driver
21/01/20 11:34:41 INFO TaskSetManager: Finished task 0.0 in stage 171.0 (TID 171) in 839 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:34:41 INFO TaskSchedulerImpl: Removed TaskSet 171.0, whose tasks have all completed, from pool 
21/01/20 11:34:41 INFO DAGScheduler: ResultStage 171 (parquet at Generate.java:61) finished in 0.878 s
21/01/20 11:34:41 INFO DAGScheduler: Job 171 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:34:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 171: Stage finished
21/01/20 11:34:41 INFO DAGScheduler: Job 171 finished: parquet at Generate.java:61, took 0.880523 s
21/01/20 11:34:41 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-6f4a626b-3ae7-4129-b90f-b9066f0eb8e1
21/01/20 11:34:41 INFO FileFormatWriter: Write Job 6462290c-f9f1-41c1-8ffb-12bf4abbb7f2 committed.
21/01/20 11:34:41 INFO FileFormatWriter: Finished processing stats for write job 6462290c-f9f1-41c1-8ffb-12bf4abbb7f2.
21/01/20 11:34:41 INFO BlockManagerInfo: Removed broadcast_171_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:34:43 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:43 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:43 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:34:43 INFO DAGScheduler: Got job 172 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:34:43 INFO DAGScheduler: Final stage: ResultStage 172 (parquet at Generate.java:61)
21/01/20 11:34:43 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:34:43 INFO DAGScheduler: Missing parents: List()
21/01/20 11:34:43 INFO DAGScheduler: Submitting ResultStage 172 (MapPartitionsRDD[689] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:34:43 INFO MemoryStore: Block broadcast_172 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:34:43 INFO MemoryStore: Block broadcast_172_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:34:43 INFO BlockManagerInfo: Added broadcast_172_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:34:43 INFO SparkContext: Created broadcast 172 from broadcast at DAGScheduler.scala:1200
21/01/20 11:34:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 172 (MapPartitionsRDD[689] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:34:43 INFO TaskSchedulerImpl: Adding task set 172.0 with 1 tasks
21/01/20 11:34:44 WARN TaskSetManager: Stage 172 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:34:44 INFO TaskSetManager: Starting task 0.0 in stage 172.0 (TID 172, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:34:44 INFO Executor: Running task 0.0 in stage 172.0 (TID 172)
21/01/20 11:34:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:44 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:44 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:34:44 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:34:44 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:34:44 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:34:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:34:44 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:34:44 INFO ParquetOutputFormat: Validation is off
21/01/20 11:34:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:34:44 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:34:44 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:34:44 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:34:44 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:34:44 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:34:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:34:45 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113443_0172_m_000000_172' to o3fs://bucket1.vol1/testdata
21/01/20 11:34:45 INFO SparkHadoopMapRedUtil: attempt_20210120113443_0172_m_000000_172: Committed
21/01/20 11:34:45 INFO Executor: Finished task 0.0 in stage 172.0 (TID 172). 2155 bytes result sent to driver
21/01/20 11:34:45 INFO TaskSetManager: Finished task 0.0 in stage 172.0 (TID 172) in 1053 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:34:45 INFO TaskSchedulerImpl: Removed TaskSet 172.0, whose tasks have all completed, from pool 
21/01/20 11:34:45 INFO DAGScheduler: ResultStage 172 (parquet at Generate.java:61) finished in 1.070 s
21/01/20 11:34:45 INFO DAGScheduler: Job 172 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:34:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 172: Stage finished
21/01/20 11:34:45 INFO DAGScheduler: Job 172 finished: parquet at Generate.java:61, took 1.071801 s
21/01/20 11:34:45 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-9816ee02-79e0-401b-8e9a-87306ab66042
21/01/20 11:34:45 INFO FileFormatWriter: Write Job 45b9e9b1-6754-4600-870b-52fd25d68d51 committed.
21/01/20 11:34:45 INFO FileFormatWriter: Finished processing stats for write job 45b9e9b1-6754-4600-870b-52fd25d68d51.
21/01/20 11:34:46 INFO BlockManagerInfo: Removed broadcast_172_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:34:47 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:47 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:34:47 INFO DAGScheduler: Got job 173 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:34:47 INFO DAGScheduler: Final stage: ResultStage 173 (parquet at Generate.java:61)
21/01/20 11:34:47 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:34:47 INFO DAGScheduler: Missing parents: List()
21/01/20 11:34:47 INFO DAGScheduler: Submitting ResultStage 173 (MapPartitionsRDD[693] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:34:47 INFO MemoryStore: Block broadcast_173 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:34:47 INFO MemoryStore: Block broadcast_173_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:34:47 INFO BlockManagerInfo: Added broadcast_173_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:34:47 INFO SparkContext: Created broadcast 173 from broadcast at DAGScheduler.scala:1200
21/01/20 11:34:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 173 (MapPartitionsRDD[693] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:34:47 INFO TaskSchedulerImpl: Adding task set 173.0 with 1 tasks
21/01/20 11:34:47 WARN TaskSetManager: Stage 173 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:34:47 INFO TaskSetManager: Starting task 0.0 in stage 173.0 (TID 173, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:34:47 INFO Executor: Running task 0.0 in stage 173.0 (TID 173)
21/01/20 11:34:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:47 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:34:47 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:34:47 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:34:47 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:34:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:34:47 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:34:47 INFO ParquetOutputFormat: Validation is off
21/01/20 11:34:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:34:47 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:34:47 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:34:47 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:34:47 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:34:47 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:34:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:34:48 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113447_0173_m_000000_173' to o3fs://bucket1.vol1/testdata
21/01/20 11:34:48 INFO SparkHadoopMapRedUtil: attempt_20210120113447_0173_m_000000_173: Committed
21/01/20 11:34:48 INFO Executor: Finished task 0.0 in stage 173.0 (TID 173). 2155 bytes result sent to driver
21/01/20 11:34:48 INFO TaskSetManager: Finished task 0.0 in stage 173.0 (TID 173) in 915 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:34:48 INFO TaskSchedulerImpl: Removed TaskSet 173.0, whose tasks have all completed, from pool 
21/01/20 11:34:48 INFO DAGScheduler: ResultStage 173 (parquet at Generate.java:61) finished in 0.956 s
21/01/20 11:34:48 INFO DAGScheduler: Job 173 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:34:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 173: Stage finished
21/01/20 11:34:48 INFO DAGScheduler: Job 173 finished: parquet at Generate.java:61, took 0.957627 s
21/01/20 11:34:48 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-5730b105-1c89-478a-a175-d3c925b6b9a8
21/01/20 11:34:48 INFO FileFormatWriter: Write Job df262bd4-1789-4686-b943-990024dedbc3 committed.
21/01/20 11:34:48 INFO FileFormatWriter: Finished processing stats for write job df262bd4-1789-4686-b943-990024dedbc3.
21/01/20 11:34:48 INFO BlockManagerInfo: Removed broadcast_173_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:34:51 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:51 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:34:51 INFO DAGScheduler: Got job 174 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:34:51 INFO DAGScheduler: Final stage: ResultStage 174 (parquet at Generate.java:61)
21/01/20 11:34:51 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:34:51 INFO DAGScheduler: Missing parents: List()
21/01/20 11:34:51 INFO DAGScheduler: Submitting ResultStage 174 (MapPartitionsRDD[697] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:34:51 INFO MemoryStore: Block broadcast_174 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:34:51 INFO MemoryStore: Block broadcast_174_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:34:51 INFO BlockManagerInfo: Added broadcast_174_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:34:51 INFO SparkContext: Created broadcast 174 from broadcast at DAGScheduler.scala:1200
21/01/20 11:34:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 174 (MapPartitionsRDD[697] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:34:51 INFO TaskSchedulerImpl: Adding task set 174.0 with 1 tasks
21/01/20 11:34:51 WARN TaskSetManager: Stage 174 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:34:51 INFO TaskSetManager: Starting task 0.0 in stage 174.0 (TID 174, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:34:51 INFO Executor: Running task 0.0 in stage 174.0 (TID 174)
21/01/20 11:34:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:51 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:34:51 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:34:51 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:34:51 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:34:51 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:34:51 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:34:51 INFO ParquetOutputFormat: Validation is off
21/01/20 11:34:51 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:34:51 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:34:51 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:34:51 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:34:51 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:34:51 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:34:51 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:34:51 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-7F485821B2CF->8df234bd-f131-4d8e-953a-b9eec8633221
21/01/20 11:34:51 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:34:52 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113451_0174_m_000000_174' to o3fs://bucket1.vol1/testdata
21/01/20 11:34:52 INFO SparkHadoopMapRedUtil: attempt_20210120113451_0174_m_000000_174: Committed
21/01/20 11:34:52 INFO Executor: Finished task 0.0 in stage 174.0 (TID 174). 2155 bytes result sent to driver
21/01/20 11:34:52 INFO TaskSetManager: Finished task 0.0 in stage 174.0 (TID 174) in 1065 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:34:52 INFO TaskSchedulerImpl: Removed TaskSet 174.0, whose tasks have all completed, from pool 
21/01/20 11:34:52 INFO DAGScheduler: ResultStage 174 (parquet at Generate.java:61) finished in 1.084 s
21/01/20 11:34:52 INFO DAGScheduler: Job 174 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:34:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 174: Stage finished
21/01/20 11:34:52 INFO DAGScheduler: Job 174 finished: parquet at Generate.java:61, took 1.085437 s
21/01/20 11:34:52 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-951494f5-8b59-4e16-bedb-10580f32ffcb
21/01/20 11:34:52 INFO FileFormatWriter: Write Job 2119b5d4-9a33-4c4f-82c8-cfc41ef93095 committed.
21/01/20 11:34:52 INFO FileFormatWriter: Finished processing stats for write job 2119b5d4-9a33-4c4f-82c8-cfc41ef93095.
21/01/20 11:34:53 INFO BlockManagerInfo: Removed broadcast_174_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:34:54 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:54 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:54 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:34:54 INFO DAGScheduler: Got job 175 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:34:54 INFO DAGScheduler: Final stage: ResultStage 175 (parquet at Generate.java:61)
21/01/20 11:34:54 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:34:54 INFO DAGScheduler: Missing parents: List()
21/01/20 11:34:54 INFO DAGScheduler: Submitting ResultStage 175 (MapPartitionsRDD[701] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:34:54 INFO MemoryStore: Block broadcast_175 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:34:54 INFO MemoryStore: Block broadcast_175_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:34:54 INFO BlockManagerInfo: Added broadcast_175_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:34:54 INFO SparkContext: Created broadcast 175 from broadcast at DAGScheduler.scala:1200
21/01/20 11:34:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 175 (MapPartitionsRDD[701] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:34:54 INFO TaskSchedulerImpl: Adding task set 175.0 with 1 tasks
21/01/20 11:34:55 WARN TaskSetManager: Stage 175 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:34:55 INFO TaskSetManager: Starting task 0.0 in stage 175.0 (TID 175, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:34:55 INFO Executor: Running task 0.0 in stage 175.0 (TID 175)
21/01/20 11:34:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:55 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:34:55 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:34:55 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:34:55 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:34:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:34:55 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:34:55 INFO ParquetOutputFormat: Validation is off
21/01/20 11:34:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:34:55 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:34:55 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:34:55 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:34:55 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:34:55 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:34:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:34:56 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113454_0175_m_000000_175' to o3fs://bucket1.vol1/testdata
21/01/20 11:34:56 INFO SparkHadoopMapRedUtil: attempt_20210120113454_0175_m_000000_175: Committed
21/01/20 11:34:56 INFO Executor: Finished task 0.0 in stage 175.0 (TID 175). 2155 bytes result sent to driver
21/01/20 11:34:56 INFO TaskSetManager: Finished task 0.0 in stage 175.0 (TID 175) in 1084 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:34:56 INFO TaskSchedulerImpl: Removed TaskSet 175.0, whose tasks have all completed, from pool 
21/01/20 11:34:56 INFO DAGScheduler: ResultStage 175 (parquet at Generate.java:61) finished in 1.103 s
21/01/20 11:34:56 INFO DAGScheduler: Job 175 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:34:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 175: Stage finished
21/01/20 11:34:56 INFO DAGScheduler: Job 175 finished: parquet at Generate.java:61, took 1.106794 s
21/01/20 11:34:56 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-7297d287-289b-49ee-80b4-5a19e45ab636
21/01/20 11:34:56 INFO FileFormatWriter: Write Job 5a7c9325-b805-450f-8a25-8882e3564ddc committed.
21/01/20 11:34:56 INFO FileFormatWriter: Finished processing stats for write job 5a7c9325-b805-450f-8a25-8882e3564ddc.
21/01/20 11:34:57 INFO BlockManagerInfo: Removed broadcast_175_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:34:58 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:58 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:58 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:34:58 INFO DAGScheduler: Got job 176 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:34:58 INFO DAGScheduler: Final stage: ResultStage 176 (parquet at Generate.java:61)
21/01/20 11:34:58 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:34:58 INFO DAGScheduler: Missing parents: List()
21/01/20 11:34:58 INFO DAGScheduler: Submitting ResultStage 176 (MapPartitionsRDD[705] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:34:58 INFO MemoryStore: Block broadcast_176 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:34:58 INFO MemoryStore: Block broadcast_176_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:34:58 INFO BlockManagerInfo: Added broadcast_176_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:34:58 INFO SparkContext: Created broadcast 176 from broadcast at DAGScheduler.scala:1200
21/01/20 11:34:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 176 (MapPartitionsRDD[705] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:34:58 INFO TaskSchedulerImpl: Adding task set 176.0 with 1 tasks
21/01/20 11:34:58 WARN TaskSetManager: Stage 176 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:34:58 INFO TaskSetManager: Starting task 0.0 in stage 176.0 (TID 176, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:34:58 INFO Executor: Running task 0.0 in stage 176.0 (TID 176)
21/01/20 11:34:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:58 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:34:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:34:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:34:58 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:34:58 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:34:58 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:34:58 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:34:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:34:58 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:34:58 INFO ParquetOutputFormat: Validation is off
21/01/20 11:34:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:34:58 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:34:58 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:34:58 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:34:58 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:34:58 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:34:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:34:59 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113458_0176_m_000000_176' to o3fs://bucket1.vol1/testdata
21/01/20 11:34:59 INFO SparkHadoopMapRedUtil: attempt_20210120113458_0176_m_000000_176: Committed
21/01/20 11:34:59 INFO Executor: Finished task 0.0 in stage 176.0 (TID 176). 2155 bytes result sent to driver
21/01/20 11:34:59 INFO TaskSetManager: Finished task 0.0 in stage 176.0 (TID 176) in 1057 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:34:59 INFO TaskSchedulerImpl: Removed TaskSet 176.0, whose tasks have all completed, from pool 
21/01/20 11:34:59 INFO DAGScheduler: ResultStage 176 (parquet at Generate.java:61) finished in 1.075 s
21/01/20 11:34:59 INFO DAGScheduler: Job 176 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:34:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 176: Stage finished
21/01/20 11:34:59 INFO DAGScheduler: Job 176 finished: parquet at Generate.java:61, took 1.076913 s
21/01/20 11:34:59 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-02ef33a0-774e-4ede-805c-fdadb0f6493c
21/01/20 11:34:59 INFO FileFormatWriter: Write Job 8c488059-709c-488a-9936-ad66a0d25be6 committed.
21/01/20 11:34:59 INFO FileFormatWriter: Finished processing stats for write job 8c488059-709c-488a-9936-ad66a0d25be6.
21/01/20 11:35:00 INFO BlockManagerInfo: Removed broadcast_176_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:35:02 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:02 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:35:02 INFO DAGScheduler: Got job 177 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:35:02 INFO DAGScheduler: Final stage: ResultStage 177 (parquet at Generate.java:61)
21/01/20 11:35:02 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:35:02 INFO DAGScheduler: Missing parents: List()
21/01/20 11:35:02 INFO DAGScheduler: Submitting ResultStage 177 (MapPartitionsRDD[709] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:35:02 INFO MemoryStore: Block broadcast_177 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:35:02 INFO MemoryStore: Block broadcast_177_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:35:02 INFO BlockManagerInfo: Added broadcast_177_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:35:02 INFO SparkContext: Created broadcast 177 from broadcast at DAGScheduler.scala:1200
21/01/20 11:35:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 177 (MapPartitionsRDD[709] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:35:02 INFO TaskSchedulerImpl: Adding task set 177.0 with 1 tasks
21/01/20 11:35:02 WARN TaskSetManager: Stage 177 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:35:02 INFO TaskSetManager: Starting task 0.0 in stage 177.0 (TID 177, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:35:02 INFO Executor: Running task 0.0 in stage 177.0 (TID 177)
21/01/20 11:35:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:02 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:35:02 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:35:02 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:35:02 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:35:02 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:35:02 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:35:02 INFO ParquetOutputFormat: Validation is off
21/01/20 11:35:02 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:35:02 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:35:02 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:35:02 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:35:02 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:35:02 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:35:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:35:03 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113502_0177_m_000000_177' to o3fs://bucket1.vol1/testdata
21/01/20 11:35:03 INFO SparkHadoopMapRedUtil: attempt_20210120113502_0177_m_000000_177: Committed
21/01/20 11:35:03 INFO Executor: Finished task 0.0 in stage 177.0 (TID 177). 2155 bytes result sent to driver
21/01/20 11:35:03 INFO TaskSetManager: Finished task 0.0 in stage 177.0 (TID 177) in 1113 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:35:03 INFO TaskSchedulerImpl: Removed TaskSet 177.0, whose tasks have all completed, from pool 
21/01/20 11:35:03 INFO DAGScheduler: ResultStage 177 (parquet at Generate.java:61) finished in 1.131 s
21/01/20 11:35:03 INFO DAGScheduler: Job 177 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:35:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 177: Stage finished
21/01/20 11:35:03 INFO DAGScheduler: Job 177 finished: parquet at Generate.java:61, took 1.133175 s
21/01/20 11:35:03 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-800ed229-1e64-47a3-8d5e-a96d619c7103
21/01/20 11:35:03 INFO FileFormatWriter: Write Job 9d507ecb-854c-496e-ac90-4a4d9484451b committed.
21/01/20 11:35:03 INFO FileFormatWriter: Finished processing stats for write job 9d507ecb-854c-496e-ac90-4a4d9484451b.
21/01/20 11:35:04 INFO BlockManagerInfo: Removed broadcast_177_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:35:06 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:06 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:06 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:35:06 INFO DAGScheduler: Got job 178 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:35:06 INFO DAGScheduler: Final stage: ResultStage 178 (parquet at Generate.java:61)
21/01/20 11:35:06 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:35:06 INFO DAGScheduler: Missing parents: List()
21/01/20 11:35:06 INFO DAGScheduler: Submitting ResultStage 178 (MapPartitionsRDD[713] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:35:06 INFO MemoryStore: Block broadcast_178 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:35:06 INFO MemoryStore: Block broadcast_178_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:35:06 INFO BlockManagerInfo: Added broadcast_178_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:35:06 INFO SparkContext: Created broadcast 178 from broadcast at DAGScheduler.scala:1200
21/01/20 11:35:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 178 (MapPartitionsRDD[713] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:35:06 INFO TaskSchedulerImpl: Adding task set 178.0 with 1 tasks
21/01/20 11:35:06 WARN TaskSetManager: Stage 178 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:35:06 INFO TaskSetManager: Starting task 0.0 in stage 178.0 (TID 178, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:35:06 INFO Executor: Running task 0.0 in stage 178.0 (TID 178)
21/01/20 11:35:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:06 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:06 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:35:06 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:35:06 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:35:06 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:35:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:35:06 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:35:06 INFO ParquetOutputFormat: Validation is off
21/01/20 11:35:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:35:06 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:35:06 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:35:06 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:35:06 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:35:06 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:35:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:35:07 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113506_0178_m_000000_178' to o3fs://bucket1.vol1/testdata
21/01/20 11:35:07 INFO SparkHadoopMapRedUtil: attempt_20210120113506_0178_m_000000_178: Committed
21/01/20 11:35:07 INFO Executor: Finished task 0.0 in stage 178.0 (TID 178). 2155 bytes result sent to driver
21/01/20 11:35:07 INFO TaskSetManager: Finished task 0.0 in stage 178.0 (TID 178) in 978 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:35:07 INFO TaskSchedulerImpl: Removed TaskSet 178.0, whose tasks have all completed, from pool 
21/01/20 11:35:07 INFO DAGScheduler: ResultStage 178 (parquet at Generate.java:61) finished in 0.997 s
21/01/20 11:35:07 INFO DAGScheduler: Job 178 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:35:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 178: Stage finished
21/01/20 11:35:07 INFO DAGScheduler: Job 178 finished: parquet at Generate.java:61, took 0.998281 s
21/01/20 11:35:07 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-397d7b35-d996-4c32-abd9-8b12aa452584
21/01/20 11:35:07 INFO FileFormatWriter: Write Job 84974368-8031-470d-b891-1c34152ee42e committed.
21/01/20 11:35:07 INFO FileFormatWriter: Finished processing stats for write job 84974368-8031-470d-b891-1c34152ee42e.
21/01/20 11:35:08 INFO BlockManagerInfo: Removed broadcast_178_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:35:09 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:09 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:09 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:35:09 INFO DAGScheduler: Got job 179 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:35:09 INFO DAGScheduler: Final stage: ResultStage 179 (parquet at Generate.java:61)
21/01/20 11:35:09 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:35:09 INFO DAGScheduler: Missing parents: List()
21/01/20 11:35:09 INFO DAGScheduler: Submitting ResultStage 179 (MapPartitionsRDD[717] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:35:09 INFO MemoryStore: Block broadcast_179 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:35:09 INFO MemoryStore: Block broadcast_179_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:35:09 INFO BlockManagerInfo: Added broadcast_179_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:35:09 INFO SparkContext: Created broadcast 179 from broadcast at DAGScheduler.scala:1200
21/01/20 11:35:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 179 (MapPartitionsRDD[717] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:35:09 INFO TaskSchedulerImpl: Adding task set 179.0 with 1 tasks
21/01/20 11:35:09 WARN TaskSetManager: Stage 179 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:35:09 INFO TaskSetManager: Starting task 0.0 in stage 179.0 (TID 179, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:35:09 INFO Executor: Running task 0.0 in stage 179.0 (TID 179)
21/01/20 11:35:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:09 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:09 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:35:09 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:35:09 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:35:09 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:35:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:35:09 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:35:09 INFO ParquetOutputFormat: Validation is off
21/01/20 11:35:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:35:09 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:35:09 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:35:09 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:35:09 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:35:09 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:35:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:35:10 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113509_0179_m_000000_179' to o3fs://bucket1.vol1/testdata
21/01/20 11:35:10 INFO SparkHadoopMapRedUtil: attempt_20210120113509_0179_m_000000_179: Committed
21/01/20 11:35:10 INFO Executor: Finished task 0.0 in stage 179.0 (TID 179). 2155 bytes result sent to driver
21/01/20 11:35:10 INFO TaskSetManager: Finished task 0.0 in stage 179.0 (TID 179) in 790 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:35:10 INFO TaskSchedulerImpl: Removed TaskSet 179.0, whose tasks have all completed, from pool 
21/01/20 11:35:10 INFO DAGScheduler: ResultStage 179 (parquet at Generate.java:61) finished in 0.831 s
21/01/20 11:35:10 INFO DAGScheduler: Job 179 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:35:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 179: Stage finished
21/01/20 11:35:10 INFO DAGScheduler: Job 179 finished: parquet at Generate.java:61, took 0.832947 s
21/01/20 11:35:10 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-b7a4c683-dc83-4465-96b5-b026abafe3b7
21/01/20 11:35:10 INFO FileFormatWriter: Write Job 577b65cd-bb16-4962-ae32-85f374715a4a committed.
21/01/20 11:35:10 INFO FileFormatWriter: Finished processing stats for write job 577b65cd-bb16-4962-ae32-85f374715a4a.
21/01/20 11:35:10 INFO BlockManagerInfo: Removed broadcast_179_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:35:13 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:13 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:13 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:35:13 INFO DAGScheduler: Got job 180 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:35:13 INFO DAGScheduler: Final stage: ResultStage 180 (parquet at Generate.java:61)
21/01/20 11:35:13 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:35:13 INFO DAGScheduler: Missing parents: List()
21/01/20 11:35:13 INFO DAGScheduler: Submitting ResultStage 180 (MapPartitionsRDD[721] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:35:13 INFO MemoryStore: Block broadcast_180 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:35:13 INFO MemoryStore: Block broadcast_180_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:35:13 INFO BlockManagerInfo: Added broadcast_180_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:35:13 INFO SparkContext: Created broadcast 180 from broadcast at DAGScheduler.scala:1200
21/01/20 11:35:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 180 (MapPartitionsRDD[721] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:35:13 INFO TaskSchedulerImpl: Adding task set 180.0 with 1 tasks
21/01/20 11:35:13 WARN TaskSetManager: Stage 180 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:35:13 INFO TaskSetManager: Starting task 0.0 in stage 180.0 (TID 180, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:35:13 INFO Executor: Running task 0.0 in stage 180.0 (TID 180)
21/01/20 11:35:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:13 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:13 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:35:13 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:35:13 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:35:13 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:35:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:35:13 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:35:13 INFO ParquetOutputFormat: Validation is off
21/01/20 11:35:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:35:13 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:35:13 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:35:13 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:35:13 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:35:13 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:35:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:35:14 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113513_0180_m_000000_180' to o3fs://bucket1.vol1/testdata
21/01/20 11:35:14 INFO SparkHadoopMapRedUtil: attempt_20210120113513_0180_m_000000_180: Committed
21/01/20 11:35:14 INFO Executor: Finished task 0.0 in stage 180.0 (TID 180). 2155 bytes result sent to driver
21/01/20 11:35:14 INFO TaskSetManager: Finished task 0.0 in stage 180.0 (TID 180) in 1030 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:35:14 INFO TaskSchedulerImpl: Removed TaskSet 180.0, whose tasks have all completed, from pool 
21/01/20 11:35:14 INFO DAGScheduler: ResultStage 180 (parquet at Generate.java:61) finished in 1.049 s
21/01/20 11:35:14 INFO DAGScheduler: Job 180 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:35:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 180: Stage finished
21/01/20 11:35:14 INFO DAGScheduler: Job 180 finished: parquet at Generate.java:61, took 1.049841 s
21/01/20 11:35:14 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-e3e807c0-9c1c-4508-9725-846ca71b865c
21/01/20 11:35:14 INFO FileFormatWriter: Write Job 0c3fe383-3869-4671-b4af-9b5adaa7df44 committed.
21/01/20 11:35:14 INFO FileFormatWriter: Finished processing stats for write job 0c3fe383-3869-4671-b4af-9b5adaa7df44.
21/01/20 11:35:15 INFO BlockManagerInfo: Removed broadcast_180_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:35:16 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:16 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:16 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:35:16 INFO DAGScheduler: Got job 181 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:35:16 INFO DAGScheduler: Final stage: ResultStage 181 (parquet at Generate.java:61)
21/01/20 11:35:16 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:35:16 INFO DAGScheduler: Missing parents: List()
21/01/20 11:35:16 INFO DAGScheduler: Submitting ResultStage 181 (MapPartitionsRDD[725] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:35:16 INFO MemoryStore: Block broadcast_181 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:35:16 INFO MemoryStore: Block broadcast_181_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:35:16 INFO BlockManagerInfo: Added broadcast_181_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:35:16 INFO SparkContext: Created broadcast 181 from broadcast at DAGScheduler.scala:1200
21/01/20 11:35:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 181 (MapPartitionsRDD[725] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:35:16 INFO TaskSchedulerImpl: Adding task set 181.0 with 1 tasks
21/01/20 11:35:16 WARN TaskSetManager: Stage 181 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:35:16 INFO TaskSetManager: Starting task 0.0 in stage 181.0 (TID 181, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:35:16 INFO Executor: Running task 0.0 in stage 181.0 (TID 181)
21/01/20 11:35:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:16 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:16 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:35:16 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:35:16 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:35:16 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:35:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:35:16 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:35:16 INFO ParquetOutputFormat: Validation is off
21/01/20 11:35:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:35:16 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:35:16 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:35:16 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:35:16 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:35:16 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:35:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:35:17 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-CB2F66BCFBDC->8df234bd-f131-4d8e-953a-b9eec8633221
21/01/20 11:35:17 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:35:17 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113516_0181_m_000000_181' to o3fs://bucket1.vol1/testdata
21/01/20 11:35:17 INFO SparkHadoopMapRedUtil: attempt_20210120113516_0181_m_000000_181: Committed
21/01/20 11:35:17 INFO Executor: Finished task 0.0 in stage 181.0 (TID 181). 2155 bytes result sent to driver
21/01/20 11:35:17 INFO TaskSetManager: Finished task 0.0 in stage 181.0 (TID 181) in 827 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:35:17 INFO TaskSchedulerImpl: Removed TaskSet 181.0, whose tasks have all completed, from pool 
21/01/20 11:35:17 INFO DAGScheduler: ResultStage 181 (parquet at Generate.java:61) finished in 0.845 s
21/01/20 11:35:17 INFO DAGScheduler: Job 181 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:35:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 181: Stage finished
21/01/20 11:35:17 INFO DAGScheduler: Job 181 finished: parquet at Generate.java:61, took 0.869208 s
21/01/20 11:35:17 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-35513275-fefe-4a3b-b7d7-4379145f8f15
21/01/20 11:35:17 INFO FileFormatWriter: Write Job fcc77ca4-046c-4b6a-aeb0-a84c0f434807 committed.
21/01/20 11:35:17 INFO FileFormatWriter: Finished processing stats for write job fcc77ca4-046c-4b6a-aeb0-a84c0f434807.
21/01/20 11:35:17 INFO BlockManagerInfo: Removed broadcast_181_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:35:20 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:20 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:20 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:35:20 INFO DAGScheduler: Got job 182 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:35:20 INFO DAGScheduler: Final stage: ResultStage 182 (parquet at Generate.java:61)
21/01/20 11:35:20 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:35:20 INFO DAGScheduler: Missing parents: List()
21/01/20 11:35:20 INFO DAGScheduler: Submitting ResultStage 182 (MapPartitionsRDD[729] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:35:20 INFO MemoryStore: Block broadcast_182 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:35:20 INFO MemoryStore: Block broadcast_182_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:35:20 INFO BlockManagerInfo: Added broadcast_182_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:35:20 INFO SparkContext: Created broadcast 182 from broadcast at DAGScheduler.scala:1200
21/01/20 11:35:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 182 (MapPartitionsRDD[729] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:35:20 INFO TaskSchedulerImpl: Adding task set 182.0 with 1 tasks
21/01/20 11:35:20 WARN TaskSetManager: Stage 182 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:35:20 INFO TaskSetManager: Starting task 0.0 in stage 182.0 (TID 182, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:35:20 INFO Executor: Running task 0.0 in stage 182.0 (TID 182)
21/01/20 11:35:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:20 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:20 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:35:20 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:35:20 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:35:20 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:35:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:35:20 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:35:20 INFO ParquetOutputFormat: Validation is off
21/01/20 11:35:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:35:20 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:35:20 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:35:20 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:35:20 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:35:20 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:35:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:35:21 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113520_0182_m_000000_182' to o3fs://bucket1.vol1/testdata
21/01/20 11:35:21 INFO SparkHadoopMapRedUtil: attempt_20210120113520_0182_m_000000_182: Committed
21/01/20 11:35:21 INFO Executor: Finished task 0.0 in stage 182.0 (TID 182). 2155 bytes result sent to driver
21/01/20 11:35:21 INFO TaskSetManager: Finished task 0.0 in stage 182.0 (TID 182) in 1106 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:35:21 INFO TaskSchedulerImpl: Removed TaskSet 182.0, whose tasks have all completed, from pool 
21/01/20 11:35:21 INFO DAGScheduler: ResultStage 182 (parquet at Generate.java:61) finished in 1.124 s
21/01/20 11:35:21 INFO DAGScheduler: Job 182 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:35:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 182: Stage finished
21/01/20 11:35:21 INFO DAGScheduler: Job 182 finished: parquet at Generate.java:61, took 1.125622 s
21/01/20 11:35:21 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-35aad76c-00f5-438d-89b4-7a9d67ad59a6
21/01/20 11:35:21 INFO FileFormatWriter: Write Job c0f60e51-8b2a-4b65-ab75-533502c9fc20 committed.
21/01/20 11:35:21 INFO FileFormatWriter: Finished processing stats for write job c0f60e51-8b2a-4b65-ab75-533502c9fc20.
21/01/20 11:35:22 INFO BlockManagerInfo: Removed broadcast_182_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:35:23 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:23 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:23 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:35:23 INFO DAGScheduler: Got job 183 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:35:23 INFO DAGScheduler: Final stage: ResultStage 183 (parquet at Generate.java:61)
21/01/20 11:35:23 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:35:23 INFO DAGScheduler: Missing parents: List()
21/01/20 11:35:23 INFO DAGScheduler: Submitting ResultStage 183 (MapPartitionsRDD[733] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:35:24 INFO MemoryStore: Block broadcast_183 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:35:24 INFO MemoryStore: Block broadcast_183_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:35:24 INFO BlockManagerInfo: Added broadcast_183_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:35:24 INFO SparkContext: Created broadcast 183 from broadcast at DAGScheduler.scala:1200
21/01/20 11:35:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 183 (MapPartitionsRDD[733] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:35:24 INFO TaskSchedulerImpl: Adding task set 183.0 with 1 tasks
21/01/20 11:35:24 WARN TaskSetManager: Stage 183 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:35:24 INFO TaskSetManager: Starting task 0.0 in stage 183.0 (TID 183, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:35:24 INFO Executor: Running task 0.0 in stage 183.0 (TID 183)
21/01/20 11:35:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:24 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:24 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:35:24 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:35:24 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:35:24 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:35:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:35:24 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:35:24 INFO ParquetOutputFormat: Validation is off
21/01/20 11:35:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:35:24 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:35:24 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:35:24 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:35:24 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:35:24 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:35:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:35:24 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113523_0183_m_000000_183' to o3fs://bucket1.vol1/testdata
21/01/20 11:35:24 INFO SparkHadoopMapRedUtil: attempt_20210120113523_0183_m_000000_183: Committed
21/01/20 11:35:24 INFO Executor: Finished task 0.0 in stage 183.0 (TID 183). 2155 bytes result sent to driver
21/01/20 11:35:24 INFO TaskSetManager: Finished task 0.0 in stage 183.0 (TID 183) in 860 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:35:24 INFO TaskSchedulerImpl: Removed TaskSet 183.0, whose tasks have all completed, from pool 
21/01/20 11:35:24 INFO DAGScheduler: ResultStage 183 (parquet at Generate.java:61) finished in 0.903 s
21/01/20 11:35:24 INFO DAGScheduler: Job 183 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:35:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 183: Stage finished
21/01/20 11:35:24 INFO DAGScheduler: Job 183 finished: parquet at Generate.java:61, took 0.904108 s
21/01/20 11:35:24 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-6fcf1b02-1cc1-4a75-83f6-6054e76f7e76
21/01/20 11:35:24 INFO FileFormatWriter: Write Job a9581bc1-6798-412f-9514-3fe1aef9291e committed.
21/01/20 11:35:24 INFO FileFormatWriter: Finished processing stats for write job a9581bc1-6798-412f-9514-3fe1aef9291e.
21/01/20 11:35:25 INFO BlockManagerInfo: Removed broadcast_183_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:35:27 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:27 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:27 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:35:27 INFO DAGScheduler: Got job 184 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:35:27 INFO DAGScheduler: Final stage: ResultStage 184 (parquet at Generate.java:61)
21/01/20 11:35:27 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:35:27 INFO DAGScheduler: Missing parents: List()
21/01/20 11:35:27 INFO DAGScheduler: Submitting ResultStage 184 (MapPartitionsRDD[737] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:35:27 INFO MemoryStore: Block broadcast_184 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:35:27 INFO MemoryStore: Block broadcast_184_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:35:27 INFO BlockManagerInfo: Added broadcast_184_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:35:27 INFO SparkContext: Created broadcast 184 from broadcast at DAGScheduler.scala:1200
21/01/20 11:35:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 184 (MapPartitionsRDD[737] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:35:27 INFO TaskSchedulerImpl: Adding task set 184.0 with 1 tasks
21/01/20 11:35:27 WARN TaskSetManager: Stage 184 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:35:27 INFO TaskSetManager: Starting task 0.0 in stage 184.0 (TID 184, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:35:27 INFO Executor: Running task 0.0 in stage 184.0 (TID 184)
21/01/20 11:35:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:27 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:27 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:35:27 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:35:27 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:35:27 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:35:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:35:27 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:35:27 INFO ParquetOutputFormat: Validation is off
21/01/20 11:35:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:35:27 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:35:27 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:35:27 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:35:27 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:35:27 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:35:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:35:28 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113527_0184_m_000000_184' to o3fs://bucket1.vol1/testdata
21/01/20 11:35:28 INFO SparkHadoopMapRedUtil: attempt_20210120113527_0184_m_000000_184: Committed
21/01/20 11:35:28 INFO Executor: Finished task 0.0 in stage 184.0 (TID 184). 2155 bytes result sent to driver
21/01/20 11:35:28 INFO TaskSetManager: Finished task 0.0 in stage 184.0 (TID 184) in 1025 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:35:28 INFO TaskSchedulerImpl: Removed TaskSet 184.0, whose tasks have all completed, from pool 
21/01/20 11:35:28 INFO DAGScheduler: ResultStage 184 (parquet at Generate.java:61) finished in 1.044 s
21/01/20 11:35:28 INFO DAGScheduler: Job 184 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:35:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 184: Stage finished
21/01/20 11:35:28 INFO DAGScheduler: Job 184 finished: parquet at Generate.java:61, took 1.045260 s
21/01/20 11:35:28 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-70c776ee-5ece-42d1-a264-525cbdb7d156
21/01/20 11:35:28 INFO FileFormatWriter: Write Job 3f36bcea-d7e7-4b17-a1f3-74189eaaacbe committed.
21/01/20 11:35:28 INFO FileFormatWriter: Finished processing stats for write job 3f36bcea-d7e7-4b17-a1f3-74189eaaacbe.
21/01/20 11:35:29 INFO BlockManagerInfo: Removed broadcast_184_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:35:31 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:31 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:35:31 INFO DAGScheduler: Got job 185 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:35:31 INFO DAGScheduler: Final stage: ResultStage 185 (parquet at Generate.java:61)
21/01/20 11:35:31 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:35:31 INFO DAGScheduler: Missing parents: List()
21/01/20 11:35:31 INFO DAGScheduler: Submitting ResultStage 185 (MapPartitionsRDD[741] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:35:31 INFO MemoryStore: Block broadcast_185 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:35:31 INFO MemoryStore: Block broadcast_185_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:35:31 INFO BlockManagerInfo: Added broadcast_185_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:35:31 INFO SparkContext: Created broadcast 185 from broadcast at DAGScheduler.scala:1200
21/01/20 11:35:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 185 (MapPartitionsRDD[741] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:35:31 INFO TaskSchedulerImpl: Adding task set 185.0 with 1 tasks
21/01/20 11:35:31 WARN TaskSetManager: Stage 185 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:35:31 INFO TaskSetManager: Starting task 0.0 in stage 185.0 (TID 185, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:35:31 INFO Executor: Running task 0.0 in stage 185.0 (TID 185)
21/01/20 11:35:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:31 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:35:31 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:35:31 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:35:31 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:35:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:35:31 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:35:31 INFO ParquetOutputFormat: Validation is off
21/01/20 11:35:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:35:31 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:35:31 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:35:31 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:35:31 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:35:31 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:35:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:35:32 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113531_0185_m_000000_185' to o3fs://bucket1.vol1/testdata
21/01/20 11:35:32 INFO SparkHadoopMapRedUtil: attempt_20210120113531_0185_m_000000_185: Committed
21/01/20 11:35:32 INFO Executor: Finished task 0.0 in stage 185.0 (TID 185). 2155 bytes result sent to driver
21/01/20 11:35:32 INFO TaskSetManager: Finished task 0.0 in stage 185.0 (TID 185) in 939 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:35:32 INFO TaskSchedulerImpl: Removed TaskSet 185.0, whose tasks have all completed, from pool 
21/01/20 11:35:32 INFO DAGScheduler: ResultStage 185 (parquet at Generate.java:61) finished in 0.981 s
21/01/20 11:35:32 INFO DAGScheduler: Job 185 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:35:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 185: Stage finished
21/01/20 11:35:32 INFO DAGScheduler: Job 185 finished: parquet at Generate.java:61, took 0.982560 s
21/01/20 11:35:32 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-d773b906-4759-4dbd-ac15-21135800fcce
21/01/20 11:35:32 INFO FileFormatWriter: Write Job 889384ea-dfa7-4180-935e-1b03645cc860 committed.
21/01/20 11:35:32 INFO FileFormatWriter: Finished processing stats for write job 889384ea-dfa7-4180-935e-1b03645cc860.
21/01/20 11:35:32 INFO BlockManagerInfo: Removed broadcast_185_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:35:34 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:34 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:34 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:35:34 INFO DAGScheduler: Got job 186 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:35:34 INFO DAGScheduler: Final stage: ResultStage 186 (parquet at Generate.java:61)
21/01/20 11:35:34 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:35:34 INFO DAGScheduler: Missing parents: List()
21/01/20 11:35:34 INFO DAGScheduler: Submitting ResultStage 186 (MapPartitionsRDD[745] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:35:34 INFO MemoryStore: Block broadcast_186 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:35:34 INFO MemoryStore: Block broadcast_186_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:35:34 INFO BlockManagerInfo: Added broadcast_186_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:35:34 INFO SparkContext: Created broadcast 186 from broadcast at DAGScheduler.scala:1200
21/01/20 11:35:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 186 (MapPartitionsRDD[745] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:35:34 INFO TaskSchedulerImpl: Adding task set 186.0 with 1 tasks
21/01/20 11:35:34 WARN TaskSetManager: Stage 186 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:35:34 INFO TaskSetManager: Starting task 0.0 in stage 186.0 (TID 186, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:35:34 INFO Executor: Running task 0.0 in stage 186.0 (TID 186)
21/01/20 11:35:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:34 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:34 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:35:34 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:35:34 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:35:34 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:35:34 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:35:34 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:35:34 INFO ParquetOutputFormat: Validation is off
21/01/20 11:35:34 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:35:34 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:35:34 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:35:34 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:35:34 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:35:34 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:35:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:35:35 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113534_0186_m_000000_186' to o3fs://bucket1.vol1/testdata
21/01/20 11:35:35 INFO SparkHadoopMapRedUtil: attempt_20210120113534_0186_m_000000_186: Committed
21/01/20 11:35:35 INFO Executor: Finished task 0.0 in stage 186.0 (TID 186). 2155 bytes result sent to driver
21/01/20 11:35:35 INFO TaskSetManager: Finished task 0.0 in stage 186.0 (TID 186) in 1044 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:35:35 INFO TaskSchedulerImpl: Removed TaskSet 186.0, whose tasks have all completed, from pool 
21/01/20 11:35:35 INFO DAGScheduler: ResultStage 186 (parquet at Generate.java:61) finished in 1.061 s
21/01/20 11:35:35 INFO DAGScheduler: Job 186 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:35:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 186: Stage finished
21/01/20 11:35:35 INFO DAGScheduler: Job 186 finished: parquet at Generate.java:61, took 1.062923 s
21/01/20 11:35:35 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-539a1f11-02cb-4a0e-8df1-565cebdbbbeb
21/01/20 11:35:35 INFO FileFormatWriter: Write Job b89d4c05-b9d2-40fe-96bc-8f776ae3f7b5 committed.
21/01/20 11:35:35 INFO FileFormatWriter: Finished processing stats for write job b89d4c05-b9d2-40fe-96bc-8f776ae3f7b5.
21/01/20 11:35:37 INFO BlockManagerInfo: Removed broadcast_186_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:35:38 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:38 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:38 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:35:38 INFO DAGScheduler: Got job 187 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:35:38 INFO DAGScheduler: Final stage: ResultStage 187 (parquet at Generate.java:61)
21/01/20 11:35:38 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:35:38 INFO DAGScheduler: Missing parents: List()
21/01/20 11:35:38 INFO DAGScheduler: Submitting ResultStage 187 (MapPartitionsRDD[749] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:35:38 INFO MemoryStore: Block broadcast_187 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:35:38 INFO MemoryStore: Block broadcast_187_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:35:38 INFO BlockManagerInfo: Added broadcast_187_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:35:38 INFO SparkContext: Created broadcast 187 from broadcast at DAGScheduler.scala:1200
21/01/20 11:35:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 187 (MapPartitionsRDD[749] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:35:38 INFO TaskSchedulerImpl: Adding task set 187.0 with 1 tasks
21/01/20 11:35:38 WARN TaskSetManager: Stage 187 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:35:38 INFO TaskSetManager: Starting task 0.0 in stage 187.0 (TID 187, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:35:38 INFO Executor: Running task 0.0 in stage 187.0 (TID 187)
21/01/20 11:35:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:38 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:38 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:35:38 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:35:38 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:35:38 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:35:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:35:38 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:35:38 INFO ParquetOutputFormat: Validation is off
21/01/20 11:35:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:35:38 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:35:38 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:35:38 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:35:38 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:35:38 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:35:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:35:38 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-D7300FCEBF33->8df234bd-f131-4d8e-953a-b9eec8633221
21/01/20 11:35:38 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:35:39 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113538_0187_m_000000_187' to o3fs://bucket1.vol1/testdata
21/01/20 11:35:39 INFO SparkHadoopMapRedUtil: attempt_20210120113538_0187_m_000000_187: Committed
21/01/20 11:35:39 INFO Executor: Finished task 0.0 in stage 187.0 (TID 187). 2155 bytes result sent to driver
21/01/20 11:35:39 INFO TaskSetManager: Finished task 0.0 in stage 187.0 (TID 187) in 893 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:35:39 INFO TaskSchedulerImpl: Removed TaskSet 187.0, whose tasks have all completed, from pool 
21/01/20 11:35:39 INFO DAGScheduler: ResultStage 187 (parquet at Generate.java:61) finished in 0.935 s
21/01/20 11:35:39 INFO DAGScheduler: Job 187 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:35:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 187: Stage finished
21/01/20 11:35:39 INFO DAGScheduler: Job 187 finished: parquet at Generate.java:61, took 0.936361 s
21/01/20 11:35:39 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-4da9d3ac-c1bd-442a-aa26-3ccb52e1cc7b
21/01/20 11:35:39 INFO FileFormatWriter: Write Job f9dae8e9-3889-47f2-9518-bce6985b73a2 committed.
21/01/20 11:35:39 INFO FileFormatWriter: Finished processing stats for write job f9dae8e9-3889-47f2-9518-bce6985b73a2.
21/01/20 11:35:39 INFO BlockManagerInfo: Removed broadcast_187_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:35:41 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:41 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:41 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:35:41 INFO DAGScheduler: Got job 188 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:35:41 INFO DAGScheduler: Final stage: ResultStage 188 (parquet at Generate.java:61)
21/01/20 11:35:41 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:35:41 INFO DAGScheduler: Missing parents: List()
21/01/20 11:35:41 INFO DAGScheduler: Submitting ResultStage 188 (MapPartitionsRDD[753] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:35:42 INFO MemoryStore: Block broadcast_188 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:35:42 INFO MemoryStore: Block broadcast_188_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:35:42 INFO BlockManagerInfo: Added broadcast_188_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:35:42 INFO SparkContext: Created broadcast 188 from broadcast at DAGScheduler.scala:1200
21/01/20 11:35:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 188 (MapPartitionsRDD[753] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:35:42 INFO TaskSchedulerImpl: Adding task set 188.0 with 1 tasks
21/01/20 11:35:42 WARN TaskSetManager: Stage 188 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:35:42 INFO TaskSetManager: Starting task 0.0 in stage 188.0 (TID 188, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:35:42 INFO Executor: Running task 0.0 in stage 188.0 (TID 188)
21/01/20 11:35:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:42 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:42 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:35:42 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:35:42 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:35:42 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:35:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:35:42 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:35:42 INFO ParquetOutputFormat: Validation is off
21/01/20 11:35:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:35:42 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:35:42 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:35:42 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:35:42 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:35:42 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:35:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:35:43 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113541_0188_m_000000_188' to o3fs://bucket1.vol1/testdata
21/01/20 11:35:43 INFO SparkHadoopMapRedUtil: attempt_20210120113541_0188_m_000000_188: Committed
21/01/20 11:35:43 INFO Executor: Finished task 0.0 in stage 188.0 (TID 188). 2155 bytes result sent to driver
21/01/20 11:35:43 INFO TaskSetManager: Finished task 0.0 in stage 188.0 (TID 188) in 1075 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:35:43 INFO TaskSchedulerImpl: Removed TaskSet 188.0, whose tasks have all completed, from pool 
21/01/20 11:35:43 INFO DAGScheduler: ResultStage 188 (parquet at Generate.java:61) finished in 1.093 s
21/01/20 11:35:43 INFO DAGScheduler: Job 188 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:35:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 188: Stage finished
21/01/20 11:35:43 INFO DAGScheduler: Job 188 finished: parquet at Generate.java:61, took 1.093840 s
21/01/20 11:35:43 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-be769fb5-b8b6-432f-a8b3-3275b2a31ccb
21/01/20 11:35:43 INFO FileFormatWriter: Write Job c8740121-6892-4671-9dc0-fbf5dfd267a3 committed.
21/01/20 11:35:43 INFO FileFormatWriter: Finished processing stats for write job c8740121-6892-4671-9dc0-fbf5dfd267a3.
21/01/20 11:35:44 INFO BlockManagerInfo: Removed broadcast_188_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:35:45 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:45 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:45 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:35:45 INFO DAGScheduler: Got job 189 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:35:45 INFO DAGScheduler: Final stage: ResultStage 189 (parquet at Generate.java:61)
21/01/20 11:35:45 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:35:45 INFO DAGScheduler: Missing parents: List()
21/01/20 11:35:45 INFO DAGScheduler: Submitting ResultStage 189 (MapPartitionsRDD[757] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:35:45 INFO MemoryStore: Block broadcast_189 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:35:45 INFO MemoryStore: Block broadcast_189_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:35:45 INFO BlockManagerInfo: Added broadcast_189_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:35:45 INFO SparkContext: Created broadcast 189 from broadcast at DAGScheduler.scala:1200
21/01/20 11:35:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 189 (MapPartitionsRDD[757] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:35:45 INFO TaskSchedulerImpl: Adding task set 189.0 with 1 tasks
21/01/20 11:35:45 WARN TaskSetManager: Stage 189 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:35:45 INFO TaskSetManager: Starting task 0.0 in stage 189.0 (TID 189, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:35:45 INFO Executor: Running task 0.0 in stage 189.0 (TID 189)
21/01/20 11:35:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:45 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:45 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:35:45 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:35:45 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:35:45 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:35:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:35:45 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:35:45 INFO ParquetOutputFormat: Validation is off
21/01/20 11:35:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:35:45 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:35:45 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:35:45 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:35:45 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:35:45 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:35:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:35:46 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113545_0189_m_000000_189' to o3fs://bucket1.vol1/testdata
21/01/20 11:35:46 INFO SparkHadoopMapRedUtil: attempt_20210120113545_0189_m_000000_189: Committed
21/01/20 11:35:46 INFO Executor: Finished task 0.0 in stage 189.0 (TID 189). 2155 bytes result sent to driver
21/01/20 11:35:46 INFO TaskSetManager: Finished task 0.0 in stage 189.0 (TID 189) in 1055 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:35:46 INFO TaskSchedulerImpl: Removed TaskSet 189.0, whose tasks have all completed, from pool 
21/01/20 11:35:46 INFO DAGScheduler: ResultStage 189 (parquet at Generate.java:61) finished in 1.075 s
21/01/20 11:35:46 INFO DAGScheduler: Job 189 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:35:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 189: Stage finished
21/01/20 11:35:46 INFO DAGScheduler: Job 189 finished: parquet at Generate.java:61, took 1.076546 s
21/01/20 11:35:46 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-60d17f87-acb4-4edd-82e3-bf5c28121ebd
21/01/20 11:35:46 INFO FileFormatWriter: Write Job a34edaaf-94dc-4ca0-aff0-b93f1420f719 committed.
21/01/20 11:35:46 INFO FileFormatWriter: Finished processing stats for write job a34edaaf-94dc-4ca0-aff0-b93f1420f719.
21/01/20 11:35:47 INFO BlockManagerInfo: Removed broadcast_189_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:35:49 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:49 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:35:49 INFO DAGScheduler: Got job 190 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:35:49 INFO DAGScheduler: Final stage: ResultStage 190 (parquet at Generate.java:61)
21/01/20 11:35:49 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:35:49 INFO DAGScheduler: Missing parents: List()
21/01/20 11:35:49 INFO DAGScheduler: Submitting ResultStage 190 (MapPartitionsRDD[761] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:35:49 INFO MemoryStore: Block broadcast_190 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:35:49 INFO MemoryStore: Block broadcast_190_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:35:49 INFO BlockManagerInfo: Added broadcast_190_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:35:49 INFO SparkContext: Created broadcast 190 from broadcast at DAGScheduler.scala:1200
21/01/20 11:35:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 190 (MapPartitionsRDD[761] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:35:49 INFO TaskSchedulerImpl: Adding task set 190.0 with 1 tasks
21/01/20 11:35:49 WARN TaskSetManager: Stage 190 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:35:49 INFO TaskSetManager: Starting task 0.0 in stage 190.0 (TID 190, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:35:49 INFO Executor: Running task 0.0 in stage 190.0 (TID 190)
21/01/20 11:35:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:49 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:35:49 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:35:49 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:35:49 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:35:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:35:49 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:35:49 INFO ParquetOutputFormat: Validation is off
21/01/20 11:35:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:35:49 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:35:49 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:35:49 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:35:49 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:35:49 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:35:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:35:50 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113549_0190_m_000000_190' to o3fs://bucket1.vol1/testdata
21/01/20 11:35:50 INFO SparkHadoopMapRedUtil: attempt_20210120113549_0190_m_000000_190: Committed
21/01/20 11:35:50 INFO Executor: Finished task 0.0 in stage 190.0 (TID 190). 2155 bytes result sent to driver
21/01/20 11:35:50 INFO TaskSetManager: Finished task 0.0 in stage 190.0 (TID 190) in 1023 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:35:50 INFO TaskSchedulerImpl: Removed TaskSet 190.0, whose tasks have all completed, from pool 
21/01/20 11:35:50 INFO DAGScheduler: ResultStage 190 (parquet at Generate.java:61) finished in 1.041 s
21/01/20 11:35:50 INFO DAGScheduler: Job 190 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:35:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 190: Stage finished
21/01/20 11:35:50 INFO DAGScheduler: Job 190 finished: parquet at Generate.java:61, took 1.041809 s
21/01/20 11:35:50 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-283b6de8-d136-4f6f-b5b7-700cc26c086b
21/01/20 11:35:50 INFO FileFormatWriter: Write Job 627c973f-a78c-4f0e-a696-f477beac2e51 committed.
21/01/20 11:35:50 INFO FileFormatWriter: Finished processing stats for write job 627c973f-a78c-4f0e-a696-f477beac2e51.
21/01/20 11:35:51 INFO BlockManagerInfo: Removed broadcast_190_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:35:52 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:52 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:53 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:35:53 INFO DAGScheduler: Got job 191 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:35:53 INFO DAGScheduler: Final stage: ResultStage 191 (parquet at Generate.java:61)
21/01/20 11:35:53 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:35:53 INFO DAGScheduler: Missing parents: List()
21/01/20 11:35:53 INFO DAGScheduler: Submitting ResultStage 191 (MapPartitionsRDD[765] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:35:53 INFO MemoryStore: Block broadcast_191 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:35:53 INFO MemoryStore: Block broadcast_191_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:35:53 INFO BlockManagerInfo: Added broadcast_191_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:35:53 INFO SparkContext: Created broadcast 191 from broadcast at DAGScheduler.scala:1200
21/01/20 11:35:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 191 (MapPartitionsRDD[765] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:35:53 INFO TaskSchedulerImpl: Adding task set 191.0 with 1 tasks
21/01/20 11:35:53 WARN TaskSetManager: Stage 191 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:35:53 INFO TaskSetManager: Starting task 0.0 in stage 191.0 (TID 191, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:35:53 INFO Executor: Running task 0.0 in stage 191.0 (TID 191)
21/01/20 11:35:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:53 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:35:53 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:35:53 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:35:53 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:35:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:35:53 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:35:53 INFO ParquetOutputFormat: Validation is off
21/01/20 11:35:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:35:53 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:35:53 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:35:53 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:35:53 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:35:53 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:35:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:35:53 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-1C6F6DA89911->27b66227-fed8-4c1d-ab57-510f5a3ce552
21/01/20 11:35:53 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:35:54 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113552_0191_m_000000_191' to o3fs://bucket1.vol1/testdata
21/01/20 11:35:54 INFO SparkHadoopMapRedUtil: attempt_20210120113552_0191_m_000000_191: Committed
21/01/20 11:35:54 INFO Executor: Finished task 0.0 in stage 191.0 (TID 191). 2155 bytes result sent to driver
21/01/20 11:35:54 INFO TaskSetManager: Finished task 0.0 in stage 191.0 (TID 191) in 1116 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:35:54 INFO TaskSchedulerImpl: Removed TaskSet 191.0, whose tasks have all completed, from pool 
21/01/20 11:35:54 INFO DAGScheduler: ResultStage 191 (parquet at Generate.java:61) finished in 1.135 s
21/01/20 11:35:54 INFO DAGScheduler: Job 191 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:35:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 191: Stage finished
21/01/20 11:35:54 INFO DAGScheduler: Job 191 finished: parquet at Generate.java:61, took 1.136279 s
21/01/20 11:35:54 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-c7074dd1-54f8-458d-9542-4301d96f33e9
21/01/20 11:35:54 INFO FileFormatWriter: Write Job a0452617-35d9-4f9c-a2cd-d5b12707db78 committed.
21/01/20 11:35:54 INFO FileFormatWriter: Finished processing stats for write job a0452617-35d9-4f9c-a2cd-d5b12707db78.
21/01/20 11:35:55 INFO BlockManagerInfo: Removed broadcast_191_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:35:56 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:56 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:56 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:35:56 INFO DAGScheduler: Got job 192 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:35:56 INFO DAGScheduler: Final stage: ResultStage 192 (parquet at Generate.java:61)
21/01/20 11:35:56 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:35:56 INFO DAGScheduler: Missing parents: List()
21/01/20 11:35:56 INFO DAGScheduler: Submitting ResultStage 192 (MapPartitionsRDD[769] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:35:56 INFO MemoryStore: Block broadcast_192 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:35:56 INFO MemoryStore: Block broadcast_192_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:35:56 INFO BlockManagerInfo: Added broadcast_192_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:35:56 INFO SparkContext: Created broadcast 192 from broadcast at DAGScheduler.scala:1200
21/01/20 11:35:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 192 (MapPartitionsRDD[769] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:35:56 INFO TaskSchedulerImpl: Adding task set 192.0 with 1 tasks
21/01/20 11:35:56 WARN TaskSetManager: Stage 192 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:35:56 INFO TaskSetManager: Starting task 0.0 in stage 192.0 (TID 192, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:35:56 INFO Executor: Running task 0.0 in stage 192.0 (TID 192)
21/01/20 11:35:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:56 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:35:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:35:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:35:56 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:35:56 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:35:56 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:35:56 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:35:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:35:56 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:35:56 INFO ParquetOutputFormat: Validation is off
21/01/20 11:35:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:35:56 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:35:56 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:35:56 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:35:56 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:35:56 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:35:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:35:57 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113556_0192_m_000000_192' to o3fs://bucket1.vol1/testdata
21/01/20 11:35:57 INFO SparkHadoopMapRedUtil: attempt_20210120113556_0192_m_000000_192: Committed
21/01/20 11:35:57 INFO Executor: Finished task 0.0 in stage 192.0 (TID 192). 2155 bytes result sent to driver
21/01/20 11:35:57 INFO TaskSetManager: Finished task 0.0 in stage 192.0 (TID 192) in 984 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:35:57 INFO TaskSchedulerImpl: Removed TaskSet 192.0, whose tasks have all completed, from pool 
21/01/20 11:35:57 INFO DAGScheduler: ResultStage 192 (parquet at Generate.java:61) finished in 1.002 s
21/01/20 11:35:57 INFO DAGScheduler: Job 192 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:35:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 192: Stage finished
21/01/20 11:35:57 INFO DAGScheduler: Job 192 finished: parquet at Generate.java:61, took 1.003215 s
21/01/20 11:35:57 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-f62e3989-bb7c-4e98-9194-c04bf249de65
21/01/20 11:35:57 INFO FileFormatWriter: Write Job 402523be-c2e6-4ad8-ae4d-fcd96efc9f86 committed.
21/01/20 11:35:57 INFO FileFormatWriter: Finished processing stats for write job 402523be-c2e6-4ad8-ae4d-fcd96efc9f86.
21/01/20 11:35:59 INFO BlockManagerInfo: Removed broadcast_192_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:36:00 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:36:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:36:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:36:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:36:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:36:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:36:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:36:00 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:36:00 INFO DAGScheduler: Got job 193 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:36:00 INFO DAGScheduler: Final stage: ResultStage 193 (parquet at Generate.java:61)
21/01/20 11:36:00 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:36:00 INFO DAGScheduler: Missing parents: List()
21/01/20 11:36:00 INFO DAGScheduler: Submitting ResultStage 193 (MapPartitionsRDD[773] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:36:00 INFO MemoryStore: Block broadcast_193 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:36:00 INFO MemoryStore: Block broadcast_193_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:36:00 INFO BlockManagerInfo: Added broadcast_193_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:36:00 INFO SparkContext: Created broadcast 193 from broadcast at DAGScheduler.scala:1200
21/01/20 11:36:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 193 (MapPartitionsRDD[773] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:36:00 INFO TaskSchedulerImpl: Adding task set 193.0 with 1 tasks
21/01/20 11:36:00 WARN TaskSetManager: Stage 193 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:36:00 INFO TaskSetManager: Starting task 0.0 in stage 193.0 (TID 193, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:36:00 INFO Executor: Running task 0.0 in stage 193.0 (TID 193)
21/01/20 11:36:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:36:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:36:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:36:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:36:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:36:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:36:00 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:36:00 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:36:00 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:36:00 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:36:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:36:00 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:36:00 INFO ParquetOutputFormat: Validation is off
21/01/20 11:36:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:36:00 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:36:00 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:36:00 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:36:00 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:36:00 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:36:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:36:01 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113600_0193_m_000000_193' to o3fs://bucket1.vol1/testdata
21/01/20 11:36:01 INFO SparkHadoopMapRedUtil: attempt_20210120113600_0193_m_000000_193: Committed
21/01/20 11:36:01 INFO Executor: Finished task 0.0 in stage 193.0 (TID 193). 2155 bytes result sent to driver
21/01/20 11:36:01 INFO TaskSetManager: Finished task 0.0 in stage 193.0 (TID 193) in 783 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:36:01 INFO TaskSchedulerImpl: Removed TaskSet 193.0, whose tasks have all completed, from pool 
21/01/20 11:36:01 INFO DAGScheduler: ResultStage 193 (parquet at Generate.java:61) finished in 0.823 s
21/01/20 11:36:01 INFO DAGScheduler: Job 193 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:36:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 193: Stage finished
21/01/20 11:36:01 INFO DAGScheduler: Job 193 finished: parquet at Generate.java:61, took 0.824818 s
21/01/20 11:36:01 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-8e194412-136c-409a-ba30-dd1311db52bd
21/01/20 11:36:01 INFO FileFormatWriter: Write Job 5cdf8051-1ef6-4e4c-aa72-b38ec7d4c1c9 committed.
21/01/20 11:36:01 INFO FileFormatWriter: Finished processing stats for write job 5cdf8051-1ef6-4e4c-aa72-b38ec7d4c1c9.
21/01/20 11:36:01 INFO BlockManagerInfo: Removed broadcast_193_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:36:03 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:36:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:36:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:36:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:36:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:36:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:36:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:36:03 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:36:03 INFO DAGScheduler: Got job 194 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:36:03 INFO DAGScheduler: Final stage: ResultStage 194 (parquet at Generate.java:61)
21/01/20 11:36:03 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:36:03 INFO DAGScheduler: Missing parents: List()
21/01/20 11:36:03 INFO DAGScheduler: Submitting ResultStage 194 (MapPartitionsRDD[777] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:36:03 INFO MemoryStore: Block broadcast_194 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:36:03 INFO MemoryStore: Block broadcast_194_piece0 stored as bytes in memory (estimated size 73.9 KiB, free 413.7 MiB)
21/01/20 11:36:03 INFO BlockManagerInfo: Added broadcast_194_piece0 in memory on test-runner-9msxc:43214 (size: 73.9 KiB, free: 413.9 MiB)
21/01/20 11:36:03 INFO SparkContext: Created broadcast 194 from broadcast at DAGScheduler.scala:1200
21/01/20 11:36:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 194 (MapPartitionsRDD[777] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:36:03 INFO TaskSchedulerImpl: Adding task set 194.0 with 1 tasks
21/01/20 11:36:03 WARN TaskSetManager: Stage 194 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:36:03 INFO TaskSetManager: Starting task 0.0 in stage 194.0 (TID 194, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:36:03 INFO Executor: Running task 0.0 in stage 194.0 (TID 194)
21/01/20 11:36:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:36:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:36:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:36:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:36:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:36:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:36:03 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:36:03 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:36:03 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:36:03 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:36:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:36:03 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:36:03 INFO ParquetOutputFormat: Validation is off
21/01/20 11:36:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:36:03 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:36:03 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:36:03 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:36:03 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:36:03 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:36:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:36:04 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113603_0194_m_000000_194' to o3fs://bucket1.vol1/testdata
21/01/20 11:36:04 INFO SparkHadoopMapRedUtil: attempt_20210120113603_0194_m_000000_194: Committed
21/01/20 11:36:04 INFO Executor: Finished task 0.0 in stage 194.0 (TID 194). 2155 bytes result sent to driver
21/01/20 11:36:04 INFO TaskSetManager: Finished task 0.0 in stage 194.0 (TID 194) in 1053 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:36:04 INFO TaskSchedulerImpl: Removed TaskSet 194.0, whose tasks have all completed, from pool 
21/01/20 11:36:04 INFO DAGScheduler: ResultStage 194 (parquet at Generate.java:61) finished in 1.071 s
21/01/20 11:36:04 INFO DAGScheduler: Job 194 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:36:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 194: Stage finished
21/01/20 11:36:04 INFO DAGScheduler: Job 194 finished: parquet at Generate.java:61, took 1.072353 s
21/01/20 11:36:04 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-a9fa5809-3382-4791-80e4-ac6ee5223b0e
21/01/20 11:36:04 INFO FileFormatWriter: Write Job 666544c8-fb87-4bc4-9262-298d500b2782 committed.
21/01/20 11:36:04 INFO FileFormatWriter: Finished processing stats for write job 666544c8-fb87-4bc4-9262-298d500b2782.
21/01/20 11:36:06 INFO BlockManagerInfo: Removed broadcast_194_piece0 on test-runner-9msxc:43214 in memory (size: 73.9 KiB, free: 413.9 MiB)
21/01/20 11:36:07 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:36:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:36:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:36:07 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:36:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:36:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:36:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:36:07 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:36:07 INFO DAGScheduler: Got job 195 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:36:07 INFO DAGScheduler: Final stage: ResultStage 195 (parquet at Generate.java:61)
21/01/20 11:36:07 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:36:07 INFO DAGScheduler: Missing parents: List()
21/01/20 11:36:07 INFO DAGScheduler: Submitting ResultStage 195 (MapPartitionsRDD[781] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:36:07 INFO MemoryStore: Block broadcast_195 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:36:07 INFO MemoryStore: Block broadcast_195_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:36:07 INFO BlockManagerInfo: Added broadcast_195_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:36:07 INFO SparkContext: Created broadcast 195 from broadcast at DAGScheduler.scala:1200
21/01/20 11:36:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 195 (MapPartitionsRDD[781] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:36:07 INFO TaskSchedulerImpl: Adding task set 195.0 with 1 tasks
21/01/20 11:36:07 WARN TaskSetManager: Stage 195 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:36:07 INFO TaskSetManager: Starting task 0.0 in stage 195.0 (TID 195, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:36:07 INFO Executor: Running task 0.0 in stage 195.0 (TID 195)
21/01/20 11:36:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:36:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:36:07 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:36:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:36:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:36:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:36:07 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:36:07 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:36:07 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:36:07 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:36:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:36:07 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:36:07 INFO ParquetOutputFormat: Validation is off
21/01/20 11:36:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:36:07 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:36:07 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:36:07 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:36:07 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:36:07 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:36:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:36:08 INFO RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-1AB5D80F7DA9->8df234bd-f131-4d8e-953a-b9eec8633221
21/01/20 11:36:08 WARN MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
21/01/20 11:36:08 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113607_0195_m_000000_195' to o3fs://bucket1.vol1/testdata
21/01/20 11:36:08 INFO SparkHadoopMapRedUtil: attempt_20210120113607_0195_m_000000_195: Committed
21/01/20 11:36:08 INFO Executor: Finished task 0.0 in stage 195.0 (TID 195). 2155 bytes result sent to driver
21/01/20 11:36:08 INFO TaskSetManager: Finished task 0.0 in stage 195.0 (TID 195) in 916 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:36:08 INFO TaskSchedulerImpl: Removed TaskSet 195.0, whose tasks have all completed, from pool 
21/01/20 11:36:08 INFO DAGScheduler: ResultStage 195 (parquet at Generate.java:61) finished in 0.956 s
21/01/20 11:36:08 INFO DAGScheduler: Job 195 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:36:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 195: Stage finished
21/01/20 11:36:08 INFO DAGScheduler: Job 195 finished: parquet at Generate.java:61, took 0.957792 s
21/01/20 11:36:08 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-47766937-2bde-47b9-9c4c-2a70a68931ad
21/01/20 11:36:08 INFO FileFormatWriter: Write Job 7021ddf5-03c6-48c7-9830-df5e99f3f978 committed.
21/01/20 11:36:08 INFO FileFormatWriter: Finished processing stats for write job 7021ddf5-03c6-48c7-9830-df5e99f3f978.
21/01/20 11:36:08 INFO BlockManagerInfo: Removed broadcast_195_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:36:11 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:36:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:36:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:36:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:36:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:36:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:36:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:36:11 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:36:11 INFO DAGScheduler: Got job 196 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:36:11 INFO DAGScheduler: Final stage: ResultStage 196 (parquet at Generate.java:61)
21/01/20 11:36:11 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:36:11 INFO DAGScheduler: Missing parents: List()
21/01/20 11:36:11 INFO DAGScheduler: Submitting ResultStage 196 (MapPartitionsRDD[785] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:36:11 INFO MemoryStore: Block broadcast_196 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:36:11 INFO MemoryStore: Block broadcast_196_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:36:11 INFO BlockManagerInfo: Added broadcast_196_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:36:11 INFO SparkContext: Created broadcast 196 from broadcast at DAGScheduler.scala:1200
21/01/20 11:36:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 196 (MapPartitionsRDD[785] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:36:11 INFO TaskSchedulerImpl: Adding task set 196.0 with 1 tasks
21/01/20 11:36:11 WARN TaskSetManager: Stage 196 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:36:11 INFO TaskSetManager: Starting task 0.0 in stage 196.0 (TID 196, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:36:11 INFO Executor: Running task 0.0 in stage 196.0 (TID 196)
21/01/20 11:36:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:36:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:36:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:36:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:36:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:36:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:36:11 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:36:11 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:36:11 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:36:11 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:36:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:36:11 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:36:11 INFO ParquetOutputFormat: Validation is off
21/01/20 11:36:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:36:11 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:36:11 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:36:11 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:36:11 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:36:11 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:36:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:36:12 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113611_0196_m_000000_196' to o3fs://bucket1.vol1/testdata
21/01/20 11:36:12 INFO SparkHadoopMapRedUtil: attempt_20210120113611_0196_m_000000_196: Committed
21/01/20 11:36:12 INFO Executor: Finished task 0.0 in stage 196.0 (TID 196). 2155 bytes result sent to driver
21/01/20 11:36:12 INFO TaskSetManager: Finished task 0.0 in stage 196.0 (TID 196) in 1062 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:36:12 INFO TaskSchedulerImpl: Removed TaskSet 196.0, whose tasks have all completed, from pool 
21/01/20 11:36:12 INFO DAGScheduler: ResultStage 196 (parquet at Generate.java:61) finished in 1.079 s
21/01/20 11:36:12 INFO DAGScheduler: Job 196 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:36:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 196: Stage finished
21/01/20 11:36:12 INFO DAGScheduler: Job 196 finished: parquet at Generate.java:61, took 1.080937 s
21/01/20 11:36:12 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-79746b46-6724-4851-9d52-6ac0ddedd460
21/01/20 11:36:12 INFO FileFormatWriter: Write Job 14e89650-352b-47e3-ad0a-8582502a2f1b committed.
21/01/20 11:36:12 INFO FileFormatWriter: Finished processing stats for write job 14e89650-352b-47e3-ad0a-8582502a2f1b.
21/01/20 11:36:13 INFO BlockManagerInfo: Removed broadcast_196_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:36:14 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:36:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:36:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:36:14 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:36:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:36:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:36:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:36:14 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:36:14 INFO DAGScheduler: Got job 197 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:36:14 INFO DAGScheduler: Final stage: ResultStage 197 (parquet at Generate.java:61)
21/01/20 11:36:14 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:36:14 INFO DAGScheduler: Missing parents: List()
21/01/20 11:36:14 INFO DAGScheduler: Submitting ResultStage 197 (MapPartitionsRDD[789] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:36:14 INFO MemoryStore: Block broadcast_197 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:36:14 INFO MemoryStore: Block broadcast_197_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:36:14 INFO BlockManagerInfo: Added broadcast_197_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:36:14 INFO SparkContext: Created broadcast 197 from broadcast at DAGScheduler.scala:1200
21/01/20 11:36:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 197 (MapPartitionsRDD[789] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:36:14 INFO TaskSchedulerImpl: Adding task set 197.0 with 1 tasks
21/01/20 11:36:14 WARN TaskSetManager: Stage 197 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:36:14 INFO TaskSetManager: Starting task 0.0 in stage 197.0 (TID 197, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:36:14 INFO Executor: Running task 0.0 in stage 197.0 (TID 197)
21/01/20 11:36:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:36:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:36:14 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:36:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:36:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:36:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:36:14 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:36:14 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:36:14 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:36:14 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:36:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:36:14 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:36:14 INFO ParquetOutputFormat: Validation is off
21/01/20 11:36:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:36:14 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:36:14 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:36:14 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:36:14 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:36:14 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:36:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:36:15 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113614_0197_m_000000_197' to o3fs://bucket1.vol1/testdata
21/01/20 11:36:15 INFO SparkHadoopMapRedUtil: attempt_20210120113614_0197_m_000000_197: Committed
21/01/20 11:36:15 INFO Executor: Finished task 0.0 in stage 197.0 (TID 197). 2155 bytes result sent to driver
21/01/20 11:36:15 INFO TaskSetManager: Finished task 0.0 in stage 197.0 (TID 197) in 1044 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:36:15 INFO TaskSchedulerImpl: Removed TaskSet 197.0, whose tasks have all completed, from pool 
21/01/20 11:36:15 INFO DAGScheduler: ResultStage 197 (parquet at Generate.java:61) finished in 1.062 s
21/01/20 11:36:15 INFO DAGScheduler: Job 197 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:36:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 197: Stage finished
21/01/20 11:36:15 INFO DAGScheduler: Job 197 finished: parquet at Generate.java:61, took 1.063362 s
21/01/20 11:36:15 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-90450e3e-5ac3-4ac1-9c01-621461735fc1
21/01/20 11:36:15 INFO FileFormatWriter: Write Job 1953863e-2fb4-4e78-bb60-2b1dea64d698 committed.
21/01/20 11:36:15 INFO FileFormatWriter: Finished processing stats for write job 1953863e-2fb4-4e78-bb60-2b1dea64d698.
21/01/20 11:36:16 INFO BlockManagerInfo: Removed broadcast_197_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:36:18 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:36:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:36:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:36:18 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:36:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:36:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:36:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:36:18 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:36:18 INFO DAGScheduler: Got job 198 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:36:18 INFO DAGScheduler: Final stage: ResultStage 198 (parquet at Generate.java:61)
21/01/20 11:36:18 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:36:18 INFO DAGScheduler: Missing parents: List()
21/01/20 11:36:18 INFO DAGScheduler: Submitting ResultStage 198 (MapPartitionsRDD[793] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:36:18 INFO MemoryStore: Block broadcast_198 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:36:18 INFO MemoryStore: Block broadcast_198_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:36:18 INFO BlockManagerInfo: Added broadcast_198_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:36:18 INFO SparkContext: Created broadcast 198 from broadcast at DAGScheduler.scala:1200
21/01/20 11:36:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 198 (MapPartitionsRDD[793] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:36:18 INFO TaskSchedulerImpl: Adding task set 198.0 with 1 tasks
21/01/20 11:36:18 WARN TaskSetManager: Stage 198 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:36:18 INFO TaskSetManager: Starting task 0.0 in stage 198.0 (TID 198, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:36:18 INFO Executor: Running task 0.0 in stage 198.0 (TID 198)
21/01/20 11:36:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:36:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:36:18 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:36:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:36:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:36:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:36:18 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:36:18 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:36:18 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:36:18 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:36:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:36:18 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:36:18 INFO ParquetOutputFormat: Validation is off
21/01/20 11:36:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:36:18 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:36:18 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:36:18 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:36:18 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:36:18 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:36:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:36:19 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113618_0198_m_000000_198' to o3fs://bucket1.vol1/testdata
21/01/20 11:36:19 INFO SparkHadoopMapRedUtil: attempt_20210120113618_0198_m_000000_198: Committed
21/01/20 11:36:19 INFO Executor: Finished task 0.0 in stage 198.0 (TID 198). 2155 bytes result sent to driver
21/01/20 11:36:19 INFO TaskSetManager: Finished task 0.0 in stage 198.0 (TID 198) in 1079 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:36:19 INFO TaskSchedulerImpl: Removed TaskSet 198.0, whose tasks have all completed, from pool 
21/01/20 11:36:19 INFO DAGScheduler: ResultStage 198 (parquet at Generate.java:61) finished in 1.097 s
21/01/20 11:36:19 INFO DAGScheduler: Job 198 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:36:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 198: Stage finished
21/01/20 11:36:19 INFO DAGScheduler: Job 198 finished: parquet at Generate.java:61, took 1.098148 s
21/01/20 11:36:19 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-a658d476-64f5-4014-a1ed-7569cfa61267
21/01/20 11:36:19 INFO FileFormatWriter: Write Job 24025b5c-669d-4bb7-8dc5-0e4b91056039 committed.
21/01/20 11:36:19 INFO FileFormatWriter: Finished processing stats for write job 24025b5c-669d-4bb7-8dc5-0e4b91056039.
21/01/20 11:36:20 INFO BlockManagerInfo: Removed broadcast_198_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:36:22 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:36:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:36:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:36:22 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:36:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:36:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:36:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:36:22 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:36:22 INFO DAGScheduler: Got job 199 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:36:22 INFO DAGScheduler: Final stage: ResultStage 199 (parquet at Generate.java:61)
21/01/20 11:36:22 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:36:22 INFO DAGScheduler: Missing parents: List()
21/01/20 11:36:22 INFO DAGScheduler: Submitting ResultStage 199 (MapPartitionsRDD[797] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:36:22 INFO MemoryStore: Block broadcast_199 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:36:22 INFO MemoryStore: Block broadcast_199_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:36:22 INFO BlockManagerInfo: Added broadcast_199_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:36:22 INFO SparkContext: Created broadcast 199 from broadcast at DAGScheduler.scala:1200
21/01/20 11:36:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 199 (MapPartitionsRDD[797] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:36:22 INFO TaskSchedulerImpl: Adding task set 199.0 with 1 tasks
21/01/20 11:36:22 WARN TaskSetManager: Stage 199 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:36:22 INFO TaskSetManager: Starting task 0.0 in stage 199.0 (TID 199, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:36:22 INFO Executor: Running task 0.0 in stage 199.0 (TID 199)
21/01/20 11:36:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:36:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:36:22 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:36:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:36:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:36:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:36:22 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:36:22 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:36:22 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:36:22 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:36:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:36:22 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:36:22 INFO ParquetOutputFormat: Validation is off
21/01/20 11:36:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:36:22 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:36:22 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:36:22 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:36:22 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:36:22 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:36:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:36:23 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113622_0199_m_000000_199' to o3fs://bucket1.vol1/testdata
21/01/20 11:36:23 INFO SparkHadoopMapRedUtil: attempt_20210120113622_0199_m_000000_199: Committed
21/01/20 11:36:23 INFO Executor: Finished task 0.0 in stage 199.0 (TID 199). 2155 bytes result sent to driver
21/01/20 11:36:23 INFO TaskSetManager: Finished task 0.0 in stage 199.0 (TID 199) in 1073 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:36:23 INFO TaskSchedulerImpl: Removed TaskSet 199.0, whose tasks have all completed, from pool 
21/01/20 11:36:23 INFO DAGScheduler: ResultStage 199 (parquet at Generate.java:61) finished in 1.090 s
21/01/20 11:36:23 INFO DAGScheduler: Job 199 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:36:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 199: Stage finished
21/01/20 11:36:23 INFO DAGScheduler: Job 199 finished: parquet at Generate.java:61, took 1.091810 s
21/01/20 11:36:23 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-cc21b12f-661e-4187-a690-d8555b1ac137
21/01/20 11:36:23 INFO FileFormatWriter: Write Job 9e8a0726-10a1-42b9-bb17-464ed01a18c7 committed.
21/01/20 11:36:23 INFO FileFormatWriter: Finished processing stats for write job 9e8a0726-10a1-42b9-bb17-464ed01a18c7.
21/01/20 11:36:24 INFO BlockManagerInfo: Removed broadcast_199_piece0 on test-runner-9msxc:43214 in memory (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:36:25 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:36:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:36:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:36:25 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:36:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:36:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:36:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:36:25 INFO SparkContext: Starting job: parquet at Generate.java:61
21/01/20 11:36:25 INFO DAGScheduler: Got job 200 (parquet at Generate.java:61) with 1 output partitions
21/01/20 11:36:25 INFO DAGScheduler: Final stage: ResultStage 200 (parquet at Generate.java:61)
21/01/20 11:36:25 INFO DAGScheduler: Parents of final stage: List()
21/01/20 11:36:25 INFO DAGScheduler: Missing parents: List()
21/01/20 11:36:25 INFO DAGScheduler: Submitting ResultStage 200 (MapPartitionsRDD[801] at parquet at Generate.java:61), which has no missing parents
21/01/20 11:36:25 INFO MemoryStore: Block broadcast_200 stored as values in memory (estimated size 205.8 KiB, free 413.7 MiB)
21/01/20 11:36:25 INFO MemoryStore: Block broadcast_200_piece0 stored as bytes in memory (estimated size 74.0 KiB, free 413.7 MiB)
21/01/20 11:36:25 INFO BlockManagerInfo: Added broadcast_200_piece0 in memory on test-runner-9msxc:43214 (size: 74.0 KiB, free: 413.9 MiB)
21/01/20 11:36:25 INFO SparkContext: Created broadcast 200 from broadcast at DAGScheduler.scala:1200
21/01/20 11:36:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 200 (MapPartitionsRDD[801] at parquet at Generate.java:61) (first 15 tasks are for partitions Vector(0))
21/01/20 11:36:25 INFO TaskSchedulerImpl: Adding task set 200.0 with 1 tasks
21/01/20 11:36:25 WARN TaskSetManager: Stage 200 contains a task of very large size (102911 KiB). The maximum recommended task size is 1000 KiB.
21/01/20 11:36:25 INFO TaskSetManager: Starting task 0.0 in stage 200.0 (TID 200, test-runner-9msxc, executor driver, partition 0, PROCESS_LOCAL, 105381192 bytes)
21/01/20 11:36:25 INFO Executor: Running task 0.0 in stage 200.0 (TID 200)
21/01/20 11:36:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:36:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:36:25 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:36:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
21/01/20 11:36:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/01/20 11:36:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/20 11:36:25 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:36:25 INFO CodecConfig: Compression: SNAPPY
21/01/20 11:36:25 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/20 11:36:25 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/20 11:36:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/20 11:36:25 INFO ParquetOutputFormat: Dictionary is on
21/01/20 11:36:25 INFO ParquetOutputFormat: Validation is off
21/01/20 11:36:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/20 11:36:25 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/20 11:36:25 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/20 11:36:25 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/20 11:36:25 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/20 11:36:25 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "data",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary data;
  required int32 index;
}

       
21/01/20 11:36:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 104858404
21/01/20 11:36:26 INFO FileOutputCommitter: Saved output of task 'attempt_20210120113625_0200_m_000000_200' to o3fs://bucket1.vol1/testdata
21/01/20 11:36:26 INFO SparkHadoopMapRedUtil: attempt_20210120113625_0200_m_000000_200: Committed
21/01/20 11:36:26 INFO Executor: Finished task 0.0 in stage 200.0 (TID 200). 2155 bytes result sent to driver
21/01/20 11:36:26 INFO TaskSetManager: Finished task 0.0 in stage 200.0 (TID 200) in 1051 ms on test-runner-9msxc (executor driver) (1/1)
21/01/20 11:36:26 INFO TaskSchedulerImpl: Removed TaskSet 200.0, whose tasks have all completed, from pool 
21/01/20 11:36:26 INFO DAGScheduler: ResultStage 200 (parquet at Generate.java:61) finished in 1.070 s
21/01/20 11:36:26 INFO DAGScheduler: Job 200 is finished. Cancelling potential speculative or zombie tasks for this job
21/01/20 11:36:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 200: Stage finished
21/01/20 11:36:26 INFO DAGScheduler: Job 200 finished: parquet at Generate.java:61, took 1.071059 s
21/01/20 11:36:26 WARN BasicOzoneFileSystem: delete: Path does not exist: o3fs://bucket1.vol1/testdata/.spark-staging-dca964e7-ffa1-4c9b-abec-0320d49c1638
21/01/20 11:36:26 INFO FileFormatWriter: Write Job 863e0361-5ce5-4993-8a47-65143ec4c1e9 committed.
21/01/20 11:36:26 INFO FileFormatWriter: Finished processing stats for write job 863e0361-5ce5-4993-8a47-65143ec4c1e9.
21/01/20 11:36:26 INFO SparkUI: Stopped Spark web UI at http://test-runner-9msxc:4040
21/01/20 11:36:26 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
21/01/20 11:36:26 INFO MemoryStore: MemoryStore cleared
21/01/20 11:36:26 INFO BlockManager: BlockManager stopped
21/01/20 11:36:27 INFO BlockManagerMaster: BlockManagerMaster stopped
21/01/20 11:36:27 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
21/01/20 11:36:27 INFO SparkContext: Successfully stopped SparkContext
21/01/20 11:36:27 INFO ShutdownHookManager: Shutdown hook called
21/01/20 11:36:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-c991cf70-fb86-46ec-95c3-decb9b5dff7a
21/01/20 11:36:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-f5614285-5b9c-4c2b-a359-8514bab6be54

real	12m26.761s
user	12m14.279s
sys	0m16.280s
Process exited with exit code 0
